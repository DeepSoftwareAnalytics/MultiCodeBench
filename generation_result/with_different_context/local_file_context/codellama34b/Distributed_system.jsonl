{"instance_id": "ba8707e206f9", "generation_result": [{"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\treturn (max(2, self._loop_wait - 2) * 1000, self._orig_kazoo_connect(*args)[1])\n\n", "CodeBleu_score": 49.17258586898197}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\treturn (max(2, self.loop_wait - 2) * 1000, self._orig_kazoo_connect(*args))\n\n", "CodeBleu_score": 49.06553676525141}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\treturn (max(2, self._loop_wait - 2) * 1000, self._orig_kazoo_connect(*args))\n\n", "CodeBleu_score": 48.69335243580273}]}
{"instance_id": "88e24a7c26a4", "generation_result": [{"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}]}
{"instance_id": "2af7d7b3c887", "generation_result": [{"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}]}
{"instance_id": "43e1bc044d8e", "generation_result": [{"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}]}
{"instance_id": "93d126ab8a22", "generation_result": [{"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}]}
{"instance_id": "f68719982812", "generation_result": [{"code": "def poll(self) -> bool\n\tif not self._next_poll or time.time() > self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        for host in self._exhibitors:\n            url = 'http://{}:{}{}'.format(host, self._exhibitor_port, self._uri_path)\n            try:\n                response = requests_get(url, timeout=self.TIMEOUT)\n                if response.ok:\n                    data = response.json()\n                    if data['port'] != self._exhibitor_port:\n                        logger.warning('exhibitor port changed from {} to {}'.format(self._exhibitor_port, data['port']))\n                        self._exhibitor_port = data['port']\n                    hosts = ','.join(['{}:{}'.format(h['hostname'], data['port']) for h in data['servers']])\n                    if hosts != self._zookeeper_hosts:\n                        logger.info('zookeeper hosts changed from {} to {}'.format(self._zookeeper_hosts, hosts))\n                        self._zookeeper_hosts = hosts\n                        return True\n            except Exception as e:\n                logger.exception('failed to poll exhibitor at {}: {}'.format(url, e))\n        return False\n\n", "CodeBleu_score": 34.749398892283736}, {"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() > self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        for host in self._exhibitors:\n            try:\n                response = requests_get(\n                    'http://{}:{}{}'.format(host, self._exhibitor_port, self._uri_path),\n                    timeout=self.TIMEOUT,\n                )\n                if response.status_code == 200:\n                    data = response.json()\n                    if 'servers' not in data or 'port' not in data:\n                        logger.warning('exhibitor returned invalid data: %s', data)\n                        continue\n                    hosts = ','.join(\n                        '{}:{}'.format(server, data['port'])\n                        for server in data['servers']\n                    )\n                    if hosts != self._zookeeper_hosts:\n                        self._zookeeper_hosts = hosts\n                        logger.info('zookeeper hosts updated to: %s', self._zookeeper_hosts)\n                        return True\n            except Exception:\n                logger.exception('failed to poll exhibitor')\n                continue\n        logger.warning('failed to get zookeeper hosts from exhibitor')\n    return False\n\n", "CodeBleu_score": 33.80647720162315}, {"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() > self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        for host in self._exhibitors:\n            try:\n                response = requests_get(\n                    host, self._exhibitor_port, self._uri_path, self.TIMEOUT\n                )\n                if response.status_code == 200:\n                    data = response.json()\n                    zookeeper_hosts = ','.join(\n                        [\n                            f'{server}:{port}'\n                            for server, port in zip(\n                                data['servers'], data['port']\n                            )\n                        ]\n                    )\n                    if zookeeper_hosts != self._zookeeper_hosts:\n                        self._zookeeper_hosts = zookeeper_hosts\n                        logger.info(\n                            f'zookeeper hosts updated to {self._zookeeper_hosts}'\n                        )\n                        return True\n            except Exception:\n                logger.exception('error polling exhibitor')\n        self._exhibitors = self._boot_exhibitors\n        return False\n\n", "CodeBleu_score": 34.521200357575694}]}
{"instance_id": "022ed3721913", "generation_result": [{"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        self.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 77.84703236539076}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        self.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 77.84703236539076}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        if self._ensemble_provider.zookeeper_hosts:\n            self._hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 73.41752664635848}]}
{"instance_id": "c7d2e1db0cda", "generation_result": [{"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n    nodeCache.getListenable().addListener(nodeCacheListener);\n}", "CodeBleu_score": 62.20116850376204}]}
{"instance_id": "c779f567d3e2", "generation_result": [{"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}]}
{"instance_id": "7f0ce4db487b", "generation_result": [{"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n        return zookeeperServer.getPayload().getMetadata();\n    }\n    return emptyMetadata();\n}", "CodeBleu_score": 67.39872746248045}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n        return zookeeperServer.getPayload().getMetadata();\n    }\n    return Collections.emptyMap();\n}", "CodeBleu_score": 66.68915248008628}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload().getMetadata();\n    }\n\n    return emptyMetadata();\n}", "CodeBleu_score": 67.39872746248045}]}
{"instance_id": "7365d194b7ee", "generation_result": [{"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData);\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n    nodeCache.getListenable().addListener(nodeCacheListener);\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 71.39392483157606}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData.getPath());\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n    nodeCache.getListenable().addListener(nodeCacheListener);\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 73.94586464202271}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData.getPath());\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n    nodeCache.getListenable().addListener(nodeCacheListener);\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener();\n    zookeeperListener.setNodeCache(nodeCache);\n    zookeeperListener.setNodeCacheListener(nodeCacheListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 71.19858340364817}]}
{"instance_id": "6f492670162b", "generation_result": [{"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.initWithNiwsConfig(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 86.97031261131865}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}]}
{"instance_id": "17c01a5f8ef8", "generation_result": [{"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        applicationContext.getBean(RegisterListenerExecutor.class).onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed\", e);\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 51.16616645223364}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        applicationContext.getBean(RegisterListenerExecutor.class).onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Failed to get RegisterListenerExecutor from ApplicationContext, skipping onRegister execution\", e);\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 51.16616645223364}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignored\");\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 91.09638065942949}]}
{"instance_id": "99392dfa2335", "generation_result": [{"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}]}
{"instance_id": "02a835946e90", "generation_result": [{"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}]}
{"instance_id": "64cf817f5945", "generation_result": [{"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = pluginAdapter.getGroup();\n    String serviceId = pluginAdapter.getServiceId();\n    String version = pluginAdapter.getVersion();\n\n    String dataId = getDataId(globalConfig);\n\n    LOG.info(\"Subscribe {} config, group={}, serviceId={}, version={}, dataId={}\", getConfigType(), group, serviceId, version, dataId);\n\n    try {\n        return zookeeperOperation.subscribeConfig(group, serviceId, version, dataId, new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                callbackConfig(config, globalConfig);\n            }\n        });\n    } catch (Exception e) {\n        LOG.error(\"Subscribe {} config failed, group={}, serviceId={}, version={}, dataId={}\", getConfigType(), group, serviceId, version, dataId, e);\n\n        return null;\n    }\n}", "CodeBleu_score": 61.408595329694336}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = pluginAdapter.getGroup();\n    String serviceId = pluginAdapter.getServiceId();\n    String dataId = getDataId(globalConfig);\n\n    LOG.info(\"Subscribe {} config, group={}, serviceId={}, dataId={}\", getConfigType(), group, serviceId, dataId);\n\n    try {\n        return zookeeperOperation.subscribeConfig(group, serviceId, dataId, new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                if (applyConfig(config)) {\n                    LOG.info(\"Call back {} config, group={}, serviceId={}, dataId={}, config={}\", getConfigType(), group, serviceId, dataId, config);\n                }\n            }\n        });\n    } catch (Exception e) {\n        LOG.error(\"Subscribe {} config failed, group={}, serviceId={}, dataId={}\", getConfigType(), group, serviceId, dataId, e);\n\n        return null;\n    }\n}", "CodeBleu_score": 51.16992883638183}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId();\n\n    LOG.info(\"Subscribe {} config, group={}, dataId={}\", getConfigType(), group, dataId);\n\n    try {\n        ZookeeperSubscribeCallback zookeeperSubscribeCallback = new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                callbackConfig(config);\n            }\n        };\n        return zookeeperOperation.subscribeConfig(group, dataId, zookeeperSubscribeCallback);\n    } catch (Exception e) {\n        LOG.error(\"Subscribe {} config failed, group={}, dataId={}\", getConfigType(), group, dataId, e);\n\n        return null;\n    }\n}", "CodeBleu_score": 53.342321349177865}]}
{"instance_id": "be408acc71f6", "generation_result": [{"code": "public CuratorFramework curatorFramework(){\n\tString zookeeperConnection = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION);\n    if (StringUtils.isEmpty(zookeeperConnection)) {\n        throw new DiscoveryException(\"Zookeeper connection string is empty\");\n    }\n\n    Integer zookeeperRetryTimes = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_TIMES, Integer.class);\n    Integer zookeeperRetryInterval = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_INTERVAL, Integer.class);\n\n    ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(zookeeperRetryInterval * 1000, zookeeperRetryTimes);\n    CuratorFramework client = CuratorFrameworkFactory.newClient(zookeeperConnection, retryPolicy);\n    client.start();\n\n    return client;\n}", "CodeBleu_score": 45.14812213946641}, {"code": "public CuratorFramework curatorFramework(){\n\tString connection = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION);\n    if (StringUtils.isEmpty(connection)) {\n        throw new DiscoveryException(\"Zookeeper connection string is empty\");\n    }\n\n    int retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class, ZookeeperConstant.ZOOKEEPER_DEFAULT_RETRY_COUNT);\n    int retrySleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_SLEEP_TIME, Integer.class, ZookeeperConstant.ZOOKEEPER_DEFAULT_RETRY_SLEEP_TIME);\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder().connectString(connection).retryPolicy(new ExponentialBackoffRetry(retrySleepTime, retryCount)).build();\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 73.77171800888613}, {"code": "public CuratorFramework curatorFramework(){\n\tString zookeeperConnection = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION);\n    if (StringUtils.isEmpty(zookeeperConnection)) {\n        throw new DiscoveryException(\"Zookeeper connection string is empty\");\n    }\n\n    String zookeeperRetryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, ZookeeperConstant.ZOOKEEPER_RETRY_COUNT_DEFAULT_VALUE);\n    String zookeeperRetrySleepMs = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_SLEEP_MS, ZookeeperConstant.ZOOKEEPER_RETRY_SLEEP_MS_DEFAULT_VALUE);\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder().connectString(zookeeperConnection).retryPolicy(new ExponentialBackoffRetry(Integer.parseInt(zookeeperRetryCount), Integer.parseInt(zookeeperRetrySleepMs))).build();\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 56.17839568937608}]}
{"instance_id": "3628d7b6f0a5", "generation_result": [{"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}]}
{"instance_id": "68a13b54ad7c", "generation_result": [{"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries(), zooKeeperClientProperties.getMaxSleepMs());\n    curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 62.600574909640116}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(),\n            zooKeeperClientProperties.getMaxRetries(),\n            zooKeeperClientProperties.getMaxSleepMs());\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 61.856214877742865}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries());\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .connectionTimeoutMs(zooKeeperClientProperties.getConnectionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 60.58921516595653}]}
{"instance_id": "018a3d7b5f59", "generation_result": [{"code": "public void lock(String path) throws DistributedLockException{\n\tInterProcessMutex lock = locksMap.get(Thread.currentThread());\n    if (lock == null) {\n        lock = new InterProcessMutex(curatorFrameworkClient.getClient(), path);\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n        locksMap.put(Thread.currentThread(), lock);\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    }\n}", "CodeBleu_score": 45.67618640620222}, {"code": "public void lock(String path) throws DistributedLockException{\n\tThread currentThread = Thread.currentThread();\n    InterProcessMutex lock = locksMap.get(currentThread);\n    if (lock == null) {\n        lock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\n        try {\n            lock.acquire();\n            locksMap.put(currentThread, lock);\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    }\n}", "CodeBleu_score": 44.56382425603742}, {"code": "public void lock(String path) throws DistributedLockException{\n\tThread currentThread = Thread.currentThread();\n    InterProcessMutex lock = locksMap.get(currentThread);\n    if (lock == null) {\n        lock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\n        try {\n            lock.acquire();\n            locksMap.put(currentThread, lock);\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    }\n}", "CodeBleu_score": 44.56382425603742}]}
{"instance_id": "2068af09c973", "generation_result": [{"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tDtpProperties.Zookeeper zookeeper = dtpProperties.getZookeeper();\n    CuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    if (PROPERTIES.equals(zookeeper.getConfigType())) {\n        return genPropertiesTypeMap(curatorFramework, nodePath);\n    } else if (JSON.equals(zookeeper.getConfigType())) {\n        String configKey = zookeeper.getConfigKey();\n        String configValue = new String(curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey)), StandardCharsets.UTF_8);\n        return ConfigHandler.getInstance().parseConfig(configValue);\n    } else {\n        throw new UnsupportedOperationException(\"Unsupported config type: \" + zookeeper.getConfigType());\n    }\n}", "CodeBleu_score": 49.6618409310688}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    DtpProperties.Zookeeper zookeeper = dtpProperties.getZookeeper();\n    if (zookeeper.getConfigType() == PROPERTIES) {\n        return genPropertiesTypeMap(curatorFramework, nodePath);\n    }\n    if (zookeeper.getConfigType() == JSON) {\n        String configKey = zookeeper.getConfigKey();\n        String configValue = new String(curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey)), StandardCharsets.UTF_8);\n        return ConfigHandler.getInstance().parseConfig(configValue);\n    }\n    return Collections.emptyMap();\n}", "CodeBleu_score": 51.67245309159041}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    if (dtpProperties.getZookeeper().getConfigType() == PROPERTIES) {\n        return genPropertiesTypeMap(curatorFramework, nodePath);\n    }\n    String configKey = dtpProperties.getZookeeper().getConfigKey();\n    String configValue = curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey));\n    return ConfigHandler.getInstance().parseConfig(configValue);\n}", "CodeBleu_score": 45.1747939407856}]}
{"instance_id": "3d3c9020d299", "generation_result": [{"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tMutablePropertySources propertySources = environment.getPropertySources();\n    OriginTrackedMapPropertySource propertySource = (OriginTrackedMapPropertySource) propertySources.get(ZK_PROPERTY_SOURCE_NAME);\n    if (propertySource != null) {\n        propertySources.remove(ZK_PROPERTY_SOURCE_NAME);\n    }\n    // 1. get dtp properties\n    DtpProperties dtpProperties = BinderHelper.bindDtpProperties(environment);\n    // 2. generate properties map\n    Map<String, Object> propertiesMap = CuratorUtil.generatePropertiesMap(dtpProperties);\n    // 3. check if the required property exists in the environment\n    if (!propertiesMap.isEmpty()) {\n        // 4. create zookeeper property source\n        propertySource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, propertiesMap);\n        propertySources.addLast(propertySource);\n    }\n}", "CodeBleu_score": 43.035505430179086}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tMutablePropertySources propertySources = environment.getPropertySources();\n    DtpProperties dtpProperties = BinderHelper.bindDtpProperties(environment);\n    Map<String, Object> dtpPropertiesMap = BinderHelper.convertToMap(dtpProperties);\n    if (dtpProperties.isZookeeperEnabled()) {\n        String zkConfig = CuratorUtil.getConfig(dtpProperties.getZookeeperConfig());\n        if (zkConfig != null) {\n            dtpPropertiesMap.putAll(BinderHelper.convertToMap(zkConfig));\n        }\n    }\n    if (propertySources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n        ((OriginTrackedMapPropertySource) propertySources.get(ZK_PROPERTY_SOURCE_NAME)).getSource().putAll(dtpPropertiesMap);\n    } else {\n        propertySources.addFirst(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, dtpPropertiesMap));\n    }\n}", "CodeBleu_score": 40.20504369574923}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = BinderHelper.bindDtpProperties(environment);\n    if (dtpProperties.getZookeeper() == null) {\n        return;\n    }\n    Map<String, Object> properties = BinderHelper.bindZookeeperProperties(dtpProperties);\n    MutablePropertySources propertySources = environment.getPropertySources();\n    OriginTrackedMapPropertySource propertySource = (OriginTrackedMapPropertySource) propertySources.get(ZK_PROPERTY_SOURCE_NAME);\n    if (propertySource == null) {\n        propertySource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties);\n        propertySources.addFirst(propertySource);\n    } else {\n        propertySource.getSource().putAll(properties);\n    }\n}", "CodeBleu_score": 45.82567292811716}]}
{"instance_id": "0a934e7885be", "generation_result": [{"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\treturn UseRouteManager(builder, provider => {\n        var clientProvider = provider.GetRequiredService<IZookeeperClientProvider>();\n        var logger = provider.GetRequiredService<ILogger<ZookeeperServiceRouteManager>>();\n        var serializer = provider.GetRequiredService<ISerializer<byte[]>>();\n        var serviceRouteFactory = provider.GetRequiredService<IServiceRouteFactory>();\n        return new ZookeeperServiceRouteManager(clientProvider, logger, serializer, configInfo, serviceRouteFactory);\n    });\n}", "CodeBleu_score": 45.68572302233792}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\treturn UseRouteManager(builder, provider =>\n    {\n        var serviceRouteFactory = GetServiceRouteFactory(configInfo);\n        var serviceCommandFactory = GetServiceCommandFactory(configInfo);\n        var serializer = provider.GetService<ISerializer<byte[]>>();\n        var logger = provider.GetService<ILogger<ZooKeeperServiceRouteManager>>();\n        var clientProvider = provider.GetService<IZookeeperClientProvider>();\n        return new ZooKeeperServiceRouteManager(clientProvider, serviceRouteFactory, serviceCommandFactory, serializer, logger);\n    });\n}", "CodeBleu_score": 43.88014745719569}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\treturn UseRouteManager(builder, provider => {\n        var result = new ZookeeperServiceRouteManager(\n            GetConfigInfo(configInfo),\n            provider.GetRequiredService<ISerializer<byte[]>>(),\n            provider.GetRequiredService<ISerializer<string>>(),\n            provider.GetRequiredService<IServiceRouteFactory>(),\n            provider.GetRequiredService<ILogger<ZookeeperServiceRouteManager>>(),\n            provider.GetRequiredService<IZookeeperClientProvider>(),\n            provider.GetRequiredService<IServiceRouteFactory>(),\n            provider.GetRequiredService<ILogger<ZookeeperServiceRouteManager>>());\n        return result;\n    });\n}", "CodeBleu_score": 49.42995427887872}]}
{"instance_id": "c30a17587331", "generation_result": [{"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider => {\n                var result = new ZooKeeperServiceCommandManager(\n                    GetConfigInfo(configInfo),\n                    provider.GetRequiredService<ISerializer<byte[]>>(),\n                    provider.GetRequiredService<ISerializer<string>>(),\n                    provider.GetRequiredService<IServiceRouteManager>(),\n                    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                    provider.GetRequiredService<IZookeeperClientProvider>());\n                return result;\n            });\n            return this;\n        }", "CodeBleu_score": 76.53540817734624}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n            {\n                var result = new ZooKeeperServiceCommandManager(\n                    GetConfigInfo(configInfo),\n                    provider.GetRequiredService<ISerializer<byte[]>>(),\n                    provider.GetRequiredService<ISerializer<string>>(),\n                    provider.GetRequiredService<IServiceRouteManager>(),\n                    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                    provider.GetRequiredService<IZookeeperClientProvider>());\n                return result;\n            });\n            return this;\n        }", "CodeBleu_score": 76.26580586950858}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n            {\n                var result = new ZooKeeperServiceCommandManager(\n                    GetConfigInfo(configInfo),\n                    provider.GetRequiredService<ISerializer<byte[]>>(),\n                    provider.GetRequiredService<ISerializer<string>>(),\n                    provider.GetRequiredService<IServiceRouteManager>(),\n                    provider.GetRequiredService<IServiceEntryManager>(),\n                    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                    provider.GetRequiredService<IZookeeperClientProvider>());\n                return result;\n            });\n            return this;\n        }", "CodeBleu_score": 82.43459901229286}]}
{"instance_id": "213cc4aa9b19", "generation_result": [{"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(string.Format(SR.Error_InvalidFilePath, nameof(path)), nameof(path));\n    }\n\n    if (provider == null) {\n        if (Path.IsPathRooted(path)) {\n            provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n            path = Path.GetFileName(path);\n        }\n    }\n\n    var source = new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 65.62026281243091}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(SR.Error_InvalidFilePath, nameof(path));\n    }\n\n    if (provider == null) {\n        if (Path.IsPathRooted(path)) {\n            provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n            path = Path.GetFileName(path);\n        }\n    }\n\n    var source = new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 65.82768995551358}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null)\n        throw new ArgumentNullException(nameof(builder));\n    if (string.IsNullOrEmpty(path))\n        throw new ArgumentException(\"File path must be a non-empty string.\", nameof(path));\n\n    if (provider == null && Path.IsPathRooted(path))\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n\n    var source = new ZookeeperConfigurationSource\n    {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 64.17685446771486}]}
{"instance_id": "15bbffa82a13", "generation_result": [{"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tvar routes = await GetRoutesAsync();\n    var descriptors = routes.Select(route => new ServiceRouteDescriptor\n    {\n        Address = route.Address.Except(Address).ToList(),\n        ServiceDescriptor = route.ServiceDescriptor\n    }).ToList();\n    await SetRoutesAsync(descriptors);\n}", "CodeBleu_score": 37.19820496935154}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tvar routes = await GetRoutesAsync();\n    foreach (var route in routes) {\n        route.Address = route.Address.Except(Address).ToList();\n    }\n    await SetRoutesAsync(routes);\n}", "CodeBleu_score": 76.56684719266867}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tvar routes = await GetRoutesAsync();\n    var descriptors = routes.Select(route => new ServiceRouteDescriptor {\n        Address = route.Address.Except(Address).ToList(),\n        ServiceDescriptor = route.ServiceDescriptor\n    }).ToList();\n    await SetRoutesAsync(descriptors);\n}", "CodeBleu_score": 37.22668418118145}]}
{"instance_id": "125d23de2a60", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}]}
{"instance_id": "b020a235d57f", "generation_result": [{"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}]}
{"instance_id": "169ed4cc2ad4", "generation_result": [{"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}]}
{"instance_id": "af01261d3199", "generation_result": [{"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");", "CodeBleu_score": 40.568729144342484}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");", "CodeBleu_score": 40.568729144342484}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");", "CodeBleu_score": 40.568729144342484}]}
{"instance_id": "fda02cec3296", "generation_result": [{"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}]}
{"instance_id": "6c3018ff0c0d", "generation_result": [{"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}]}
{"instance_id": "60ff973ba60a", "generation_result": [{"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}]}
{"instance_id": "4597b75f2b3d", "generation_result": [{"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tif (watchedEvent.get_Type() == EventType.NodeDataChanged) {\n        var (successEvent, zooKeeper) = await _zooKeeperCall();\n        try {\n            var newData = await zooKeeper.getDataAsync(_path, true);\n            _action(_currentData, newData);\n            _currentData = newData;\n        }\n        catch (KeeperException.NoNodeException) {\n            //ignore\n        }\n        finally {\n            successEvent.Set();\n        }\n    }\n}", "CodeBleu_score": 56.501415985079184}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (_, zooKeeper) = await _zooKeeperCall();\n    switch (watchedEvent.get_Type()) {\n        case EventType.NodeDataChanged:\n            var data = await zooKeeper.getDataAsync(_path, true);\n            _action(_currentData, data.Data);\n            _currentData = data.Data;\n            break;\n        default:\n            _action(_currentData, null);\n            break;\n    }\n}", "CodeBleu_score": 58.07800355634496}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (_, zooKeeper) = await _zooKeeperCall();\n    switch (watchedEvent.get_Type()) {\n        case EventType.NodeDataChanged:\n            var data = await zooKeeper.getDataAsync(Path, false);\n            _action(_currentData, data.Data);\n            _currentData = data.Data;\n            zooKeeper.existsAsync(Path, this);\n            break;\n    }\n}", "CodeBleu_score": 56.65545884692731}]}
{"instance_id": "cb4f0e22a0d5", "generation_result": [{"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.Register(provider => {\n        var logger = provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>();\n        var addressSelector = provider.GetRequiredService<IZookeeperAddressSelector>();\n        var healthCheckService = provider.GetRequiredService<IHealthCheckService>();\n        var serializer = provider.GetRequiredService<ISerializer<byte[]>>();\n        var config = GetConfigInfo(configInfo);\n        return new DefaultZookeeperClientProvider(config, logger, addressSelector, healthCheckService, serializer);\n    }).As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 50.261557735281606}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.Register(provider => {\n        var factory = provider.GetRequiredService<Func<string, IZookeeperClient>>();\n        var logger = provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>();\n        var addressSelector = provider.GetRequiredService<IZookeeperAddressSelector>();\n        var healthCheckService = provider.GetRequiredService<IHealthCheckService>();\n        var client = factory(configInfo.ConnectionString);\n        var provider = new DefaultZookeeperClientProvider(client, addressSelector, healthCheckService, logger);\n        return provider;\n    }).As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 41.92965902080489}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.Register(provider => {\n        var factory = provider.GetRequiredService<Func<string, IZookeeperClient>>();\n        var logger = provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>();\n        var addressSelector = provider.GetRequiredService<IZookeeperAddressSelector>();\n        var healthCheckService = provider.GetRequiredService<IHealthCheckService>();\n        return new DefaultZookeeperClientProvider(factory, logger, addressSelector, healthCheckService, configInfo);\n    }).As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 52.72733031105824}]}
{"instance_id": "80709a4b2b69", "generation_result": [{"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tvar path = _configInfo.MqttRoutePath;\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation($\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\u914d\u7f6e\u3002\");\n        var client = zooKeeper.Item1;\n        var index = 0;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n        while (childrens.Count() > 1)\n        {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n            if (await client.existsAsync(nodePath) == null)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u6dfb\u52a0zk\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await client.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        foreach (var route in routes)\n        {\n            var nodePath = $\"{path}/{route.Id}\";\n            var nodeData = _serializer.Serialize(route);\n            if (await client.existsAsync(nodePath) == null)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u6dfb\u52a0zk\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await client.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0zk\u8282\u70b9\uff1a{nodePath}\u3002\");\n                var onlineData = (", "CodeBleu_score": 61.717152015586365}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tvar path = _configInfo.MqttRoutePath;\n    if (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation($\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\uff0c\u8def\u7531\u5730\u5740\uff1a{path}\u3002\");\n\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var client = zooKeeper.Item2;\n        if (await client.existsAsync(path) == null) {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n            await client.createAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        }\n\n        var serviceRouteCaches = routes.ToArray();\n        foreach (var serviceRouteCache in serviceRouteCaches) {\n            var nodePath = $\"{path}{serviceRouteCache.Id}\";\n            var nodeData = _serializer.Serialize(serviceRouteCache);\n            if (await client.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9{nodePath}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await client.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9{nodePath}\u7684\u6570\u636e\u3002\");\n\n                var onlineData = (await client.getDataAsync(nodePath)).Data;\n                if (!DataEquals(nodeData, onlineData)) {\n                    await client.setDataAsync(nodePath, nodeData);\n                }\n            }\n        }\n\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\");\n    }", "CodeBleu_score": 74.00944473845267}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        var path = _configInfo.MqttRoutePath;\n        if (await zooKeeper.Item2.existsAsync(path) == null)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n            await zooKeeper.Item2.createAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        }\n        await zooKeeper.Item2.createAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        foreach (var route in routes)\n        {\n            var nodePath = $\"{path}/{route.Id}\";\n            var nodeData = _serializer.Serialize(route);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u6dfb\u52a0\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u3002\");\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if (!DataEquals(nodeData, onlineData))\n                    await zooKeeper.Item2.setDataAsync(nodePath, nodeData);", "CodeBleu_score": 81.54136025686792}]}
{"instance_id": "69ff4d3427cd", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}]}
{"instance_id": "8856055e4872", "generation_result": [{"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar path = _configInfo.CachePath;\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var zooKeeperClient = zooKeeper.Item1;\n        if (!await zooKeeperClient.ExistsAsync(path)) {\n            await zooKeeperClient.CreateAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        }\n        foreach (var cacheDescriptor in cacheDescriptors) {\n            var nodePath = $\"{path}{cacheDescriptor.Id}\";\n            if (!await zooKeeperClient.ExistsAsync(nodePath)) {\n                await zooKeeperClient.CreateAsync(nodePath, cacheDescriptor.ToByteArray(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                var bytes = (await zooKeeperClient.GetDataAsync(nodePath)).ToArray();\n                var descriptor = bytes.ToServiceCacheDescriptor();\n                if (cacheDescriptor.Version != descriptor.Version) {\n                    await zooKeeperClient.SetDataAsync(nodePath, cacheDescriptor.ToByteArray());\n                }\n            }\n        }\n    }\n    _logger.LogInformation(\"\u7f13\u5b58\u66f4\u65b0\u6210\u529f\u3002\");\n}", "CodeBleu_score": 50.395978786879304}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar path = _configInfo.CachePath;\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        if (await zooKeeper.CreateAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT)) {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5df2\u5c1d\u8bd5\u521b\u5efa\u3002\");\n        }\n\n        var childrens = await zooKeeper.GetChildrenAsync(path);\n        foreach (var cacheDescriptor in cacheDescriptors) {\n            var nodePath = $\"{path}/{cacheDescriptor.Id}\";\n            if (!childrens.Children.Contains(cacheDescriptor.Id)) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9{nodePath}\u4e0d\u5b58\u5728\uff0c\u5df2\u5c1d\u8bd5\u521b\u5efa\u3002\");\n                if (await zooKeeper.CreateAsync(nodePath, _serializer.Serialize(cacheDescriptor), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT)) {\n                    if (_logger.IsEnabled(LogLevel.Information))\n                        _logger.LogInformation($\"\u7f13\u5b58\u8282\u70b9\uff1a{nodePath}\u521b\u5efa\u6210\u529f\u3002\");\n                }\n            }\n            else {\n                var bytes = (await zooKeeper.GetDataAsync(nodePath)).ToArray();\n                var cache = _serializer.Deserialize<ServiceCacheDescriptor>(bytes);\n                if (cache.Equals(cacheDescriptor))\n                    continue;\n                var online = cache.Online;\n                cache = cacheDescriptor;\n                cache.Online = online;\n                if (await zooKeeper.SetDataAsync(nodePath, _serializer.Serialize(cache))) {\n                    if (_logger.IsEnabled(LogLevel.Information))\n                        _logger.LogInformation($\"\u7f13\u5b58\u8282\u70b9\uff1a{nodePath}\u66f4\u65b0\u6210\u529f\u3002\");\n                }", "CodeBleu_score": 62.10395389980494}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar path = _configInfo.CachePath;\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        var client = zooKeeper.Item1;\n        if (await client.ExistsAsync(path) == Stat.Empty)\n        {\n            await client.CreateAsync(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        }\n        foreach (var cacheDescriptor in cacheDescriptors)\n        {\n            var nodePath = $\"{path}/{cacheDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(cacheDescriptor);\n            if (await client.ExistsAsync(nodePath) == Stat.Empty)\n            {\n                await client.CreateAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else\n            {\n                var onlineData = (await client.GetDataAsync(nodePath)).ToArray();\n                if (!DataEquals(nodeData, onlineData))\n                {\n                    await client.SetDataAsync(nodePath, nodeData);\n                }\n            }\n        }\n    }\n    _logger.LogInformation(\"\u7f13\u5b58\u914d\u7f6e\u66f4\u65b0\u6210\u529f\u3002\");\n}", "CodeBleu_score": 56.803675133374256}]}
{"instance_id": "57d92c17de83", "generation_result": [{"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}]}
{"instance_id": "b276a07da453", "generation_result": [{"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses) {\n        _healthCheckService.Monitor(address);\n        var task = _healthCheckService.IsHealth(address);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        var ipAddress = address as IpAddressModel;\n        result.Add(CreateZooKeeper(ipAddress));\n    }\n    return result;\n}", "CodeBleu_score": 80.78045108112244}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses) {\n        _healthCheckService.Monitor(address);\n        var task = _healthCheckService.IsHealth(address);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        var ipAddress = address as IpAddressModel;\n        result.Add(CreateZooKeeper(ipAddress));\n    }\n    return result;\n}", "CodeBleu_score": 80.78045108112244}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses) {\n        _healthCheckService.Monitor(address);\n        var task = _healthCheckService.IsHealth(address);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        var ipAddress = address as IpAddressModel;\n        var tuple = CreateZooKeeper(ipAddress);\n        result.Add(tuple);\n    }\n    return result;\n}", "CodeBleu_score": 77.31047338626333}]}
{"instance_id": "dbb6463e7ee8", "generation_result": [{"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}]}
{"instance_id": "0d9f311aa482", "generation_result": [{"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = getZooKeeper();\n    if (zooKeeper != null) {\n        try {\n            zooKeeper.register(new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                }\n            });\n            if (waitForShutdownTimeoutMs > 0) {\n                zooKeeper.close(waitForShutdownTimeoutMs);\n            } else {\n                zooKeeper.close();\n            }\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        }\n    }\n}", "CodeBleu_score": 49.90055442531181}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = getZooKeeper();\n    if (zooKeeper != null) {\n        zooKeeper.register(new Watcher() {\n            @Override\n            public void process(WatchedEvent event) {\n                // dummy watcher to clear the default watcher\n            }\n        });\n\n        try {\n            if (waitForShutdownTimeoutMs > 0) {\n                zooKeeper.close(waitForShutdownTimeoutMs);\n            } else {\n                zooKeeper.close();\n            }\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        }\n    }\n}", "CodeBleu_score": 48.81538287494079}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = getZooKeeper();\n    if (zooKeeper != null) {\n        zooKeeper.register(new Watcher() {\n            @Override\n            public void process(WatchedEvent event) {\n            }\n        });\n        if (waitForShutdownTimeoutMs == 0) {\n            zooKeeper.close();\n        } else {\n            final CountDownLatch latch = new CountDownLatch(1);\n            zooKeeper.close(waitForShutdownTimeoutMs, () -> latch.countDown());\n            latch.await();\n        }\n    }\n}", "CodeBleu_score": 42.09100407283749}]}
{"instance_id": "d57ef4592303", "generation_result": [{"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}]}
{"instance_id": "d722f6d0beb6", "generation_result": [{"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}]}
{"instance_id": "8492e37ce46e", "generation_result": [{"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}]}
{"instance_id": "4a01f5365731", "generation_result": [{"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long currentSessionId = client.getZooKeeper().getSessionId();\n        if (currentSessionId != sessionId) {\n            log.info(\"Session ID has changed from \" + sessionId + \" to \" + currentSessionId + \" - deleting old protected node: \" + foundNode);\n            client.delete().forPath(foundNode);\n            sessionId = currentSessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 45.37315176428884}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long newSessionId = client.getZooKeeper().getSessionId();\n        if (newSessionId != sessionId) {\n            log.info(\"Session has changed during protected mode, deleting old node: \" + foundNode);\n            client.delete().forPath(foundNode);\n            sessionId = newSessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 44.631385363259504}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long currentSessionId = client.getZooKeeper().getSessionId();\n        if (currentSessionId != sessionId) {\n            log.info(\"Session has changed during a protected mode create, deleting old node: \" + foundNode);\n            client.delete().forPath(foundNode);\n            sessionId = currentSessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 44.92117879538065}]}
{"instance_id": "126d0922a699", "generation_result": [{"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tOperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = RetryLoop.callWithRetry(\n            client.getZookeeperClient(),\n            new Callable<Stat>() {\n                @Override\n                public Stat call() throws Exception {\n                    return client.getZooKeeper().setACL(path, aclList, version);\n                }\n            }\n    );\n    trace.setPath(path).setStat(resultStat).commit();\n    return resultStat;\n}", "CodeBleu_score": 87.46424996935558}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tOperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = RetryLoop.callWithRetry(\n            client.getZookeeperClient(),\n            new Callable<Stat>() {\n                @Override\n                public Stat call() throws Exception {\n                    return client.getZooKeeper().setACL(path, aclList, version);\n                }\n            }\n    );\n    trace.setPath(path).setStat(resultStat).commit();\n    return resultStat;\n}", "CodeBleu_score": 87.46424996935558}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tStat resultStat = null;\n    OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    try {\n        resultStat = RetryLoop.callWithRetry(\n                client.getZookeeperClient(),\n                new Callable<Stat>() {\n                    @Override\n                    public Stat call() throws Exception {\n                        return client.getZooKeeper().setACL(path, aclList, version);\n                    }\n                }\n        );\n        trace.setPath(path).setStat(resultStat).commit();\n    } catch (Exception e) {\n        trace.setException(e).commit();\n        throw e;\n    }\n    return resultStat;\n}", "CodeBleu_score": 75.72208925684066}]}
{"instance_id": "e07b079ff61f", "generation_result": [{"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkArgument(thisId != null, \"thisId cannot be null\");\n    this.membershipPath = Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    this.thisId = thisId;\n    pen = new PersistentNode(client, CreateMode.EPHEMERAL, false, membershipPath, thisId.getBytes(), payload);\n    cache = CuratorCache.bridgeBuilder(client, membershipPath).\n            withData(parentPathFilter(membershipPath)).\n            build();\n    cache.start();\n}", "CodeBleu_score": 56.551920789140475}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tthis.thisId = Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    this.membershipPath = Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkArgument(ZKPaths.getPathAndNode(membershipPath).getPath().length() != 0, \"membershipPath must have a parent\");\n    Preconditions.checkArgument(ZKPaths.getPathAndNode(membershipPath).getNode().length() != 0, \"membershipPath must have a node\");\n\n    pen = new PersistentNode(client, CreateMode.EPHEMERAL, false, membershipPath, payload);\n    cache = CuratorCache.bridgeBuilder(client, membershipPath).withData(parentPathFilter(membershipPath)).build();\n    cache.start();\n}", "CodeBleu_score": 53.64544009037904}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n\tPreconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n\tPreconditions.checkNotNull(thisId, \"thisId cannot be null\");\n\tPreconditions.checkNotNull(payload, \"payload cannot be null\");\n\tthis.membershipPath = membershipPath;\n\tthis.thisId = thisId;\n\ttry {\n\t\tclient.create().creatingParentsIfNeeded().forPath(ZKPaths.makePath(membershipPath, thisId), payload);\n\t} catch (Exception e) {\n\t\tThreadUtils.checkInterrupted(e);\n\t\tthrow Throwables.propagate(e);\n\t}\n\tpen = new PersistentNode(client, CreateMode.EPHEMERAL, false, membershipPath, thisId, payload);\n\tcache = CuratorCache.bridgeBuilder(client, membershipPath).withData(parentPathFilter(membershipPath)).build();\n}", "CodeBleu_score": 54.5547121127921}]}
{"instance_id": "d11c76b2b912", "generation_result": [{"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client);\n\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            System.out.println(\"Created node name is: \" + name);\n            // now watch the node\n            async.watched().getData().forPath(path).whenComplete((data, eventException) -> {\n                if (eventException != null) {\n                    // there was a problem\n                    eventException.printStackTrace();\n                } else {\n                    System.out.println(\"Node changed: \" + new String(data));\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 77.87985576392954}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client);\n\n    // create a node at the given path with the given payload asynchronously\n    async.create()\n            .forPath(path)\n            .whenComplete((name, exception) -> {\n                if (exception != null) {\n                    // there was a problem\n                    exception.printStackTrace();\n                } else {\n                    System.out.println(\"Created node name is: \" + name);\n                    // setup a watch on the node\n                    async.getChildren()\n                            .watched()\n                            .forPath(path)\n                            .whenComplete((children, event) -> {\n                                if (event != null) {\n                                    // there was an event\n                                    System.out.println(\"Event received: \" + event);\n                                }\n                            });\n                }\n            });\n}", "CodeBleu_score": 71.70477247176844}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path with the given payload asynchronously\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            System.out.println(\"Created node name is: \" + name);\n            // watch the node for changes\n            async.watched().getData().forPath(path).whenComplete((data, eventException) -> {\n                if (eventException != null) {\n                    // there was a problem\n                    eventException.printStackTrace();\n                } else {\n                    // the data has changed\n                    System.out.println(\"Data has changed to: \" + new String(data));\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 76.54386947126436}]}
{"instance_id": "4571de52ad5d", "generation_result": [{"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZooKeeperClient().getZooKeeper().multi(transaction.getOps());\n    OpResult firstResult = resultList.get(0);\n    if (firstResult.getType() == ZooDefs.OpCode.error) {\n        OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n        int errorCode = errorResult.getErr();\n        throw KeeperException.create(errorCode);\n    }\n    return resultList;\n}", "CodeBleu_score": 52.78493054533919}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZooKeeperClient().getZooKeeper().multi(transaction.getOps());\n    if (resultList.size() > 0) {\n        OpResult firstResult = resultList.get(0);\n        if (firstResult.getType() == ZooDefs.OpCode.error) {\n            OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n            throw KeeperException.create(KeeperException.Code.get(errorResult.getErr()));\n        }\n    }\n    return resultList;\n}", "CodeBleu_score": 55.42317878773942}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZooKeeperClient().getZooKeeper().multi(transaction.getOps());\n    if (resultList.size() == 0) {\n        throw new KeeperException.ConnectionLossException();\n    }\n\n    OpResult firstResult = resultList.get(0);\n    if (firstResult.getType() == ZooDefs.OpCode.error) {\n        OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n        throw KeeperException.create(KeeperException.Code.get(errorResult.getErr()));\n    }\n\n    return resultList;\n}", "CodeBleu_score": 54.0853438813504}]}
{"instance_id": "99263700ce6d", "generation_result": [{"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\t// this shows how to get all instances for all services\n    Collection<String> serviceNames = serviceDiscovery.queryForNames();\n    for (String serviceName : serviceNames) {\n        Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n        System.out.println(serviceName + \\\" has \\\" + instances.size() + \\\" instances\\\");\n        for (ServiceInstance<InstanceDetails> instance : instances) {\n            outputInstance(instance);\n        }\n    }\n}", "CodeBleu_score": 47.54568664481806}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\tCollection<String> serviceNames;\n    try {\n        serviceNames = serviceDiscovery.queryForNames();\n    } catch (Exception e) {\n        e.printStackTrace();\n        return;\n    }\n    for (String serviceName : serviceNames) {\n        Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n        System.out.println(serviceName + \\\" : \\\");\n        for (ServiceInstance<InstanceDetails> instance : instances) {\n            outputInstance(instance);\n        }\n    }\n}", "CodeBleu_score": 49.563425180156585}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\tCollection<String> serviceNames = serviceDiscovery.queryForNames();\n    for (String serviceName : serviceNames) {\n        Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n        System.out.println(serviceName + \\\" has \\\" + instances.size() + \\\" instances\\\");\n        for (ServiceInstance<InstanceDetails> instance : instances) {\n            outputInstance(instance);\n        }\n    }\n}", "CodeBleu_score": 47.332213630031916}]}
{"instance_id": "891a390fa04d", "generation_result": [{"code": "private List<String> filterAndSortChildren(List<String> children){\n\treturn Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String s) {\n            return !READY_NODE.equals(s);\n        }\n    })).stream().sorted().collect(Collectors.toList());\n}", "CodeBleu_score": 42.27512474284561}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\tList<String> filtered = Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String s) {\n            return !READY_NODE.equals(s);\n        }\n    }));\n    Collections.sort(filtered);\n    return filtered;\n}", "CodeBleu_score": 64.03796410009856}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\treturn children\n    .stream()\n    .filter(child -> !child.equals(READY_NODE))\n    .sorted()\n    .collect(Collectors.toList());\n}", "CodeBleu_score": 16.185222304253944}]}
{"instance_id": "f25af95a13b6", "generation_result": [{"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}]}
{"instance_id": "269eaac5cec6", "generation_result": [{"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}]}
{"instance_id": "c25db637a176", "generation_result": [{"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    Objects.requireNonNull(pathList);\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        pathList.forEach(path -> zookeeperMap.get(serverId).delete(path));\n    }\n}", "CodeBleu_score": 80.83093707310802}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    assertPathListNotNull(pathList);\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        pathList.forEach(path -> zookeeperMap.get(serverId).deleteAsync(path));\n    }\n}", "CodeBleu_score": 78.92916452824964}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    assertPathListNotEmpty(pathList);\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        zookeeperMap.get(serverId).delete(pathList);\n    }\n}", "CodeBleu_score": 86.80545361097091}]}
{"instance_id": "3ecb1ce78f90", "generation_result": [{"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to \" + url + \"...\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .aclList(List.of())\n            .maxRetries(0)\n            .connectionTimeout(0)\n            .retryIntervalTime(0)\n            .sessionTimeout(0)\n            .build();\n    var connection = factory.createAsync(params, List.of());\n    writer.write(\"Connected to \" + url);\n    var zooKeeperMain = new ZooKeeperMain(connection);\n    return new Terminal(id, url, connection, zooKeeperMain);\n}", "CodeBleu_score": 66.20834411435817}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to \" + url + \"...\");\n    CuratorZookeeperConnectionFactory factory = new CuratorZookeeperConnectionFactory();\n    ZookeeperParams params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .aclList(aclList)\n            .maxRetries(maxRetries)\n            .connectionTimeout(connectionTimeout)\n            .retryIntervalTime(retryIntervalTime)\n            .sessionTimeout(sessionTimeout)\n            .build();\n    var connection = factory.createAsync(params, listeners);\n    writer.write(\"Successfully connected to \" + url);\n    ZooKeeperMain main = new ZooKeeperMain(connection);\n    return new Terminal(id, url, connection, main);\n}", "CodeBleu_score": 59.700368848829676}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to \" + url + \"...\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .build();\n    var connection = factory.createAsync(params);\n    writer.write(\"Successfully connected to \" + url + \".\");\n    return new Terminal(id, url, connection, new ZooKeeperMain(connection));\n}", "CodeBleu_score": 53.929013011535844}]}
{"instance_id": "a3606e777eb5", "generation_result": [{"code": "public void blockUntilConnected(){\n\tint retries = 0;\n    while (!proxySocket.isBound()) {\n        try {\n            log.info(\"waiting for ssh-tunnel to be established\");\n            Thread.sleep(1000);\n            retries++;\n            if (retries > 6) {\n                throw new IllegalStateException(\"ssh-tunnel connection failed\");\n            }\n        } catch (InterruptedException e) {\n            log.error(\"create ssh-tunnel failed\", e);\n            this.close();\n            throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n        }\n    }\n}", "CodeBleu_score": 43.025115030386026}, {"code": "public void blockUntilConnected(){\n\tint retryCount = 0;\n    while (retryCount < 6) {\n        try {\n            if (proxySocket.isClosed()) {\n                throw new IllegalStateException(\"SSH tunnel closed\");\n            }\n            if (proxySocket.isBound()) {\n                return;\n            }\n            Thread.sleep(1000);\n            retryCount++;\n        } catch (InterruptedException e) {\n            log.error(\"blockUntilConnected failed\", e);\n            this.close();\n            throw new IllegalStateException(\"blockUntilConnected failed\", e);\n        }\n    }\n    this.close();\n    throw new IllegalStateException(\"SSH tunnel connection failed\");\n}", "CodeBleu_score": 42.5439186225815}, {"code": "public void blockUntilConnected(){\n\tfor (int i = 0; i < 6; i++) {\n        try {\n            if (proxySocket.isClosed()) {\n                log.info(\"ssh-tunnel is closed, retry {}\", i);\n                createAsync();\n            } else {\n                log.info(\"ssh-tunnel is connected\");\n                return;\n            }\n        } catch (Exception e) {\n            log.error(\"create ssh-tunnel failed\", e);\n            this.close();\n            throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n        }\n        try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e) {\n            log.error(\"sleep failed\", e);\n        }\n    }\n    throw new IllegalStateException(\"create ssh-tunnel failed\");\n}", "CodeBleu_score": 42.15312079042947}]}
{"instance_id": "a4e5b886b72b", "generation_result": [{"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node error\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 91.83551671500399}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 96.41257553887742}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 96.41257553887742}]}
{"instance_id": "4fdbc9ec2e75", "generation_result": [{"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig.getServerId());\n        treeItemCache.remove(id);\n    });\n}", "CodeBleu_score": 72.2324732972751}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        treeItemCache.remove(id);\n    });\n}", "CodeBleu_score": 59.43696483466169}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig.getServerId());\n        treeItemCache.remove(id);\n    });\n}", "CodeBleu_score": 72.2324732972751}]}
{"instance_id": "6bf0d7ac9390", "generation_result": [{"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}]}
{"instance_id": "635dea120e55", "generation_result": [{"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}]}
{"instance_id": "8627d161f5fc", "generation_result": [{"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}]}
{"instance_id": "dce20e143e04", "generation_result": [{"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}]}
{"instance_id": "d89f8e6bfb31", "generation_result": [{"code": "private String response(Socket client){\n\tfinal StringBuilder sb = new StringBuilder();\n    BufferedReader reader = null;\n    try {\n        reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n        String line;\n        while ((line = reader.readLine()) != null) {\n            sb.append(line).append(\"\n\");\n        }\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    } finally {\n        try {\n            if (reader != null) {\n                reader.close();\n            }\n            client.close();\n        } catch (IOException e) {\n            throw new IllegalStateException(e);\n        }\n    }\n    return sb.toString();\n}", "CodeBleu_score": 54.88340439151222}, {"code": "private String response(Socket client){\n\tfinal StringBuilder sb = new StringBuilder();\n    try (final BufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()))) {\n        String line;\n        while ((line = reader.readLine()) != null) {\n            sb.append(line).append(\"\n\");\n        }\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    }\n    return sb.toString();\n}", "CodeBleu_score": 48.943487098622896}, {"code": "private String response(Socket client){\n\tfinal StringBuilder builder = new StringBuilder();\n    try (BufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()))) {\n        String line;\n        while ((line = reader.readLine()) != null) {\n            builder.append(line);\n            builder.append(\"\n\");\n        }\n        return builder.toString();\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 52.63612818338268}]}
{"instance_id": "b0a277c5e3c0", "generation_result": [{"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\ttry {\n        RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n        CuratorFramework client = CuratorFrameworkFactory.newClient(params.getUrl(), retryPolicy);\n        client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n            @Override\n            public void stateChanged(CuratorFramework client, ConnectionState newState) {\n                log.info(\"zookeeper connection state changed to {}\", newState);\n                if (newState == ConnectionState.LOST) {\n                    params.getServerListeners().forEach(ServerListener::disconnected);\n                }\n                if (newState == ConnectionState.CONNECTED) {\n                    params.getServerListeners().forEach(ServerListener::connected);\n                }\n            }\n        });\n        client.start();\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n        return new CuratorZookeeperConnection(client, params);\n    } catch (Exception e) {\n        log.error(\"failed to create zookeeper connection\", e);\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 39.75803503190226}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tCuratorFramework client = CuratorFrameworkFactory.builder()\n            .connectString(params.getHost())\n            .retryPolicy(new ExponentialBackoffRetry(1000, 3))\n            .connectionTimeoutMs(5000)\n            .sessionTimeoutMs(5000)\n            .aclProvider(new ACLProvider() {\n                @Override\n                public List<ACL> getDefaultAcl() {\n                    return ZooDefs.Ids.OPEN_ACL_UNSAFE;\n                }\n\n                @Override\n                public List<ACL> getAclForPath(String path) {\n                    return ZooDefs.Ids.OPEN_ACL_UNSAFE;\n                }\n            })\n            .authorization(params.getAuths().stream().map(auth -> new AuthInfo(auth.getScheme(), auth.getAuth().getBytes())).toArray(AuthInfo[]::new))\n            .build();\n\n    client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n        @Override\n        public void stateChanged(CuratorFramework client, ConnectionState newState) {\n            if (newState == ConnectionState.LOST) {\n                log.warn(\"connection lost\");\n            } else if (newState == ConnectionState.SUSPENDED) {\n                log.warn(\"connection suspended\");\n            } else if (newState == ConnectionState.RECONNECTED) {\n                log.warn(\"reconnected\");\n            }\n        }\n    });\n\n    client.start();\n\n    try {\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n    } catch (InterruptedException e) {\n        log.error(\"connection interrupted\", e);\n        client.close();\n        throw new RuntimeException(e);\n    }\n\n    return new CuratorZookeeperConnection(params, client);\n}", "CodeBleu_score": 40.30067552258074}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tCuratorFramework client = createClient(params);\n    try {\n        client.start();\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n        return new CuratorZookeeperConnection(client, params);\n    } catch (Exception e) {\n        client.close();\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 53.38270796531065}]}
{"instance_id": "b144bcd67462", "generation_result": [{"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tfinal String connectionString = getClient().getZookeeperClient().getCurrentConnectionString();\n    if (isSync.compareAndSet(false, true)) {\n        log.info(\"start synchronization for {}\", connectionString);\n        final CuratorTreeCacheListener curatorTreeCacheListener = new CuratorTreeCacheListener(listeners);\n        treeCache.getListenable().addListener(curatorTreeCacheListener);\n        try {\n            treeCache.start();\n        } catch (Exception e) {\n            log.error(\"synchronization failed\", e);\n            treeCache.close();\n            isSync.set(false);\n        }\n    } else {\n        log.info(\"synchronization is already in progress for {}\", connectionString);\n    }\n}", "CodeBleu_score": 50.35593244131835}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tfinal String connectionString = getClient().getZookeeperClient().getCurrentConnectionString();\n    if (isSync.compareAndSet(false, true)) {\n        log.info(\"start syncing zookeeper nodes for connection: {}\", connectionString);\n        treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n        try {\n            treeCache.start();\n        } catch (Exception e) {\n            log.error(\"error occurred while syncing zookeeper nodes for connection: {}\", connectionString, e);\n            treeCache.close();\n            isSync.set(false);\n        }\n    } else {\n        log.info(\"zookeeper nodes for connection: {} is already syncing\", connectionString);\n    }\n}", "CodeBleu_score": 57.40705329608924}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tfinal String connectionString = getClient().getZookeeperClient().getCurrentConnectionString();\n    if (isSync.compareAndSet(false, true)) {\n        log.info(\"start synchronizing zookeeper nodes from {}\", connectionString);\n        final CuratorTreeCacheListener listener = new CuratorTreeCacheListener(listeners);\n        treeCache.getListenable().addListener(listener);\n        try {\n            treeCache.start();\n        } catch (Exception e) {\n            log.error(\"synchronizing zookeeper nodes from {} failed\", connectionString, e);\n            treeCache.close();\n        }\n    } else {\n        log.info(\"synchronizing zookeeper nodes from {} is already in progress\", connectionString);\n    }\n}", "CodeBleu_score": 51.47472415551094}]}
{"instance_id": "6452f272f2d7", "generation_result": [{"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n    pathChildrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n    pathChildrenCache.getListenable().addListener(listener);\n}", "CodeBleu_score": 92.2373669030745}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache childrenCache = new PathChildrenCache(client, path, true);\n    childrenCache.getListenable().addListener(listener);\n    childrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n}", "CodeBleu_score": 72.64346522330871}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n    pathChildrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n    pathChildrenCache.getListenable().addListener(listener);\n}", "CodeBleu_score": 92.2373669030745}]}
{"instance_id": "17f786e147c3", "generation_result": [{"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start\");\n    for (String path : pathList) {\n        try {\n            curatorClient.deletePath(path);\n            logger.info(\"Unregister service success: {}\", path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n}", "CodeBleu_score": 54.279204417291126}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start\");\n    for (String path : pathList) {\n        try {\n            curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n}", "CodeBleu_score": 64.9644336795974}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start...\");\n    for (String path : pathList) {\n        try {\n            this.curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    this.curatorClient.close();\n}", "CodeBleu_score": 75.0307979717487}]}
{"instance_id": "32fec6cfa353", "generation_result": [{"code": "public void addListener(IZkStateListener listener){\n\tif (client != null) {\n        client.subscribeStateChanges(listener);\n    } else {\n        completableFuture.whenComplete(this::makeClientReady);\n        completableFuture.whenComplete((zkClient, throwable) -> {\n            if (zkClient != null) {\n                zkClient.subscribeStateChanges(listener);\n            }\n        });\n    }\n}", "CodeBleu_score": 52.82821117050794}, {"code": "public void addListener(IZkStateListener listener){\n\tif (client == null) {\n        logger.error(\"Zkclient is not connected yet! Please invoke the start() method first!\");\n        return;\n    }\n    if (listener != null) {\n        client.subscribeStateChanges(listener);\n    }\n}", "CodeBleu_score": 30.565792132195806}, {"code": "public void addListener(IZkStateListener listener){\n\tif (listener != null) {\n        if (state == KeeperState.SyncConnected) {\n            listener.handleStateChanged(KeeperState.SyncConnected);\n        }\n        client.subscribeStateChanges(listener);\n    }\n}", "CodeBleu_score": 31.298950768204836}]}
{"instance_id": "619d57562a76", "generation_result": [{"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\ttry {\n        CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n                .connectString(serverAddr)\n                .retryPolicy(new RetryNTimes(1000, 5000))\n                .connectionTimeoutMs(5000);\n        if (authority != null && !authority.trim().isEmpty()) {\n            builder = builder.authorization(\"digest\", authority.getBytes());\n        }\n        client = builder.build();\n        client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n            @Override\n            public void stateChanged(CuratorFramework client, ConnectionState state) {\n                if (state == ConnectionState.LOST) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.DISCONNECTED);\n                } else if (state == ConnectionState.CONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.CONNECTED);\n                } else if (state == ConnectionState.RECONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.RECONNECTED);\n                }\n            }\n        });\n        client.start();\n    } catch (Exception e) {\n        throw new IllegalStateException(e.getMessage(), e);\n    }\n}", "CodeBleu_score": 90.3331808751063}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\ttry {\n        CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n                .connectString(serverAddr)\n                .retryPolicy(new RetryNTimes(1000, 5000))\n                .connectionTimeoutMs(5000);\n        if (authority != null) {\n            builder = builder.authorization(\"digest\", authority.getBytes());\n        }\n        client = builder.build();\n        client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n            @Override\n            public void stateChanged(CuratorFramework client, ConnectionState state) {\n                if (state == ConnectionState.LOST) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.DISCONNECTED);\n                } else if (state == ConnectionState.CONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.CONNECTED);\n                } else if (state == ConnectionState.RECONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.RECONNECTED);\n                }\n            }\n        });\n        client.start();\n    } catch (Exception e) {\n        throw new IllegalStateException(e.getMessage(), e);\n    }\n}", "CodeBleu_score": 89.52777369571244}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tsuper(serverAddr, authority);\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n            .connectString(serverAddr)\n            .retryPolicy(new RetryNTimes(Integer.MAX_VALUE, 1000))\n            .connectionTimeoutMs(5000);\n    if (authority != null) {\n        builder = builder.authorization(\"digest\", authority.getBytes());\n    }\n    client = builder.build();\n    client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n        @Override\n        public void stateChanged(CuratorFramework client, ConnectionState state) {\n            if (state == ConnectionState.LOST) {\n                CuratorZookeeperClient.this.stateChanged(StateListener.DISCONNECTED);\n            } else if (state == ConnectionState.CONNECTED) {\n                CuratorZookeeperClient.this.stateChanged(StateListener.CONNECTED);\n            } else if (state == ConnectionState.RECONNECTED) {\n                CuratorZookeeperClient.this.stateChanged(StateListener.RECONNECTED);\n            }\n        }\n    });\n    client.start();\n    try {\n        client.blockUntilConnected();\n    } catch (InterruptedException e) {\n        throw new IllegalStateException(e.getMessage(), e);\n    }\n}", "CodeBleu_score": 85.18492022277113}]}
{"instance_id": "9e7fb18946e4", "generation_result": [{"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}]}
{"instance_id": "2c3fae642654", "generation_result": [{"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}]}
{"instance_id": "0b5ab34e1dd9", "generation_result": [{"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}]}
{"instance_id": "19fc5847a7c5", "generation_result": [{"code": "def make_digest_acl_credential(username, password)\n\tcredential = username + \":\" + password\n    sha1_hash = hashlib.sha1(credential).hexdigest()\n    return username + \":\" + b64encode(sha1_hash)\n\n", "CodeBleu_score": 39.65850835167763}, {"code": "def make_digest_acl_credential(username, password)\n\tusername_password = \"%s:%s\" % (username, password)\n    username_password_hash = hashlib.sha1(username_password.encode('utf-8')).digest()\n    username_password_hash_b64 = b64encode(username_password_hash).decode('utf-8')\n    return \"%s:%s\" % (username, username_password_hash_b64)\n\n", "CodeBleu_score": 42.047264042451324}, {"code": "def make_digest_acl_credential(username, password)\n\tusername_password = '%s:%s' % (username, password)\n    username_password_hash = hashlib.sha1(username_password).digest()\n    username_password_hash_b64 = b64encode(username_password_hash)\n    credential = '%s:%s' % (username, username_password_hash_b64)\n    return credential\n\n", "CodeBleu_score": 35.67768678484369}]}
{"instance_id": "4a062391deb0", "generation_result": [{"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}]}
{"instance_id": "aaac7523d2b5", "generation_result": [{"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}]}
{"instance_id": "bed0c998ac4c", "generation_result": [{"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}]}
{"instance_id": "b10e2c2c410b", "generation_result": [{"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}]}
{"instance_id": "42b3d26a3540", "generation_result": [{"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}]}
{"instance_id": "738974d4e9c0", "generation_result": [{"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n    self.participating = False\n\n", "CodeBleu_score": 68.89514515801572}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = True\n\n", "CodeBleu_score": 61.0238923784431}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = True\n", "CodeBleu_score": 61.0238923784431}]}
{"instance_id": "a321fef7d100", "generation_result": [{"code": "public Result start(){\n\tMap map = new HashMap();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (status) {\n            map.put(\"is_running\", true);\n            return new Result(true, map, \"FTP \u670d\u52a1\u5df2\u542f\u52a8\");\n        } else {\n            boolean start = hdfsOverFtpServer.startServer();\n            map.put(\"is_running\", start);\n            return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n        }\n    } catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 47.267761836178494}, {"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (!status) {\n            hdfsOverFtpServer.startServer();\n        }\n        status = hdfsOverFtpServer.statusServer();\n        map.put(\"is_running\", status);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 53.50249271720769}, {"code": "public Result start(){\n\tMap map = new HashMap();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (status) {\n            map.put(\"is_running\", true);\n            return new Result(true, map, \"FTP \u670d\u52a1\u5df2\u7ecf\u542f\u52a8\");\n        }\n        hdfsOverFtpServer.startServer();\n        map.put(\"is_running\", true);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 49.196380907986004}]}
{"instance_id": "24b088689114", "generation_result": [{"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 97.12561037093238}, {"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 91.93822004293011}, {"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 91.93822004293011}]}
{"instance_id": "af0884ffda66", "generation_result": [{"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n\t\treturn null;\n\t}\n\n\tAuthorizationRequest newRequest = null;\n\tfor (Authority authority : authorities) {\n\t\tnewRequest = authority.authorize(request);\n\t\tif (newRequest == null) {\n\t\t\treturn null;\n\t\t}\n\t\trequest = newRequest;\n\t}\n\treturn request;\n}", "CodeBleu_score": 39.30959383409663}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n\t\treturn null;\n\t}\n\n\tfor (Authority authority : authorities) {\n\t\tAuthorizationRequest authRequest = authority.authorize(request);\n\t\tif (authRequest == null) {\n\t\t\treturn null;\n\t\t}\n\t\trequest = authRequest;\n\t}\n\n\treturn request;\n}", "CodeBleu_score": 35.683905532351936}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n\t\treturn null;\n\t}\n\tfor (Authority authority : authorities) {\n\t\trequest = authority.authorize(request);\n\t\tif (request == null) {\n\t\t\treturn null;\n\t\t}\n\t}\n\treturn request;\n}", "CodeBleu_score": 35.12344282834362}]}
{"instance_id": "920d28a0fe76", "generation_result": [{"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}]}
{"instance_id": "a2b04232d387", "generation_result": [{"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}]}
{"instance_id": "3407087c9af6", "generation_result": [{"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}]}
{"instance_id": "91227000a139", "generation_result": [{"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}]}
{"instance_id": "aec4623134b5", "generation_result": [{"code": "public void configure(){\n\tisConfigured = true;\n\n\tuserDataProp = new BaseProperties();\n\n\tif (userDataFile.exists()) {\n\n\t\ttry {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t} catch (IOException ex) {\n\t\t\tthrow new FtpServerConfigurationException(\"Failed to load user data file \\\"\" + userDataFile.getAbsolutePath() + \"\\\"\", ex);\n\t\t}\n\t}\n\n\t// convert any deprecated properties\n\tfor (Object keyObj : userDataProp.keySet()) {\n\t\tString key = (String) keyObj;\n\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\tString newKey = PREFIX + key.substring(DEPRECATED_PREFIX.length());\n\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\tuserDataProp.remove(key);\n\t\t}\n\t}\n\n}", "CodeBleu_score": 45.43071630520655}, {"code": "public void configure(){\n\tisConfigured = true;\n\n\tuserDataProp = new BaseProperties();\n\n\tif (userDataFile.exists()) {\n\n\t\ttry {\n\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\n\t\t} catch (IOException ex) {\n\n\t\t\tthrow new FtpServerConfigurationException(\"Cannot load user data file \\\"\" + userDataFile.getAbsolutePath() + \"\\\"\", ex);\n\n\t\t}\n\n\t}\n\n\tconvertDeprecatedProperties();\n\n}", "CodeBleu_score": 39.31954692265956}, {"code": "public void configure(){\n\tisConfigured = true;\n\n\tuserDataProp = new BaseProperties();\n\n\tif (userDataFile.exists()) {\n\t\ttry {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\"Failed to load user data file \\\"\" + userDataFile + \"\\\"\", e);\n\t\t}\n\t}\n\n\t// convert deprecated properties\n\tfor (Object key : userDataProp.keySet()) {\n\t\tString keyStr = (String) key;\n\t\tif (keyStr.startsWith(DEPRECATED_PREFIX)) {\n\t\t\tString newKey = PREFIX + keyStr.substring(DEPRECATED_PREFIX.length());\n\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(keyStr));\n\t\t\tuserDataProp.remove(keyStr);\n\t\t}\n\t}\n}", "CodeBleu_score": 47.40765930921981}]}
{"instance_id": "b5a2587f14d0", "generation_result": [{"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}]}
{"instance_id": "5cf2989cf513", "generation_result": [{"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 82.33147782493528}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 84.58771483281748}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.setPasswordEncryptor(new Md5PasswordEncryptor());\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 74.41075893992152}]}
{"instance_id": "ce383a9a6497", "generation_result": [{"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.debug(\"open path: {} bufferSize: {}\", path, bufferSize);\n    Path qualifiedPath = makeQualified(path);\n    FSDataInputStream inputStream = seaweedFileSystemStore.open(qualifiedPath, bufferSize);\n    if (inputStream == null) {\n        LOG.warn(\"open null for path: {}\", path);\n    }\n    return inputStream;\n}", "CodeBleu_score": 34.32679536259141}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.debug(\"open({}, {}\", path, bufferSize);\n    Path qualifiedPath = makeQualified(path);\n    LOG.debug(\"open: qualified path: {}\", qualifiedPath);\n    try {\n        return new FSDataInputStream(new BufferedFSInputStream(seaweedFileSystemStore.open(qualifiedPath), bufferSize));\n    } catch (IOException e) {\n        LOG.warn(\"open: failed to open file: {}\", path, e);\n        return null;\n    }\n}", "CodeBleu_score": 34.25044647581379}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.debug(\"open path: {} bufferSize: {}\", path, bufferSize);\n    Path qualifiedPath = makeQualified(path);\n    FSDataInputStream inputStream = seaweedFileSystemStore.open(qualifiedPath, bufferSize);\n    if (inputStream == null) {\n        LOG.warn(\"open null for path: {}\", qualifiedPath);\n        return null;\n    }\n    return inputStream;\n}", "CodeBleu_score": 36.71429801823544}]}
{"instance_id": "f0fc12072c02", "generation_result": [{"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}]}
{"instance_id": "5104c071af46", "generation_result": [{"code": "public boolean hasCapability(String capability){\n\tString lower = capability.toLowerCase(Locale.ENGLISH);\n    return lower.equals(\"hsync\") && supportFlush || lower.equals(\"hflush\") && supportFlush;\n}", "CodeBleu_score": 29.95616458638582}, {"code": "public boolean hasCapability(String capability){\n\treturn capability.toLowerCase(Locale.ENGLISH).equals(\"hsync\")\n      || capability.toLowerCase(Locale.ENGLISH).equals(\"hflush\");\n}", "CodeBleu_score": 41.30519964376674}, {"code": "public boolean hasCapability(String capability){\n\tString lower = capability.toLowerCase(Locale.ENGLISH);\n  return lower.equals(\"hsync\") && supportFlush ||\n      lower.equals(\"hflush\") && supportFlush;\n}", "CodeBleu_score": 30.35212686236719}]}
{"instance_id": "6989c860c79b", "generation_result": [{"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (lastError != null) {\n        throw lastError;\n    }\n    if (offset > buffer.position()) {\n        throw new IllegalArgumentException(\"Offset=\" + offset + \" > buffer.position()=\" + buffer.position());\n    }\n    if (offset <= lastFlushOffset) {\n        return;\n    }\n    buffer.flip();\n    buffer.position((int) offset);\n    buffer.limit(buffer.position() + (int) (buffer.position() - offset));\n    if (buffer.remaining() == 0) {\n        return;\n    }\n    final ByteBuffer toWrite = buffer.slice();\n    buffer.position(buffer.limit());\n    buffer.compact();\n    lastTotalAppendOffset += toWrite.remaining();\n    final long currentOffset = lastTotalAppendOffset;\n    final long currentLength = toWrite.remaining();\n    final long currentPosition = position;\n    position += currentLength;\n    lastFlushOffset = offset;\n    final WriteOperation writeOperation = new WriteOperation(toWrite, currentOffset, currentLength, currentPosition);\n    writeOperations.add(writeOperation);\n    if (writeOperations.size() >= maxConcurrentRequestCount) {\n        waitForFlush();\n    }\n    completionService.submit(new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n            try {\n                filerClient.write(path, replication, collection, toWrite, currentOffset, currentLength, currentPosition, shouldSaveMetadata, entry);\n            } catch (IOException e) {\n                lastError = e;\n            } finally {\n                writeOperations.remove(writeOperation);\n            }\n            return null;\n        }\n    });\n}", "CodeBleu_score": 32.23357207453348}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (this.lastError != null) {\n        throw this.lastError;\n    }\n    if (offset <= this.lastFlushOffset) {\n        return;\n    }\n    if (this.lastTotalAppendOffset > offset) {\n        throw new IOException(\"flush offset \" + offset + \" is smaller than lastTotalAppendOffset \" + this.lastTotalAppendOffset);\n    }\n    if (this.lastTotalAppendOffset < offset) {\n        throw new IOException(\"flush offset \" + offset + \" is larger than lastTotalAppendOffset \" + this.lastTotalAppendOffset);\n    }\n    if (this.lastTotalAppendOffset == offset) {\n        return;\n    }\n    if (this.buffer.position() > 0) {\n        this.buffer.flip();\n        this.buffer.limit((int) (offset - this.lastTotalAppendOffset));\n        final ByteBuffer toWrite = this.buffer.slice();\n        this.buffer.clear();\n        this.buffer.position((int) (offset - this.lastTotalAppendOffset));\n        this.buffer.limit(this.buffer.capacity());\n        this.lastTotalAppendOffset = offset;\n        this.writeOperations.add(new WriteOperation(toWrite, offset));\n    }\n    if (this.writeOperations.size() > 0) {\n        final Future<Void> future = this.completionService.poll();\n        if (future != null) {\n            try {\n                future.get();\n            } catch (InterruptedException | ExecutionException ex) {\n                throw new IOException(ex);\n            }\n        }\n        final WriteOperation writeOperation = this.writeOperations.peek();\n        if (writeOperation != null && writeOperation.getOffset() == offset) {\n            try {\n                this.completionService.submit(new Callable<Void>() {\n                    @Override\n                    public Void call() throws Exception {\n                        try {\n                            SeaweedOutputStream.this.writeOperationToService(writeOperation);\n                        } catch (IOException ex) {\n                            SeaweedOutputStream.this.lastError = ex;\n                        }\n                        return null;\n                    }", "CodeBleu_score": 43.80654903770543}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (lastError != null) {\n        throw lastError;\n    }\n    if (offset > buffer.position()) {\n        throw new IllegalArgumentException(\"Offset \" + offset + \" is greater than buffered bytes count \" + buffer.position());\n    }\n    if (offset <= lastFlushOffset) {\n        return;\n    }\n\n    final long size = buffer.position() - offset;\n    final long start = position + offset;\n    final long end = start + size;\n    final ByteBuffer slice = buffer.duplicate();\n    slice.position((int) offset);\n    slice.limit((int) (offset + size));\n    final WriteOperation writeOperation = new WriteOperation(start, end, slice);\n    writeOperations.add(writeOperation);\n    completionService.submit(writeOperation);\n\n    try {\n        if (shouldSaveMetadata) {\n            FilerProto.Entry entry = this.entry.build();\n            entry = entry.toBuilder().setAttributes(entry.getAttributes().toBuilder().setMtime(System.currentTimeMillis() / 1000L)).build();\n            filerClient.updateEntry(entry);\n        }\n    } catch (Exception ex) {\n        LOG.error(\"update filer entry {}\", ex);\n        lastError = new IOException(ex);\n        throw lastError;\n    }\n\n    lastFlushOffset = offset;\n}", "CodeBleu_score": 40.89158464457892}]}
{"instance_id": "56e8fb9de5da", "generation_result": [{"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.762212712361983}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.762212712361983}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.762212712361983}]}
{"instance_id": "bf27d955c38b", "generation_result": [{"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}]}
{"instance_id": "5c7a311273ae", "generation_result": [{"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 20.504924780674337}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 20.504924780674337}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 20.504924780674337}]}
{"instance_id": "d8f07e66f690", "generation_result": [{"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"user can not be null\");\n\t}\n\n\tString homeDir = user.getHomeDirectory();\n\tif (homeDir == null) {\n\t\tthrow new IllegalArgumentException(\"user home directory can not be null\");\n\t}\n\n\thomePath = homeDir;\n\tthis.user = user;\n}", "CodeBleu_score": 64.7271401834771}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"user can not be null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"user home directory can not be null\");\n\t}\n\tthis.homePath = user.getHomeDirectory();\n\tthis.user = user;\n}", "CodeBleu_score": 80.92668269441138}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"user can not be null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"user home directory can not be null\");\n\t}\n\thomePath = user.getHomeDirectory();\n\tthis.user = user;\n}", "CodeBleu_score": 74.76401978375091}]}
{"instance_id": "27807db0eb0f", "generation_result": [{"code": "public boolean changeWorkingDirectory(String dir){\n\tif (dir == null) {\n\t\tthrow new IllegalArgumentException(\"dir can not be null\");\n\t}\n\n\tPath newPath;\n\tif (dir.startsWith(File.separator)) {\n\t\tnewPath = new Path(dir);\n\t} else {\n\t\tnewPath = new Path(currPath, dir);\n\t}\n\n\tif (newPath.toString().equals(homePath)) {\n\t\tcurrPath = File.separator;\n\t\treturn true;\n\t}\n\n\tif (newPath.toString().startsWith(homePath)) {\n\t\tcurrPath = newPath.toString().substring(homePath.length());\n\t\tif (currPath.startsWith(File.separator)) {\n\t\t\tcurrPath = currPath.substring(1);\n\t\t}\n\t\treturn true;\n\t}\n\n\treturn false;\n}", "CodeBleu_score": 42.503730001360616}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tPath newDir = new Path(currPath, dir);\n\tif (newDir.equals(homePath)) {\n\t\tcurrPath = File.separator;\n\t} else {\n\t\tcurrPath = newDir.toString();\n\t}\n\tFtpFile fileObj = new HdfsFileObject(homePath, currPath, user);\n\tif (fileObj.isDirectory()) {\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 26.08894921852748}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tif (dir.equals(\"..\")) {\n\t\tif (!currPath.equals(homePath)) {\n\t\t\tcurrPath = currPath.substring(0, currPath.lastIndexOf(File.separator));\n\t\t}\n\t} else if (dir.equals(\".\")) {\n\t\t// do nothing\n\t} else if (dir.startsWith(File.separator)) {\n\t\tcurrPath = dir;\n\t} else {\n\t\tcurrPath = currPath + File.separator + dir;\n\t}\n\treturn true;\n}", "CodeBleu_score": 22.373489580974432}]}
{"instance_id": "be7aba23d184", "generation_result": [{"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.filerClient = new FilerClient(host, port, grpcPort, conf);\n    this.conf = conf;\n    String accessMode = conf.get(SEAWEED_ACCESS_MODE, SEAWEED_ACCESS_MODE_DEFAULT);\n    filerClient.setAccessMode(accessMode);\n}", "CodeBleu_score": 36.814721653569634}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    filerClient = new FilerClient(host, port, grpcPort, conf);\n    filerClient.setAccessMode(conf.get(SEAWEED_ACCESS_MODE, SEAWEED_ACCESS_MODE_DEFAULT));\n}", "CodeBleu_score": 27.078963227828716}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n        String accessMode = conf.get(ACCESS_MODE_KEY, ACCESS_MODE_DIRECT);\n        switch (accessMode) {\n            case ACCESS_MODE_DIRECT:\n                filerClient = new FilerClient(host, port, grpcPort);\n                break;\n            case ACCESS_MODE_PUBLIC_URL:\n                filerClient = new FilerClient(host, port, grpcPort, conf.get(PUBLIC_URL_KEY));\n                break;\n            case ACCESS_MODE_FILER_PROXY:\n                filerClient = new FilerClient(host, port, grpcPort, conf.get(PUBLIC_URL_KEY), conf.get(PROXY_HOST_KEY), conf.getInt(PROXY_PORT_KEY, 80));\n                break;\n            default:\n                throw new IllegalArgumentException(\"Access mode \" + accessMode + \" not supported\");\n        }\n    }", "CodeBleu_score": 40.992872515947376}]}
{"instance_id": "7943a1723c23", "generation_result": [{"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}]}
{"instance_id": "36634cee9ed3", "generation_result": [{"code": "public void configure(JobConf conf){\n\tthis.conf = conf;\n    try {\n        hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n        LOG.warn(\"Unable to retrieve the hostname. Defaulting to \\\"localhost\\\"\", e);\n        hostName = \"localhost\";\n    }\n    threadsPerMap = conf.getInt(\"test.io.threads.per.map\", 1);\n    filesPerThread = conf.getInt(\"test.io.files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap);\n    for (int i = 0; i < threadsPerMap; i++) {\n        pool.execute(new Runnable() {\n            @Override\n            public void run() {\n                Thread.currentThread().setName(\"IOMapperBase \" + hostName + \" Thread \" + Thread.currentThread().getId());\n                Thread.currentThread().setDaemon(true);\n            }\n        });\n    }\n}", "CodeBleu_score": 48.18473780681439}, {"code": "public void configure(JobConf conf){\n\tsetConf(conf);\n    try {\n        hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n        hostName = \"localhost\";\n    }\n    threadsPerMap = conf.getInt(\"test.io.threads.per.map\", 1);\n    filesPerThread = conf.getInt(\"test.io.files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap);\n    for (int i = 0; i < threadsPerMap; i++) {\n        pool.execute(new DaemonThread());\n    }\n}", "CodeBleu_score": 59.566638876881626}, {"code": "public void configure(JobConf conf){\n\tsuper.configure(conf);\n    try {\n        hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n        LOG.warn(\"Unable to retrieve the hostname. Defaulting to localhost\", e);\n        hostName = \"localhost\";\n    }\n    threadsPerMap = conf.getInt(\"test.io.threads.per.map\", 1);\n    filesPerThread = conf.getInt(\"test.io.files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap);\n    for (int i = 0; i < threadsPerMap; i++) {\n        pool.execute(new DaemonThread());\n    }\n}", "CodeBleu_score": 51.37919346706117}]}
{"instance_id": "53cd9065153d", "generation_result": [{"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n    if (emptierFs != null) {\n        return;\n    }\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    try {\n        ugi.doAs(new PrivilegedExceptionAction<Void>() {\n            @Override\n            public Void run() throws Exception {\n                emptierFs = new JuiceFileSystemImpl(uri, conf);\n                return null;\n            }\n        });\n    } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted\", e);\n    } catch (Exception e) {\n        throw new IOException(\"Failed to create JuiceFileSystemImpl\", e);\n    }\n    BgTaskUtil.runBgTask(10, TimeUnit.MINUTES, new Runnable() {\n        @Override\n        public void run() {\n            try {\n                ugi.doAs(new PrivilegedExceptionAction<Void>() {\n                    @Override\n                    public Void run() throws Exception {\n                        emptierFs.delete(new Path(conf.getTrash()), true);\n                        return null;\n                    }\n                });\n            } catch (Exception e) {\n                LOG.error(\"Failed to empty trash\", e);\n            }\n        }\n    });\n}", "CodeBleu_score": 36.29604217158442}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tif (emptierFs != null) {\n        return;\n    }\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    try {\n        ugi.doAs(new PrivilegedExceptionAction<Void>() {\n            public Void run() throws Exception {\n                final String host = uri.getHost();\n                if (host == null) {\n                    return null;\n                }\n                Configuration confCopy = new Configuration(conf);\n                confCopy.set(\"fs.defaultFS\", uri.toString());\n                confCopy.set(\"fs.trash.interval\", \"10\");\n                confCopy.set(\"fs.trash.checkpoint.interval\", \"10\");\n                final FileSystem fs = FileSystem.get(confCopy);\n                if (!(fs instanceof JuiceFileSystem)) {\n                    return null;\n                }\n                if (JuiceFileSystem.this.emptierFs != null) {\n                    return null;\n                }\n                synchronized (JuiceFileSystem.this) {\n                    if (JuiceFileSystem.this.emptierFs != null) {\n                        return null;\n                    }\n                    JuiceFileSystem.this.emptierFs = fs;\n                }\n                BgTaskUtil.startBgTask(\"TrashEmptier-\" + host, new Runnable() {\n                    public void run() {\n                        try {\n                            FsShell shell = new FsShell(confCopy);\n                            shell.setConf(confCopy);\n                            shell.run(new String[]{\"-expunge\"});\n                        } catch (Exception e) {\n                            LOG.warn(\"Trash emptier thread exception: \", e);\n                        }\n                    }\n                }, 10, TimeUnit.MINUTES);\n                return null;\n            }\n        });\n    } catch (InterruptedException e) {\n        throw new IOException(e);\n    } catch (IOException e2) {\n        throw e2;\n    } catch (Exception e3) {\n        throw new IOException(e3);\n    }\n}", "CodeBleu_score": 33.809015050030766}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n    if (emptierFs != null) {\n        return;\n    }\n    try {\n        UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {\n            public Void run() throws Exception {\n                Configuration newConf = new Configuration(conf);\n                newConf.set(\"fs.defaultFS\", uri.toString());\n                FileSystem fs = FileSystem.get(newConf);\n                if (fs instanceof JuiceFileSystem) {\n                    emptierFs = fs;\n                }\n                return null;\n            }\n        });\n    } catch (InterruptedException e) {\n        throw new IOException(e);\n    } catch (IOException e) {\n        throw new IOException(e);\n    }\n    if (emptierFs == null) {\n        throw new IOException(\"Failed to create JuiceFileSystemImpl instance as superuser\");\n    }\n    BgTaskUtil.startBgTask(new Runnable() {\n        public void run() {\n            try {\n                emptierFs.deleteOnExit(new Path(emptierFs.getHomeDirectory(), \".Trash\"));\n            } catch (IOException e) {\n                LOG.error(\"Failed to delete .Trash\", e);\n            }\n        }\n    }, 10, TimeUnit.MINUTES);\n}", "CodeBleu_score": 33.68362955078049}]}
{"instance_id": "ede797f6a8f2", "generation_result": [{"code": "public static void setContext(String context) throws Exception{\n\tCallerContext currentContext = CallerContext.getCurrent();\n    if (currentContext == null || !currentContext.isContextValid()) {\n        CallerContext.setCurrent(new CallerContext.Builder(context).build());\n    } else if (!currentContext.getContext().contains(context)) {\n        CallerContext.setCurrent(new CallerContext.Builder(currentContext.getContext() + \",\" + context).build());\n    }\n}", "CodeBleu_score": 43.94974176189265}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext callerContext = CallerContext.getCurrent();\n    if (callerContext == null || !callerContext.isContextValid()) {\n        callerContext = new CallerContext.Builder(context).build();\n    } else if (!callerContext.getContext().contains(context)) {\n        callerContext = new CallerContext.Builder(callerContext.getContext() + \",\" + context).build();\n    }\n    CallerContext.setCurrent(callerContext);\n}", "CodeBleu_score": 52.084407738385195}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext currentContext = CallerContext.getCurrent();\n    if (currentContext == null || !currentContext.isContextValid()) {\n        CallerContext.setCurrent(new CallerContext.Builder(context).build());\n    } else if (!currentContext.getContext().contains(context)) {\n        CallerContext.setCurrent(new CallerContext.Builder(currentContext.getContext() + \",\" + context).build());\n    }\n}", "CodeBleu_score": 43.94974176189265}]}
{"instance_id": "56968a526f6a", "generation_result": [{"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}]}
{"instance_id": "f33adb3e8cb6", "generation_result": [{"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}]}
{"instance_id": "893c46ab9675", "generation_result": [{"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}]}
{"instance_id": "3da0b4d8ae16", "generation_result": [{"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tMap<String, String> hbaseCfgMap = JSON.parseObject(hbaseCfgString, new TypeReference<HashMap<String, String>>() {});\n    String zkQuorum = hbaseCfgMap.get(HConstants.ZOOKEEPER_QUORUM);\n    String zkClientPort = hbaseCfgMap.get(HConstants.ZOOKEEPER_CLIENT_PORT);\n    if (StringUtils.isBlank(zkClientPort)) {\n        zkClientPort = \"2181\";\n    }\n    zkQuorum = zkQuorum + \":\" + zkClientPort;\n    String znodeParent = hbaseCfgMap.get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (StringUtils.isBlank(znodeParent)) {\n        znodeParent = HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT;\n    }\n    return new Pair<>(zkQuorum, znodeParent);\n}", "CodeBleu_score": 58.06434165551973}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tif (StringUtils.isBlank(hbaseCfgString)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE,\n                \"The value of the configuration item [\" + HBaseKey.HBASE_CONFIG + \"] is not set\");\n    }\n\n    Configuration hbaseConfig = Configuration.from(hbaseCfgString);\n    String zkParent = hbaseConfig.getString(HConstants.ZOOKEEPER_ZNODE_PARENT,\n            HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);\n    String[] zkHosts = hbaseConfig.getString(HConstants.ZOOKEEPER_QUORUM).split(\",\");\n    for (int i = 0; i < zkHosts.length; i++) {\n        String zkHost = zkHosts[i];\n        if (!zkHost.contains(\":\")) {\n            zkHosts[i] = zkHost + \":2181\";\n        }\n    }\n    return Pair.of(StringUtils.join(zkHosts, \",\"), zkParent);\n}", "CodeBleu_score": 38.446983458798066}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tMap<String, String> hbaseConfigMap = JSON.parseObject(hbaseCfgString, new TypeReference<HashMap<String, String>>() {});\n    String zookeeperQuorum = hbaseConfigMap.get(HConstants.ZOOKEEPER_QUORUM);\n    String clientPort = hbaseConfigMap.get(HConstants.ZOOKEEPER_CLIENT_PORT);\n    if (StringUtils.isBlank(clientPort)) {\n        clientPort = \"2181\";\n    }\n    zookeeperQuorum = zookeeperQuorum + \":\" + clientPort;\n    String zookeeperZnodeParent = hbaseConfigMap.get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (StringUtils.isBlank(zookeeperZnodeParent)) {\n        zookeeperZnodeParent = HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT;\n    }\n    return new Pair<>(zookeeperQuorum, zookeeperZnodeParent);\n}", "CodeBleu_score": 54.53018205266872}]}
{"instance_id": "860cae2bb7f4", "generation_result": [{"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\ttry {\n        if (cfg.getNullMode().equals(Key.NULL_MODE_DELETE)) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The nullMode parameter of HbaseSQLWriter only supports the default value.\");\n        }\n        if (cfg.getWriteMode().equals(Key.WRITE_MODE_DELETE)) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The writeMode parameter of HbaseSQLWriter only supports the insert mode.\");\n        }\n        if (cfg.getBatchSize() < 1) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The batchSize parameter of HbaseSQLWriter must be greater than 1.\");\n        }\n        if (cfg.getBatchByteSize() < 1) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The batchByteSize parameter of HbaseSQLWriter must be greater than 1.\");\n        }\n        if (cfg.getBatchSize() > cfg.getBatchByteSize()) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The batchSize parameter of HbaseSQLWriter must be less than or equal to the batchByteSize parameter.\");\n        }\n        if (cfg.getColumn().size() == 0) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The column parameter of HbaseSQLWriter must be specified.\");\n        }\n        if (cfg.getColumn().size() != cfg.getColumnValue().size()) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \"The size of the column parameter of HbaseSQLWriter must be equal to the size of the columnValue parameter.\");\n        }\n        if (cfg.getColumn().contains(\"rowkey\")) {\n            throw AddaxException.asAddax", "CodeBleu_score": 29.263094713893416}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\ttry {\n        // \u83b7\u53d6\u8868\u5bf9\u8c61\n        getThinClientPTable(cfg);\n        // \u6821\u9a8c\u5217\n        validateColumns(cfg);\n    } catch (SQLException e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_PHOENIX_CONNECTION_ERROR, e);\n    }\n}", "CodeBleu_score": 7.133252912262364}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\tif (cfg.getNullMode().equals(Key.NULL_MODE_SKIP) && cfg.getWriteMode().equals(Key.WRITE_MODE_INSERT)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"writeMode can not be insert when nullMode is skip\");\n    }\n\n    // \u5efa\u7acb\u8fde\u63a5\n    try (Connection conn = getConnection(cfg)) {\n        try (Statement stmt = conn.createStatement()) {\n            // \u68c0\u6d4b\u8868\u662f\u5426\u5b58\u5728\n            String tableName = cfg.getTableName();\n            String schema = cfg.getSchema();\n            String fullTableName = StringUtils.isBlank(schema) ? tableName : schema + \".\" + tableName;\n            String query = \"SELECT * FROM \" + fullTableName + \" LIMIT 1\";\n            stmt.executeQuery(query);\n\n            // \u68c0\u6d4b\u5217\u662f\u5426\u5b58\u5728\n            ResultSetMetaData rsMetaData = stmt.executeQuery(\"SELECT * FROM \" + fullTableName + \" LIMIT 1\").getMetaData();\n            List<String> columns = cfg.getColumns();\n            for (String col : columns) {\n                try {\n                    rsMetaData.findColumn(col);\n                } catch (SQLException e) {\n                    throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                            \"The column name [\" + col + \"] does not exist in the table [\" + fullTableName + \"]\");\n                }\n            }\n        }\n    } catch (SQLException e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE, e);\n    }\n}", "CodeBleu_score": 47.15389391091473}]}
{"instance_id": "61721c327cf6", "generation_result": [{"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}]}
{"instance_id": "9ceb7f0bb717", "generation_result": [{"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}]}
{"instance_id": "247fc8882f6f", "generation_result": [{"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}]}
{"instance_id": "ac87e1b9a54e", "generation_result": [{"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}]}
{"instance_id": "5e6a66d06162", "generation_result": [{"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}]}
{"instance_id": "c450f959616c", "generation_result": [{"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.640464131616895}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.640464131616895}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.640464131616895}]}
{"instance_id": "1bf487d2cc64", "generation_result": [{"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}]}
{"instance_id": "680728cbc3ec", "generation_result": [{"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}]}
{"instance_id": "ce50bd53abef", "generation_result": [{"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tbyte[] rawRowkey = cell.getRowArray();\n        int rowkeyOffset = cell.getRowOffset();\n        int rowkeyLength = cell.getRowLength();\n        byte[] cfAndQualifierName = CellUtil.cloneQualifier(cell);\n        byte[] familyName = Bytes.split(cfAndQualifierName, colonByte)[0];\n        byte[] qualifierName = Bytes.split(cfAndQualifierName, colonByte)[1];\n        byte[] value = CellUtil.cloneValue(cell);\n        long timestamp = cell.getTimestamp();\n        int valueLength = cell.getValueLength();\n        String key = Bytes.toString(rawRowkey, rowkeyOffset, rowkeyLength);\n\n        // \u8bbe\u7f6erowkey\n        ColumnType rowkeyType = ColumnType.getByTypeName(configuration.getString(HBaseKey.ROWKEY_TYPE));\n        if (rowkeyType == null) {\n            throw AddaxException.asAddaxException(Hbase20xReaderErrorCode.ILLEGAL_VALUE,\n                    String.format(\"Hbasewriter \u4e0d\u652f\u6301\u60a8\u914d\u7f6e\u7684 rowkeytype \u7c7b\u578b:%s, \u4ec5\u652f\u6301%s\",\n                            configuration.getString(HBaseKey.ROWKEY_TYPE),\n                            ColumnType.getSupportedMap().keySet()));\n        }\n        record.addColumn(new StringColumn(key));\n\n        // \u8bbe\u7f6efamily:qualifier\n        String familyQualifier = new String(familyName, StandardCharsets.UTF_8) + \":\" + new String(qualifierName, StandardCharsets.UTF_8);\n        record.addColumn(new StringColumn(familyQualifier));\n\n        // \u8bbe\u7f6e\u65f6\u95f4\u6233\n        record.addColumn(new LongColumn(timestamp));\n\n        // \u8bbe\u7f6e\u503c\n        ColumnType valueType = ColumnType.getByTypeName(familyQualifierMap.get(Bytes.toString(familyName)).get(Bytes.toString(qualifierName)));\n        if (valueType == null) {\n            throw AddaxException.asAddaxException(Hbase20xReaderErrorCode.ILLEGAL_VALUE,\n                    String.format(\"Hbasewriter \u4e0d", "CodeBleu_score": 45.903798812898486}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tbyte[] rawRowkey = CellUtil.cloneRow(cell);\n    String rowkey = Bytes.toString(rawRowkey);\n    record.addColumn(new LongColumn(HBaseConstant.HBASE_ROWKEY_NAME));\n    record.addColumn(new StringColumn(rowkey));\n    record.addColumn(new LongColumn(HBaseConstant.HBASE_TS_NAME));\n    record.addColumn(new LongColumn(cell.getTimestamp()));\n    byte[] rawFamily = CellUtil.cloneFamily(cell);\n    String family = Bytes.toString(rawFamily);\n    byte[] rawQualifier = CellUtil.cloneQualifier(cell);\n    String qualifier = Bytes.toString(rawQualifier);\n    byte[] value = CellUtil.cloneValue(cell);\n    record.addColumn(new StringColumn(family));\n    record.addColumn(new StringColumn(qualifier));\n    record.addColumn(new StringColumn(Bytes.toString(value)));\n}", "CodeBleu_score": 32.23216233496942}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tbyte[] rawRowkey = CellUtil.cloneRow(cell);\n        byte[] cfAndQualifierName = Bytes.split(CellUtil.cloneQualifier(cell),\n                colonByte, 2)[0];\n        byte[] familyName = Bytes.split(cfAndQualifierName, colonByte)[0];\n        byte[] qualifierName = Bytes.split(cfAndQualifierName, colonByte)[1];\n        byte[] value = CellUtil.cloneValue(cell);\n        long timestamp = cell.getTimestamp();\n        String key = Bytes.toString(rawRowkey);\n        String family = Bytes.toString(familyName);\n        String qualifier = Bytes.toString(qualifierName);\n        String type = this.columnMetaData.get(family + \":\" + qualifier);\n        if (StringUtils.isBlank(type)) {\n            type = this.defaultColumnType;\n        }\n        String dateformat = this.columnMetaData.get(family + \":\" + qualifier\n                + \":dateformat\");\n        if (StringUtils.isBlank(dateformat)) {\n            dateformat = this.defaultDateFormat;\n        }\n        record.addColumn(new StringColumn(key));\n        record.addColumn(new StringColumn(family));\n        record.addColumn(new StringColumn(qualifier));\n        record.addColumn(new LongColumn(timestamp));\n        Hbase20xHelper.convertCellToLine(cell, record, type, dateformat);\n    }", "CodeBleu_score": 45.22591663732323}]}
{"instance_id": "29e8f3a99f0c", "generation_result": [{"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}]}
{"instance_id": "f5ad51545b10", "generation_result": [{"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}]}
{"instance_id": "8618eef8b0e9", "generation_result": [{"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = new UUID(record.key().getMostSignificantBits(), record.key().getLeastSignificantBits());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 42.204839168763044}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 43.124700079387935}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = new UUID(record.key().getMostSignificantBits(), record.key().getLeastSignificantBits());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 42.204839168763044}]}
{"instance_id": "46484df184dd", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}]}
{"instance_id": "4ac56ef9f89a", "generation_result": [{"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}]}
{"instance_id": "e13a82b26338", "generation_result": [{"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"[{}] Unsubscribing from all topics\", getTopic());\n        consumer.unsubscribe();\n    } else {\n        if (topicNames.size() == 1) {\n            String topicName = topicNames.get(0);\n            log.info(\"[{}] Subscribing to topic '{}'\", getTopic(), topicName);\n            admin.createTopicIfNotExists(topicName);\n            consumer.subscribe(Collections.singletonList(topicName));\n        } else {\n            log.info(\"[{}] Subscribing to topics {}\", getTopic(), topicNames);\n            admin.createTopicsIfNotExist(topicNames);\n            consumer.subscribe(topicNames);\n        }\n    }\n}", "CodeBleu_score": 47.84307566235514}, {"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"[{}] Unsubscribing from all topics\", getTopic());\n        consumer.unsubscribe();\n    } else {\n        log.info(\"[{}] Subscribing to topics: {}\", getTopic(), topicNames);\n        admin.createTopics(topicNames);\n        consumer.subscribe(topicNames);\n    }\n}", "CodeBleu_score": 55.540830323248024}, {"code": "protected void doSubscribe(List<String> topicNames){\n\tif (!topicNames.isEmpty()) {\n        for (String topicName : topicNames) {\n            try {\n                admin.createTopicIfNotExists(topicName);\n            } catch (IOException e) {\n                log.error(\"Failed to create topic [{}]\", topicName, e);\n            }\n        }\n        consumer.subscribe(topicNames);\n    } else {\n        consumer.unsubscribe();\n        log.info(\"No topics configured for client group [{}]\", groupId);\n    }\n}", "CodeBleu_score": 58.36286349515949}]}
{"instance_id": "8a20d0b48703", "generation_result": [{"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"[{}] Polling for {} ms\", getTopic(), durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    if (records.isEmpty()) {\n        log.trace(\"[{}] No records found\", getTopic());\n    } else {\n        log.trace(\"[{}] Polling took {} ms\", getTopic(), stopWatch.getTotalTimeMillis());\n    }\n    if (statsService != null) {\n        statsService.recordPollDuration(groupId, stopWatch.getTotalTimeMillis());\n    }\n    List<ConsumerRecord<String, byte[]>> recordsToProcess = new ArrayList<>();\n    for (ConsumerRecord<String, byte[]> record : records) {\n        recordsToProcess.add(record);\n    }\n    return recordsToProcess;\n}", "CodeBleu_score": 67.65346119670423}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch sw = new StopWatch();\n    sw.start();\n    log.trace(\"[{}] Polling for {} ms\", getTopic(), durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    sw.stop();\n    if (records.isEmpty()) {\n        log.trace(\"[{}] Polling returned no data. Time spent: {}\", getTopic(), sw.getTotalTimeMillis());\n        return Collections.emptyList();\n    } else {\n        List<ConsumerRecord<String, byte[]>> data = new ArrayList<>(records.count());\n        records.forEach(data::add);\n        log.trace(\"[{}] Polling returned {} records. Time spent: {}\", getTopic(), records.count(), sw.getTotalTimeMillis());\n        return data;\n    }\n}", "CodeBleu_score": 71.7836018684093}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch sw = new StopWatch();\n    sw.start();\n    log.trace(\"Polling [{}], duration [{}] ms\", getTopic(), durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    sw.stop();\n    if (records.isEmpty()) {\n        log.trace(\"[{}] No records found\", getTopic());\n    } else {\n        log.trace(\"[{}] Polling took [{}] ms\", getTopic(), sw.getTotalTimeMillis());\n    }\n    if (statsService != null) {\n        statsService.recordPoll(groupId, sw.getTotalTimeMillis(), records.count());\n    }\n    List<ConsumerRecord<String, byte[]>> result = new ArrayList<>();\n    for (ConsumerRecord<String, byte[]> record : records) {\n        result.add(record);\n    }\n    return result;\n}", "CodeBleu_score": 64.1062221500458}]}
{"instance_id": "328e4c39a8e1", "generation_result": [{"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}]}
{"instance_id": "35f835e8c387", "generation_result": [{"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}]}
{"instance_id": "111a11189434", "generation_result": [{"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}]}
{"instance_id": "f5d2487100a9", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}]}
{"instance_id": "886264f5ca27", "generation_result": [{"code": "private void startLogScheduling(){\n\tlong timeout = kafkaSettings.getResponseTimeout();\n        statsPrintScheduler.scheduleWithFixedDelay(this::logStats, timeout, timeout, TimeUnit.MILLISECONDS);\n    }", "CodeBleu_score": 8.202885759569508}, {"code": "private void startLogScheduling(){\n\tlong timeoutDuration = TimeUnit.MILLISECONDS.convert(kafkaSettings.getOperationTimeout(), TimeUnit.SECONDS);\n    statsPrintScheduler.scheduleWithFixedDelay(() -> {\n        try {\n            if (statsConfig.getPrintInterval() > 0) {\n                printStats();\n            }\n        } catch (Exception e) {\n            log.warn(\"Failed to print consumer group statistics. Group ID: [{}]\", e.getMessage());\n            log.trace(\"Failed to print consumer group statistics. Group ID: [{}]\", e.getMessage(), e);\n        }\n    }, timeoutDuration, statsConfig.getPrintInterval(), TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 18.9699611332749}, {"code": "private void startLogScheduling(){\n\tlong timeoutMs = kafkaSettings.getResponseTimeoutMs();\n    Duration duration = Duration.ofMillis(timeoutMs);\n    this.statsPrintScheduler.scheduleWithFixedDelay(() -> {\n        try {\n            if (!statsConfig.getEnabled()) {\n                return;\n            }\n            if (statsConfig.getPrintIntervalMs() > 0 && System.currentTimeMillis() % statsConfig.getPrintIntervalMs() != 0) {\n                return;\n            }\n            monitoredGroups.forEach(groupId -> {\n                try {\n                    List<GroupOffset> groupOffsets = getGroupOffsets(groupId);\n                    List<TopicPartition> endOffsets = getEndOffsets(groupOffsets);\n                    List<TopicPartition> topicPartitions = getTopicPartitions(endOffsets);\n                    List<TopicPartition> topicPartitionsWithLag = getTopicPartitionsWithLag(groupOffsets, endOffsets);\n                    if (topicPartitionsWithLag.size() > 0) {\n                        log.warn(\"[{}][{}] Consumer group has lagging topics: {}\", ServiceType.TB_QUEUE.name(), groupId, topicPartitionsWithLag);\n                    }\n                } catch (Exception e) {\n                    log.warn(\"[{}][{}] Failed to get consumer group statistics: {}\", ServiceType.TB_QUEUE.name(), groupId, e.getMessage());\n                    log.trace(\"[{}][{}] Failed to get consumer group statistics: \", ServiceType.TB_QUEUE.name(), groupId, e);\n                }\n            });\n        } catch (Exception e) {\n            log.warn(\"[{}][{}] Failed to get consumer group statistics: {}\", ServiceType.TB_QUEUE.name(), groupId, e.getMessage());\n            log.trace(\"[{}][{}] Failed to get consumer group statistics: \", ServiceType.TB_QUEUE.name(), groupId, e);\n        }\n    }, 0, timeoutMs, TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 38.8016067052839}]}
{"instance_id": "81553181cdc8", "generation_result": [{"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}]}
{"instance_id": "3c8bf3ff31b7", "generation_result": [{"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\t// create a scoped name for the metric\n    String name = getScopedName(metricName, group);\n\n    // create tags for the metric\n    Map<String, String> tags = getTags(metricName, group);\n\n    // create a metric event for the metric\n    MetricEvent event = new MetricEvent(name, tags);\n\n    // add the metric and its event to the corresponding collection\n    if (metric instanceof Counter) {\n        counters.put((Counter) metric, event);\n    } else if (metric instanceof Gauge) {\n        gauges.put((Gauge<?>) metric, event);\n    } else if (metric instanceof Histogram) {\n        histograms.put((Histogram) metric, event);\n    } else if (metric instanceof Meter) {\n        meters.put((Meter) metric, event);\n    } else {\n        LOG.warn(\"Unknown metric type: {}. This indicator will not be monitored.\", metric);\n    }\n}", "CodeBleu_score": 68.25964680248312}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tMetricEvent event = new MetricEvent(group, metricName, CHARACTER_FILTER);\n    if (metric instanceof Counter) {\n        counters.put((Counter) metric, event);\n    } else if (metric instanceof Gauge) {\n        gauges.put((Gauge<?>) metric, event);\n    } else if (metric instanceof Histogram) {\n        histograms.put((Histogram) metric, event);\n    } else if (metric instanceof Meter) {\n        meters.put((Meter) metric, event);\n    } else {\n        LOG.warn(\"Cannot add unknown metric type {}. This indicates that the metric type is not supported by this reporter.\", metric.getClass().getName());\n    }\n}", "CodeBleu_score": 66.83662149368791}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\t// create a scoped name for the metric\n    String name = createScopedName(metricName, group);\n    // create a map with tags derived from the metric name and group\n    Map<String, String> tags = createTags(metricName, group);\n    // create a MetricEvent\n    MetricEvent event = new MetricEvent(name, tags);\n    // add the metric and its event to the corresponding collection\n    if (metric instanceof Counter) {\n        counters.put((Counter) metric, event);\n    } else if (metric instanceof Gauge) {\n        gauges.put((Gauge<?>) metric, event);\n    } else if (metric instanceof Histogram) {\n        histograms.put((Histogram) metric, event);\n    } else if (metric instanceof Meter) {\n        meters.put((Meter) metric, event);\n    } else {\n        LOG.warn(\"Unknown metric type: {}\", metric.getClass());\n    }\n}", "CodeBleu_score": 68.73829604974253}]}
{"instance_id": "77c6addc9b99", "generation_result": [{"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}]}
{"instance_id": "a48971ffb3e9", "generation_result": [{"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}]}
{"instance_id": "c2080c299bb0", "generation_result": [{"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time > 0) {\n        consumer.setStartFromSpecificOffsets(getOffsets(props, time));\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 62.44794293761529}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time > 0) {\n        Map<KafkaTopicPartition, Long> specificStartOffsets = getSpecificStartOffsets(props, time);\n        consumer.setStartFromSpecificOffsets(specificStartOffsets);\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 71.05388644965426}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(\n            topic,\n            new MetricSchema(),\n            props);\n    if (time != 0L) {\n        consumer.setStartFromTimestamp(time);\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 70.20449774628975}]}
{"instance_id": "5e828afbe8e4", "generation_result": [{"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\t// set the group ID to the timestamp\n    props.setProperty(\"group.id\", parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time);\n    // create a consumer with the properties\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    // get the partitions for the topic\n    List<PartitionInfo> partitionInfos = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    // build a map of partition to timestamp\n    Map<TopicPartition, Long> partitionTimestamp = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfos) {\n        partitionTimestamp.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n    }\n    // get the offsets for the partitions\n    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(partitionTimestamp);\n    // build a map of KafkaTopicPartition to offsets\n    Map<KafkaTopicPartition, Long> partitionOffsets = new HashMap<>();\n    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n        partitionOffsets.put(new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), entry.getValue().offset());\n    }\n    // close the consumer\n    consumer.close();\n    return partitionOffsets;\n}", "CodeBleu_score": 61.97504487862927}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\t// set the consumer group ID using the provided time\n    props.put(\"group.id\", parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + \"-\" + time);\n\n    // create a consumer with the properties\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n\n    // retrieve partition information for the specified topic\n    List<PartitionInfo> partitionInfos = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n\n    // build a map of partition to timestamp\n    Map<TopicPartition, Long> partitionToTimestamp = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfos) {\n        partitionToTimestamp.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n    }\n\n    // fetch offsets for the given times\n    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(partitionToTimestamp);\n\n    // build a map of KafkaTopicPartition to offsets\n    Map<KafkaTopicPartition, Long> partitionOffsets = new HashMap<>();\n    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n        TopicPartition topicPartition = entry.getKey();\n        OffsetAndTimestamp offsetAndTimestamp = entry.getValue();\n        if (offsetAndTimestamp != null) {\n            partitionOffsets.put(new KafkaTopicPartition(topicPartition.topic(), topicPartition.partition()), offsetAndTimestamp.offset());\n        }\n    }\n\n    // close the consumer\n    consumer.close();\n\n    return partitionOffsets;\n}", "CodeBleu_score": 58.38713244921063}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tString groupId = parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time;\n    props.put(\"group.id\", groupId);\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    List<PartitionInfo> partitionInfos = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    Map<TopicPartition, Long> partitionTimes = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfos) {\n        partitionTimes.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n    }\n    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(partitionTimes);\n    Map<KafkaTopicPartition, Long> partitionOffsets = new HashMap<>();\n    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n        if (entry.getValue() != null) {\n            partitionOffsets.put(new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), entry.getValue().offset());\n        }\n    }\n    consumer.close();\n    return partitionOffsets;\n}", "CodeBleu_score": 57.713069622336874}]}
{"instance_id": "28da89f56f67", "generation_result": [{"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    MetricEvent event = new MetricEvent();\n    event.setTimestamp(System.currentTimeMillis());\n    event.setName(\"metric\");\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\"tag1\", \"value1\");\n    tags.put(\"tag2\", \"value2\");\n    event.setTags(tags);\n    Map<String, Object> fields = new HashMap<>();\n    fields.put(\"count\", 1.0);\n    fields.put(\"cpu\", 0.9);\n    event.setFields(fields);\n\n    producer.send(new ProducerRecord<String, String>(topic, null, null, GsonUtil.toJson(event)));\n    System.out.println(\"Sent data: \" + GsonUtil.toJson(event));\n\n    producer.flush();\n}", "CodeBleu_score": 63.242811886619556}, {"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    MetricEvent event = new MetricEvent();\n    event.setTimestamp(System.currentTimeMillis());\n    event.setName(\"metric\");\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\"tag1\", \"value1\");\n    tags.put(\"tag2\", \"value2\");\n    event.setTags(tags);\n    Map<String, Object> fields = new HashMap<>();\n    fields.put(\"field1\", 1);\n    fields.put(\"field2\", 2.0);\n    fields.put(\"field3\", true);\n    event.setFields(fields);\n\n    String data = GsonUtil.toJson(event);\n    producer.send(new ProducerRecord<>(topic, data));\n    producer.flush();\n\n    System.out.println(\"Data sent: \" + data);\n}", "CodeBleu_score": 61.87273300999795}, {"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    MetricEvent event = new MetricEvent();\n    event.setTimestamp(System.currentTimeMillis());\n    event.setName(\"metric\");\n    event.addTag(\"tag1\", \"value1\");\n    Map<String, Object> fields = new HashMap<>();\n    fields.put(\"field1\", 1);\n    fields.put(\"field2\", 2.0);\n    fields.put(\"field3\", true);\n    event.setFields(fields);\n\n    ProducerRecord<String, String> record =\n            new ProducerRecord<>(topic, null, event.getTimestamp(),\n                    null, GsonUtil.toJson(event));\n    producer.send(record);\n    producer.flush();\n    System.out.println(\"Sent data: \" + GsonUtil.toJson(event));\n}", "CodeBleu_score": 64.35187983972408}]}
{"instance_id": "14bffe09c25a", "generation_result": [{"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().isMoreSpecificThan(org.apache.logging.log4j.Level.toLevel(this.level))) {\n        if (!event.getLoggerName().contains(\"xxx\")) {\n            try {\n                String message = new String(this.getLayout().toByteArray(event));\n                LogEvent logEvent = new LogEvent();\n                logEvent.setLevel(event.getLevel().toString());\n                logEvent.setLog(message);\n                logEvent.setAppId(this.appId);\n                logEvent.setContainerId(this.containerId);\n                logEvent.setContainerType(this.containerType);\n                logEvent.setTaskName(this.taskName);\n                logEvent.setTaskId(this.taskId);\n                logEvent.setSource(this.source);\n                logEvent.setNodeIp(this.nodeIp);\n                logEvent.setLogTime(event.getTimeMillis());\n                logEvent.setLogThreadName(event.getThreadName());\n                logEvent.setLogThreadPriority(event.getThreadPriority());\n                logEvent.setLogLoggerName(event.getLoggerName());\n                logEvent.setLogLoggerFqcn(event.getLoggerFqcn());\n                logEvent.setLogMarker(event.getMarker() != null ? event.getMarker().toString() : \"\");\n                logEvent.setLogThrown(event.getThrown() != null ? ExceptionUtil.getStackTrace(event.getThrown()) : \"\");\n                logEvent.setLogThrownProxy(event.getThrownProxy() != null ? ExceptionUtil.getStackTrace(event.getThrownProxy()) : \"\");\n                logEvent.setLogSource(new String(event.getSource().getBytes()));\n                logEvent.setLogSourceClassName(event.getSource().getClassName());\n                logEvent.setLogSourceMethodName(event.getSource().getMethodName());\n                logEvent.setLogSourceLineNumber(event.getSource().getLineNumber());\n                logEvent.setLogContextStack(event.getContextStack().asList());\n                logEvent.setLogContextData(event.getContextData().toMap());\n                logEvent.", "CodeBleu_score": 34.7607891917538}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().toString().equals(level) && !event.getLoggerName().contains(\"xxx\")) {\n        try {\n            LogEvent logEvent = new LogEvent();\n            logEvent.setLogId(UUID.randomUUID().toString());\n            logEvent.setSource(source);\n            logEvent.setAppId(appId);\n            logEvent.setContainerId(containerId);\n            logEvent.setContainerType(containerType);\n            logEvent.setTaskName(taskName);\n            logEvent.setTaskId(taskId);\n            logEvent.setLogTime(event.getTimeMillis());\n            logEvent.setLogLevel(event.getLevel().toString());\n            logEvent.setLogThreadName(event.getThreadName());\n            logEvent.setLogClassName(event.getLoggerName());\n            logEvent.setLogContent(event.getMessage().getFormattedMessage());\n            logEvent.setLogStack(ExceptionUtil.getStack(event.getThrown()));\n            logEvent.setLogHostIp(nodeIp);\n            logEvent.setLogHostName(InetAddress.getLocalHost().getHostName());\n            logEvent.setLogHostAddress(InetAddress.getLocalHost().getHostAddress());\n            producer.send(new ProducerRecord<>(topic, JacksonUtil.toJson(logEvent)));\n        } catch (JsonProcessingException e) {\n            log.warn(\"Error while serializing log event\", e);\n        } catch (Exception e) {\n            log.warn(\"Error while sending log event to Kafka\", e);\n        }\n    }\n}", "CodeBleu_score": 50.49090982984301}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().toString().equals(level) && !event.getLoggerName().contains(\"xxx\")) {\n        try {\n            LogEvent logEvent = new LogEvent();\n            logEvent.setId(UUID.randomUUID().toString());\n            logEvent.setTimestamp(System.currentTimeMillis());\n            logEvent.setSource(source);\n            logEvent.setAppId(appId);\n            logEvent.setContainerId(containerId);\n            logEvent.setContainerType(containerType);\n            logEvent.setTaskName(taskName);\n            logEvent.setTaskId(taskId);\n            logEvent.setNodeIp(nodeIp);\n            logEvent.setLevel(event.getLevel().toString());\n            logEvent.setLoggerName(event.getLoggerName());\n            logEvent.setThreadName(event.getThreadName());\n            logEvent.setThreadId(event.getThreadId());\n            logEvent.setThreadPriority(event.getThreadPriority());\n            logEvent.setMessage(event.getMessage().getFormattedMessage());\n            logEvent.setThrown(ExceptionUtil.getThrowableList(event.getThrown()));\n            logEvent.setContextMap(event.getContextMap());\n            logEvent.setContextStack(event.getContextStack().asList());\n            logEvent.setSource(source);\n            producer.send(new ProducerRecord<>(topic, JacksonUtil.writeValueAsString(logEvent)));\n        } catch (JsonProcessingException e) {\n            log.warn(\"Error processing log event: {}\", event, e);\n        }\n    }\n}", "CodeBleu_score": 43.14020734553425}]}
{"instance_id": "a9e0786fb5c1", "generation_result": [{"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"Unknown topic suffix method provided: %s\", method)\n}", "CodeBleu_score": 94.37882455205906}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"Unknown topic suffix method provided: %s\", method)\n}", "CodeBleu_score": 94.37882455205906}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"Unknown topic suffix method provided: %s\", method)\n}", "CodeBleu_score": 94.37882455205906}]}
{"instance_id": "9b88d720d48a", "generation_result": [{"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif routingTag, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn routingTag, nil\n\t\t}\n\t}\n\tif k.RoutingKey == \"random\" {\n\t\tuuID, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn uuID.String(), nil\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 82.38126471826128}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif routingTag, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn routingTag, nil\n\t\t}\n\t}\n\n\tif k.RoutingKey == \"random\" {\n\t\tu, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn u.String(), nil\n\t}\n\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 86.16664749845812}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif routingTag, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn routingTag, nil\n\t\t}\n\t}\n\tif k.RoutingKey == \"random\" {\n\t\tu, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn u.String(), nil\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 86.16664749845812}]}
{"instance_id": "dc35e80e7f8d", "generation_result": [{"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}]}
{"instance_id": "daf09ba144b2", "generation_result": [{"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}]}
{"instance_id": "8ecf2a1efead", "generation_result": [{"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}]}
{"instance_id": "eba9dafa99ab", "generation_result": [{"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id != \".\", LogkafkaIdInvalid)\n  checkCondition(logkafka_id != \"..\", LogkafkaIdInvalid)\n  checkCondition(logkafka_id.length <= maxNameLength, LogkafkaIdTooLong)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), InvalidCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, InvalidCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 75.17891860124485}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id != \".\", LogkafkaIdDot)\n  checkCondition(logkafka_id != \"..\", LogkafkaIdDotDot)\n  checkCondition(logkafka_id.length <= maxPathLength, LogkafkaIdTooLong)\n  illRgxPath.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      rgx.findFirstIn(logkafka_id) match {\n        case Some(t) =>\n          checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n        case None =>\n          checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n      }\n  }\n}", "CodeBleu_score": 72.38982922951544}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(!logkafka_id.equals(\".\"), LogkafkaIdIsDot)\n  checkCondition(!logkafka_id.equals(\"..\"), LogkafkaIdIsDotDot)\n  checkCondition(logkafka_id.length <= maxNameLength, LogkafkaIdTooLong(logkafka_id))\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 79.85937422007913}]}
{"instance_id": "45d5aefd3f6c", "generation_result": [{"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}]}
{"instance_id": "a535ce861222", "generation_result": [{"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}]}
{"instance_id": "ad37d3914acc", "generation_result": [{"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 28.760438521551706}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 28.760438521551706}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 28.760438521551706}]}
{"instance_id": "3ecdb2c1534f", "generation_result": [{"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}]}
{"instance_id": "5955ade118b9", "generation_result": [{"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}]}
{"instance_id": "26fbc2ba1d28", "generation_result": [{"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}]}
{"instance_id": "adf40b68c93e", "generation_result": [{"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}]}
{"instance_id": "07ff1ca41c93", "generation_result": [{"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tif (null == this.isTopicCreationEnabled) {\n        final String topicCreationEnable = (String) config.get(\"topic.creation.enable\");\n        final Version kafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n        if (kafkaConnectVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) < 0) {\n            throw new DebeziumException(String.format(\"The current Kafka Connect version %s does not support topic creation.\", kafkaConnectVersion));\n        }\n        this.isTopicCreationEnabled = (null == topicCreationEnable) || Boolean.parseBoolean(topicCreationEnable);\n    }\n    return this.isTopicCreationEnabled;\n}", "CodeBleu_score": 55.474093387037996}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\ttry {\n        if (null == herder) {\n            herder = getHerder();\n        }\n        final Version connectVersion = parseVersion(AppInfoParser.getVersion());\n        final boolean isTopicCreationEnabled = Boolean.parseBoolean(config.getOrDefault(\"topic.creation.enable\", \"true\").toString());\n        return connectVersion.greaterThanOrEqualTo(TOPIC_CREATION_KAFKA_VERSION) && isTopicCreationEnabled;\n    } catch (Exception e) {\n        throw new DebeziumException(e);\n    }\n}", "CodeBleu_score": 50.15382312912098}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tfinal String topicCreationEnable = (String) config.get(TOPIC_CREATION_ENABLE_CONFIG);\n    final Version kafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n    return kafkaConnectVersion.greaterThanOrEqualTo(TOPIC_CREATION_KAFKA_VERSION) && Boolean.parseBoolean(topicCreationEnable);\n}", "CodeBleu_score": 43.287749861669724}]}
{"instance_id": "e2cc420e1e63", "generation_result": [{"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal String clientId = config.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(\n                String.format(\"The '%s' property must be specified\", CLIENT_ID_CONFIG));\n    }\n\n    // Set up the Kafka AdminClient for managing offsets\n    final Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, clientId + \"-admin\");\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n\n    // Create the offset backing store, which manages offsets in a Kafka topic\n    final Map<String, Object> offsetProps = new HashMap<>(config);\n    offsetProps.put(CLIENT_ID_CONFIG, clientId);\n    return new KafkaOffsetBackingStore(admin, offsetProps, converterForOffsetStore());\n}", "CodeBleu_score": 47.85742907219917}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal String clientId = config.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(\"Missing mandatory configuration: \" + CLIENT_ID_CONFIG);\n    }\n    final Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.remove(CLIENT_ID_CONFIG);\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n    final JsonConverter converter = new JsonConverter();\n    converter.configure(Collections.singletonMap(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"false\"), true);\n    return new KafkaOffsetBackingStore(admin, converter, clientId);\n}", "CodeBleu_score": 42.01726622761603}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal String clientId = config.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(String.format(\"Missing configuration '%s'\", CLIENT_ID_CONFIG));\n    }\n    final Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, clientId + \"-admin\");\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n    return new KafkaOffsetBackingStore(converterForOffsetStore(), admin, clientId, adminProps);\n}", "CodeBleu_score": 45.697042187633066}]}
{"instance_id": "4ecb2279231e", "generation_result": [{"code": "private T readKey(JsonNode node) throws IOException{\n\tif (!node.isObject()) {\n        throw new IllegalArgumentException(\"Key is not an object\");\n    }\n    JsonNode payload = node.get(PAYLOAD_FIELD);\n    if (payload != null) {\n        node = payload;\n    }\n    Iterator<String> fieldNames = node.fieldNames();\n    if (fieldNames.hasNext()) {\n        String fieldName = fieldNames.next();\n        if (fieldNames.hasNext()) {\n            return reader.readValue(node);\n        } else {\n            return reader.readValue(node.get(fieldName));\n        }\n    } else {\n        return reader.readValue(node);\n    }\n}", "CodeBleu_score": 50.75779620868756}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (!node.isObject()) {\n        throw new RuntimeException(\"Expected a JSON object for the key but got \" + node.getNodeType().name());\n    }\n\n    if (node.has(PAYLOAD_FIELD)) {\n        node = node.get(PAYLOAD_FIELD);\n    }\n\n    Iterator<String> fieldNames = node.fieldNames();\n    if (!fieldNames.hasNext()) {\n        throw new RuntimeException(\"Expected a JSON object with a single field name for the key but got \" + node);\n    }\n\n    String fieldName = fieldNames.next();\n    if (fieldNames.hasNext()) {\n        return reader.readValue(node);\n    }\n    else {\n        return reader.readValue(node.get(fieldName));\n    }\n}", "CodeBleu_score": 50.01778853760766}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        node = node.get(PAYLOAD_FIELD);\n    }\n    if (node.isObject()) {\n        Iterator<String> fieldNames = node.fieldNames();\n        if (fieldNames.hasNext()) {\n            String fieldName = fieldNames.next();\n            if (fieldNames.hasNext()) {\n                return reader.readValue(node);\n            } else {\n                return reader.readValue(node.get(fieldName));\n            }\n        } else {\n            return reader.readValue(node);\n        }\n    } else {\n        return reader.readValue(node);\n    }\n}", "CodeBleu_score": 49.443245524880986}]}
{"instance_id": "b8ba1bfc7b5e", "generation_result": [{"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder();\n    boolean changed = false;\n    for (int i = 0; i < topicName.length(); i++) {\n        char c = topicName.charAt(i);\n        if (isValidTopicNameChar(c)) {\n            sb.append(c);\n        } else {\n            sb.append(REPLACEMENT_CHAR);\n            changed = true;\n        }\n    }\n    if (changed) {\n        LOGGER.warn(\"The topic name '{}' contains invalid characters, so it was sanitized to '{}'\", topicName, sb);\n        return sb.toString();\n    }\n    return topicName;\n}", "CodeBleu_score": 60.017296539074195}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder(topicName.length());\n    boolean changed = false;\n    for (char c : topicName.toCharArray()) {\n        if (c == '_') {\n            sb.append(REPLACEMENT_CHAR);\n            changed = true;\n        } else if (c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c == '-' || c == '.' || c == '_') {\n            sb.append(c);\n        } else {\n            sb.append(REPLACEMENT_CHAR);\n            changed = true;\n        }\n    }\n    if (changed) {\n        LOGGER.warn(\"The topic name '{}' contains invalid characters and has been replaced with '{}'\", topicName, sb.toString());\n        return sb.toString();\n    }\n    return topicName;\n}", "CodeBleu_score": 56.14261702288342}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder();\n    boolean changed = false;\n    for (int i = 0; i < topicName.length(); i++) {\n        char ch = topicName.charAt(i);\n        if (isValidTopicNameCharacter(ch)) {\n            sb.append(ch);\n        }\n        else {\n            sb.append(REPLACEMENT_CHAR);\n            changed = true;\n        }\n    }\n    if (changed) {\n        LOGGER.warn(\"The topic name '{}' for data collection '{}' contains invalid characters, so it has been sanitized to '{}'\",\n                topicName, id, sb);\n        return sb.toString();\n    }\n    return topicName;\n}", "CodeBleu_score": 59.09409309609606}]}
{"instance_id": "9cf0d74c098d", "generation_result": [{"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n        this.connectorConfig = connectorConfig;\n        this.maxRetries = determineMaxRetries(connectorType, connectorConfig);\n        if (replacedErrorHandler != null) {\n            this.retries = replacedErrorHandler.retries;\n        }\n        else {\n            this.retries = 0;\n        }\n    }", "CodeBleu_score": 62.097391951573414}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n        this.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n        this.maxRetries = connectorConfig.getInteger(CommonConnectorConfig.MAX_RETRIES);\n        if (replacedErrorHandler != null) {\n            this.retries = replacedErrorHandler.retries;\n        }\n        else {\n            this.retries = 0;\n        }\n    }", "CodeBleu_score": 68.83654856764664}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n        this.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n        this.maxRetries = connectorConfig.getInteger(CommonConnectorConfig.MAX_RETRIES);\n        if (replacedErrorHandler != null) {\n            this.retries = replacedErrorHandler.retries;\n        }\n        else {\n            this.retries = 0;\n        }\n    }", "CodeBleu_score": 68.83654856764664}]}
{"instance_id": "8ab4bf3a4dd8", "generation_result": [{"code": "public void validate(Configuration configuration, Field.Set fields){\n\tif (configuration == null) {\n        throw new ConfigException(\"Configuration is null\");\n    }\n    if (fields == null) {\n        throw new ConfigException(\"Fields are null\");\n    }\n    final Map<String, ConfigValue> values = configuration.validate(fields);\n    if (values != null && !values.isEmpty()) {\n        for (ConfigValue value : values.values()) {\n            if (value.errorMessages().size() > 0) {\n                throw new ConfigException(value.name(), value.errorMessages().get(0));\n            }\n        }\n    }\n}", "CodeBleu_score": 43.426008880544536}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tif (configuration == null) {\n        throw new ConfigException(\"Configuration is null\");\n    }\n    if (fields == null) {\n        throw new ConfigException(\"Fields is null\");\n    }\n    final Map<String, ConfigValue> values = configuration.validate(fields);\n    for (ConfigValue value : values.values()) {\n        if (value.errorMessages().size() > 0) {\n            throw new ConfigException(value.name(), value.errorMessages().get(0));\n        }\n    }\n}", "CodeBleu_score": 45.72567287445683}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tif (configuration == null) {\n        throw new ConfigException(\"Configuration cannot be null\");\n    }\n    if (fields == null) {\n        throw new ConfigException(\"Fields cannot be null\");\n    }\n    if (!configuration.validateAndRecord(fields, (configName, field, error) -> {\n        throw new ConfigException(configName, field.name(), error);\n    })) {\n        throw new ConfigException(\"Error validating configuration.\");\n    }\n}", "CodeBleu_score": 28.387261716486133}]}
{"instance_id": "860405dc9abb", "generation_result": [{"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}]}
{"instance_id": "681a0415df83", "generation_result": [{"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n\n    if (!source.schema().field(SNAPSHOT_KEY) != null) {\n        return null;\n    }\n\n    final String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 40.691165652924994}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n\n    final Schema schema = source.schema();\n    final Field field = schema.field(SNAPSHOT_KEY);\n    if (field == null) {\n        return null;\n    }\n\n    final Object snapshotString = source.get(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n\n    return SnapshotRecord.valueOf(snapshotString.toString().toUpperCase());\n}", "CodeBleu_score": 40.4083000062046}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n    final Schema schema = source.schema();\n    if (schema == null) {\n        return null;\n    }\n    final Field field = schema.field(SNAPSHOT_KEY);\n    if (field == null) {\n        return null;\n    }\n    final String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 46.46496229222081}]}
{"instance_id": "85ef1162e38a", "generation_result": [{"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tString[] parts = version.split(\"\\\\.\");\n    if (parts.length > 2) {\n        parts = new String[] { parts[0], parts[1] };\n    }\n    String[] hyphenParts = parts[1].split(\"-\");\n    return Optional.of(Float.parseFloat(hyphenParts[0]));\n}", "CodeBleu_score": 53.76992905442985}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\ttry {\n        String[] parts = version.split(\"\\\\.\");\n        if (parts.length > 2) {\n            parts = new String[]{parts[0], parts[1]};\n        }\n        String[] hyphenParts = parts[1].split(\"-\");\n        return Optional.of(Float.parseFloat(hyphenParts[0]));\n    } catch (Exception e) {\n        return Optional.empty();\n    }\n}", "CodeBleu_score": 66.49729432486046}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tString[] parts = version.split(\"\\\\.\");\n    if (parts.length > 2) {\n        parts = new String[] {parts[0], parts[1]};\n    }\n    String[] hyphenParts = parts[1].split(\"-\");\n    return Optional.of(Float.parseFloat(hyphenParts[0]));\n}", "CodeBleu_score": 53.61903359412743}]}
{"instance_id": "64346d69eba0", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.flatMap(c -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, c)))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 71.40535793143783}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.flatMap(newConnector ->\n            kafkaConnectService.createConnector(getCluster(clusterName), connectName, newConnector)\n                .map(ResponseEntity::ok)))\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 70.66194637705108}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.map(dto -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, dto)))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 71.8858951851314}]}
{"instance_id": "4c446fd64db6", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}]}
{"instance_id": "d34a49f3ca1b", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}]}
{"instance_id": "199cfe41b374", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}]}
{"instance_id": "88380ae300c4", "generation_result": [{"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n      .initializers(new DynamicConfigOperations())\n      .run(args);\n}", "CodeBleu_score": 40.8226072471737}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n            .initializers(new DynamicConfigOperations())\n            .run(args);\n}", "CodeBleu_score": 41.92511731620986}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder()\n        .sources(KafkaUiApplication.class)\n        .initializers(new DynamicConfigOperations())\n        .run(args);\n}", "CodeBleu_score": 35.57979121815457}]}
{"instance_id": "3f022876f737", "generation_result": [{"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\treturn search == null ? connector -> true : connector ->\n        Stream.of(connector.getName(), connector.getType(), connector.getState().getValue())\n            .anyMatch(s -> StringUtils.containsIgnoreCase(s, search));\n}", "CodeBleu_score": 44.55474572809792}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\treturn search == null ? connector -> true :\n        connector -> connector.getConnector().getName().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getConfig().entrySet().stream()\n                .anyMatch(entry -> entry.getValue().toLowerCase().contains(search.toLowerCase())) ||\n            connector.getTasks().stream()\n                .anyMatch(task -> task.getStatus().getWorkerId().toLowerCase().contains(search.toLowerCase())) ||\n            connector.getTasks().stream()\n                .anyMatch(task -> task.getStatus().getTrace().toLowerCase().contains(search.toLowerCase()));\n}", "CodeBleu_score": 30.979862634839904}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tif (StringUtils.isBlank(search)) {\n        return connectorInfo -> true;\n    }\n\n    return connectorInfo -> {\n        final String searchLower = search.toLowerCase();\n        return Stream.of(\n                connectorInfo.getConnector().getName(),\n                connectorInfo.getConnector().getConfig().get(\"connector.class\"),\n                connectorInfo.getConnector().getConfig().get(\"name\")\n        ).anyMatch(s -> StringUtils.isNotBlank(s) && s.toLowerCase().contains(searchLower));\n    };\n}", "CodeBleu_score": 39.17254194383517}]}
{"instance_id": "9754aa1ec96c", "generation_result": [{"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return new HashMap<>();\n    }\n    Map<String, Object> flattened = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      if (prefix != null) {\n        key = prefix + \".\" + key;\n      }\n      if (entry.getValue() instanceof Map) {\n        flattened.putAll(flattenClusterProperties(key, (Map<String, Object>) entry.getValue()));\n      } else {\n        flattened.put(key, entry.getValue());\n      }\n    }\n    return flattened;\n  }", "CodeBleu_score": 50.40029924618929}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tMap<String, Object> result = new HashMap<>();\n    if (propertiesMap == null) {\n      return result;\n    }\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      if (prefix != null) {\n        key = prefix + \".\" + key;\n      }\n      if (value instanceof Map) {\n        result.putAll(flattenClusterProperties(key, (Map<String, Object>) value));\n      } else {\n        result.put(key, value);\n      }\n    }\n    return result;\n  }", "CodeBleu_score": 50.73982512338975}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tMap<String, Object> flattened = new HashMap<>();\n\n    if (propertiesMap != null) {\n      for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n        String key = entry.getKey();\n        if (prefix != null) {\n          key = prefix + \".\" + key;\n        }\n        Object value = entry.getValue();\n        if (value instanceof Map) {\n          flattened.putAll(flattenClusterProperties(key, (Map<String, Object>) value));\n        } else {\n          flattened.put(key, value);\n        }\n      }\n    }\n\n    return flattened;\n  }", "CodeBleu_score": 57.015379188904305}]}
{"instance_id": "99266677c846", "generation_result": [{"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(\n        SslConfigs.SSL_KEY_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"The password of the private key in the key store file. \"\n            + \"This is required for client-auth.\"\n    );\n    configDef.define(\n        SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"The store password for the keystore file. \"\n            + \"This is required for client-auth.\"\n    );\n    configDef.define(\n        SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"The password for the trust store file.\"\n    );\n    configDef.define(\n        SaslConfigs.SASL_JAAS_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"JAAS login context parameters for SASL connections. \"\n            + \"This may be prefixed with \\\"${substitution:prop}\\\" to use a property from the environment.\"\n    );\n    configDef.define(\n        SaslConfigs.SASL_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"Password for SASL/PLAIN, SASL/SCRAM, SASL/DIGEST-MD5, or SASL/CRAM-MD5 if needed.\"\n    );\n    return configDef.configKeys().values().stream()\n        .filter(k -> k.type == ConfigDef.Type.PASSWORD)\n        .map(k -> k.name)\n        .collect(Collectors.toSet());\n  }", "CodeBleu_score": 49.64697463609539}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(\n        SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_KEYSTORE_TYPE_CONFIG,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SaslConfigs.SASL_JAAS_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    configDef.define(\n        SaslConfigs.SASL_MECHANISM,\n        ConfigDef.Type.STRING,\n        \"\",\n        ConfigDef.Importance.LOW,\n        \"\"\n    );\n    return configDef.", "CodeBleu_score": 43.73941088190648}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(\n        SslConfigs.SSL_KEY_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"SSL private key password\"\n    );\n    configDef.define(\n        SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"SSL keystore password\"\n    );\n    configDef.define(\n        SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"SSL truststore password\"\n    );\n    configDef.define(\n        SaslConfigs.SASL_JAAS_CONFIG,\n        ConfigDef.Type.PASSWORD,\n        null,\n        ConfigDef.Importance.HIGH,\n        \"SASL JAAS config\"\n    );\n    return configDef.configKeys().values().stream()\n        .filter(configKey -> configKey.type == ConfigDef.Type.PASSWORD)\n        .map(configKey -> configKey.name)\n        .collect(Collectors.toSet());\n  }", "CodeBleu_score": 51.646178199334955}]}
{"instance_id": "deebb6fcb0a8", "generation_result": [{"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\treturn adminClient.getAclOperations()\n        .map(Set::containsAll)\n        .map(allowed -> allowed ? ClusterFeature.KAFKA_ACL_EDIT : null)\n        .defaultIfEmpty(null);\n}", "CodeBleu_score": 17.867291440112705}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\treturn adminClient.getAuthorizedOperations()\n        .map(Set::containsAll)\n        .map(allowed -> allowed ? ClusterFeature.KAFKA_ACL_EDIT : null);\n}", "CodeBleu_score": 16.990562613933815}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\treturn adminClient.getAuthorizedOperations()\n        .map(authorizedOperations -> {\n            Set<AclOperation> operations = authorizedOperations.get(clusterDescription.getClusterId());\n            return operations.contains(AclOperation.ALTER) || operations.contains(AclOperation.ALL);\n        })\n        .flatMap(allowed -> allowed ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT) : Mono.empty());\n}", "CodeBleu_score": 41.98455212121488}]}
{"instance_id": "a5d8cc77955f", "generation_result": [{"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tvar offsetsToReadFrom = new TreeMap<TopicPartition, Long>();\n    if (prevRange.isEmpty()) {\n        offsetsToReadFrom.putAll(seekOperations.getSeekToOffsets());\n    } else {\n        offsetsToReadFrom.putAll(\n            prevRange.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().getTo()))\n        );\n    }\n\n    var messagesToPollPerPartition =\n        Math.min(messagesPerPage / offsetsToReadFrom.size(), pollingSettings.getMaxMessagesPerPartition());\n\n    var nextRange = new TreeMap<TopicPartition, FromToOffset>();\n    for (var entry : offsetsToReadFrom.entrySet()) {\n        var topicPartition = entry.getKey();\n        var offset = entry.getValue();\n        var fromOffset = offset - messagesToPollPerPartition;\n        fromOffset = Math.max(fromOffset, 0);\n        nextRange.put(topicPartition, new FromToOffset(fromOffset, offset));\n    }\n\n    return nextRange;\n}", "CodeBleu_score": 43.65348021483504}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tTreeMap<TopicPartition, FromToOffset> nextRange = new TreeMap<>(Comparator.comparing(TopicPartition::topic)\n      .thenComparingInt(TopicPartition::partition));\n  seekOperations.forEach((topicPartition, fromToOffset) -> {\n    nextRange.put(topicPartition, fromToOffset);\n  });\n  if (prevRange.isEmpty()) {\n    nextRange.forEach((topicPartition, fromToOffset) -> {\n      if (fromToOffset.getFrom() == null) {\n        // If from is null, it means we are polling from the end of the topic partition.\n        // The end offset is the offset of the next message, so we subtract 1 to get the offset of the last message.\n        long from = consumer.endOffset(topicPartition) - 1;\n        nextRange.put(topicPartition, new FromToOffset(from, fromToOffset.getTo()));\n      }\n    });\n  } else {\n    nextRange.forEach((topicPartition, fromToOffset) -> {\n      if (fromToOffset.getFrom() == null) {\n        // If from is null, it means we are polling from the end of the topic partition.\n        // The end offset is the offset of the next message, so we subtract 1 to get the offset of the last message.\n        long from = consumer.endOffset(topicPartition) - 1;\n        nextRange.put(topicPartition, new FromToOffset(from, fromToOffset.getTo()));\n      }\n    });\n  }\n  if (nextRange.isEmpty()) {\n    return nextRange;\n  }\n  int messagesToPollPerPartition = messagesPerPage / nextRange.size();\n  nextRange.forEach((topicPartition, fromToOffset) -> {\n    long from = fromToOffset.getFrom();\n    long to = fromToOffset.getTo();\n    long start = Math.max(from - messagesToPollPerPartition, 0);\n    nextRange.put(topicPartition, new FromToOffset(start, to));\n  });\n  return nextRange;\n}", "CodeBleu_score": 39.03152882992437}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tvar offsets = new TreeMap<TopicPartition, Long>();\n    if (prevRange.isEmpty()) {\n        offsets.putAll(seekOperations.getSeekToOffsets());\n    } else {\n        offsets.putAll(\n                prevRange.entrySet().stream()\n                        .collect(\n                                Collectors.toMap(\n                                        Map.Entry::getKey,\n                                        e -> e.getValue().getFrom() - messagesPerPage\n                                )\n                        )\n        );\n    }\n\n    var messagesPerPartition = messagesPerPage / offsets.size();\n\n    return offsets.entrySet().stream()\n            .collect(\n                    Collectors.toMap(\n                            Map.Entry::getKey,\n                            e -> {\n                                var from = Math.max(e.getValue(), offsets.firstEntry().getValue());\n                                var to = Math.max(e.getValue(), offsets.firstEntry().getValue() + messagesPerPartition);\n                                return new FromToOffset(from, to);\n                            },\n                            (v1, v2) -> v1,\n                            () -> new TreeMap<>(Comparator.comparing(TopicPartition::topic).thenComparing(TopicPartition::partition))\n                    )\n            );\n}", "CodeBleu_score": 39.269651458044194}]}
{"instance_id": "c4c657caeba5", "generation_result": [{"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}]}
{"instance_id": "dffe85dbb47b", "generation_result": [{"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}]}
{"instance_id": "ea146a7bc30c", "generation_result": [{"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tfinal List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(entry -> new Journal.Entry(entry.getSourceNodeId(), entry.getRawMessage()))\n            .collect(Collectors.toList());\n\n    final AtomicLong messageBytes = new AtomicLong(0);\n    journalEntries.forEach(entry -> messageBytes.addAndGet(entry.getMessage().length));\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Could not write messages to journal\", e);\n    }\n\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(messageBytes.get());\n}", "CodeBleu_score": 59.8925389133957}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tfinal List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(entry -> {\n                final int messageSize = entry.getRawMessage().length;\n                metrics.messageBytes().mark(messageSize);\n                return new Journal.Entry(entry.getEventId(), entry.getRawMessage());\n            })\n            .collect(Collectors.toList());\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        metrics.failedWrites().mark();\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(journalEntries.stream().mapToInt(entry -> entry.message().length).sum());\n}", "CodeBleu_score": 59.827113934961616}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tfinal List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(event -> new Journal.Entry(event.getEvent(), event.getEvent().getTimestamp()))\n            .collect(Collectors.toList());\n\n    final AtomicLong totalBytes = new AtomicLong();\n    journalEntries.forEach(entry -> totalBytes.addAndGet(entry.getMessage().length));\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            journalFilled.release();\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(totalBytes.get());\n}", "CodeBleu_score": 57.83640778820116}]}
{"instance_id": "944e77614042", "generation_result": [{"code": "public void acknowledge(List<Message> messages){\n\tfinal Optional<Long> maxMessageQueueId = messages.stream()\n            .map(Message::getId)\n            .max(Long::compare);\n\n    maxMessageQueueId.ifPresent(kafkaJournal::acknowledge);\n    markAcknowledged(messages.size());\n}", "CodeBleu_score": 35.3655142855985}, {"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxMessageQueueId = messages.stream().map(Message::getId).max(Long::compareTo);\n    maxMessageQueueId.ifPresent(kafkaJournal::acknowledge);\n    metrics.mark(messages.size());\n}", "CodeBleu_score": 31.572035969743123}, {"code": "public void acknowledge(List<Message> messages){\n\tfinal Optional<Long> maxMessageQueueId = messages.stream()\n            .map(Message::getId)\n            .max(Long::compareTo);\n\n    if (maxMessageQueueId.isPresent()) {\n        kafkaJournal.acknowledge(maxMessageQueueId.get());\n    }\n\n    updateMetrics(messages.size());\n}", "CodeBleu_score": 37.7056646970823}]}
{"instance_id": "981280879490", "generation_result": [{"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}]}
{"instance_id": "3ca34bfd2d64", "generation_result": [{"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 13.064465675200484}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 13.064465675200484}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 13.064465675200484}]}
{"instance_id": "e1d07a1fa986", "generation_result": [{"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}]}
{"instance_id": "8b30a387069f", "generation_result": [{"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}]}
{"instance_id": "ee338b61e75d", "generation_result": [{"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        LOGGER.info(\"start to close kafka admin-client of cluster-phy-id: \" + clusterPhyId);\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        for (AdminClient adminClient : adminClientList) {\n            try {\n                adminClient.close();\n            } catch (Exception e) {\n                LOGGER.error(\"close kafka admin-client failed, cluster-phy-id: \" + clusterPhyId, e);\n            }\n        }\n\n        LOGGER.info(\"close kafka admin-client success, cluster-phy-id: \" + clusterPhyId);\n    }\n}", "CodeBleu_score": 44.85712794024747}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        LOGGER.info(\"start to close kafka admin-client for cluster: \" + clusterPhyId);\n        try {\n            for (AdminClient adminClient : adminClientList) {\n                adminClient.close();\n            }\n            LOGGER.info(\"close kafka admin-client for cluster: \" + clusterPhyId + \" success\");\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka admin-client for cluster: \" + clusterPhyId + \" failed\", e);\n        } finally {\n            KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n        }\n    }\n}", "CodeBleu_score": 46.150544783880655}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        LOGGER.info(\"begin to close kafka admin-client for cluster-phy-id: \" + clusterPhyId);\n        for (AdminClient adminClient : adminClientList) {\n            try {\n                adminClient.close();\n            } catch (Exception e) {\n                LOGGER.error(\"close kafka admin-client failed for cluster-phy-id: \" + clusterPhyId, e);\n            }\n        }\n        LOGGER.info(\"close kafka admin-client for cluster-phy-id: \" + clusterPhyId + \" success\");\n        KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n    }\n}", "CodeBleu_score": 44.82808312628197}]}
{"instance_id": "6f0b935b971c", "generation_result": [{"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean success = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n            success = false;\n        }\n    }\n    return success;\n}", "CodeBleu_score": 86.86976203698602}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean success = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            success = false;\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n        }\n    }\n\n    return success;\n}", "CodeBleu_score": 75.96304066960002}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean success = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            success = false;\n        }\n    }\n    return success;\n}", "CodeBleu_score": 75.27404327231915}]}
{"instance_id": "b5475aa94c1a", "generation_result": [{"code": "public void stop() {\n\tif (app != null) {\n\t\tint attempts = 0;\n\t\twhile (attempts < 5) {\n\t\t\tattempts++;\n\t\t\ttry {\n\t\t\t\tapp.stop();\n\t\t\t\tapp = null;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e1) {\n\t\t\t\t\tthrow new IllegalStateException(e1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 50.17704745083187}, {"code": "public void stop() {\n\tif (app != null) {\n        app.stop();\n        for (int i = 0; i < 5; i++) {\n            if (!app.isRunning()) {\n                app = null;\n                return;\n            }\n            try {\n                Thread.sleep(1000);\n            }\n            catch (InterruptedException e) {\n                throw new IllegalStateException(\"Thread interrupted while waiting for app to stop\", e);\n            }\n        }\n        throw new IllegalStateException(\"App failed to stop in 5 seconds\");\n    }\n}", "CodeBleu_score": 44.69408661906178}, {"code": "public void stop() {\n\tif (app != null) {\n\t\tapp.stop();\n\t\tint count = 0;\n\t\twhile (app.isActive() && count < 5) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 64.60742053948908}]}
{"instance_id": "5da405de9e3d", "generation_result": [{"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner eurekaServerRunner = new EurekaServerRunner(config);\n\teurekaServerRunner.enableTls();\n\teurekaServerRunner.setKeyStore(caCert.getAbsolutePath(), KEY_STORE_PASSWORD);\n\teurekaServerRunner.setTrustStore(caCert.getAbsolutePath(), KEY_STORE_PASSWORD);\n\teurekaServerRunner.start();\n\treturn eurekaServerRunner;\n}", "CodeBleu_score": 46.27029740059714}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner runner = new EurekaServerRunner(config);\n\trunner.enableTls();\n\trunner.setKeyStore(caCert, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\trunner.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\trunner.start();\n\treturn runner;\n}", "CodeBleu_score": 53.88559895556024}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner server = new EurekaServerRunner(config, false);\n\tserver.enableTls();\n\tserver.setKeyStore(caCert, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tserver.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tserver.start();\n\treturn server;\n}", "CodeBleu_score": 64.97689839959801}]}
{"instance_id": "19cbf70751b2", "generation_result": [{"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config);\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.setServiceName(\"testservice\");\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 37.00213550039335}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config);\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 36.84733482640462}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config);\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 36.84733482640462}]}
{"instance_id": "d6ccff5c0536", "generation_result": [{"code": "static void createCertificates() throws Exception{\n\tKeyTool caKeyTool = new KeyTool(new File(\"ca-key.p12\"), KEY_STORE_PASSWORD, \"ca\", KEY_PASSWORD);\n\tKeyTool serverKeyTool = new KeyTool(new File(\"server-key.p12\"), KEY_STORE_PASSWORD, \"server\", KEY_PASSWORD);\n\tKeyTool clientKeyTool = new KeyTool(new File(\"client-key.p12\"), KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tKeyTool wrongClientKeyTool = new KeyTool(new File(\"wrong-client-key.p12\"), KEY_STORE_PASSWORD, \"wrong-client\", WRONG_PASSWORD);\n\n\tcaKeyTool.createKeyStore();\n\tserverKeyTool.createKeyStore();\n\tclientKeyTool.createKeyStore();\n\twrongClientKeyTool.createKeyStore();\n\n\tcaKeyTool.createCertificate(\"ca\", \"CA\");\n\tserverKeyTool.createCertificate(\"server\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\tclientKeyTool.createCertificate(\"client\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\twrongClientKeyTool.createCertificate(\"wrong-client\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\n\tcaKeyTool.createCertificate(\"server\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\tcaKeyTool.createCertificate(\"client\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\tcaKeyTool.createCertificate(\"wrong-client\", \"CN=localhost, OU=Test, O=Test, L=Test, ST=Test, C=DE\");\n\n\tcaKeyTool.saveKeyAndCert(caCert);\n\tserverKeyTool.saveKeyAndCert(serverCert);\n\tclientKeyTool.saveKeyAndCert(clientCert);\n\twrongClient", "CodeBleu_score": 24.64844216006671}, {"code": "static void createCertificates() throws Exception{\n\tKeyTool caKeyTool = new KeyTool();\n\tcaKeyTool.setKeyStore(caCert, KEY_STORE_PASSWORD);\n\tcaKeyTool.generateKeyPair(\"ca\", \"RSA\");\n\tcaKeyTool.generateSelfSignedCertificate(\"ca\", \"RSA\", 365);\n\tcaKeyTool.saveKeyStore();\n\n\tKeyTool serverKeyTool = new KeyTool();\n\tserverKeyTool.setKeyStore(serverCert, KEY_STORE_PASSWORD);\n\tserverKeyTool.generateKeyPair(\"server\", \"RSA\");\n\tserverKeyTool.generateCertificate(\"server\", \"RSA\", caKeyTool, \"ca\", 365);\n\tserverKeyTool.saveKeyStore();\n\n\tKeyTool clientKeyTool = new KeyTool();\n\tclientKeyTool.setKeyStore(clientCert, KEY_STORE_PASSWORD);\n\tclientKeyTool.generateKeyPair(\"client\", \"RSA\");\n\tclientKeyTool.generateCertificate(\"client\", \"RSA\", caKeyTool, \"ca\", 365);\n\tclientKeyTool.saveKeyStore();\n\n\tKeyTool wrongCaKeyTool = new KeyTool();\n\twrongCaKeyTool.setKeyStore(wrongCaCert, KEY_STORE_PASSWORD);\n\twrongCaKeyTool.generateKeyPair(\"ca\", \"RSA\");\n\twrongCaKeyTool.generateSelfSignedCertificate(\"ca\", \"RSA\", 365);\n\twrongCaKeyTool.saveKeyStore();\n\n\tKeyTool wrongClientKeyTool = new KeyTool();\n\twrongClientKeyTool.setKeyStore(wrongClientCert, KEY_STORE_PASSWORD);\n\twrongClientKeyTool.generateKeyPair(\"client\", \"RSA\");\n\twrongClientKeyTool.generateCertificate(\"client\", \"RSA\", wrongCaKeyTool, \"ca\", 365);\n\twrongClientKeyTool.saveKeyStore();\n}", "CodeBleu_score": 20.131746186733352}, {"code": "static void createCertificates() throws Exception{\n\tKeyTool caKeyTool = new KeyTool(null);\n\tcaKeyTool.generateKey(\"ca\", \"ca\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\tcaKeyTool.generateKey(\"server\", \"server\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\tcaKeyTool.generateKey(\"client\", \"client\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\n\tcaKeyTool.selfSignCertificate(\"ca\", \"ca\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\tcaKeyTool.signCertificate(\"server\", \"ca\", \"server\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\tcaKeyTool.signCertificate(\"client\", \"ca\", \"client\", KEY_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\n\tcaCert = saveCert(\"ca\", caKeyTool);\n\tserverCert = saveKeyAndCert(\"server\", caKeyTool);\n\tclientCert = saveKeyAndCert(\"client\", caKeyTool);\n\n\tKeyTool wrongCaKeyTool = new KeyTool(null);\n\twrongCaKeyTool.generateKey(\"ca\", \"ca\", WRONG_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\twrongCaKeyTool.generateKey(\"client\", \"client\", WRONG_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\n\twrongCaKeyTool.selfSignCertificate(\"ca\", \"ca\", WRONG_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\twrongCaKeyTool.signCertificate(\"client\", \"ca\", \"client\", WRONG_PASSWORD, KEY_STORE_PASSWORD, \"RSA\", 1024);\n\n\twrongCaCert = saveCert(\"ca\", wrongCaKeyTool);\n\twrong", "CodeBleu_score": 19.14226177195696}]}
{"instance_id": "d2eeaa6a6bbe", "generation_result": [{"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong endTime = System.currentTimeMillis() + (seconds * 1000);\n    while (System.currentTimeMillis() < endTime) {\n        if (assertion.getAsBoolean()) {\n            return;\n        }\n        try {\n            Thread.sleep(100);\n        } catch (InterruptedException e) {\n            throw new RuntimeException(e);\n        }\n    }\n    throw new RuntimeException(\"Assertion not met within time limit\");\n}", "CodeBleu_score": 42.027640046569495}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong timeLimit = System.currentTimeMillis() + seconds * 1000;\n\twhile (System.currentTimeMillis() < timeLimit) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t}\n\tthrow new RuntimeException(\"Assertion failed\");\n}", "CodeBleu_score": 43.72565844709433}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong start = System.currentTimeMillis();\n\tlong end = start + (seconds * 1000);\n\twhile (System.currentTimeMillis() < end) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t}\n\tthrow new RuntimeException(\"Assertion failed in \" + seconds + \" seconds\");\n}", "CodeBleu_score": 47.415290167808664}]}
{"instance_id": "6bb261b0e4e1", "generation_result": [{"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX509v3CertificateBuilder builder = new JcaX509v3CertificateBuilder(\n\t\t\t\t\tnew X500Name(ca),\n\t\t\t\t\tBigInteger.valueOf(System.currentTimeMillis()),\n\t\t\t\t\tnew Date(System.currentTimeMillis() - ONE_DAY),\n\t\t\t\t\tnew Date(System.currentTimeMillis() + TEN_YEARS),\n\t\t\t\t\tnew X500Name(ca),\n\t\t\t\t\tkeyPair.getPublic());\n\t\t\tbuilder.addExtension(Extension.keyUsage, true, new KeyUsage(KeyUsage.digitalSignature | KeyUsage.keyEncipherment | KeyUsage.keyCertSign));\n\t\t\tbuilder.addExtension(Extension.basicConstraints, true, new BasicConstraints(true));\n\t\t\tContentSigner signer = new JcaContentSignerBuilder(\"SHA1withRSA\").setProvider(\"BC\").build(keyPair.getPrivate());\n\t\t\tX509CertificateHolder holder = builder.build(signer);\n\t\t\treturn new JcaX509CertificateConverter().setProvider(\"BC\").getCertificate(holder);\n\t\t}", "CodeBleu_score": 40.16838639153038}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX500Name subject = new X500Name(\"CN=\" + ca);\n\treturn createCert(keyPair.getPublic(), keyPair.getPrivate(), subject, subject);\n}", "CodeBleu_score": 21.91583516783367}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX509v3CertificateBuilder certBuilder = new JcaX509v3CertificateBuilder(\n\t\t\tnew X500Name(ca),\n\t\t\tBigInteger.valueOf(new SecureRandom().nextInt()),\n\t\t\tnew Date(System.currentTimeMillis() - ONE_DAY),\n\t\t\tnew Date(System.currentTimeMillis() + TEN_YEARS),\n\t\t\tnew X500Name(ca),\n\t\t\tkeyPair.getPublic());\n\tContentSigner contentSigner = new JcaContentSignerBuilder(\"SHA256withRSA\").build(keyPair.getPrivate());\n\tcertBuilder.addExtension(Extension.keyUsage, true, new KeyUsage(KeyUsage.digitalSignature | KeyUsage.keyEncipherment | KeyUsage.keyCertSign));\n\tcertBuilder.addExtension(Extension.basicConstraints, true, new BasicConstraints(true));\n\tX509CertificateHolder holder = certBuilder.build(contentSigner);\n\treturn new JcaX509CertificateConverter().getCertificate(holder);\n}", "CodeBleu_score": 40.32286978183722}]}
{"instance_id": "e4f9c32a7c9e", "generation_result": [{"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient eurekaHttpClient = this.eurekaHttpClient.get();\n\tif (eurekaHttpClient == null) {\n\t\ttry {\n\t\t\teurekaHttpClient = (EurekaHttpClient) ReflectionUtils.getField(eurekaTransportField, this);\n\t\t\tthis.eurekaHttpClient.compareAndSet(null, eurekaHttpClient);\n\t\t}\n\t\tcatch (IllegalAccessException e) {\n\t\t\tlog.error(\"Cannot access EurekaHttpClient\", e);\n\t\t}\n\t}\n\treturn eurekaHttpClient;\n}", "CodeBleu_score": 46.119540838693574}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient eurekaHttpClient = this.eurekaHttpClient.get();\n    if (eurekaHttpClient == null) {\n        try {\n            eurekaHttpClient = (EurekaHttpClient) ReflectionUtils.getField(this.eurekaTransportField, this);\n            this.eurekaHttpClient.set(eurekaHttpClient);\n        }\n        catch (IllegalAccessException ex) {\n            log.error(\"Cannot get EurekaHttpClient from DiscoveryClient. Registration status update will not work.\", ex);\n        }\n    }\n    return eurekaHttpClient;\n}", "CodeBleu_score": 47.15361198321361}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient eurekaHttpClient = this.eurekaHttpClient.get();\n\t\tif (eurekaHttpClient == null) {\n\t\t\tObject eurekaTransport = ReflectionUtils.getField(eurekaTransportField, this);\n\t\t\tif (eurekaTransport != null) {\n\t\t\t\teurekaHttpClient = ReflectionUtils.getField(ReflectionUtils.findField(eurekaTransport.getClass(), \"registrationClient\"), eurekaTransport);\n\t\t\t\tthis.eurekaHttpClient.compareAndSet(null, eurekaHttpClient);\n\t\t\t}\n\t\t}\n\t\treturn eurekaHttpClient;\n\t}", "CodeBleu_score": 42.95373455216578}]}
{"instance_id": "5c404f04e34c", "generation_result": [{"code": "public List<String> getServices(){\n\tApplications applications = this.eurekaClient.getApplications();\n\tList<Application> registered = applications.getRegisteredApplications();\n\tif (registered.isEmpty()) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> names = new ArrayList<>(registered.size());\n\tfor (Application app : registered) {\n\t\tif (!app.getInstances().isEmpty()) {\n\t\t\tnames.add(app.getName().toLowerCase());\n\t\t}\n\t}\n\treturn names;\n}", "CodeBleu_score": 66.60055333435432}, {"code": "public List<String> getServices(){\n\tList<Application> applications = getApplications();\n\tif (applications.isEmpty()) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> names = new ArrayList<>();\n\tfor (Application application : applications) {\n\t\tif (!application.getInstances().isEmpty()) {\n\t\t\tnames.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn names;\n}", "CodeBleu_score": 48.24719025981652}, {"code": "public List<String> getServices(){\n\tApplications applications = this.eurekaClient.getApplications();\n    if (applications == null) {\n        return Collections.emptyList();\n    }\n    List<Application> registered = applications.getRegisteredApplications();\n    List<String> names = new ArrayList<>();\n    for (Application app : registered) {\n        if (!app.getInstances().isEmpty()) {\n            names.add(app.getName().toLowerCase());\n        }\n    }\n    return names;\n}", "CodeBleu_score": 86.98346801022684}]}
{"instance_id": "a21251337ade", "generation_result": [{"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\tif (entry.getValue() instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor discoveryCompositeHealthContributor = (DiscoveryCompositeHealthContributor) entry\n\t\t\t\t\t.getValue();\n\t\t\tfor (NamedContributor<HealthContributor> indicator : discoveryCompositeHealthContributor.getContributors()) {\n\t\t\t\tif (!(indicator.getContributor() instanceof EurekaHealthIndicator)) {\n\t\t\t\t\tthis.healthContributors.put(indicator.getName(), indicator.getContributor());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tthis.healthContributors.put(entry.getKey(), entry.getValue());\n\t\t}\n\t}\n}", "CodeBleu_score": 46.91409584683551}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n        String name = entry.getKey();\n        HealthContributor value = entry.getValue();\n        if (value instanceof DiscoveryCompositeHealthContributor) {\n            DiscoveryCompositeHealthContributor discoveryCompositeHealthContributor = (DiscoveryCompositeHealthContributor) value;\n            for (NamedContributor<HealthContributor> healthContributor : discoveryCompositeHealthContributor.getContributors()) {\n                if (!(healthContributor.getContributor() instanceof EurekaHealthIndicator)) {\n                    this.healthContributors.put(name, healthContributor.getContributor());\n                }\n            }\n        } else {\n            this.healthContributors.put(name, value);\n        }\n    }\n}", "CodeBleu_score": 40.227691714439615}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n        if (entry.getValue() instanceof DiscoveryCompositeHealthContributor) {\n            DiscoveryCompositeHealthContributor discoveryCompositeHealthContributor = (DiscoveryCompositeHealthContributor) entry.getValue();\n            for (NamedContributor<HealthContributor> namedContributor : discoveryCompositeHealthContributor.getContributors()) {\n                if (!(namedContributor.getContributor() instanceof EurekaHealthIndicator)) {\n                    this.healthContributors.put(namedContributor.getName(), namedContributor.getContributor());\n                }\n            }\n        }\n        else {\n            this.healthContributors.put(entry.getKey(), entry.getValue());\n        }\n    }\n}", "CodeBleu_score": 45.48841853369155}]}
{"instance_id": "998fee829929", "generation_result": [{"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.size() == 0) {\n        throw new RuntimeException(\"No instance available for provider-application\");\n    }\n    URI uri = instances.get(0).getUri();\n    return restTemplate.postForObject(uri + \"/echo\", null, String.class);\n}", "CodeBleu_score": 45.1542767028574}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No instance available for provider-application\");\n    }\n    URI uri = instances.get(0).getUri();\n    return restTemplate.postForObject(uri + \"/echo\", null, String.class);\n}", "CodeBleu_score": 43.32545699436467}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No instance available for provider-application\");\n    }\n    URI uri = instances.get(0).getUri();\n    return restTemplate.postForObject(uri + \"/echo\", null, String.class);\n}", "CodeBleu_score": 43.32545699436467}]}
{"instance_id": "31166ae7d3dc", "generation_result": [{"code": "public String world(){\n\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n    if (serviceInstance != null) {\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 95.5159801104738}, {"code": "public String world(){\n\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n    if (serviceInstance != null) {\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 95.5159801104738}, {"code": "public String world(){\n\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n    if (serviceInstance != null) {\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 95.5159801104738}]}
{"instance_id": "d7714f9cf0e9", "generation_result": [{"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Received register request for service {} with IP {} and port {}\", serviceName, ip, port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.put(serviceName, Collections.synchronizedSet(new HashSet<>()));\n    }\n    serverMap.get(serviceName).add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 61.566819752999194}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service {} with IP {} and port {}\", serviceName, ip, port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.put(serviceName, Collections.synchronizedSet(new HashSet<>()));\n    }\n    serverMap.get(serviceName).add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 62.10537145697225}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service {} with IP {} and port {}\", serviceName, ip, port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.put(serviceName, Collections.synchronizedSet(new HashSet<>()));\n    }\n    serverMap.get(serviceName).add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 62.10537145697225}]}
{"instance_id": "45f91ed1c560", "generation_result": [{"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Server server = new Server(ip, port);\n    if (serverMap.get(serviceName).remove(server)) {\n        return true;\n    }\n    return false;\n}", "CodeBleu_score": 62.561489186616}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Server server = new Server(ip, port);\n    if (serverMap.containsKey(serviceName.toLowerCase())) {\n        serverMap.get(serviceName).remove(server);\n    }\n    return true;\n}", "CodeBleu_score": 67.71232156781961}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    serverMap.putIfAbsent(serviceName.toLowerCase(), Collections.synchronizedSet(new HashSet<>()));\n    Server server = new Server(ip, port);\n    serverMap.get(serviceName).remove(server);\n    return true;\n}", "CodeBleu_score": 65.41892418121358}]}
{"instance_id": "f671d9564e40", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null && attributes.containsKey(\"defaultConfiguration\")) {\n        String name = \"__defaultRibbonClientConfiguration\";\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addPropertyValue(\"name\", name);\n        builder.addPropertyValue(\"configuration\", attributes.get(\"defaultConfiguration\"));\n        registry.registerBeanDefinition(name, builder.getBeanDefinition());\n    }\n    registerClientConfiguration(registry, metadata, attributes);\n}", "CodeBleu_score": 49.5130914070989}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null && attributes.containsKey(\"defaultConfiguration\")) {\n        String name = \"__defaultRibbonClientConfiguration\";\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addPropertyValue(\"name\", name);\n        builder.addPropertyValue(\"configuration\", attributes.get(\"defaultConfiguration\"));\n        registry.registerBeanDefinition(name, builder.getBeanDefinition());\n    }\n}", "CodeBleu_score": 50.53733393021908}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null && attributes.containsKey(\"defaultConfiguration\")) {\n        String name = \"__defaultRibbonClientConfiguration\";\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addPropertyValue(\"name\", name);\n        builder.addPropertyValue(\"configuration\", attributes.get(\"defaultConfiguration\"));\n        registry.registerBeanDefinition(name, builder.getBeanDefinition());\n    }\n    registerClientConfiguration(registry, metadata, attributes);\n}", "CodeBleu_score": 49.5130914070989}]}
{"instance_id": "dbc92d8f146f", "generation_result": [{"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n\tif (loadBalancer == null) {\n\t\treturn null;\n\t}\n\tServer server = loadBalancer.chooseServer(serviceId);\n\tif (server == null) {\n\t\treturn null;\n\t}\n\treturn new TutuServiceInstance(server.getHost(), server.getPort(), serviceId);\n}", "CodeBleu_score": 48.90849950163318}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n    Server server = loadBalancer.chooseServer();\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort());\n}", "CodeBleu_score": 62.14708396883941}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n    Server server = loadBalancer.chooseServer(null);\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort());\n}", "CodeBleu_score": 63.24428230851468}]}
{"instance_id": "362eadbd3c5e", "generation_result": [{"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\tString scheme = original.getScheme();\n\tString host = server.getHost();\n\tint port = server.getPort();\n\tString path = original.getPath();\n\tString query = original.getQuery();\n\tString fragment = original.getFragment();\n\n\ttry {\n\t\treturn new URI(scheme, null, host, port, path, query, fragment);\n\t} catch (URISyntaxException e) {\n\t\tthrow new RuntimeException(e);\n\t}\n}", "CodeBleu_score": 36.00213663434012}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n        String scheme = original.getScheme();\n        if (StrUtil.isBlank(scheme)) {\n            scheme = \"http\";\n        }\n        String query = original.getQuery();\n        if (StrUtil.isNotBlank(query)) {\n            query = \"?\" + query;\n        }\n        return new URI(scheme, null, server.getHost(), server.getPort(), original.getPath(), null, query);\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 40.96542657644009}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\tString host = server.getHost();\n    int port = server.getPort();\n    String path = original.getPath();\n    String query = original.getQuery();\n    String scheme = original.getScheme();\n    StringBuilder sb = new StringBuilder();\n    sb.append(scheme).append(\"://\").append(host);\n    if (port != -1 && port != 80) {\n        sb.append(\":\").append(port);\n    }\n    if (StrUtil.isNotBlank(path)) {\n        sb.append(path);\n    }\n    if (StrUtil.isNotBlank(query)) {\n        sb.append(\"?\").append(query);\n    }\n    try {\n        return new URI(sb.toString());\n    }\n    catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 56.42823684833933}]}
{"instance_id": "6801d2b2e4c8", "generation_result": [{"code": "private List<TutuServer> getServer(){\n\t// \u6784\u5efa\u8bf7\u6c42\u53c2\u6570\n    Map<String, Object> param = new HashMap<>();\n    param.put(\"serviceName\", serviceId);\n    // \u67e5\u8be2\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    String result = HttpUtil.get(discoveryProperties.getServerAddr() + \"/list\", param);\n    // \u89e3\u6790\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    List<TutuServer> tutuServers = JSON.parseArray(result, TutuServer.class);\n    // \u6253\u5370\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    logger.info(\"{} instance list: {}\", serviceId, tutuServers.stream().map(TutuServer::toString).collect(Collectors.joining(\",\")));\n    return tutuServers;\n}", "CodeBleu_score": 53.31288286620318}, {"code": "private List<TutuServer> getServer(){\n\tString url = discoveryProperties.getServerAddr() + \"/list?serviceName=\" + serviceId;\n    String response = HttpUtil.get(url);\n    JSONObject jsonObject = JSON.parseObject(response);\n    List<JSONObject> serverList = jsonObject.getJSONArray(\"serverList\").toJavaList(JSONObject.class);\n    List<TutuServer> tutuServerList = serverList.stream().map(server -> {\n        String ip = server.getString(\"ip\");\n        int port = server.getInteger(\"port\");\n        TutuServer tutuServer = new TutuServer(ip, port);\n        Map<String, String> metadata = new HashMap<>();\n        metadata.put(\"server\", ip + \":\" + port);\n        metadata.put(\"serviceId\", serviceId);\n        metadata.put(\"serviceName\", serviceId);\n        metadata.put(\"serviceAddr\", discoveryProperties.getServerAddr());\n        metadata.put(\"serviceType\", \"TUTU\");\n        tutuServer.setMetadata(metadata);\n        return tutuServer;\n    }).collect(Collectors.toList());\n    return tutuServerList;\n}", "CodeBleu_score": 45.08738233999359}, {"code": "private List<TutuServer> getServer(){\n\tList<TutuServer> tutuServers = null;\n    try {\n        String url = discoveryProperties.getServerAddr() + \"/list?serviceName=\" + serviceId;\n        String response = HttpUtil.get(url);\n        JSONObject jsonObject = JSON.parseObject(response);\n        if (jsonObject.getInteger(\"code\") == 200) {\n            List<JSONObject> data = jsonObject.getJSONArray(\"data\").toJavaList(JSONObject.class);\n            tutuServers = data.stream().map(json -> {\n                String ip = json.getString(\"ip\");\n                int port = json.getInteger(\"port\");\n                return new TutuServer(ip, port);\n            }).collect(Collectors.toList());\n        }\n    } catch (Exception e) {\n        logger.error(\"getServer error\", e);\n    }\n    return tutuServers;\n}", "CodeBleu_score": 45.74326196304139}]}
{"instance_id": "ab2fb77484fd", "generation_result": [{"code": "public Route getMatchingRoute(String path){\n\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n        String pattern = entry.getKey();\n        ZuulProperties.ZuulRoute route = entry.getValue();\n        if (pathMatcher.match(pattern, path)) {\n            String targetPath = path.replaceFirst(pattern, route.getPath());\n            return new Route(route.getId(), targetPath, route.getServiceId(), route.isStripPrefix());\n        }\n    }\n    return null;\n}", "CodeBleu_score": 64.63113071783432}, {"code": "public Route getMatchingRoute(String path){\n\tfor (String routePath : this.zuulProperties.getRoutes().keySet()) {\n        if (this.pathMatcher.match(routePath, path)) {\n            String targetPath = this.pathMatcher.extractPathWithinPattern(routePath, path);\n            String serviceId = this.zuulProperties.getRoutes().get(routePath).getServiceId();\n            return new Route(routePath, serviceId, targetPath, this.zuulProperties.isStripPrefix());\n        }\n    }\n    return null;\n}", "CodeBleu_score": 36.0215275558823}, {"code": "public Route getMatchingRoute(String path){\n\tfor (String routePath : zuulProperties.getRoutes().keySet()) {\n        if (pathMatcher.match(routePath, path)) {\n            String targetPath = path.replaceFirst(routePath, \"\");\n            return new Route(zuulProperties.getRoutes().get(routePath), targetPath);\n        }\n    }\n    return null;\n}", "CodeBleu_score": 38.822499383703665}]}
{"instance_id": "822071925513", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tClassPathFeignClientsScanner scanner = new ClassPathFeignClientsScanner(registry);\n    // \u8bbe\u7f6e\u626b\u63cf\u7684\u57fa\u7840\u5305\n    scanner.setResourceLoader(this.resourceLoader);\n    scanner.setEnvironment(this.environment);\n    scanner.registerFilters();\n    // \u626b\u63cf @FeignClient \u6ce8\u89e3\n    Set<String> basePackages = getBasePackages(importingClassMetadata);\n    for (String basePackage : basePackages) {\n        scanner.doScan(basePackage);\n    }\n}", "CodeBleu_score": 14.99393936044523}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\t// \u83b7\u53d6@EnableFeignClients\u6ce8\u89e3\u7684\u7c7b\n\tString className = importingClassMetadata.getClassName();\n\t// \u83b7\u53d6@EnableFeignClients\u6ce8\u89e3\u7684\u7c7b\u6240\u5728\u7684\u5305\n\tString packageName = ClassUtils.getPackageName(className);\n\t// \u83b7\u53d6@EnableFeignClients\u6ce8\u89e3\u7684\u7c7b\u6240\u5728\u7684\u5305\u4e0b\u6240\u6709\u7684\u7c7b\n\tSet<Class<?>> classes = ClassUtil.scanPackageByAnnotation(packageName, FeignClient.class);\n\tfor (Class<?> clazz : classes) {\n\t\t// \u521b\u5efaGenericBeanDefinition\u5bf9\u8c61\n\t\tGenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n\t\t// \u8bbe\u7f6eFeign\u5ba2\u6237\u7aef\u5de5\u5382\u7c7b\n\t\tbeanDefinition.setBeanClass(FeignClientFactoryBean.class);\n\t\t// \u8bbe\u7f6eFeign\u5ba2\u6237\u7aef\u7684\u63a5\u53e3\n\t\tbeanDefinition.getPropertyValues().add(\"type\", clazz);\n\t\t// \u8bbe\u7f6eFeign\u5ba2\u6237\u7aef\u7684\u540d\u79f0\n\t\tbeanDefinition.getPropertyValues().add(\"contextId\", clazz.getName());\n\t\t// \u5c06Feign\u5ba2\u6237\u7aef\u7684\u540d\u79f0\u4f5c\u4e3abean\u7684\u540d\u79f0\u6ce8\u518c\u5230bean\u5bb9\u5668\u4e2d\n\t\tregistry.registerBeanDefinition(clazz.getName(), beanDefinition);\n\t}\n}", "CodeBleu_score": 65.39333816437897}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tClassPathFeignClientScanner scanner = new ClassPathFeignClientScanner(registry, new FeignClientFactoryBean());\n    // \u8bbe\u7f6e\u626b\u63cf\u7684\u57fa\u7840\u5305\n    scanner.setResourceLoader(this.resourceLoader);\n    scanner.setEnvironment(this.environment);\n    scanner.registerFilters();\n    // \u626b\u63cf\u6307\u5b9a\u5305\u4e0b\u7684\u6240\u6709\u7c7b\n    Set<BeanDefinitionHolder> beanDefinitions = scanner.doScan(this.basePackages, this.basePackageClasses);\n    for (BeanDefinitionHolder definitionHolder : beanDefinitions) {\n        GenericBeanDefinition definition = (GenericBeanDefinition) definitionHolder.getBeanDefinition();\n        // \u83b7\u53d6\u6ce8\u89e3\u4e0a\u7684\u914d\u7f6e\n        String name = definition.getBeanClassName();\n        FeignClientFactoryBean feignClientFactoryBean = new FeignClientFactoryBean();\n        feignClientFactoryBean.setName(name);\n        feignClientFactoryBean.setType(ClassUtils.getUserClass(ClassUtils.resolveClassName(name, this.beanClassLoader)));\n        feignClientFactoryBean.setContextId(name);\n        // \u5c06FeignClientFactoryBean\u6ce8\u5165\u5230bean\u5bb9\u5668\u4e2d\n        registry.registerBeanDefinition(name, definition);\n    }\n}", "CodeBleu_score": 38.29135214292824}]}
{"instance_id": "3d678e3bf3c8", "generation_result": [{"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}]}
