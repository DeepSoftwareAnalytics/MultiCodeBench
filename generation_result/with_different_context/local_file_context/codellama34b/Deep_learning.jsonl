{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    if cached_ldsr_model is not None:\n        print(\"Using cached model\")\n        model = cached_ldsr_model\n    else:\n        # Load model from path\n        print(\"Loading model from path\")\n        model = instantiate_from_config(self.config.model)\n        # Load state dict from path\n        sd_path = self.config.model.sd_path\n        if sd_path is not None:\n            sd_path = os.path.expanduser(sd_path)\n            if sd_path.endswith(\".safetensors\"):\n                sd_hijack.load_safetensors(model, sd_path)\n            else:\n                sd = torch.load(sd_path, map_location=\"cpu\")\n                if \"state_dict\" in sd:\n                    sd = sd[\"state_dict\"]\n                if ismap(sd):\n                    model.load_state_dict(sd)\n                else:\n                    model.load_state_dict(sd[\"model\"])\n        # Move model to device\n        model = model.to(self.device)\n        # Configure model\n        OmegaConf.set_struct(self.config.model, False)\n        self.config.model.device = self.device\n        self.config.model.half_attention = half_attention\n        self.config.model.memory_format = self.config.memory_format\n        OmegaConf.set_struct(self.config.model, True)\n        model.eval()\n        # Cache model\n        if self.config.cache_model:\n            cached_ldsr_model = model\n    return {\"model\": model}\n\n", "CodeBleu_score": 45.61773342960778}, {"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    if self.config.cached_model_path is not None and os.path.exists(self.config.cached_model_path):\n        # Load the cached model\n        print(f'Loading cached model from {self.config.cached_model_path}')\n        model = torch.load(self.config.cached_model_path, map_location=devices.get_device())\n        # Check if the model is valid\n        if model.config.model_id == self.config.model_id:\n            # Set the model to evaluation mode\n            model.eval()\n            # Move the model to the appropriate device\n            model = model.to(devices.get_device())\n            # Optionally use half-precision attention\n            if half_attention:\n                model.use_half_attention()\n            # Optionally use memory format optimization\n            if self.config.memory_format_optimization:\n                model.use_memory_format_optimization()\n            # Cache the model for future use\n            if self.config.cache_model:\n                global cached_ldsr_model\n                cached_ldsr_model = model\n            # Return the model\n            return model\n        else:\n            print(f'Cached model is invalid, loading from {self.config.model_path}')\n    # Load the model from the specified path\n    if self.config.model_path.endswith('.safetensors'):\n        # Load the model from a safetensors file\n        print(f'Loading model from {self.config.model_path}')\n        model = torch.load(self.config.model_path, map_location=devices.get_device())\n    else:\n        # Load the model from a PyTorch-compatible file\n        print(f'Loading model from {self.config.model_path}')\n        model = torch.load(self.config.model_path, map_location=devices.get_device())\n    # Set the model to evaluation mode\n    model.eval()\n    # Move the model to the appropriate device\n    model = model.to(devices.get_device", "CodeBleu_score": 34.519106472024305}, {"code": "def load_model_from_config(self, half_attention)\n\t# Load model from cache if available\n    if cached_ldsr_model is not None:\n        model = cached_ldsr_model\n    else:\n        # Load model from path\n        model_path = os.path.join(self.model_path, self.model_name)\n        if model_path.endswith('.safetensors'):\n            model = torch.load(model_path, map_location=devices.get_device())\n        else:\n            model = torch.load(model_path, map_location=devices.get_device())\n            model.eval()\n            if half_attention:\n                model = model.half()\n            if self.memory_format_opt:\n                model = model.to_memory_format(torch.channels_last)\n        # Cache model\n        if self.cache_model:\n            cached_ldsr_model = model\n    return model\n\n", "CodeBleu_score": 34.101062784028024}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            print(f\"{context}: Using EMA weights.\")\n        self.model_ema.swap_parameters()\n    yield\n    if self.use_ema:\n        if context is not None:\n            print(f\"{context}: Using regular (non-EMA) weights.\")\n        self.model_ema.swap_parameters()\n\n", "CodeBleu_score": 58.5404339604906}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            print(f\"{context}: Using EMA weights.\")\n        weights = self.model_ema.average_parameters()\n        if context is not None:\n            print(f\"{context}: Restoring model weights.\")\n        self.model_ema.restore_parameters()\n        try:\n            yield\n        finally:\n            if context is not None:\n                print(f\"{context}: Restoring EMA weights.\")\n            self.model_ema.store_parameters()\n            self.model_ema.copy_parameters(weights)\n    else:\n        yield\n\n", "CodeBleu_score": 62.15778497946561}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            print(f\"{context}: Using EMA weights.\")\n        self.model_ema.swap_parameters()\n        yield\n        self.model_ema.swap_parameters()\n        if context is not None:\n            print(f\"{context}: Using regular (non-EMA) weights.\")\n    else:\n        yield\n\n", "CodeBleu_score": 54.351954488761365}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path, map_location=\\\"cpu\\\")[\\\"state_dict\\\"]\n    if ignore_keys is not None:\n        sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    print(f\\\"Restored from {path}\\\")\n    if len(missing) > 0:\n        print(f\\\"Missing keys when loading {path}: {missing}\\\")\n    if len(unexpected) > 0:\n        print(f\\\"Unexpected keys when loading {path}: {unexpected}\\\")\n\n", "CodeBleu_score": 35.93917904902143}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path, map_location=\\\"cpu\\\")\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    print(f\\\"Restored from {path}\\\")\n    if len(missing) > 0:\n        print(f\\\"Missing keys when restoring model from {path}: {missing}\\\")\n    if len(unexpected) > 0:\n        print(f\\\"Unexpected keys when restoring model from {path}: {unexpected}\\\")\n\n", "CodeBleu_score": 36.47620874382732}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path, map_location=\\\"cpu\\\")\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    print(f\\\"Restored from {path} with missing keys {missing_keys} and unexpected keys {unexpected_keys}.\\\")\n\n", "CodeBleu_score": 24.116961190152036}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\tif self.batch_resize_range is not None:\n        new_size = np.random.randint(self.batch_resize_range[0], self.batch_resize_range[1])\n        input = F.interpolate(input, size=new_size, mode='bilinear', align_corners=False)\n\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n\n    if self.monitor is not None:\n        with torch.no_grad():\n            self.monitor.update(diff.detach())\n\n    if return_pred_indices:\n        return dec, diff, quant\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 49.66102658144661}, {"code": "def forward(self, input, return_pred_indices=False)\n\twith self.ema_scope(context=\\\"forward\\\"):\n        if self.training and self.batch_resize_range is not None:\n            resize_amount = np.random.randint(*self.batch_resize_range)\n        else:\n            resize_amount = 0\n        if resize_amount != 0:\n            input = F.interpolate(input, scale_factor=1+resize_amount, mode=\\\"bilinear\\\", align_corners=False)\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        if self.monitor is not None:\n            with torch.no_grad():\n                self.monitor.update(diff.detach())\n        if return_pred_indices:\n            _, pred_indices = self.quantize.embed_code(quant)\n            return dec, diff, pred_indices\n        else:\n            return dec, diff\n\n", "CodeBleu_score": 43.33912450433688}, {"code": "def forward(self, input, return_pred_indices=False)\n\twith self.ema_scope() as ema:\n        if self.training and self.batch_resize_range is not None:\n            resize_amt = int(torch.randint(self.batch_resize_range[0], self.batch_resize_range[1], (1,)).item())\n            input = F.interpolate(input, (input.shape[2] // resize_amt, input.shape[3] // resize_amt), mode=\\\"bilinear\\\")\n        quant, diff, (_, _, indices) = self.encode(input)\n        dec = self.decode(quant)\n        if self.training and self.monitor is not None:\n            assert self.monitor in {\\\"kl\\\", \\\"l2\\\"}\n            if self.monitor == \\\"kl\\\":\n                # compute KL between approximate posterior and N(0, I)\n                kl = -0.5 * torch.sum(1 + 2 * torch.log(torch.abs(diff)) - diff.pow(2) - diff.pow(2).mean(dim=1).mean(dim=1), dim=1)\n                kl = kl.mean()\n                self.log(f\\\"{self.monitor}_div\\\", kl, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n            elif self.monitor == \\\"l2\\\":\n                l2 = ((input - dec) ** 2).mean()\n                self.log(f\\\"{self.monitor}_loss\\\", l2, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n        if self.monitor == \\\"l2\\\":\n            return dec, diff, l2\n        else:\n            return dec, diff, kl\n    else:\n        return dec, diff, None\n\n", "CodeBleu_score": 36.39401035685369}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    assert x.shape[1] == 3\n    if self.batch_resize_range is not None:\n        assert len(x.shape) == 4\n        size = random.randint(*self.batch_resize_range)\n        assert size >= x.shape[2]\n        x = F.interpolate(x, size, mode=\\\"bicubic\\\", align_corners=False)\n    x = x.permute(0, 2, 3, 1).contiguous()\n    x = x.to(memory_format=torch.contiguous_format).float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 41.89462421529919}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if x.ndim == 2:\n        x = x[..., None]\n    if x.ndim == 3:\n        x = x.transpose(2, 0, 1)\n    if x.ndim == 4:\n        x = x.contiguous()\n    else:\n        raise ValueError(f'Wrong input dimensions: {x.ndim}')\n    if self.batch_resize_range is not None:\n        size = x.shape[-1]\n        assert size == x.shape[-2], 'Input is not square.'\n        resize_factor = self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * np.cos(\n            np.pi * self.global_step / self.trainer.max_steps)\n        new_size = int(size * resize_factor)\n        x = F.interpolate(x, (new_size, new_size), mode='bicubic', align_corners=True)\n    return x.detach()\n\n", "CodeBleu_score": 47.636257558451696}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.batch_resize_range is not None:\n        curr_bs = x.shape[0]\n        curr_step = self.trainer.global_step\n        curr_total_steps = self.trainer.max_steps\n        resize_factor = self.batch_resize_range[1] - self.batch_resize_range[0]\n        curr_resize = (resize_factor * curr_step / curr_total_steps) + self.batch_resize_range[0]\n        x = F.interpolate(x, scale_factor=curr_resize, mode=\\\"bicubic\\\", align_corners=False)\n        print(f\\\"Resized {k} from {curr_bs} to {x.shape[0]}\\\")\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 55.64597946171691}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n\n    if optimizer_idx == 0:\n        # autoencoder\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                        return_pred_indices=True)\n\n        # VQ\n        if self.model_type == \\\"vqvae\\\":\n            _, _, [_, _, ind], _ = self.encode(x)\n            ind_loss = (ind.flatten(1).float().mean(1) * qloss).mean()\n            aeloss = aeloss + ind_loss\n            log_dict_ae[\\\"ind_loss\\\"] = ind_loss\n\n        # log\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n\n    if optimizer_idx == 1:\n        # discriminator\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return discloss\n\n", "CodeBleu_score": 64.05420414105922}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    if optimizer_idx == 0:\n        # autoencoder\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                        n_batches=len(self.trainer.train_dataloader()))\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n\n    if optimizer_idx == 1:\n        # discriminator\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            n_batches=len(self.trainer.train_dataloader()))\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return discloss\n\n", "CodeBleu_score": 66.91145078064802}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n\n    if optimizer_idx == 0:\n        # autoencoding loss\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                        use_ema=self.use_ema)\n\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        self.log(\\\"loss_ae\\\", aeloss, prog_bar=False, logger=True, on_step=True, on_epoch=True, sync_dist=True)\n        return aeloss\n\n    if optimizer_idx == 1:\n        # discriminator loss\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            use_ema=self.use_ema)\n\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        self.log(\\\"loss_disc\\\", discloss, prog_bar=False, logger=True, on_step=True, on_epoch=True, sync_dist=True)\n        return discloss\n\n", "CodeBleu_score": 61.690040023508466}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        # find the factorization that minimizes the sum of the factors\n        m, n = 1, dimension\n        while n > 1:\n            if dimension % n == 0:\n                m, n = n, dimension // n\n            else:\n                n -= 1\n        if m > n:\n            m, n = n, m\n    else:\n        # use the specified factor\n        if dimension % factor == 0:\n            m, n = factor, dimension // factor\n        else:\n            raise ValueError(\"The specified factor does not divide the dimension.\")\n    return m, n\n\n", "CodeBleu_score": 42.531389730178795}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        m = 1\n        n = dimension\n        while n > 1:\n            n, r = divmod(n, 2)\n            if r == 0:\n                m *= 2\n            elif factor > 0 and m * 2 > factor:\n                break\n    else:\n        m = dimension\n        n = 1\n    return m, n\n\n", "CodeBleu_score": 38.720360460672374}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor < 0:\n            factor = abs(factor)\n        m = 1\n        n = dimension\n        while m < n:\n            if n % m == 0:\n                if m <= factor and n // m <= factor:\n                    return m, n // m\n                elif m <= factor:\n                    n = n // m\n                elif n // m <= factor:\n                    m = m * 2\n                else:\n                    m = m * 2\n                    n = n // m\n            else:\n                m = m + 1\n        return m, n // m\n\n", "CodeBleu_score": 39.29189632594815}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif updown == 'up':\n        weight = self.weight\n    elif updown == 'down':\n        weight = self.weight_orig\n    else:\n        raise ValueError(f'updown must be either \"up\" or \"down\", but got {updown}')\n\n    if weight.device != orig_weight.device or weight.dtype != orig_weight.dtype:\n        orig_weight = orig_weight.to(weight.device).type(weight.dtype)\n\n    weight = weight + orig_weight\n    weight = F.normalize(weight, dim=1)\n    weight = weight * self.scale\n    weight = weight - orig_weight\n\n    return weight\n\n", "CodeBleu_score": 28.688002121945892}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif self.dora_scale is None:\n        return orig_weight\n\n    if self.dora_scale == 0:\n        return orig_weight\n\n    if self.dora_scale == 1:\n        return orig_weight\n\n    if self.dora_scale == 2:\n        return orig_weight\n\n    if self.dora_scale == 3:\n        return orig_weight\n\n    if self.dora_scale == 4:\n        return orig_weight\n\n    if self.dora_scale == 5:\n        return orig_weight\n\n    if self.dora_scale == 6:\n        return orig_weight\n\n    if self.dora_scale == 7:\n        return orig_weight\n\n    if self.dora_scale == 8:\n        return orig_weight\n\n    if self.dora_scale == 9:\n        return orig_weight\n\n    if self.dora_scale == 10:\n        return orig_weight\n\n    if self.dora_scale == 11:\n        return orig_weight\n\n    if self.dora_scale == 12:\n        return orig_weight\n\n    if self.dora_scale == 13:\n        return orig_weight\n\n    if self.dora_scale == 14:\n        return orig_weight\n\n    if self.dora_scale == 15:\n        return orig_weight\n\n    if self.dora_scale == 16:\n        return orig_weight\n\n    if self.dora_scale == 17:\n        return orig_weight\n\n    if self.dora_scale == 18:\n        return orig_weight\n\n    if self.dora_scale == 19:\n        return orig_weight\n\n    if self.dora_scale == 20:\n        return orig_weight\n\n    if self.dora_scale == 21:\n        return orig_weight\n\n    if self.dora_scale == 22:\n        return orig_weight\n\n    if self.dora_scale == 23:\n        return orig_weight", "CodeBleu_score": 11.71587887596018}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif self.dora_scale is None:\n        return orig_weight\n\n    if self.dora_scale == 0:\n        return orig_weight\n\n    if self.dora_scale == 1:\n        return orig_weight\n\n    if self.dora_scale == 2:\n        return orig_weight\n\n    if self.dora_scale == 3:\n        return orig_weight\n\n    if self.dora_scale == 4:\n        return orig_weight\n\n    if self.dora_scale == 5:\n        return orig_weight\n\n    if self.dora_scale == 6:\n        return orig_weight\n\n    if self.dora_scale == 7:\n        return orig_weight\n\n    if self.dora_scale == 8:\n        return orig_weight\n\n    if self.dora_scale == 9:\n        return orig_weight\n\n    if self.dora_scale == 10:\n        return orig_weight\n\n    if self.dora_scale == 11:\n        return orig_weight\n\n    if self.dora_scale == 12:\n        return orig_weight\n\n    if self.dora_scale == 13:\n        return orig_weight\n\n    if self.dora_scale == 14:\n        return orig_weight\n\n    if self.dora_scale == 15:\n        return orig_weight\n\n    if self.dora_scale == 16:\n        return orig_weight\n\n    if self.dora_scale == 17:\n        return orig_weight\n\n    if self.dora_scale == 18:\n        return orig_weight\n\n    if self.dora_scale == 19:\n        return orig_weight\n\n    if self.dora_scale == 20:\n        return orig_weight\n\n    if self.dora_scale == 21:\n        return orig_weight\n\n    if self.dora_scale == 22:\n        return orig_weight\n\n    if self.dora_scale == 23:\n        return orig_weight", "CodeBleu_score": 11.71587887596018}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.588275561456065}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.588275561456065}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.588275561456065}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\tweight = self.weight.to(orig_weight.device)\n    ex_bias = self.ex_bias.to(orig_weight.device) if self.ex_bias is not None else None\n    return self.calc_updown_inner(orig_weight, weight, ex_bias)\n\n", "CodeBleu_score": 30.106461408784806}, {"code": "def calc_updown(self, orig_weight)\n\tif self.ex_bias is not None:\n        self.ex_bias = self.ex_bias.to(orig_weight.device)\n    self.weight = self.weight.to(orig_weight.device)\n    return self.calc_updown_core(orig_weight)\n\n", "CodeBleu_score": 41.605378988015275}, {"code": "def calc_updown(self, orig_weight)\n\tif self.weight is None:\n        return orig_weight\n    if self.ex_bias is None:\n        self.ex_bias = torch.zeros(orig_weight.shape[0], device=orig_weight.device)\n    self.weight = self.weight.to(orig_weight.device)\n    self.ex_bias = self.ex_bias.to(orig_weight.device)\n    return self.calc_updown_inner(orig_weight)\n\n", "CodeBleu_score": 42.733412703060615}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\treturn [x for x in self.parameters() if x.requires_grad]\n\n", "CodeBleu_score": 6.191786672500702}, {"code": "def trainables(self)\n\treturn [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.194787406674239}, {"code": "def trainables(self)\n\treturn [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.194787406674239}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 19.105375929763866}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 19.105375929763866}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 19.105375929763866}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 17.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 17.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 17.069674113110185}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\twith cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, disk_min_file_size=256 * 1024)\n        return caches[subsection]\n\n", "CodeBleu_score": 55.99384779446477}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_path = os.path.join(cache_dir, subsection)\n    if not os.path.exists(cache_path):\n        os.makedirs(cache_path)\n    return diskcache.Cache(cache_path, diskcache.JSONDisk, size_limit=4 * 1024 * 1024 * 1024, min_file_size=256 * 1024)\n\n", "CodeBleu_score": 52.1276479061771}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_path = os.path.join(cache_dir, subsection)\n    c = diskcache.Cache(cache_path, diskcache.DEFAULT_SIZE, diskcache.DEFAULT_COUNT, diskcache.DEFAULT_MIN_FILE_SIZE)\n    return c\n\n", "CodeBleu_score": 43.87686706918947}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.05913597407847}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 67.05913597407847}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj", "CodeBleu_score": 67.05913597407847}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime, data = cache_obj[title]\n        if mtime > cached_mtime:\n            del cache_obj[title]\n            cache_obj[title] = mtime, func()\n            dump_cache()\n    else:\n        data = func()\n        cache_obj[title] = mtime, data\n        dump_cache()\n    return data\n\n", "CodeBleu_score": 22.081261251069286}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj and cache_obj[title][0] >= mtime:\n        return cache_obj[title][1]\n    data = func()\n    cache_obj[title] = (mtime, data)\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 11.182187038162029}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime, data = cache_obj[title]\n        if cached_mtime < mtime:\n            del cache_obj[title]\n        else:\n            return data\n    data = func()\n    cache_obj[title] = (mtime, data)\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 21.03954846612954}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in format_exception(*sys.exc_info())[\"traceback\"]:\n            print(\"    \", *line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 62.925091363453724}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 73.7159209257337}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in traceback.format_exc().splitlines():\n            print(\"    \", line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 63.89181240157289}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        success, negative_prompt = extract_style_text_from_prompt(\n            style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 65.04567035902238}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n    elif style.prompt is None:\n        success, negative_prompt = extract_style_text_from_prompt(\n            style.negative_prompt, negative_prompt\n        )\n        return success, prompt, negative_prompt\n    elif style.negative_prompt is None:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        return success, prompt, negative_prompt\n    else:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n        success, negative_prompt = extract_style_text_from_prompt(\n            style.negative_prompt, negative_prompt\n        )\n        if not success:\n            return False, prompt, negative_prompt\n        return True, prompt, negative_prompt\n\n", "CodeBleu_score": 59.26567008817999}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 66.93784359600453}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 49.74640122118473}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]", "CodeBleu_score": 49.74640122118473}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 49.74640122118473}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n    change_db = target_dBFS - 20 * np.log10(rms / int16_max)\n    if (change_db < 0 and increase_only) or (change_db > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_db / 20))\n\n", "CodeBleu_score": 69.12986220124291}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n    change_db = target_dBFS - 20 * np.log10(rms / int16_max)\n    if (change_db < 0 and increase_only) or (change_db > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_db / 20))\n\n", "CodeBleu_score": 69.12986220124291}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n    change_in_dBFS = target_dBFS - 20 * np.log10(rms / int16_max)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 68.6639206854843}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model, _device\n    _device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=_device)\n    checkpoint = torch.load(weights_fpath, map_location=_device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded pre-trained model \\\"%s\\\" trained to step %d.\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 44.54190374525114}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model, _device\n    _device = device if device is not None else torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", _device)\n    _model = SpeakerEncoder(device=_device).to(_device)\n    checkpoint = torch.load(weights_fpath, map_location=_device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded step {} from {}\".format(checkpoint[\"step\"], weights_fpath))\n\n", "CodeBleu_score": 42.89314540352712}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model\n    global _device\n\n    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n    _model = SpeakerEncoder(device=_device)\n    checkpoint = torch.load(weights_fpath, map_location=_device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded speaker encoder \\\"{}\\\" trained to step {}\".format(\n        weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 54.18748975113039}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\tif _model is None:\n        raise Exception(\\\"The model must be loaded before inference can be performed, please call load_model().\\\")\n    frames_batch = torch.from_numpy(frames_batch).float().to(_device)\n    embeds_batch = _model.embed_frames(frames_batch).detach().cpu().numpy()\n    return embeds_batch\n\n", "CodeBleu_score": 58.70958942825395}, {"code": "def embed_frames_batch(frames_batch)\n\tif not is_loaded():\n        raise Exception(\\\"The model must be loaded before inference can be performed. Call load_model().\\\")\n    frames_batch = torch.from_numpy(frames_batch).float()\n    frames_batch = frames_batch.to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 47.5600454192318}, {"code": "def embed_frames_batch(frames_batch)\n\tif not is_loaded():\n        raise Exception(\\\"The model must be loaded before inference can be performed. Call load_model() before calling embed_frames().\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds_batch = _model(frames_batch).detach().cpu().numpy()\n    return embeds_batch\n\n", "CodeBleu_score": 51.076818930731825}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n\n    # Compute how many frames separate two partial utterances\n    frames_per_partial = (sampling_rate * partial_utterance_n_frames) // hop_length\n    frames_per_hop = (sampling_rate * hop_length_seconds) // hop_length\n    assert frames_per_hop > 0\n\n    # Compute how many partials we can expect from the audio\n    n_partials = int(np.ceil(n_samples / (frames_per_partial * frame_shift_ms / 1000)))\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    for i in range(n_partials):\n        # Compute where the wav and mel starts and ends\n        # for the current partial utterance\n        start = i * frames_per_partial\n        end = (i + 1) * frames_per_partial\n        mel_range = np.array([start, end]) * frame_shift_ms / 1000\n        wav_range = mel_range * hop_length_seconds * sampling_rate\n\n        # Prepare the wav slice\n        wav_start = int(wav_range[0])\n        wav_end = int(wav_range[1])\n        assert len(wav_range) == 2\n        assert wav_end - wav_start >= 0\n        wav_slice = (wav_start, wav_end)\n\n        # Prepare the mel slice\n        mel_start = int(mel_range[0])\n        mel_end = int(mel_range[1])\n        assert len(mel_range) == 2\n        assert mel_end - mel_start >= 0\n        mel_slice = (mel_start, mel_end)\n\n        # Store this partial\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n\n    # Check that the number of frames matches\n    assert len(wav_slices) ==", "CodeBleu_score": 36.062520713233205}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n\n    if partial_utterance_n_frames > n_samples:\n        raise ValueError(\"Can\\\"t have more partial utterances than entire utterance\")\n\n    # Compute how many frames separate two partial utterances\n    samples_per_frame = int((sampling_rate * frame_length_ms) / 1000)\n    frame_step = int(samples_per_frame * (1 - overlap))\n\n    # Compute how many partial utterances we can expect from the audio\n    n_frames = int(np.ceil((n_samples - partial_utterance_n_frames) / frame_step + 1))\n\n    # Compute the slices\n    wav_slices = [(i * frame_step, i * frame_step + partial_utterance_n_frames)\n                  for i in range(n_frames)]\n    mel_slices = [(i * frame_step // samples_per_frame,\n                   i * frame_step // samples_per_frame + partial_utterance_n_frames // samples_per_frame)\n                  for i in range(n_frames)]\n\n    # Evaluate whether extra padding is necessary\n    last_wav_slice = wav_slices[-1]\n    coverage = (n_samples - last_wav_slice[0]) / (last_wav_slice[1] - last_wav_slice[0])\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 46.25241641339546}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 < overlap < 1.0\n    assert 0.0 <= min_pad_coverage <= 1.0\n    assert partial_utterance_n_frames % 2 == 1\n\n    frame_step = int((1.0 - overlap) * sampling_rate // hop_length)\n    n_frames = int(np.ceil(n_samples / hop_length))\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(np.round(n_frames / partial_utterance_n_frames)))\n    for i in range(steps):\n        start_index = int(np.round(i * partial_utterance_n_frames * frame_step))\n        end_index = min(start_index + partial_utterance_n_frames * frame_step, n_samples)\n        if end_index == n_samples:\n            pad_left = pad_right = 0\n        else:\n            # Pad the remaining samples\n            pad_left = int((end_index - n_samples) // 2)\n            pad_right = n_samples - end_index - pad_left\n\n        wav_slices.append((max(0, start_index - pad_left),\n                           min(n_samples, end_index + pad_right)))\n\n        mel_slices.append((max(0, start_index - pad_left),\n                           min(n_samples, end_index + pad_right)))\n\n    # Evaluate whether extra padding is necessary\n    if len(mel_slices) > 1 and mel_slices[-1][1] - mel_slices[-1][0] < min_pad_coverage * n_samples:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 39.09144187600786}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\tn_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Compute the centroids of the current embeddings for each speaker\n    centroids_incl = torch.zeros(n_speakers, n_utterances, model_embedding_size).to(self.loss_device)\n    centroids_excl = torch.zeros(n_speakers, model_embedding_size).to(self.loss_device)\n    for i in range(n_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_excl[i] = torch.mean(embeds[i], dim=0)\n\n    # Compute similarities\n    similarities = torch.zeros(n_speakers, n_utterances, n_utterances).to(self.loss_device)\n    for i in range(n_speakers):\n        centroid_incl = centroids_incl[i]\n        centroid_excl = centroids_excl[i]\n        for j in range(n_utterances):\n            similarities[i, j] = 1 - torch.cosine_similarity(embeds[i, j], centroid_incl[j], dim=0)\n            similarities[i, j] += 1 - torch.cosine_similarity(embeds[i, j], centroid_excl, dim=0)\n\n    # Mask out the similarity of an embedding to itself\n    mask = np.ones((n_utterances, n_utterances), dtype=np.float32)\n    mask = np.triu(mask, 1)  # Set upper triangle to zeros\n    mask = torch.from_numpy(mask).to(self.loss_device)\n    similarities = similarities * mask\n\n    # Scale and bias the similarities\n    similarities = (similarities + 1) / 2  # Scale to 0..1\n    similarities = similarities * self.similarity_weight +", "CodeBleu_score": 34.55513528005804}, {"code": "def similarity_matrix(self, embeds)\n\t# Compute the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the centroids for each utterance\n    centroids_incl = torch.zeros(num_speakers, num_utterances, model_embedding_size).to(self.loss_device)\n    centroids_excl = torch.zeros(num_speakers, num_utterances, model_embedding_size).to(self.loss_device)\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            centroids_incl[i, j, :] = torch.mean(embeds[i, :j+1, :], dim=0)\n            centroids_excl[i, j, :] = torch.mean(embeds[i, :j, :], dim=0)\n\n    # Compute the similarity matrix\n    similarity_matrix = torch.zeros(num_speakers, num_utterances, num_utterances).to(self.loss_device)\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            # Compute the similarity scores for the current utterance\n            similarity_scores = torch.nn.functional.cosine_similarity(embeds[i, j, :].unsqueeze(0), centroids_incl[i, :j+1, :], dim=1)\n            # Update the similarity matrix with the similarity scores\n            similarity_matrix[i, j, :j+1] = similarity_scores\n\n    # Mask the similarity scores of the current utterance\n    mask = np.zeros((num_utterances, num_utterances), dtype=np.bool)\n    mask[np.triu_indices(num_utterances, k=1)] = True\n    similarity_matrix = similarity_matrix.masked_fill_(torch.tensor(mask).bool().to(self.loss_device), -np.inf)\n\n    # Scale and bias the similarity", "CodeBleu_score": 31.880070417954386}, {"code": "def similarity_matrix(self, embeds)\n\t# Compute the number of speakers and utterances per speaker in the batch\n    num_speakers = len(embeds)\n    num_utterances = [len(embeds[speaker]) for speaker in range(num_speakers)]\n\n    # Initialize the similarity matrix and the mask matrix\n    similarity_matrix = torch.zeros(num_speakers, num_speakers).to(self.loss_device)\n    mask_matrix = np.ones((num_speakers, num_speakers)) - np.eye(num_speakers)\n\n    # Compute the centroids for each speaker using their embeddings\n    for speaker in range(num_speakers):\n        centroids_incl = torch.sum(embeds[speaker], dim=0, keepdim=True) / (num_utterances[speaker] + 1)\n        centroids_excl = torch.sum(embeds[speaker], dim=0, keepdim=True) / num_utterances[speaker]\n\n        # Compute the similarity scores using the centroids and update the similarity matrix\n        for i in range(num_speakers):\n            if mask_matrix[speaker, i]:\n                similarity_matrix[speaker, i] = torch.mm(embeds[speaker], centroids_excl[i]).item()\n                if speaker == i:\n                    # If the centroid to compare against is the same embedding, use the other centroid instead\n                    similarity_matrix[speaker, i] = torch.mm(embeds[speaker], centroids_incl[i]).item()\n\n    # Scale and bias the cosine similarities to turn them into similarities between -1 and 1\n    # This allows the use of cross entropy loss\n    similarity_matrix = self.similarity_weight * similarity_matrix + self.similarity_bias\n\n    return similarity_matrix\n\n", "CodeBleu_score": 31.1830346412076}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.view(-1, speakers_per_batch)\n\n    # Ground truth labels\n    labels = torch.arange(speakers_per_batch).unsqueeze(0).repeat(utterances_per_speaker, 1).view(-1)\n    labels = labels.long().to(self.loss_device)\n\n    # Loss\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Equal Error Rate (EER)\n    with torch.no_grad():\n        fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), sim_matrix.detach().cpu().numpy())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer", "CodeBleu_score": 50.49932258072059}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape to a vector of size speakers_per_batch * (speakers_per_batch - 1)\n    sim_matrix_reshaped = sim_matrix.view(speakers_per_batch, -1)\n\n    # Compute ground truth labels\n    labels = torch.from_numpy(np.repeat(range(speakers_per_batch), speakers_per_batch)).long().to(self.loss_device)\n\n    # Compute loss\n    loss = self.loss_fn(sim_matrix_reshaped, labels)\n\n    # Compute EER\n    with torch.no_grad():\n        fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), sim_matrix_reshaped.cpu().numpy())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer", "CodeBleu_score": 47.42902204354003}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape similarity matrix to a vector of size (speakers_per_batch * (speakers_per_batch - 1))\n    sim_matrix_reshaped = sim_matrix.view(speakers_per_batch, -1)\n    # Compute the ground truth labels\n    labels = torch.arange(speakers_per_batch).view(-1, 1).repeat(1, utterances_per_speaker).view(-1)\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix_reshaped, labels)\n    # Compute the EER\n    with torch.no_grad():\n        preds = sim_matrix_reshaped.detach().cpu().numpy()\n        labels = labels.detach().cpu().numpy()\n        fpr, tpr, thresholds = roc_curve(labels, preds, pos_label=speakers_per_batch - 1)\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n\n", "CodeBleu_score": 50.015738933582114}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\tif self.disabled:\n        return\n\n    # Log the model parameters\n    model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(f\"Number of model parameters: {params}\")\n\n    # Log the data parameters\n    dataset_parameters = [\n        \"dataset_dir\",\n        \"speaker_dirs\",\n        \"speaker_embedding_dir\",\n        \"speaker_embedding_file\",\n        \"speaker_embedding_dim\",\n        \"speaker_embedding_lifespan\",\n        \"speaker_embedding_extension\",\n        \"cache_dir\",\n        \"cache_size\",\n        \"cache_threshold\",\n        \"cache_files\",\n        \"cache_files_lifespan\",\n        \"cache_files_extension\",\n        \"audio_extension\",\n        \"audio_sample_rate\",\n        \"audio_sample_width\",\n        \"audio_channels\",\n        \"audio_normalize\",\n        \"audio_trim\",\n        \"audio_trim_top_db\",\n        \"audio_trim_frame_margin\",\n        \"audio_min_duration\",\n        \"audio_max_duration\",\n        \"audio_filterbank_type\",\n        \"audio_filterbank_size\",\n        \"audio_filterbank_count\",\n        \"audio_dither\",\n        \"audio_pad\",\n        \"audio_preemphasis\",\n        \"audio_preemphasis_coeff\",\n        \"audio_window_type\",\n        \"audio_window_size\",\n        \"audio_window_stride\",\n        \"audio_window_dither\",\n        \"audio_window_preemphasis\",\n        \"audio_window_normalize\",\n        \"audio_window_max_level\",\n        \"audio_window_zero_mean\",\n        \"audio_mel_type\",\n        \"audio_mel_size\",\n        \"audio_mel_count\",\n        \"audio_mel_dither\",\n        \"audio_mel_min\",\n        \"audio_mel_max\",\n        \"audio_", "CodeBleu_score": 14.950025695863605}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n\n    # Log the model parameters\n    model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n    n_params = sum([np.prod(p.size()) for p in model_parameters])\n    self.implementation_string += \\\"Model Parameters (%d):\\\" % n_params\n    for name, param in self.model.named_parameters():\n        if param.requires_grad:\n            self.implementation_string += \\\"\\", "CodeBleu_score": 16.142867666408556}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n\n    # Log the model parameters\n    model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(f\"Total trainable parameters: {params}\")\n\n    # Log the data parameters\n    data_parameters = filter(lambda p: p.requires_grad, self.data_loader.dataset.parameters())\n    params = sum([np.prod(p.size()) for p in data_parameters])\n    print(f\"Total trainable data parameters: {params}\")\n\n    # Log the model and data parameters\n    model_parameters = list(self.model.parameters())\n    data_parameters = list(self.data_loader.dataset.parameters())\n    parameters = model_parameters + data_parameters\n\n    # Get the parameter names and values\n    param_names = [p.name for p in parameters]\n    param_values = [p.data.view(-1).cpu().numpy() for p in parameters]\n\n    # Format the parameter string\n    param_string = \\\"\\\"\n    for name, value in zip(param_names, param_values):\n        param_string += f\\\"{name}: {value.shape} = {value}\\", "CodeBleu_score": 26.3875460768147}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    if len(embeds.shape) == 2:\n        embeds = np.expand_dims(embeds, 0)\n    n_speakers = min(max_speakers, len(utterances_per_speaker))\n    speaker_ids = list(utterances_per_speaker.keys())[:n_speakers]\n    utterances_per_speaker = {speaker_id: utterances_per_speaker[speaker_id] for speaker_id in speaker_ids}\n    embeds = np.concatenate([embeds[speaker_id] for speaker_id in speaker_ids], 0)\n    embeds = embeds.reshape((embeds.shape[0], -1))\n    colors = np.array([colormap[i] for i in range(n_speakers)])\n    colors = np.concatenate([colors] * [len(embeds[i]) for i in range(n_speakers)], 0)\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    self.projection_win = self.vis.scatter(\n        projected[:, 0],\n        projected[:, 1],\n        opts=dict(\n            legend=list(utterances_per_speaker.keys()),\n            markersize=1,\n            title=\\\"Step %d, UMAP projection of %d speakers\\\" % (step, n_speakers),\n            xlabel=\\\"Dimension 1\\\",\n            ylabel=\\\"Dimension 2\\\",\n        ),\n        win=self.projection_win,\n    )\n    if out_fpath is not None:\n        plt.switch_backend(\\\"agg\\\")\n        plt.figure(figsize=(10, 10))\n        plt.scatter(projected[:, 0], projected[:, 1], c=colors, s=0.1)\n        plt.legend(utterances_per_speaker.keys(), loc=\\\"upper left\\\", bbox_to_anchor=(1, 1))\n        plt.tight_layout()\n        plt.savefig", "CodeBleu_score": 48.002769069466275}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n\n    # Limit the number of speakers to visualize\n    speakers = list(utterances_per_speaker.keys())[:max_speakers]\n    utterances = sum((list(l) for s, l in utterances_per_speaker.items() if s in speakers), [])\n\n    # Flatten the speaker embeddings and assign colors\n    embeds = np.array([embeds[u].detach().cpu().numpy() for u in utterances])\n    colors = np.array([colormap[s_id % colormap.shape[0]] for s_id, s in enumerate(speakers) for u in utterances_per_speaker[s]])\n\n    # Reduce the dimensionality of the speaker embeddings with UMAP\n    reducer = umap.UMAP(n_components=2, random_state=0, metric=\\\"cosine\\\")\n    projection = reducer.fit_transform(embeds)\n\n    # Plot the embeddings\n    plt.figure(figsize=(10, 10))\n    plt.scatter(projection[:, 0], projection[:, 1], c=colors, s=2)\n    plt.gca().set_aspect(\\\"equal\\\", adjustable=\\\"box\\\")\n    plt.title(\\\"Utterance embeddings (step %d)\\\" % step)\n    plt.xlabel(\\\"UMAP dim 0\\\")\n    plt.ylabel(\\\"UMAP dim 1\\\")\n    plt.tight_layout()\n    plt.close()\n\n    # Save to file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n\n    # Update the visualization\n    self.projection_win = self.vis.matplot(plt, win=self.projection_win, opts={\\\"title\\\": \\\"UMAP projections\\\"})", "CodeBleu_score": 47.79887814142878}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    if len(embeds) == 0:\n        print(\"WARNING: Cannot draw an empty projection.\")\n        return\n\n    # Limit the number of speakers to visualize\n    speakers = list(embeds.keys())[:max_speakers]\n    labels = list(range(len(speakers)))\n    colors = colormap[labels]\n\n    # Flatten the embeddings and project them\n    flat_embeds = np.array([embed.flatten() for label, embed in embeds.items() if label in speakers])\n    projection = umap.UMAP().fit_transform(flat_embeds)\n\n    # Draw the projection\n    fig = plt.figure(figsize=(16, 10))\n    ax = fig.add_subplot(111)\n    scatter = ax.scatter(projection[:, 0], projection[:, 1], c=colors, alpha=0.5, s=10)\n    legend = ax.legend(handles=scatter.legend_elements()[0], labels=speakers, loc=\\\"upper left\\\", title=\\\"Speakers\\\")\n    ax.add_artist(legend)\n    ax.set_title(\\\"Step %d\\\" % step)\n    ax.set_xlabel(\\\"UMAP dim 0\\\")\n    ax.set_ylabel(\\\"UMAP dim 1\\\")\n\n    # Draw the utterance count for each speaker\n    for i, speaker in enumerate(speakers):\n        count = utterances_per_speaker[speaker]\n        ax.annotate(str(count), (projection[i, 0], projection[i, 1]))\n\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        print(\\\"Saved projection to %s\\\" % out_fpath)\n\n    # Show or update the plot\n    if self.projection_win is None:\n        self.projection_win = self.vis.matplot(plt, opts={\\\"title\\\": \\\"Speaker embeddings\\\"})\n    else:\n        self.", "CodeBleu_score": 43.59981278012566}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tbatch_size, time_steps, _ = x.shape\n    speaker_embedding = speaker_embedding.repeat(time_steps, 1).view(time_steps, batch_size, -1).transpose(0, 1)\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 11.96553599050686}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tbatch_size, time_steps, _ = x.shape\n    speaker_embedding = speaker_embedding.repeat(time_steps, 1)\n    speaker_embedding = speaker_embedding.reshape(time_steps, 1, -1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 16.925382146014037}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tbatch_size, time_steps, _ = x.shape\n    speaker_embedding = speaker_embedding.repeat(1, time_steps, 1)\n    speaker_embedding = speaker_embedding.view(batch_size, time_steps, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 29.545483174040776}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.236418047148106}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 79.54784684590057}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 79.54784684590057}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 79.54784684590057}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 79.44757356575354}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result.replace(\"}{\", \" \")", "CodeBleu_score": 76.81859257014594}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result.strip()\n\n", "CodeBleu_score": 76.32817322431288}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + [i for i in 'abcdefghijklmnopqrstuvwxyz '] + ['<eos>'], fontdict={'fontsize': 14})\n    ax.set_yticklabels([''] + [i for i in 'abcdefghijklmnopqrstuvwxyz '] + ['<eos>'], fontdict={'fontsize': 14})\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 49.17384072398156}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(15, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap=\"viridis\")\n    ax.set_xticklabels([\"\"] + [i for i in range(attn.shape[1])], fontdict={\"fontsize\": 14})\n    ax.set_yticklabels([\"\"] + [i for i in range(attn.shape[0])], fontdict={\"fontsize\": 14})\n    plt.savefig(path + \".png\", bbox_inches=\"tight\")\n    plt.close(fig)\n\n", "CodeBleu_score": 48.45707403881929}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    from matplotlib import rc\n    rc('font', **{'family': 'sans-serif', 'sans-serif': ['Helvetica']})\n    rc('text', usetex=True)\n    plt.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    cax = ax.matshow(attn.T, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + [f'${i}$' for i in range(len(attn))], fontsize=14)\n    ax.set_yticklabels([''] + [f'${i}$' for i in range(len(attn))], fontsize=14)\n    ax.xaxis.set_ticks_position('bottom')\n    plt.savefig(path + '.png', dpi=200, bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 41.53310428387572}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('Time (s)', fontsize=12, color='#777777')\n    ax.set_ylabel('Frequency (Hz)', fontsize=12, color='#777777')\n    ax.tick_params(axis='both', which='major', labelsize=10, colors='#777777')\n    ax.imshow(array, interpolation='nearest', aspect='auto')\n    plt.show()\n\n", "CodeBleu_score": 33.03891003872415}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('Time', fontsize=20, color='black')\n    ax.set_ylabel('Frequency', fontsize=20, color='black')\n    ax.tick_params(axis='x', colors='black', labelsize=15)\n    ax.tick_params(axis='y', colors='black', labelsize=15)\n    ax.imshow(array, interpolation='nearest', aspect='auto')\n    plt.show()\n\n", "CodeBleu_score": 35.157828804922964}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('Time', fontsize=12, color='#777777')\n    ax.set_ylabel('Frequency', fontsize=12, color='#777777')\n    ax.tick_params(axis='x', labelsize=7, colors='#777777')\n    ax.tick_params(axis='y', labelsize=7, colors='#777777')\n    plt.imshow(array, interpolation='nearest', aspect='auto')\n    plt.show()\n\n", "CodeBleu_score": 34.392739957265164}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.50379425444722}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.50379425444722}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.50379425444722}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 21.848677605787085}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 21.848677605787085}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 21.848677605787085}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus_clone = taus.clone().detach().requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n    log_alpha = alpha.log()\n    log_alpha.backward(torch.ones_like(log_alpha))\n    log_alpha_grad = taus_clone.grad.clone().detach()\n    taus_clone.grad.zero_()\n    return log_alpha_grad * alpha\n\n", "CodeBleu_score": 20.806016177807926}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus = taus.clone().detach()\n    taus.requires_grad = True\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = alpha.log()\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad.clone().detach()\n    taus.grad.zero_()\n    return log_alpha.exp() * grad_log_alpha\n\n", "CodeBleu_score": 41.41841468642821}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t# Clone taus, and set their gradient to 1.\n    taus_clone = taus.clone().detach().requires_grad_(True)\n    # Calculate alpha.\n    alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n    # Calculate the gradient of log(alpha) with respect to taus.\n    log_alpha = alpha.log()\n    log_alpha.backward(torch.ones_like(log_alpha))\n    # Calculate the integrand.\n    integrand = alpha * taus_clone.grad\n    # Clear the gradient of taus.\n    taus_clone.grad.zero_()\n    return integrand\n\n", "CodeBleu_score": 26.31594813966923}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 37.02760638509862}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 37.02760638509862}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 37.02760638509862}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\tif len(x.shape) > target_dims:\n        raise ValueError(\"The target number of dimensions is fewer than the current number of dimensions.\")\n    if len(x.shape) == target_dims:\n        return x\n    else:\n        if x.device.type == \"mps\":\n            return x.clone().detach().expand(*x.shape, *[1 for _ in range(target_dims - len(x.shape))])\n        else:\n            return x.expand(*x.shape, *[1 for _ in range(target_dims - len(x.shape))])\n\n", "CodeBleu_score": 32.22531345982016}, {"code": "def append_dims(x, target_dims)\n\tif x.dim() > target_dims:\n        raise ValueError(f\"Target dimensions {target_dims} must be greater than or equal to the current dimensions {x.dim()}\")\n    elif x.dim() == target_dims:\n        return x\n    else:\n        if x.device.type == 'mps':\n            return x.detach().clone().expand(tuple(1 for _ in range(target_dims - x.dim())))\n        else:\n            return x.expand(tuple(1 for _ in range(target_dims - x.dim())))\n\n", "CodeBleu_score": 31.50432033758547}, {"code": "def append_dims(x, target_dims)\n\tif len(x.shape) > target_dims:\n        raise ValueError(f\"Target dimensions {target_dims} is smaller than the current dimensions {len(x.shape)}\")\n    if len(x.shape) == target_dims:\n        return x\n    else:\n        num_dims_to_append = target_dims - len(x.shape)\n        if x.device.type == 'mps':\n            return x.clone().detach().expand(*([1] * num_dims_to_append), *x.shape)\n        else:\n            return x.expand(*([1] * num_dims_to_append), *x.shape)\n\n", "CodeBleu_score": 30.79799421652648}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            actual_digest = hashlib.sha256(path.read_bytes()).hexdigest()\n            if digest != actual_digest:\n                raise RuntimeError(\n                    f\"Digest of {path} does not match the expected one. Got {actual_digest} but expected {digest}.\"\n                )\n        return path\n    with urllib.request.urlopen(url) as response, path.open(\"wb\") as file:\n        shutil.copyfileobj(response, file)\n    if digest is not None:\n        actual_digest = hashlib.sha256(path.read_bytes()).hexdigest()\n        if digest != actual_digest:\n            raise RuntimeError(\n                f\"Digest of {path} does not match the expected one. Got {actual_digest} but expected {digest}.\"\n            )\n    return path\n\n", "CodeBleu_score": 46.81906782047477}, {"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        print(f'Downloading {path} from {url}')\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        sha256 = hashlib.sha256()\n        with open(path, 'rb') as f:\n            while chunk := f.read(1024):\n                sha256.update(chunk)\n        if sha256.hexdigest() != digest:\n            raise RuntimeError(f'Digest mismatch for {path}. Expected {digest} but got {sha256.hexdigest()}')\n    return path\n\n", "CodeBleu_score": 45.20166515282534}, {"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            with open(path, 'rb') as f:\n                hash = hashlib.sha256(f.read()).hexdigest()\n            if hash != digest:\n                raise RuntimeError(f'Hash mismatch for {path}! Expected {digest} but got {hash}')\n        return path\n    with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n        shutil.copyfileobj(response, f)\n    if digest is not None:\n        with open(path, 'rb') as f:\n            hash = hashlib.sha256(f.read()).hexdigest()\n        if hash != digest:\n            raise RuntimeError(f'Hash mismatch for {path}! Expected {digest} but got {hash}')\n    return path\n\n", "CodeBleu_score": 59.69920905981628}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\tif not isinstance(model, nn.Module):\n        raise ValueError(\"model must be an instance of nn.Module\")\n    if mode is None:\n        raise ValueError(\"mode must be True or False\")\n    if not isinstance(mode, bool):\n        raise ValueError(\"mode must be True or False\")\n    old_modes = {}\n    for module in model.modules():\n        old_modes[module] = module.training\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for module, old_mode in old_modes.items():\n            module.train(old_mode)\n\n", "CodeBleu_score": 27.89412223558212}, {"code": "def train_mode(model, mode=True)\n\tif mode:\n        model.train()\n    else:\n        model.eval()\n    original_modes = {}\n    for module in model.modules():\n        original_modes[module] = module.training\n        module.training = mode\n    try:\n        yield\n    finally:\n        for module in model.modules():\n            module.training = original_modes[module]\n\n", "CodeBleu_score": 33.44169459709024}, {"code": "def train_mode(model, mode=True)\n\tif mode is None:\n        yield\n    else:\n        modules = model.modules()\n        orig_modes = [module.training for module in modules]\n        try:\n            model.train(mode)\n            yield\n        finally:\n            for module, orig_mode in zip(modules, orig_modes):\n                module.train(orig_mode)\n\n", "CodeBleu_score": 40.795594755540066}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\tfor p_ema, p_model in zip(averaged_model.parameters(), model.parameters()):\n        p_ema.data.mul_(decay).add_(p_model.data, alpha=1 - decay)\n    for b_ema, b_model in zip(averaged_model.buffers(), model.buffers()):\n        b_ema.data.copy_(b_model.data)\n\n", "CodeBleu_score": 25.986540536314063}, {"code": "def ema_update(model, averaged_model, decay)\n\tfor ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data = decay * ema_param.data + (1 - decay) * param.data\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        ema_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.446719147211144}, {"code": "def ema_update(model, averaged_model, decay)\n\tfor ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        if ema_buffer.data.shape == buffer.data.shape:\n            ema_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 30.691844788451693}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Unknown activation type.\")\n\n", "CodeBleu_score": 39.08494140870455}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(\"Invalid activation type\")\n\n    if antialias:\n        act = Activation1d(act)\n\n    return act\n\n", "CodeBleu_score": 68.0697532345598}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(nn.Identity())\n        else:\n            return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n", "CodeBleu_score": 40.10683462033068}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\tvq = self.codebook(idx)\n    if dim != -1:\n        vq = vq.transpose(dim, -1)\n    return vq\n\n", "CodeBleu_score": 37.441031627632256}, {"code": "def idx2vq(self, idx, dim=-1)\n\tz_e_x = self.codebook(idx)\n    if dim != -1:\n        z_e_x = z_e_x.transpose(dim, -1)\n    return z_e_x\n\n", "CodeBleu_score": 38.15989721629658}, {"code": "def idx2vq(self, idx, dim=-1)\n\treturn self.codebook(idx, dim=dim)\n", "CodeBleu_score": 15.568059306831882}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            logpy.info(f\"Using EMA weights for {context}.\")\n        params = [p.detach().clone() for p in self.parameters()]\n        for p, p_ema in zip(self.parameters(), self.model_ema.parameters()):\n            p.data = p_ema.data.clone()\n        yield\n        if context is not None:\n            logpy.info(f\"Restoring original weights after {context}.\")\n        for p, p_orig in zip(self.parameters(), params):\n            p.data = p_orig.data.clone()\n    else:\n        yield\n\n", "CodeBleu_score": 49.006301470819714}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            logpy.info(f\"Using EMA weights for {context}.\")\n        self.store_params()\n        self.load_ema_params()\n        yield\n        if context is not None:\n            logpy.info(f\"Restoring original weights after {context}.\")\n        self.load_params()\n    else:\n        yield\n\n", "CodeBleu_score": 44.26713485733572}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        self.model_ema.store(self)\n        self.model_ema.swap(self)\n        if context is not None:\n            logpy.info(f\"Using EMA weights for {context}.\")\n    try:\n        yield\n    finally:\n        if self.use_ema:\n            self.model_ema.restore(self)\n            if context is not None:\n                logpy.info(f\"Restored original weights after {context}.\")\n\n", "CodeBleu_score": 60.62492194212146}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be one-dimensional'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be divisible by 2'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 76.15185999493768}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be 1-dimensional'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 76.61678323463887}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be a 1D tensor'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    timesteps = timesteps.float()\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 71.88882317205673}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tmin_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    min_encodings.scatter_(1, indices, 1)\n\n    z_q = torch.matmul(min_encodings, self.embedding.weight).view(shape)\n\n    return z_q\n\n", "CodeBleu_score": 21.393219744757662}, {"code": "def get_codebook_entry(self, indices, shape)\n\tone_hot = F.one_hot(indices, num_classes=self.n_e).float()\n    z_q = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 52.760039576985506}, {"code": "def get_codebook_entry(self, indices, shape)\n\tmin_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    min_encodings.scatter_(1, indices, 1)\n\n    z_q = torch.matmul(min_encodings, self.embedding.weight).view(shape)\n\n    return z_q\n\n", "CodeBleu_score": 21.393219744757662}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.966114539827704}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.966114539827704}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.966114539827704}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\timg = img.astype(np.float32)\n    shift = shift.astype(np.float32)\n    # random jitter value for each RGB channel\n    r = np.random.randint(-shift[0], shift[0], 3)\n    # add the jitter value to the image\n    img = img + r\n    # clip the result to ensure pixel values remain between 0 and 1\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 48.7947854377108}, {"code": "def color_jitter(img, shift)\n\timg = img.astype(np.float32)\n    color_jitter_val = np.random.randint(-shift, shift, 3)\n    img[:, :, 0] += color_jitter_val[0]\n    img[:, :, 1] += color_jitter_val[1]\n    img[:, :, 2] += color_jitter_val[2]\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 29.460366460717612}, {"code": "def color_jitter(img, shift)\n\timg = img.astype(np.float32)\n    shift = np.array(shift, dtype=np.float32)\n    r = np.random.uniform(-shift[0], shift[0], 3)\n    img += r\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 36.8196323360267}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 20.043001311601575}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 20.043001311601575}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 20.043001311601575}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\tif len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    for exp in batch:\n        self.exps.remove(exp)\n    return batch\n\n", "CodeBleu_score": 44.196333290598155}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    for exp in batch:\n        self.exps.remove(exp)\n    return batch\n\n", "CodeBleu_score": 49.569905926049515}, {"code": "def next_batch(self, sample_size=1)\n\tif len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n    sampled_exps = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in sampled_exps]\n    return sampled_exps\n\n", "CodeBleu_score": 54.76500051115576}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\tif len(self.evaluated_configs) == 0:\n        return None\n\n    config_list = []\n    for config in self.evaluated_configs:\n        config_list.append(flatten_config(config))\n\n    config_array = np.array(config_list)\n    perf_array = np.array(self.evaluated_perf)\n    sorted_perf_indices = np.argsort(perf_array)\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        sorted_perf_indices = sorted_perf_indices[::-1]\n\n    config_array = config_array[sorted_perf_indices]\n    config_array = config_array[:, :, 0]\n\n    self.cost_model.fit(config_array, perf_array)\n    estimated_perf = self.cost_model.predict(config_array)\n    estimated_perf_indices = np.argsort(estimated_perf)\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        estimated_perf_indices = estimated_perf_indices[::-1]\n\n    estimated_top_config_indices = sorted_perf_indices[estimated_perf_indices]\n    return estimated_top_config_indices\n\n", "CodeBleu_score": 36.346604416300096}, {"code": "def find_estimated_top_configs(self)\n\tall_configs = [e['ds_config'] for e in self.exps]\n    num_all_configs = len(all_configs)\n    dims = dict_to_dims(self.tuning_space)\n    logger.info(f\"Create config dim: {dims}, all configs: {num_all_configs}\")\n    configs = []\n    for i in range(num_all_configs):\n        config = all_configs[i]\n        config = flatten_dict(config)\n        config = {k: v for k, v in config.items() if isinstance(v, numbers.Number)}\n        configs.append(config)\n    configs = np.array(configs)\n    logger.info(f\"configs: {configs}\")\n    if len(self.evaluated_configs) > 0:\n        logger.info(f\"evaluated configs: {self.evaluated_configs}\")\n        configs = np.concatenate((configs, self.evaluated_configs), axis=0)\n    logger.info(f\"configs: {configs}\")\n    perf = self.cost_model.predict(configs)\n    logger.info(f\"perf: {perf}\")\n    if len(self.evaluated_perf) > 0:\n        perf = np.concatenate((perf, self.evaluated_perf), axis=0)\n    logger.info(f\"perf: {perf}\")\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        sorted_idx = np.argsort(perf)\n    else:\n        sorted_idx = np.argsort(perf)[::-1]\n    logger.info(f\"sorted_idx: {sorted_idx}\")\n    top_k_idx = sorted_idx[:self.top_k]\n    logger.info(f\"top_k_idx: {top_k_idx}\")\n    return top_k_idx\n\n", "CodeBleu_score": 43.46147030081063}, {"code": "def find_estimated_top_configs(self)\n\tconfig_values = np.array([flatten_config(config) for config in self.all_configs])\n    estimates = self.cost_model.predict(config_values)\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        return np.argsort(estimates)\n    else:\n        return np.argsort(estimates)[::-1]\n\n", "CodeBleu_score": 17.84518856353247}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.81087182166186}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.81087182166186}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.81087182166186}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp[\"exp_id\"]\n    exp_name = exp[\"name\"]\n    exp_script = exp[\"script\"]\n    exp_args = exp[\"args\"]\n    exp_args_str = exp[\"args_str\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_ds_config = exp[\"ds_config\"]\n    exp_ds_config_str = exp[\"ds_config_str\"]\n    exp_master_port = exp[\"master_port\"]\n    exp_hosts = exp[\"hosts\"]\n    exp_num_gpus = exp[\"num_gpus\"]\n    exp_num_nodes = exp[\"num_nodes\"]\n    exp_num_processes = exp[\"num_processes\"]\n    exp_num_machines = exp[\"num_machines\"]\n    exp_num_machine_nodes = exp[\"num_machine_nodes\"]\n    exp_num_machine_gpus = exp[\"num_machine_gpus\"]\n\n    if exp_master_port is None:\n        exp_master_port = 29500\n\n    if exp_num_gpus is None:\n        exp_num_gpus = self.num_gpus_per_node\n\n    if exp_num_nodes is None:\n        exp_num_nodes = len(exp_hosts)\n\n    if exp_num_processes is None:\n        exp_num_processes = exp_num_nodes * exp_num_gpus\n\n    if exp_num_machines is None:\n        exp_num_machines = len(self.nodes)\n\n    if exp_num_machine_nodes is None:\n        exp_num_machine_nodes = int(exp_num_machines / exp_num_nodes)\n\n    if exp_num_machine_gpus is None:\n        exp_num_machine_gpus = int(exp_num_machines / exp_num_gpus)\n\n    exp[\"master_port\"] = exp_master_port\n    exp[\"result_dir\"] = exp_result_dir\n    exp[\"ds_config\"] = exp_ds_config\n    exp[\"ds_config_str\"]", "CodeBleu_score": 21.49779028511651}, {"code": "def run_job(self, exp: dict, reservations)\n\t# update user arguments based on argument mappings\n    if self.arg_mappings:\n        args = exp[\"ds_args\"]\n        for k, v in self.arg_mappings.items():\n            if k in args:\n                args[k] = v\n\n    # set the experiment's master port and result directory\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_p2p_bw_limit\"] = 0\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allreduce_p2p_bw_limit\"] = 0\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allgather_p2p_bw_limit\"] = 0\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_reduce_scatter_p2p_bw_limit\"] = 0\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allgather_bucket_size\"] = 500000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_reduce_scatter_bucket_size\"] = 500000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allgather_partitions\"] = 1\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_reduce_scatter_partitions\"] = 1\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_gather_list_size\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allgather_num_groups\"] = 1\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_reduce_scatter_num_groups\"] = 1\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_allgather_algorithm\"] = \"recursive\"\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_reduce_scatter_algorithm\"] = \"recursive\"\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage", "CodeBleu_score": 19.121391528524896}, {"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp[\"exp_id\"]\n    exp[\"master_port\"] = self.get_master_port()\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp[\"name\"])\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_live_parameters\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_reuse_distance\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_prefetch_bucket_size\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_param_persistence_threshold\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_gradients\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_weight\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_bucket_cap_bytes\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_partition_size\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_min_param_size\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_live_parameters\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_max_reuse_distance\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_prefetch_bucket_size\"] = 1000000000\n    exp[\"ds_config\"][\"zero_optimization\"][\"stage3_param_persistence_", "CodeBleu_score": 10.96237122740879}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\tsystem_info = {\n        \\\"\\\"\\\"System\\\"\\\"\\\": {\n            \\\"\\\"\\\"OS\\\"\\\"\\\": platform.system(),\n            \\\"\\\"\\\"OS Version\\\"\\\"\\\": platform.version(),\n            \\\"\\\"\\\"Architecture\\\"\\\"\\\": platform.architecture(),\n            \\\"\\\"\\\"Processor\\\"\\\"\\\": platform.processor(),\n            \\\"\\\"\\\"Python Version\\\"\\\"\\\": platform.python_version()\n        }\n    }\n    return system_info\n\n", "CodeBleu_score": 24.86264047829995}, {"code": "def system_info()\n\tsystem_info = {\n        \\\"OS\\\": platform.system(),\n        \\\"architecture\\\": platform.architecture(),\n        \\\"version\\\": platform.version(),\n        \\\"processor\\\": platform.processor(),\n        \\\"python version\\\": platform.python_version(),\n    }\n    return system_info\n\n", "CodeBleu_score": 43.381828164299016}, {"code": "def system_info()\n\tsystem_info = {\n        \"system\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version()\n    }\n    return system_info\n\n", "CodeBleu_score": 58.011155035785045}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\ttry:\n        import torch\n        import torch.cuda\n\n        if torch.cuda.is_available():\n            return {\n                \"CUDA\": True,\n                \"CUDA_version\": torch.version.cuda,\n                \"GPUs\": torch.cuda.get_device_name(0),\n            }\n        else:\n            return {\"CUDA\": False}\n    except ImportError:\n        return {\"CUDA\": False}\n\n", "CodeBleu_score": 31.874317661064705}, {"code": "def cuda_info()\n\tif torch.cuda.is_available():\n        return {\n            \"devices\": torch.cuda.get_device_name(0),\n            \"cuda\": torch.backends.cudnn.version(),\n            \"cudnn\": torch.backends.cudnn.version(),\n        }\n    else:\n        return {\"devices\": None, \"cuda\": None, \"cudnn\": None}\n\n", "CodeBleu_score": 36.73537673937163}, {"code": "def cuda_info()\n\treturn {\n        \"devices\": torch.cuda.device_count(),\n        \"available\": torch.cuda.is_available(),\n        \"cuda\": torch.version.cuda,\n    }\n\n", "CodeBleu_score": 39.13333283710034}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tclass_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n    if class_id_to_class_name is not None:\n        class_id_to_class_name = {int(k): v for k, v in class_id_to_class_name.items()}\n    class_name_to_accuracy = {}\n    num_samples = 0\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n        if class_id_to_class_name is not None:\n            class_name = class_id_to_class_name[class_name]\n        embedding = encoder_manager.embed_utterance(audio_file)\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        if class_name not in class_name_to_accuracy:\n            class_name_to_accuracy[class_name] = {\n                \"num_samples\": 0,\n                \"num_correct\": 0\n            }\n        class_name_to_accuracy[class_name][\"num_samples\"] += 1\n        if predicted_class_name == class_name:\n            class_name_to_accuracy[class_name][\"num_correct\"] += 1\n        num_samples += 1\n    class_name_to_accuracy = {\n        class_name: {\n            \"accuracy\": float(class_name_to_accuracy[class_name][\"num_correct\"]) / class_name_to_accuracy[class_name][\"num_samples\"],\n            \"num_samples\": class_name_to_accuracy[class_name][\"num_samples\"]\n        }\n        for class_name in class_name_to_accuracy\n    }\n    total_accuracy = sum([class_name_to_accuracy[class_name][\"accuracy\"] *", "CodeBleu_score": 34.97621427896675}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tencoder_config = encoder_manager.config\n    class_name_key = encoder_config.class_name_key\n    class_id_to_name_mapping = encoder_config.class_id_to_name_mapping\n    class_names = []\n    class_accuracy = []\n    total_accuracy = 0\n    total_items = 0\n    for item in tqdm(dataset_items):\n        audio_file = item[0]\n        true_class_name = item[1]\n        predicted_class_name = None\n        if class_name_key is not None:\n            predicted_class_name = item[2]\n        if class_id_to_name_mapping is not None:\n            predicted_class_name = class_id_to_name_mapping[predicted_class_name]\n        if predicted_class_name is not None:\n            if predicted_class_name not in class_names:\n                class_names.append(predicted_class_name)\n                class_accuracy.append(0)\n            class_index = class_names.index(predicted_class_name)\n            class_accuracy[class_index] += 1\n            if true_class_name == predicted_class_name:\n                total_accuracy += 1\n            total_items += 1\n    class_accuracy = [accuracy / total_items for accuracy in class_accuracy]\n    print(f'Class accuracy: {class_accuracy}')\n    print(f'Total accuracy: {total_accuracy / total_items}')\n\n", "CodeBleu_score": 27.92483968675443}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tclass_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\", None)\n    class_name_to_id_mapping = encoder_manager.config.get(\"class_name_to_id_mapping\", None)\n    if class_id_to_name_mapping is not None:\n        class_id_to_name_mapping = {int(k): v for k, v in class_id_to_name_mapping.items()}\n    if class_name_to_id_mapping is not None:\n        class_name_to_id_mapping = {k: int(v) for k, v in class_name_to_id_mapping.items()}\n\n    class_names = []\n    true_class_names = []\n    predicted_class_names = []\n    for item in tqdm(dataset_items):\n        class_names.append(item[class_name_key])\n        true_class_names.append(item[class_name_key])\n        if class_id_to_name_mapping is not None:\n            item[class_name_key] = class_id_to_name_mapping[item[class_name_key]]\n        if class_name_to_id_mapping is not None:\n            item[class_name_key] = class_name_to_id_mapping[item[class_name_key]]\n        embedding = encoder_manager.embed_utterance(**item)\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        predicted_class_names.append(predicted_class_name)\n\n    class_names = list(set(class_names))\n    class_accuracy = {}\n    for class_name in class_names:\n        true_class_name_count = true_class_names.count(class_name)\n        predicted_class_name_count = predicted_class_names.count(class_name)\n        class_accuracy[class_name] = predicted_class_name_count / true_class_", "CodeBleu_score": 34.61404325520096}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\taudio_path = pathlib.Path(audio_path)\n    output_path = pathlib.Path(args.output_dir) / audio_path.relative_to(args.input_dir)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not args.force:\n        return str(output_path), False\n    speech_detected = remove_silence(str(audio_path), str(output_path), args.vad_threshold, args.vad_buffer_ms, args.vad_window_ms)\n    return str(output_path), speech_detected\n\n", "CodeBleu_score": 35.343404152507595}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\taudio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.replace(args.input, args.output)\n    output_path.mkdir(parents=True, exist_ok=True)\n    output_path = output_path / audio_path.name\n    if output_path.exists() and not args.force:\n        print(f\"{output_path} already exists, skipping.\")\n        return output_path, False\n    audio, speech = remove_silence(audio_path, args.vad_model, args.vad_utils, args.vad_mode, args.vad_window_length, args.vad_padding, args.vad_required_padding, args.vad_aggressiveness, args.vad_noise_floor_db)\n    torch.save(audio, output_path)\n    return output_path, speech\n\n", "CodeBleu_score": 38.986308202409795}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\toutput_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_dir = pathlib.Path(output_path).parent\n    if output_dir.exists() and not args.force:\n        return output_path, False\n    output_dir.mkdir(parents=True, exist_ok=True)\n    speech_detected = remove_silence(audio_path, output_path, args.vad_model, args.vad_utils, args.frame_shift_ms,\n                                     args.frame_length_ms, args.silence_threshold, args.min_silence_duration_ms,\n                                     args.min_utterance_duration_ms)\n    return output_path, speech_detected\n\n", "CodeBleu_score": 42.58243232140556}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    file_list = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as pool:\n        with tqdm(total=len(file_list)) as progress_bar:\n            for _ in pool.imap_unordered(resample_file, [(f, output_sr) for f in file_list]):\n                progress_bar.update(1)\n    print(f\"Resampled {len(file_list)} files to {output_sr} sr.\")\n\n", "CodeBleu_score": 40.627420240464005}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    input_files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as pool:\n        list(\n            tqdm(\n                pool.imap(resample_file, zip(input_files, [output_sr] * len(input_files))),\n                total=len(input_files),\n            )\n        )\n    print(\"Done!\")\n\n", "CodeBleu_score": 33.4430879175421}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(output_dir, f\"**/*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as p:\n        list(\n            tqdm(\n                p.imap(resample_file, zip(files, [output_sr] * len(files))),\n                total=len(files),\n            )\n        )\n    print(f\"Resampled {len(files)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 33.02475609041279}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    eval_loss_list = []\n    eval_loss_dict = {}\n\n    with torch.no_grad():\n        for batch_index, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            eval_loss += loss.item()\n            eval_loss_list.append(loss.item())\n            eval_loss_dict[batch_index] = loss.item()\n\n    eval_loss /= len(data_loader)\n    eval_loss_list = np.array(eval_loss_list)\n    eval_loss_dict = pd.DataFrame(eval_loss_dict, index=[\"batch_index\"]).T\n\n    if global_step > 0:\n        wandb.log({\"eval_loss\": eval_loss}, step=global_step)\n        wandb.log({\"eval_loss_list\": eval_loss_list}, step=global_step)\n        wandb.log({\"eval_loss_dict\": eval_loss_dict}, step=global_step)\n\n    print(f\" > Eval loss: {eval_loss:.6f}\")\n    return eval_loss\n\n", "CodeBleu_score": 26.99652545587692}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n\n    eval_loss = 0\n    eval_steps = 0\n\n    for batch_idx, (x, y, _) in enumerate(data_loader):\n\n        # reshape input to group samples of each class\n        x = x.view(-1, x.shape[-1])\n        y = y.view(-1, y.shape[-1])\n\n        # dispatch data to GPU if available\n        x, y = x.to(device), y.to(device)\n\n        # compute loss\n        y_pred = model(x)\n        loss = criterion(y_pred, y)\n\n        eval_loss += loss.item()\n        eval_steps += 1\n\n        if batch_idx == 0:\n            # generate and log a UMAP plot of the embeddings from the last batch\n            plot_embeddings(\n                y_pred.detach().cpu().numpy(),\n                meta_data_eval,\n                c.model_dir,\n                global_step,\n                c.model_params.get(\"use_torch_spec\", False),\n            )\n\n    eval_loss /= eval_steps\n\n    # log evaluation statistics\n    logger.log_evaluation(eval_loss, global_step)\n\n    model.train()\n\n    return eval_loss\n\n", "CodeBleu_score": 28.654451370159624}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    log_evaluation_stats(eval_loss, global_step)\n    return eval_loss\n\n", "CodeBleu_score": 15.679679962561732}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class = find_module(model_name, [os.path.join(os.path.dirname(__file__), \"configs\"), os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"configs\")])\n    return config_class()\n\n", "CodeBleu_score": 12.620756282086344}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = f\"{model_name.upper()}Config\"\n    config_class_path = f\"TTS.config.{model_name.lower()}_config\"\n    config_class_module = find_module(config_class_path, f\"{model_name.lower()}_config.py\")\n    config_class = getattr(config_class_module, config_class_name)\n    return config_class\n\n", "CodeBleu_score": 13.958327371572594}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = model_name + \"Config\"\n    config_class_path = None\n\n    # check if the model is \"xtts\" to import its specific configuration\n    if model_name == \"xtts\":\n        config_class_path = \"TTS.config.xtts_config\"\n\n    # search through predefined paths to find and assign the configuration class\n    if not config_class_path:\n        config_class_path = find_module(config_class_name)\n\n    # handle ModuleNotFoundError exceptions\n    if not config_class_path:\n        raise ModuleNotFoundError(f\"The configuration class for the model {model_name} is not found!\")\n\n    # import the configuration class\n    config_class = getattr(__import__(config_class_path, fromlist=[config_class_name]), config_class_name)\n\n    return config_class\n\n", "CodeBleu_score": 40.892408138522974}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\tif config_path.endswith(\\\".yaml\\\"):\n        with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_path.endswith(\\\".json\\\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\" [!] Unknown file type: {config_path}\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class(**config_dict)\n    return config", "CodeBleu_score": 36.02626051356341}, {"code": "def load_config(config_path: str) -> Coqpit\n\t# determine the file type\n    file_extension = config_path.split(\".\")[-1]\n    if file_extension == \"yaml\" or file_extension == \"yml\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    elif file_extension == \"json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown config file type: {file_extension}\")\n\n    # process the model name\n    model_name = _process_model_name(config_dict)\n\n    # register the config class\n    config_class = register_config(model_name)\n\n    # create a config instance\n    config = config_class()\n\n    # populate the config instance\n    config.load_from_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 49.213552520992}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = {}\n    if config_path.endswith(\\\".yml\\\"):\n        with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\\\".json\\\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise NotImplementedError(\\\" [!] Only .json and .yml configs are supported.\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 39.913684219145566}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t# walk the path and get all files in the directory structure\n    for root, dirs, files in os.walk(basePath, topdown=True):\n        for relFilePath in files:\n            if contains is not None and contains not in relFilePath:\n                continue\n            absFilePath = os.path.join(root, relFilePath)\n            if validExts is None or os.path.splitext(absFilePath)[1].lower() in validExts:\n                yield absFilePath\n\n", "CodeBleu_score": 40.21035576236231}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# get all files in the sub-tree\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            if validExts is None or os.path.splitext(file)[1].lower() in validExts:\n                yield os.path.join(root, file)\n\n", "CodeBleu_score": 38.777720968854524}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# check if basePath is a file\n    if os.path.isfile(basePath):\n        if validExts is None or os.path.splitext(basePath)[1] in validExts:\n            yield basePath\n        return\n\n    # check if basePath is a valid directory\n    if not os.path.isdir(basePath):\n        return\n\n    # walk through the directory structure recursively\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            filepath = os.path.join(root, file)\n            if validExts is None or os.path.splitext(filepath)[1] in validExts:\n                yield filepath\n\n", "CodeBleu_score": 42.24655836268828}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the paths to the checkpoint, config, and vocab files.\"\n    try:\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint, map_location='cpu'))\n        model.eval()\n        if torch.cuda.is_available():\n            model.cuda()\n        model.vocab = torch.load(xtts_vocab)\n        global XTTS_MODEL\n        XTTS_MODEL = model\n        return \"XTTS model loaded successfully!\"\n    except Exception as e:\n        return \"Failed to load the model. Please check the paths to the checkpoint, config, and vocab files.\"\n\n", "CodeBleu_score": 46.57722998274241}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Please provide the required paths to the checkpoint, config, and vocabulary.\"\n    try:\n        print(\"Loading XTTS model...\")\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        model.eval()\n        model.load_state_dict(torch.load(xtts_checkpoint))\n        model.vocab = torch.load(xtts_vocab)\n        if torch.cuda.is_available():\n            model.cuda()\n        print(\"XTTS model loaded.\")\n        return \"XTTS model loaded.\"\n    except Exception as e:\n        print(e)\n        traceback.print_exc()\n        return \"Error loading XTTS model.\"\n\n", "CodeBleu_score": 52.72221138996195}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the paths to the model checkpoint, config, and vocabulary.\"\n    try:\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device('cpu')))\n        model.load_vocab(xtts_vocab)\n        if torch.cuda.is_available():\n            model = model.cuda()\n        return \"XTTS model loaded successfully!\"\n    except Exception as e:\n        return f\"Error loading the model: {e}\"\n\n", "CodeBleu_score": 45.70692440135631}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif not isinstance(x, torch.Tensor):\n        x = torch.from_numpy(x).unsqueeze(0).to(self.device)\n    if self.torch_stft:\n        x = self.get_torch_mel_spectrogram_class(self.audio_config)(x)\n        x = x.transpose(1, 2)\n        x = x[:, :, : self.audio_config[\"num_mel_bins\"]]\n        x = x.unsqueeze(0)\n    num_frames = min(num_frames, x.shape[2])\n    offset = x.shape[2] - num_frames\n    offset_frames = np.linspace(0, offset, num_eval, dtype=np.int)\n    embeddings = []\n    for i in offset_frames:\n        x_frame = x[:, :, i : i + num_frames]\n        embedding = self.inference(x_frame, l2_norm=l2_norm)\n        embeddings.append(embedding)\n    embeddings = torch.cat(embeddings, dim=0)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 48.058119084646165}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tx = x.unsqueeze(0)\n    if hasattr(self, \"torch_mel_spectrogram\"):\n        x = self.torch_mel_spectrogram(x)\n        if x.shape[2] < num_frames:\n            num_frames = x.shape[2]\n        if num_frames % self.hop_length != 0:\n            num_frames -= num_frames % self.hop_length\n        if num_frames < self.hop_length:\n            num_frames = self.hop_length\n        if num_eval > num_frames:\n            num_eval = num_frames\n        offset = np.linspace(0, x.shape[2] - num_frames, num_eval, dtype=int)\n        batch = []\n        for i in range(num_eval):\n            batch.append(x[:, :, offset[i] : offset[i] + num_frames])\n        batch = torch.cat(batch, dim=0)\n        embeddings = self.forward(batch, l2_norm)\n        if return_mean:\n            embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings)\n        return embeddings\n    else:\n        raise RuntimeError(\"You must specify a torch mel spectrogram class to use this function.\")\n\n", "CodeBleu_score": 40.9487247212134}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tx_len = x.shape[1]\n    if self.torch_stft:\n        num_frames = int(num_frames / self.audio_config[\"hop_length\"])\n    if num_frames > x_len:\n        num_frames = x_len\n    offset = np.linspace(0, x_len - self.audio_config[\"hop_length\"], num_frames)\n    offset = offset.astype(np.int32)\n    if num_eval > num_frames:\n        num_eval = num_frames\n    offset = offset[:num_eval]\n    embeddings = []\n    for i in range(len(offset)):\n        if self.torch_stft:\n            x_i = x[:, offset[i] : offset[i] + self.audio_config[\"hop_length\"] * self.audio_config[\"win_length\"]]\n        else:\n            x_i = x[:, offset[i] : offset[i] + self.audio_config[\"win_length\"]]\n        embeddings.append(self.forward(x_i.unsqueeze(0), l2_norm))\n    embeddings = torch.cat(embeddings, dim=0)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 45.18791510407764}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.20790427293335}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.20790427293335}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.20790427293335}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Get the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(len(embeddings)):\n        num_utterances_per_class[i % num_classes_in_batch] += 1\n\n    # Limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transformation\n    reducer = umap.UMAP()\n    transformed = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(10, 10))\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            transformed[i * num_utterances_per_class[i] : (i + 1) * num_utterances_per_class[i], 0],\n            transformed[i * num_utterances_per_class[i] : (i + 1) * num_utterances_per_class[i], 1],\n            s=1,\n            c=colormap[i],\n            alpha=0.5,\n            label=i,\n        )\n\n    # Adjust the aspect ratio and title\n    ax.set_aspect(\"equal\", \"datalim\")\n    ax.set_title(\"UMAP projection\")\n\n    # Save and return the figure\n    fig.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 42.89528219782619}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\tnum_utterances = embeddings.shape[0]\n    num_classes = num_classes_in_batch.shape[0]\n    num_utterances_per_class = num_utterances // num_classes\n\n    if num_classes > 10:\n        num_classes = 10\n        num_utterances_per_class = 100\n\n    embeddings_per_class = [\n        embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class]\n        for i in range(num_classes)\n    ]\n\n    embeddings_per_class = np.concatenate(embeddings_per_class, axis=0)\n\n    reducer = umap.UMAP(n_neighbors=15, n_components=2, metric=\"cosine\")\n    projected = reducer.fit_transform(embeddings_per_class)\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\", \"datalim\")\n\n    for i in range(num_classes):\n        ax.scatter(\n            projected[i * num_utterances_per_class : (i + 1) * num_utterances_per_class][\n                :, 0\n            ],\n            projected[i * num_utterances_per_class : (i + 1) * num_utterances_per_class][\n                :, 1\n            ],\n            c=colormap[i],\n            alpha=0.5,\n            edgecolors=\"none\",\n        )\n\n    plt.savefig(\"umap.png\")\n    plt.close()\n\n    return fig", "CodeBleu_score": 40.966083479267546}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\tnum_classes = len(num_classes_in_batch)\n\n    if num_classes > 10:\n        num_classes_in_batch = num_classes_in_batch[:10]\n        num_classes = 10\n\n    num_utts_per_class = [\n        sum(num_classes_in_batch[i : i + 1]) for i in range(num_classes)\n    ]\n\n    reducer = umap.UMAP(\n        n_components=2,\n        n_neighbors=10,\n        min_dist=0.0,\n        metric=\"cosine\",\n        random_state=42,\n    )\n    umap_embeddings = reducer.fit_transform(embeddings)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_title(\"UMAP projection\", fontsize=16)\n    ax.set_aspect(\"equal\")\n\n    for i in range(num_classes):\n        ax.scatter(\n            umap_embeddings[\n                sum(num_utts_per_class[:i]) : sum(num_utts_per_class[: i + 1]), 0\n            ],\n            umap_embeddings[\n                sum(num_utts_per_class[:i]) : sum(num_utts_per_class[: i + 1]), 1\n            ],\n            c=colormap[i],\n            s=10,\n            alpha=0.5,\n            edgecolors=\"none\",\n        )\n\n    fig.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 40.221949871854626}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim = F.cosine_similarity(dvecs[spkr, utt], new_centroids, dim=-1)\n            cos_sim = torch.clamp(cos_sim, min=1e-8)\n            cos_sim_matrix.append(cos_sim)\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 29.797338826021676}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tN, M, D = dvecs.shape\n    cos_sim_matrix = []\n    for spkr in range(N):\n        spkr_cos_sim = []\n        for utt in range(M):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim = F.cosine_similarity(dvecs[spkr, utt], new_centroids, dim=-1)\n            spkr_cos_sim.append(torch.clamp(cos_sim, min=1e-8))\n        cos_sim_matrix.append(torch.stack(spkr_cos_sim))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 38.13296653158259}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr in range(dvecs.size(0)):\n        spkr_cos_sim = []\n        for utt in range(dvecs.size(1)):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = F.cosine_similarity(dvecs[spkr, utt], new_centroids)\n            sim = torch.clamp(sim, min=1e-8)\n            spkr_cos_sim.append(sim)\n        cos_sim_matrix.append(torch.stack(spkr_cos_sim))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 37.58411753308288}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            loss = -F.log_softmax(cos_sim_matrix[spkr_idx][utt_idx], dim=0)[\n                spkr_idx\n            ]\n            losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 32.85929324006849}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tloss = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            cs_row = cos_sim_matrix[spkr_idx][utt_idx]\n            exp_cs_row = torch.exp(cs_row)\n            sum_exp_cs_row = torch.sum(exp_cs_row)\n            softmax_cs_row = exp_cs_row / sum_exp_cs_row\n            neg_log_likelihood = -torch.log(softmax_cs_row[spkr_idx][utt_idx])\n            loss.append(neg_log_likelihood)\n    return torch.stack(loss)\n", "CodeBleu_score": 29.67107068363922}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for i, embedding in enumerate(dvecs):\n        for j, utterance in enumerate(embedding):\n            losses.append(F.log_softmax(cos_sim_matrix[i, j], dim=0))\n    return torch.stack(losses)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 28.461670026033374}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            # Exclude the current centroid\n            new_centroids = self.calc_new_centroids(dvecs, centroids, j, i)\n            # Compute the contrast loss\n            L_row.append(\n                torch.clamp(\n                    torch.sum(\n                        F.logsigmoid(\n                            torch.mm(\n                                dvecs[j, i].unsqueeze(1).transpose(0, 1),\n                                new_centroids.transpose(0, 1),\n                            )\n                        )\n                    ),\n                    1e-6,\n                )\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 44.519408291553056}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            # Exclude the current centroid\n            excl = torch.cat(\n                (cos_sim_matrix[j, :i], cos_sim_matrix[j, i + 1 :])\n            )\n            # Compute the contrast loss\n            L_row.append(\n                F.binary_cross_entropy_with_logits(\n                    cos_sim_matrix[j, i], excl, reduction=\"none\"\n                )\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 52.07351612958857}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(\n                -torch.log(\n                    torch.sigmoid(\n                        torch.clamp(\n                            cos_sim_matrix[j, i, j]\n                            - torch.max(cos_sim_matrix[j, i, :]),\n                            1e-6,\n                        )\n                    )\n                )\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 45.60160542055531}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\tassert x.ndim == 3, \"Input must have shape (num_speakers, num_utts_per_speaker, dvec_feats)\"\n    assert x.shape[1] >= 2, \"Must have at least two utterances per speaker\"\n    dvecs = x\n    N, M, _ = dvecs.shape\n    centroids = torch.mean(dvecs, 1)\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n    L = self.embed_loss(dvecs, cos_sim_matrix)\n    L = torch.mean(L, 1)\n    L = L * self.w + self.b\n    return L.mean()\n\n", "CodeBleu_score": 43.73677937562864}, {"code": "def forward(self, x, _label=None)\n\tN, M, D = x.shape\n    assert N >= 2, \"GE2ELoss needs at least two speakers\"\n    assert M >= 2, \"GE2ELoss needs at least two utterances per speaker\"\n    assert D > 0, \"GE2ELoss needs the embedding dimension to be positive\"\n    # Calculate centroids\n    centroids = torch.mean(x, 1)\n    # Make the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # Scale and shift the cosine similarity matrix\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # Calculate the loss\n    loss = self.embed_loss(x, cos_sim_matrix)\n    # Return the mean loss\n    return torch.mean(torch.mean(loss, 1), 0)", "CodeBleu_score": 59.37693626576722}, {"code": "def forward(self, x, _label=None)\n\tassert x.ndim == 3, f\"Input data must be 3-dimensional, received {x.ndim} dimensions.\"\n    assert x.shape[1] > 1, f\"GE2E loss requires at least two utterances per speaker, received {x.shape[1]}.\"\n\n    dvecs = x\n    N, M, D = dvecs.shape\n\n    centroids = torch.mean(dvecs, 1)\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n\n    # scale and shift cosine similarity matrix\n    cos_sim_matrix = self.w * (cos_sim_matrix + self.b)\n\n    # compute loss\n    L = self.embed_loss(dvecs, cos_sim_matrix)\n\n    # average loss over all utterances\n    L = torch.mean(L)\n\n    return L", "CodeBleu_score": 42.984843527380114}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoice_dirs = [\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\")\n    ] + extra_voice_dirs\n    voices = {}\n    for voice_dir in voice_dirs:\n        for subdir, _, files in os.walk(voice_dir):\n            if not files:\n                continue\n            if any(file.endswith(\".npz\") for file in files):\n                voices[subdir] = [\n                    os.path.join(subdir, file) for file in files if file.endswith(\".npz\")\n                ]\n            else:\n                voices[subdir] = [\n                    os.path.join(subdir, file)\n                    for file in files\n                    if file.endswith(\".mp3\") or file.endswith(\".wav\")\n                ]\n    return voices\n\n", "CodeBleu_score": 27.366137196996764}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoice_dirs = [\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"voices\\\")\n    ] + extra_voice_dirs\n    voices = {}\n    for voice_dir in voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            subdir_path = os.path.join(voice_dir, subdir)\n            if os.path.isdir(subdir_path):\n                voice_files = glob(os.path.join(subdir_path, \\\"*.npz\\\"))\n                if not voice_files:\n                    voice_files = glob(os.path.join(subdir_path, \\\"*.wav\\\"))\n                    voice_files += glob(os.path.join(subdir_path, \\\"*.mp3\\\"))\n                voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 31.26754984102696}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    voice_dirs = [\n        os.path.join(os.path.dirname(os.path.realpath(__file__)), \"voices\"),\n        *extra_voice_dirs,\n    ]\n    for voice_dir in voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            if subdir in voices:\n                continue\n            subdir_path = os.path.join(voice_dir, subdir)\n            if not os.path.isdir(subdir_path):\n                continue\n            npz_files = glob(os.path.join(subdir_path, \"*.npz\"))\n            if len(npz_files) > 0:\n                voices[subdir] = npz_files\n                continue\n            wav_files = glob(os.path.join(subdir_path, \"*.wav\"))\n            mp3_files = glob(os.path.join(subdir_path, \"*.mp3\"))\n            voices[subdir] = wav_files + mp3_files\n    return voices\n\n", "CodeBleu_score": 28.794625347200242}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tvoices = get_voices(extra_voice_dirs)\n    if voice == \\\"random\\\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\\\"The voice {voice} is not available in the specified directories. Available voices: {list(voices.keys())}\\\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple voice files found for {voice}. Please specify a single voice file.\\\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\\\".npz\\\"):\n        return load_npz(voice_path)\n    else:\n        # convert audio to npz\n        npz_file = convert_audio(voice_path)\n        # recursively call load_voice\n        return load_voice(model, npz_file)\n\n", "CodeBleu_score": 39.697057846755015}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tvoices = get_voices(extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found. Available voices are: {voices.keys()}\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voice files found for voice {voice}. Please remove all but one.\"\n        )\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        if not os.path.exists(voice_path):\n            raise ValueError(f\"Voice {voice_path} does not exist.\")\n        # generate npz file\n        voice_path_npz = voice_path.replace(\".wav\", \".npz\").replace(\".mp3\", \".npz\")\n        if not os.path.exists(voice_path_npz):\n            logger.info(f\"Generating npz file for voice {voice_path}\")\n            audio, sr = librosa.load(voice_path, sr=None)\n            if sr != model.sample_rate:\n                audio = convert_audio(audio, sr, model.sample_rate)\n            audio = torch.from_numpy(audio).float()\n            with torch.no_grad():\n                with inference_mode(model):\n                    model.eval()\n                    model.setup_inference(audio)\n                    model.infer(audio)\n            semantic = model.semantic.detach().cpu().numpy()\n            coarse = model.coarse.detach().cpu().numpy()\n            fine = model.fine.detach().cpu().numpy()\n            np.savez(voice_path_npz, semantic_prompt=semantic, coarse_prompt=coarse, fine_prompt=fine)\n        else:\n            logger.info(f\"Loading npz file for voice {voice_path}\")\n        semantic, coarse, fine = load_", "CodeBleu_score": 40.5160982436947}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tvoices = get_voices(extra_voice_dirs)\n    if voice in voices:\n        voice_paths = voices[voice]\n        if len(voice_paths) > 1:\n            raise ValueError(f\\\"Found multiple voices for {voice}!\\\")\n        voice_path = voice_paths[0]\n        if voice_path.endswith(\\\".npz\\\"):\n            semantic, coarse, fine = load_npz(voice_path)\n        else:\n            # convert audio file to npz\n            semantic, coarse, fine = generate_npz(model, voice_path)\n        return semantic, coarse, fine\n    return None, None, None\n\n", "CodeBleu_score": 32.1478023031083}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t# Extract the context\n    context = context[:, :, : max(lens)]\n    # Pad the contexts to the longest length\n    context = nn.utils.rnn.pad_sequence(context, batch_first=True)\n    # Apply convolutional layers\n    for conv in self.convolutions:\n        context = self.dropout(F.relu(conv(context)))\n    # Apply LSTM\n    if self.lstm_type != \"\":\n        context, _ = self.bilstm(context)\n    # Apply dense layer\n    if self.use_linear:\n        context = self.dense(context)\n    return context\n\n", "CodeBleu_score": 36.00284526376094}, {"code": "def run_padded_sequence(self, context, lens)\n\tbatch_size, max_len, dim = context.shape\n    if lens is None:\n        lens = torch.ones(batch_size, dtype=torch.long) * max_len\n    lens = lens.to(context.device)\n    padded_sequence = torch.zeros(batch_size, max_len, self.out_dim).to(context.device)\n    for batch_idx in range(batch_size):\n        context_batch = context[batch_idx, : lens[batch_idx], :]\n        for conv_layer in self.convolutions:\n            context_batch = self.dropout(F.relu(conv_layer(context_batch)))\n        if self.lstm_type != \"\":\n            context_batch = context_batch.transpose(0, 1)\n            context_batch, _ = self.bilstm(context_batch)\n            context_batch = context_batch.transpose(0, 1)\n        if self.use_linear:\n            context_batch = self.dense(context_batch)\n        padded_sequence[batch_idx, : context_batch.shape[0], :] = context_batch\n    return padded_sequence\n\n", "CodeBleu_score": 31.653072106462655}, {"code": "def run_padded_sequence(self, context, lens)\n\tcontext = self.dropout(context)\n    context = context.transpose(1, 2)\n    for conv in self.convolutions:\n        context = conv(context)\n        context = F.relu(context)\n    context = self.dropout(context)\n    context = context.transpose(1, 2)\n    if self.lstm_type != \"\":\n        context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n        context, _ = self.bilstm(context)\n        context, _ = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    if self.use_linear:\n        context = self.dense(context)\n    return context\n\n", "CodeBleu_score": 34.28786359941106}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    context = context[sorted_indices]\n    context = self.run_padded_sequence(context, sorted_lens)\n    _, unsort_indices = torch.sort(sorted_indices)\n    context = context[unsort_indices]\n    return context\n\n", "CodeBleu_score": 10.957686864459381}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, indices = torch.sort(lens, descending=True)\n    context_sorted = context[indices]\n    context_sorted = self.run_padded_sequence(context_sorted, sorted_lens)\n    context = context_sorted[indices]\n    return context\n\n", "CodeBleu_score": 8.387306117988196}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tif lens is None:\n        lens = torch.ones(context.size()[0], dtype=torch.int32)\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n    context = fn(context)\n    context = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n    _, inverse_indices = torch.sort(indices)\n    context = context[inverse_indices]\n    return context\n\n", "CodeBleu_score": 46.4396612059947}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 46.689212035154924}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    if mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'senet-')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 47.69601334121906}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'senet-')\n    if mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 47.69601334121906}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\tnum_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    if self.is_training:\n        num_samples_per_worker = math.ceil(\n            num_samples_per_worker / self.repeats\n        )\n    if self.batch_size is not None:\n        num_samples_per_worker = (\n            math.ceil(num_samples_per_worker / self.batch_size)\n            * self.batch_size\n        )\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 41.79560848930552}, {"code": "def _num_samples_per_worker(self)\n\tnum_samples_per_worker = math.ceil(self.num_samples * self.repeats / self.global_num_workers)\n    if self.is_training and self.batch_size > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 31.629743718256652}, {"code": "def _num_samples_per_worker(self)\n\tnum_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    if self.repeats > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.repeats)\n    if self.is_training and self.batch_size > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 43.437208397092505}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=True, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 81.27417747636765}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    labels = []\n    filenames = []\n    class_to_idx_map = {}\n    for root, subdirs, files in os.walk(folder, topdown=True, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        if class_to_idx is None:\n            labels.append(label)\n        else:\n            label_idx = class_to_idx.get(label)\n            if label_idx is None:\n                continue\n            labels.append(label_idx)\n\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n\n    if class_to_idx is None:\n        if labels and len(set(labels)) == len(labels):\n            # No class to index map provided and all labels are unique, use them as is\n            class_to_idx_map = {l: l for l in labels}\n        else:\n            # No class to index map provided and some labels are not unique, build a mapping\n            unique_labels = list(set(labels))\n            unique_labels.sort(key=natural_key)\n            class_to_idx_map = {l: idx for idx, l in enumerate(unique_labels)}\n\n    if sort:\n        labels.sort(key=natural_key)\n        filenames.sort(key=natural_key)\n\n    return filenames, labels, class_to_idx_map\n\n", "CodeBleu_score": 56.1130862750034}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif not types:\n        types = get_img_extensions()\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames), key=lambda x: natural_key(x[0])))\n        class_to_idx = {c: i for i, c in enumerate(sorted(set(labels)))}\n    else:\n        if sort:\n            filenames = [f for _, f in sorted(zip(labels, filenames), key=lambda x: natural_key(x[0]))]\n        class_to_idx = class_to_idx\n    class_to_idx = {k: v for k, v in sorted(class_to_idx.items(), key=lambda x: natural_key(x[0]))}\n    targets = [class_to_idx[l] for l in labels]\n    return filenames, targets, class_to_idx\n\n", "CodeBleu_score": 62.6514709954196}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.initial_seed() % 2 ** 32\n\n", "CodeBleu_score": 54.00585338778634}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2 ** 32\n    else:\n        return wds.utils.seed % 2 ** 32\n\n", "CodeBleu_score": 47.99251437103263}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is None:\n        return wds.utils.get_rank()\n    else:\n        return worker_info.id\n\n", "CodeBleu_score": 47.21937287757041}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\n", "CodeBleu_score": 85.83676294835062}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\n", "CodeBleu_score": 85.83676294835062}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tif img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"RGB\":\n            img = img.point(lut)\n        else:\n            img = img.point(lut, \"L\")\n    return img\n\n\n", "CodeBleu_score": 64.54619998826618}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = [x] * n\n        x = tuple(x)\n    elif len(x) != n:\n        x = [np.mean(x)] * n\n        x = tuple(x)\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 37.564880771812966}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = [x] * n\n        _logger.warning(f'Adapting input to {n} channels.')\n    elif len(x) != n:\n        x = [np.mean(x)] * n\n        _logger.warning(f'Adapting input to {n} channels.')\n    elif len(x) == n:\n        assert True\n    else:\n        assert False\n    return x\n\n", "CodeBleu_score": 45.19694491582533}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        _logger.warning(f'The length of x ({len(x)}) does not match the required number of channels ({n}).')\n        x = tuple(repeat(np.mean(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 53.141912157363244}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n", "CodeBleu_score": 64.8572898609103}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n            tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size],\n                            resize_method)[0]\n\n    return image\n\n", "CodeBleu_score": 64.24303492742436}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32,\n    )\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack(\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\n    )\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 60.90060110915078}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 43.23220992916319}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tbbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], interpolation)[0])\n\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 45.2912303500748}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 43.23220992916319}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return results\n\n", "CodeBleu_score": 63.63886931384117}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    self.predictor(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n    return results\n\n", "CodeBleu_score": 59.00068392796325}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\ttry:\n        self.predictor.predict(model, eval_dataloader, output_file)\n        outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n        results = self.metric.compute_metrics(outputs)\n        self.metric.print_computed_metrics(results)\n        return {\"results\": results, \"metric\": self.metric}\n    except FileNotFoundError:\n        print(\"[missing]\", self.predictor.pred_dir)\n        return {}\n\n", "CodeBleu_score": 40.61538958172955}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"Rank@1: {:.4f} - Rank@5: {:.4f} - Mean IoU: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )\n\n", "CodeBleu_score": 51.03438211031745}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"R@1: {:.4f} - R@5: {:.4f} - mIoU: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )\n\n\n def _eval_predictions(self, predictions, targets):\n    \"\"\"This function evaluates the provided predictions and targets and returns the rank1, rank5, and miou values for the predictions.", "CodeBleu_score": 45.38998196879387}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"Rank@1: {:.4f} - Rank@5: {:.4f} - Mean IoU: {:.4f}\".format(rank1, rank5, miou)\n    )\n\n", "CodeBleu_score": 42.36476316588016}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\tranks = []\n    ious = []\n    for i, segment in enumerate(segments):\n        gt = data[i]\n        if len(gt) == 0:\n            continue\n        iou_max = 0\n        rank_min = 1000000\n        for g in gt:\n            iou = self._iou(segment, g)\n            if iou > iou_max:\n                iou_max = iou\n            rank = self._rank(segment, g)\n            if rank < rank_min:\n                rank_min = rank\n        ranks.append(rank_min)\n        ious.append(iou_max)\n    return np.mean(ranks), np.mean(ious), np.mean(ious)", "CodeBleu_score": 26.611922150777534}, {"code": "def _eval_predictions(self, segments, data)\n\trank1, rank5, iou = 0, 0, 0\n    for seg, gt in zip(segments, data):\n        iou += self._iou(seg, gt)\n        rank1 += self._rank(seg, gt)\n        rank5 += self._rank(seg[:5], gt)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    iou /= len(segments)\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 18.136273048919637}, {"code": "def _eval_predictions(self, segments, data)\n\trank1 = 0.0\n    rank5 = 0.0\n    mean_iou = 0.0\n    for i, segment in enumerate(segments):\n        gt = data[i]\n        ious = [_iou(pred, gt) for pred in segment]\n        ranks = [_rank(segment, gt) for pred in segment]\n        rank1 += min(ranks)\n        rank5 += min(ranks[:5])\n        mean_iou += sum(ious)/len(ious)\n    return rank1/len(segments), rank5/len(segments), mean_iou/len(segments)\n\n", "CodeBleu_score": 19.66357900518611}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tscores = []\n    model.eval()\n    model.to(0)\n    for batch in tqdm(eval_dataloader, desc=\"Prediction\"):\n        with torch.no_grad():\n            batch = self.transfer_batch_to_device(batch, 0)\n            outputs = model(**batch)\n            scores.append(self.post_eval_hook(outputs))\n    scores = self.finalize_scores(scores)\n    if output_file:\n        self.save_scores(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 44.281567780854694}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tmodel.eval()\n    scores = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = self.transfer_to_device(batch)\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2]}\n            outputs = model(**inputs)\n            scores.append(outputs)\n    return self.process_output(scores, eval_dataloader, output_file)\n\n", "CodeBleu_score": 30.113119707157836}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# initialize an empty list to store the predictions\n    predictions = []\n    # set the model to evaluation mode\n    model.eval()\n    # transfer the model to GPU 0\n    model = model.to(0)\n    # iterate over the dataloader\n    for batch in tqdm(eval_dataloader):\n        # transfer data to GPU 0\n        batch = {k: v.to(0) for k, v in batch.items()}\n        # make predictions without computing gradients\n        with torch.no_grad():\n            outputs = model(**batch)\n        # update the predictions\n        predictions.update(self(outputs))\n    # finalize the predictions\n    predictions = self.finalize(predictions)\n    # optionally save the predictions to an output file\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            json.dump(predictions, f, indent=2)\n    # return the predictions\n    return predictions\n\n", "CodeBleu_score": 48.32281636641624}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.2318132436907}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n    builtin_keys = {\n        \"loss\",\n        \"nll_loss\",\n        \"ntokens\",\n        \"nsentences\",\n        \"sample_size\",\n    }\n\n    for key in logging_outputs[0]:\n        if key not in builtin_keys:\n            val = sum(log.get(key, 0) for log in logging_outputs)\n            metrics.log_scalar(key, val / sample_size, sample_size)\n\n", "CodeBleu_score": 52.38513033246803}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.2318132436907}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\ttest_pipeline = cfg.test_pipeline\n    if 'meta_keys' in test_pipeline[0].keys():\n        meta_keys = test_pipeline[0]['meta_keys']\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n            test_pipeline[0]['meta_keys'] = meta_keys\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            test_pipeline[i]['type'] = 'mmdet.InferencerLoader'\n            break\n    else:\n        raise ValueError('LoadImageFromFile not found in test_pipeline')\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 34.79533717365196}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\ttest_pipeline = cfg.test_pipeline\n    meta_keys = test_pipeline[0]['meta_keys']\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    loader_index = -1\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            loader_index = i\n            break\n    if loader_index == -1:\n        raise ValueError(\n            'Cannot find the loader in the test pipeline! Please check the '\n            'test pipeline configuration.')\n    test_pipeline[loader_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 36.012694843611136}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\ttest_pipeline = cfg.test_pipeline\n    if 'img_id' in test_pipeline[0].keys():\n        test_pipeline[0].pop('img_id')\n    load_idx = -1\n    for idx, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_idx = idx\n            break\n    assert load_idx >= 0, 'Cannot find the LoadImageFromFile transform in the test pipeline'\n    test_pipeline[load_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 30.24149223004712}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 25.36751069417026}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 25.36751069417026}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 25.36751069417026}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 32.621363875579135}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 32.621363875579135}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 32.621363875579135}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 20.62138728861922}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 20.62138728861922}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 20.62138728861922}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.454766421265926}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.454766421265926}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.454766421265926}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.78393299731138}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.78393299731138}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.78393299731138}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.22929475931163}, {"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.22929475931163}, {"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.22929475931163}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(\n            \"Loading %s from %s to %s with sample rate %s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n        )\n        waveform, sample_rate = self.load(\n            path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype\n        )\n        return waveform, False\n    except Exception as error:\n        logger.error(\"Error while loading %s: %s\", path, error)\n        return -1.0, True\n\n", "CodeBleu_score": 37.116183094328626}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        # Load audio data.\n        data = self.load(path, offset, duration, sample_rate, dtype)\n        # Log success.\n        logger.info(\n            \"Loaded waveform from %s with offset %s, duration %s and sample rate %s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n        )\n        # Return data and no error flag.\n        return data, False\n    except Exception as error:\n        # Log error.\n        logger.error(\n            \"Failed to load waveform from %s with offset %s, duration %s and sample rate %s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n        )\n        # Log error.\n        logger.exception(error)\n        # Return error flag.\n        return -1.0, True\n\n", "CodeBleu_score": 29.888667974689742}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        # Load audio data from file.\n        wf, sr = self.load(\n            audio_descriptor=path,\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        )\n        # Log the loading process.\n        logger.info(\n            \"Loaded %s with sample rate %s and dtype %s\",\n            path,\n            sr,\n            dtype,\n        )\n        # Return the audio data and a False flag indicating no error.\n        return wf, False\n    except Exception as error:\n        # Log the error.\n        logger.error(\n            \"Unable to load %s due to %s\",\n            path,\n            error,\n        )\n        # Return a default value of -1.0 and a True flag indicating an error.\n        return -1.0, True\n\n", "CodeBleu_score": 36.55084136007565}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels <= 0:\n        raise ValueError(f\"n_channels must be a positive integer, but got {n_channels}\")\n    if n_channels == 1:\n        return waveform[:, 0]\n    if waveform.shape.rank == 1:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape[1] == n_channels:\n        return waveform\n    if waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    return to_n_channels(tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])[:, :n_channels], n_channels)\n\n", "CodeBleu_score": 38.8021442989389}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels < 1:\n        raise ValueError(\"n_channels must be a positive integer\")\n\n    if waveform.shape[1] == n_channels:\n        return waveform\n\n    if waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n\n    return tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])[:, :n_channels]\n\n", "CodeBleu_score": 45.19097534774708}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif waveform.shape[-1] == n_channels:\n        return waveform\n    if waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    return tf.tile(waveform, [1, n_channels // waveform.shape[-1]])\n\n", "CodeBleu_score": 46.17337081355032}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\treturn from_float32_to_uint8(\n        spectrogram_to_db(spectrogram, db_range=db_range, **kwargs)\n    )\n\n", "CodeBleu_score": 24.928276221167454}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = gain_to_db(spectrogram, **kwargs)\n    spectrogram = tf.math.maximum(spectrogram, tf.reduce_max(spectrogram) - db_range)\n    return from_float32_to_uint8(spectrogram)\n\n", "CodeBleu_score": 40.13665860110009}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tif \"epsilon\" not in kwargs:\n        kwargs[\"epsilon\"] = 10e-10\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    spectrogram_db = tf.cast(\n        from_float32_to_uint8(spectrogram_db), dtype=tf.float32\n    )\n    return spectrogram_db\n\n", "CodeBleu_score": 51.05502203562218}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_length = frame_length\n    window = hann_window(window_length, periodic=True, dtype=waveform.dtype)\n    window = window ** window_exponent\n    stfts = stft(waveform, frame_length, frame_step, window_fn=window)\n    spectrogram = tf.abs(stfts) ** spec_exponent\n    return tf.transpose(spectrogram)\n\n", "CodeBleu_score": 35.32489466788806}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\thann_window = tf.signal.hann_window(frame_length, periodic=True)\n    hann_window = tf.pow(hann_window, window_exponent)\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n    )\n    stfts = tf.pow(tf.abs(stfts), spec_exponent)\n    return tf.transpose(stfts)\n\n", "CodeBleu_score": 39.85363824250368}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_size = frame_length\n    window = hann_window(window_size, periodic=True) ** window_exponent\n    stfts = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=window_size,\n        window_fn=lambda frame_length, dtype: tf.cast(window, dtype),\n        pad_end=False,\n    )\n    spectrogram = tf.math.pow(tf.math.abs(stfts), spec_exponent)\n    return tf.transpose(spectrogram, perm=[0, 2, 1])\n\n", "CodeBleu_score": 39.56182071641838}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-parentheses\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-boolean-expressions\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-boolean-expressions\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-boolean-expressions\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-instance-attributes", "CodeBleu_score": 15.230481727574752}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tnew_time_dim = int(spectrogram.shape[0] * factor)\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [new_time_dim, spectrogram.shape[1]],\n        method=method,\n        preserve_aspect_ratio=False,\n    )\n    if new_time_dim > spectrogram.shape[0]:\n        spectrogram = tf.pad(\n            spectrogram,\n            [[0, new_time_dim - spectrogram.shape[0]], [0, 0]],\n            mode=\"constant\",\n        )\n    else:\n        spectrogram = spectrogram[:new_time_dim, :]\n    return spectrogram\n\n", "CodeBleu_score": 33.073454442672414}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n    new_time_dim = tf.cast(tf.math.round(tf.cast(spectrogram.shape[0], tf.float32) * factor), tf.int32)\n    stretched = tf.image.resize(spectrogram, [new_time_dim, spectrogram.shape[1]], method=method)\n    if new_time_dim < spectrogram.shape[0]:\n        return stretched[:spectrogram.shape[0], :]\n    elif new_time_dim > spectrogram.shape[0]:\n        return tf.pad(stretched, [[0, spectrogram.shape[0] - new_time_dim], [0, 0]])\n    else:\n        return stretched\n\n", "CodeBleu_score": 40.1708956611833}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# The resizing factor is calculated based on the semitone shift factor\n    resize_factor = tf.pow(2.0, semitone_shift / 12.0)\n    # The resizing factor is used to resize the spectrogram along the frequency axis\n    ps_spec = tf.image.resize_images(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resize_factor, tf.int32)],\n        method=method,\n        align_corners=True,\n    )\n    # The resized spectrogram is padded to maintain the original dimensions\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1])\n\n", "CodeBleu_score": 34.48971163425294}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tF = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * 2 ** (semitone_shift / 12), tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [F_ps, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, F, F)\n\n", "CodeBleu_score": 37.96643782771717}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tF = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * 2 ** (semitone_shift / 12), tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [F_ps, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, F, F)\n\n", "CodeBleu_score": 37.96643782771717}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    if params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 35.960517705512615}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    if params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 40.73345998928395}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    if params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 35.960517705512615}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\tsha256_hash = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256_hash.update(data)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 34.44644883002955}, {"code": "def compute_file_checksum(path)\n\tsha256_hash = hashlib.sha256()\n    with open(path, 'rb') as file:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: file.read(4096), b''):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 47.05626842221085}, {"code": "def compute_file_checksum(path)\n\tsha256_hash = hashlib.sha256()\n    with open(path, 'rb') as file:\n        while True:\n            data = file.read(65536)\n            if not data:\n                break\n            sha256_hash.update(data)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 34.11127942465684}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    logger.info(\"Retrieving checksum index from %s\", url)\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 39.1057468151671}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    with httpx.Client() as client:\n        response = client.get(url)\n        response.raise_for_status()\n        checksum_index: Dict[str, str] = response.json()\n        if name not in checksum_index:\n            raise ValueError(f\"Model {name} not found in checksum index\")\n        return checksum_index[name]\n\n", "CodeBleu_score": 44.32787105746927}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    logger.debug(f\"Getting checksum for model {name} from {url}\")\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums: Dict[str, str] = response.json()\n    if name not in checksums:\n        raise ValueError(f\"Checksum not found for model {name}\")\n    return checksums[name]\n\n", "CodeBleu_score": 43.22113303237655}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    with NamedTemporaryFile() as temporary_file:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                temporary_file.write(chunk)\n        temporary_file.flush()\n        logger.info(f\"Downloaded model {name} to {temporary_file.name}\")\n        checksum: str = compute_file_checksum(temporary_file.name)\n        if checksum != self.checksum(name):\n            raise ValueError(\n                f\"Checksum mismatch for model {name}: expected {self.checksum(name)}, got {checksum}\"\n            )\n        with tarfile.open(temporary_file.name, \"r:gz\") as archive:\n            archive.extractall(path)\n    logger.info(f\"Extracted model {name} to {path}\")", "CodeBleu_score": 48.95258286098932}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    with NamedTemporaryFile() as file:\n        logger.info(\"Downloading model archive from %s\", url)\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n        logger.info(\"Model archive downloaded to %s\", file.name)\n        file.seek(0)\n        checksum: str = compute_file_checksum(file.name)\n        if checksum != self.checksum(name):\n            raise RuntimeError(\"Checksum mismatch\")\n        logger.info(\"Checksum verified\")\n        with tarfile.open(file.name, \"r:gz\") as archive:\n            archive.extractall(path)\n        logger.info(\"Model extracted to %s\", path)", "CodeBleu_score": 51.36448822595114}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    logger.info(\"Downloading %s from %s\", name, url)\n    with NamedTemporaryFile(delete=False) as file:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n    logger.info(\"Downloaded %s\", name)\n    checksum: str = self.checksum(name)\n    logger.info(\"Expected checksum for %s: %s\", name, checksum)\n    file_checksum: str = compute_file_checksum(file.name)\n    logger.info(\"Computed checksum for %s: %s\", name, file_checksum)\n    if file_checksum != checksum:\n        raise ValueError(f\"Checksum for {name} does not match\")\n    logger.info(\"Extracting %s to %s\", name, path)\n    with tarfile.open(file.name, \"r:gz\") as archive:\n        archive.extractall(path)\n    os.remove(file.name)\n    logger.info(\"Extracted %s\", name)\n\n", "CodeBleu_score": 52.47466425781173}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            with loader.open_text(resources, f\\\"{name}.json\\\") as fd:\n                return json.load(fd)\n        except FileNotFoundError as e:\n            raise SpleeterError(f\\\"Embedded configuration {name} not found.\\\"\n                                ) from e\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as fd:\n            return json.load(fd)\n    else:\n        raise SpleeterError(f\\\"Configuration file {descriptor} not found.\\\"\n                            )", "CodeBleu_score": 52.73286520269814}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            with loader.open_text(resources, name) as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration '{name}' not found.\\\")\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(f\\\"Configuration file '{descriptor}' not found.\\\")", "CodeBleu_score": 54.212592414265885}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            with loader.open_text(resources, f\\\"{name}.json\\\") as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise SpleeterError(\n                f\\\"Embedded configuration {descriptor} not found.\\\"\n            )\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as file:\n            return json.load(file)\n    raise SpleeterError(f\\\"Configuration {descriptor} not found.\\\")\n\n", "CodeBleu_score": 53.44305901483067}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"Concatenation axis should be either 0 or 1, but got {concat_axis}.\"\n        )\n    if not tensor_dict:\n        return {}\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.expand_dims(tensor, axis=0) for key, tensor in tensor_dict.items()\n        }\n    tensor_dict = {key: tensor.numpy() for key, tensor in tensor_dict.items()}\n    tensor_df = pd.DataFrame(tensor_dict)\n    tensor_df = pd.concat([tensor_df], axis=concat_axis)\n    tensor_df = func(tensor_df)\n    tensor_dict = tensor_df.to_dict(orient=\"list\")\n    tensor_dict = {key: tf.convert_to_tensor(value) for key, value in tensor_dict.items()}\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.squeeze(tensor, axis=0) for key, tensor in tensor_dict.items()\n        }\n    return tensor_dict\n\n", "CodeBleu_score": 43.73122424873218}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            \"Concatenation axis must be either 0 (vertical) or 1 (horizontal).\"\n        )\n    if len(tensor_dict) == 0:\n        raise ValueError(\"Empty dictionary.\")\n    if not all(\n        [\n            tensor_dict[key].shape[concat_axis] == tensor_dict[list(tensor_dict.keys())[0]].shape[concat_axis]\n            for key in tensor_dict.keys()\n        ]\n    ):\n        raise ValueError(\"Tensors must have the same shape along the concatenation axis.\")\n    if concat_axis == 0:\n        tensor_dict = {key: tf.expand_dims(tensor_dict[key], axis=0) for key in tensor_dict}\n    tensor_concat = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    tensor_processed = func(tensor_concat)\n    if concat_axis == 0:\n        tensor_processed = tf.squeeze(tensor_processed, axis=0)\n    tensor_dict_processed = {\n        key: tensor_processed[i] for i, key in enumerate(tensor_dict.keys())\n    }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 44.72206685111602}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in (0, 1):\n        raise ValueError(\n            f\"Concatenation axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=1)\n    return dict(zip(tensor_dict.keys(), processed_tensors))\n\n", "CodeBleu_score": 39.273917360367165}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tif tensor.dtype != tf.float32:\n        raise ValueError(\n            \"The tensor must be of type float32. Got {}\".format(tensor.dtype)\n        )\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor_scaled = tf.cast(tensor_scaled * 255, tf.uint8)\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 63.692878016540334}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor_scaled = tf.cast(tensor_scaled * 255, tf.uint8)\n    return {tensor_key: tensor_scaled, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 58.814255296855556}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor_scaled = tf.cast(tensor_scaled * 255, tf.uint8)\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 64.41147150160948}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_len = tf.shape(tensor)[0]\n    pad_len = segment_len - tensor_len % segment_len\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [tf.shape(padded_tensor)[0] // segment_len, segment_len, -1])\n\n", "CodeBleu_score": 26.091863866339267}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_shape = tf.shape(tensor)\n    tensor_len = tensor_shape[0]\n    tensor_dim = tensor_shape[1]\n    pad_len = tf.cast(tf.math.ceil(tensor_len / segment_len), tf.int32) * segment_len - tensor_len\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tensor_dim])\n\n", "CodeBleu_score": 27.789800642602376}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\tpad_len = tf.cast(\n        tf.math.ceil(tf.cast(tf.shape(tensor)[0], tf.float32) / segment_len),\n        tf.int32,\n    )\n    padded_tensor = tf.pad(\n        tensor,\n        [[0, pad_len * segment_len - tf.shape(tensor)[0]]],\n        mode=\"CONSTANT\",\n    )\n    return tf.reshape(padded_tensor, [pad_len, segment_len] + tensor.shape[1:])\n\n", "CodeBleu_score": 31.63517470589229}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tnum_extra_rows = tf.math.floormod(frame_length - tf.shape(instr_spec)[0], frame_length)\n    extension = tf.zeros(shape=(num_extra_rows, tf.shape(instr_spec)[1]), dtype=tf.float32)\n    extended_instr_spec = tf.concat([instr_spec, extension], axis=0)\n    return tf.reshape(extended_instr_spec, [tf.shape(extended_instr_spec)[0] // F, -1])\n\n", "CodeBleu_score": 21.509441465601387}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# Calculate the number of extra rows needed for padding\n    pad_size = tf.math.floormod(frame_length - tf.shape(instr_spec)[0], frame_length)\n    # Create an extension of zeros\n    pad_spec = tf.zeros([pad_size, tf.shape(instr_spec)[1]])\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = tf.concat([instr_spec, pad_spec], 0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    reshaped_spec = tf.reshape(padded_spec, [tf.shape(padded_spec)[0] // frame_length, frame_length * F])\n    return reshaped_spec\n\n", "CodeBleu_score": 17.819222622171015}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# Calculate the number of extra rows needed for padding\n    num_pad = tf.math.floormod(frame_length - tf.shape(instr_spec)[0], frame_length)\n    # Create an extension of zeros\n    extension = tf.zeros((num_pad, tf.shape(instr_spec)[1]), dtype=tf.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], 0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, (F, -1))\n    return instr_spec\n\n", "CodeBleu_score": 18.12746960387493}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset", "CodeBleu_score": 36.57982072299581}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 36.57982072299581}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 26.170329590788334}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for dim_idx, dim_len in enumerate(target_shape):\n        if dim_len is not None:\n            result = result and (tf.shape(tensor_tf)[dim_idx] == dim_len)\n    return result\n\n", "CodeBleu_score": 34.698563791717326}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result = result and (tensor_tf.shape[i] == target_length)\n    return result\n\n", "CodeBleu_score": 41.25079838157243}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_dim_len in enumerate(target_shape):\n        if target_dim_len is not None:\n            result = result and (tensor_tf.shape[i] == target_dim_len)\n    return result\n\n", "CodeBleu_score": 34.00784128904885}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize dataset builder\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n\n    # Build dataset\n    return builder.build(\n        train_csv=train_csv,\n        cache_dir=cache_dir,\n        batch_size=batch_size,\n        n_chunks_per_song=n_chunks_per_song,\n        data_augmentation=data_augmentation,\n        cache=cache,\n    )\n\n", "CodeBleu_score": 20.698657301072096}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize the dataset builder\n    dataset_builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=30,\n        random_seed=123,\n    )\n\n    # Build the dataset\n    dataset = dataset_builder.build(\n        train_csv=\"train.csv\",\n        cache_dir=None,\n        batch_size=32,\n        n_chunks_per_song=3,\n        augment_data=True,\n        cache_data=False,\n    )\n\n    return dataset\n\n", "CodeBleu_score": 35.641572430434046}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n\n    # Initialize the dataset builder\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=chunk_duration,\n        random_seed=random_seed,\n    )\n\n    # Build the dataset\n    dataset = builder.build(\n        train_csv,\n        cache_dir,\n        batch_size,\n        n_chunks_per_song,\n        data_augmentation,\n        cache_refresh,\n        cache_format,\n    )\n    return dataset\n\n", "CodeBleu_score": 37.76603415092702}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params)\n    model_dir = model_provider.get_model_dir()\n    params = model_provider.update_parameters(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        session_config=session_config,\n        save_summary_steps=0,\n        save_checkpoints_steps=0,\n    )\n    model_fn_instance = model_fn.ModelFn(\n        params=params,\n        MWF=MWF,\n        builder=EstimatorSpecBuilder(\n            params=params,\n            MWF=MWF,\n        ),\n        input_provider_factory=InputProviderFactory(params=params),\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_fn_instance,\n        config=run_config,\n        params=params,\n    )\n\n", "CodeBleu_score": 27.27887349215804}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params)\n    model_dir = model_provider.model_dir\n    params = model_provider.params\n    if MWF:\n        params['MWF'] = MWF\n    params['model_dir'] = model_dir\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        save_summary_steps=0,\n        session_config=session_config,\n        keep_checkpoint_max=1,\n        tf_random_seed=42)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        config=run_config,\n        params=params)\n    return estimator\n\n", "CodeBleu_score": 47.760726272545725}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params['model_path'])\n    model_params = model_provider.load_json(params['model_filename'])\n    model_params.update(params)\n    model_params['MWF'] = MWF\n    model_params['model_filename'] = params['model_filename']\n    model_params['model_path'] = params['model_path']\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.allow_soft_placement = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_provider.model_dir,\n        tf_random_seed=model_params['random_seed'],\n        save_checkpoints_steps=model_params['save_checkpoints_steps'],\n        keep_checkpoint_max=model_params['keep_checkpoint_max'],\n        log_step_count_steps=model_params['log_step_count_steps'],\n        session_config=config,\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_provider.model_dir,\n        params=model_params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 33.431605274769375}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.shape[1] == 1:\n        waveform = to_stereo(waveform)\n    self._data_generator.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor})\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 51.88564762902491}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t# Ensure stereo\n    if waveform.shape[1] == 1:\n        waveform = to_stereo(waveform)\n\n    # Update data\n    self._data_generator.update_data({\n        \\\"waveform\\\": waveform,\n        \\\"audio_id\\\": audio_descriptor.id,\n    })\n\n    # Get prediction\n    prediction = next(self._get_prediction_generator())\n\n    # Remove audio_id\n    del prediction[\\\"audio_id\\\"]\n\n    return prediction\n\n\n", "CodeBleu_score": 48.0404144862721}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.ndim == 1:\n        waveform = to_stereo(waveform)\n    self._data_generator.update_data(\n        (\n            (waveform, audio_descriptor.id),\n            (audio_descriptor.duration, audio_descriptor.sample_rate),\n        )\n    )\n    prediction = next(self._get_prediction_generator())\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 49.47684668381318}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n\n\n", "CodeBleu_score": 94.44046099762427}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only. TensorFlow backend.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.44046099762427}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only. Tweak this function to support other backbones.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n\n\n", "CodeBleu_score": 94.44046099762427}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    K.clear_session()\n\n    # Define the input layer\n    input_layer = KL.Input(shape=(x_train.shape[1:]))\n\n    # Define the convolutional layers\n    conv1 = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    conv2 = KL.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(conv1)\n\n    # Define the max-pooling layer\n    pool1 = KL.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Flatten the output of the previous layer\n    flat1 = KL.Flatten()(pool1)\n\n    # Define the dense layers\n    dense1 = KL.Dense(units=128, activation='relu')(flat1)\n    dense2 = KL.Dense(units=num_classes, activation='softmax')(dense1)\n\n    # Create the model\n    model = KM.Model(inputs=input_layer, outputs=dense2)\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 29.975800121110552}, {"code": "def build_model(x_train, num_classes)\n\t# Reset TensorFlow graph\n    K.clear_session()\n\n    # Define input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # Define model\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=128, activation='relu')(x)\n    x = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # Create model\n    model = KM.Model(inputs=input_layer, outputs=x)\n\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 33.12861051198144}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the graph\n    K.clear_session()\n\n    # Define the input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # Define the first convolutional layer\n    conv1 = KL.Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_layer)\n    relu1 = KL.Activation('relu')(conv1)\n    pool1 = KL.MaxPooling2D(pool_size=(2, 2))(relu1)\n\n    # Define the second convolutional layer\n    conv2 = KL.Conv2D(filters=32, kernel_size=(3, 3), padding='same')(pool1)\n    relu2 = KL.Activation('relu')(conv2)\n    pool2 = KL.MaxPooling2D(pool_size=(2, 2))(relu2)\n\n    # Flatten the output of the second convolutional layer\n    flatten = KL.Flatten()(pool2)\n\n    # Define the dense layer\n    dense = KL.Dense(units=128, activation='relu')(flatten)\n\n    # Define the output layer\n    output = KL.Dense(units=num_classes, activation='softmax')(dense)\n\n    # Create the model\n    model = KM.Model(inputs=input_layer, outputs=output)\n\n    # Compile the model\n    model.compile(optimizer=keras.optimizers.Adam(),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 40.50858625134923}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\tboxes = []\n    # if mask is None:\n    #     return boxes\n    # mask = np.squeeze(mask)\n    # for i in range(mask.shape[-1]):\n    #     mask_layer = mask[:, :, i]\n    #     horizontal_indices = np.where(np.any(mask_layer, axis=0))[0]\n    #     vertical_indices = np.where(np.any(mask_layer, axis=1))[0]\n    #     if horizontal_indices.shape[0]:\n    #         x1, x2 = horizontal_indices[[0, -1]]\n    #         y1, y2 = vertical_indices[[0, -1]]\n    #         x2 += 1\n    #         y2 += 1\n    #         boxes.append([x1, y1, x2, y2])\n    #     else:\n    #         boxes.append([0, 0, 0, 0])\n    return boxes\n\n", "CodeBleu_score": 0.864487454212114}, {"code": "def extract_bboxes(mask)\n\t# Calculate bounding boxes for each mask layer\n    bboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box\n        horizontal_indices = np.where(np.any(m, axis=0))[0]\n        vertical_indices = np.where(np.any(m, axis=1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes.append([y1, x1, y2, x2])\n    return np.array(bboxes).astype(np.int32)\n\n", "CodeBleu_score": 60.13940483724042}, {"code": "def extract_bboxes(mask)\n\t# Calculate bounding boxes for each mask layer\n    bboxes = []\n    for mask_layer in range(mask.shape[2]):\n        mask_layer = mask[:, :, mask_layer]\n        # Bounding box\n        horizontal_indices = np.where(np.any(mask_layer, axis=0))[0]\n        vertical_indices = np.where(np.any(mask_layer, axis=1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes.append([y1, x1, y2, x2])\n    return np.array(bboxes).astype(np.int32)\n\n", "CodeBleu_score": 57.61762634811459}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(\n        0.0, y2 - y1) * np.maximum(0.0, x2 - x1)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 52.04698317660829}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\tarea1 = boxes1[:, 2] * boxes1[:, 3]\n    area2 = boxes2[:, 2] * boxes2[:, 3]\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        box1 = boxes1[i]\n        y1 = np.maximum(box1[0], boxes2[:, 0])\n        y2 = np.minimum(box1[2], boxes2[:, 2])\n        x1 = np.maximum(box1[1], boxes2[:, 1])\n        x2 = np.minimum(box1[3], boxes2[:, 3])\n        intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n        union = area1[i] + area2 - intersection\n        overlaps[i, :] = intersection / union\n    return overlaps\n\n", "CodeBleu_score": 47.10000176715023}, {"code": "def compute_overlaps(boxes1, boxes2)\n\tarea1 = boxes1[:, 2] * boxes1[:, 3]\n    area2 = boxes2[:, 2] * boxes2[:, 3]\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        box1 = boxes1[i]\n        y1 = np.maximum(box1[0], boxes2[:, 0])\n        y2 = np.minimum(box1[2], boxes2[:, 2])\n        x1 = np.maximum(box1[1], boxes2[:, 1])\n        x2 = np.minimum(box1[3], boxes2[:, 3])\n        intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n        union = area1[i] + area2 - intersection\n        overlaps[i, :] = intersection / union\n    return overlaps\n\n", "CodeBleu_score": 47.10000176715023}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# Areas of anchors and GT boxes\n    area1 = boxes1[:, 2] * boxes1[:, 3]\n    area2 = boxes2[:, 2] * boxes2[:, 3]\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 59.55296719007704}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else [None] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.37052674878079}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else [None] * len(images)\n    n_images = len(images)\n    if cols > n_images:\n        cols = n_images\n    rows = n_images // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 75.85041244512925}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles or ['Image (%d)' % i for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 70.47522906619787}]}
