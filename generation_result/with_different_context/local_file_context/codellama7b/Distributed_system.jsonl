{"instance_id": "ba8707e206f9", "generation_result": [{"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\tloop_wait = self._loop_wait\n    if loop_wait < 2:\n        loop_wait = 2\n    return (loop_wait * 1000 - 2, self._orig_kazoo_connect(*args))\n\n", "CodeBleu_score": 46.34950378695854}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\treturn (self.loop_wait * 1000 - 2000, self._orig_kazoo_connect(*args))\n\n", "CodeBleu_score": 43.5790011904385}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\tloop_wait = self._loop_wait\n    if loop_wait is None:\n        loop_wait = 2\n    wait_time = max(2, loop_wait - 2) * 1000\n    return (wait_time, self._orig_kazoo_connect(*args))\n\n", "CodeBleu_score": 51.537718439321154}]}
{"instance_id": "88e24a7c26a4", "generation_result": [{"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}]}
{"instance_id": "2af7d7b3c887", "generation_result": [{"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}]}
{"instance_id": "43e1bc044d8e", "generation_result": [{"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}]}
{"instance_id": "93d126ab8a22", "generation_result": [{"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}]}
{"instance_id": "f68719982812", "generation_result": [{"code": "def poll(self) -> bool\n\tif self._next_poll is None:\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    elif time.time() > self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        return True\n    return False\n\n", "CodeBleu_score": 16.13782574696128}, {"code": "def poll(self) -> bool\n\tif self._next_poll is not None and self._next_poll > time.time():\n        return False\n\n    self._next_poll = time.time() + self._poll_interval\n\n    logger.info('Polling exhibitor')\n\n    for exhibitor in self._exhibitors:\n        try:\n            response = requests_get(f'http://{exhibitor}:{self._exhibitor_port}{self._uri_path}', timeout=self.TIMEOUT)\n            if response.status_code == 200:\n                data = json.loads(response.text)\n                zookeeper_hosts = data['servers']\n                if self._zookeeper_hosts != zookeeper_hosts:\n                    logger.info(f'ZooKeeper hosts changed from {self._zookeeper_hosts} to {zookeeper_hosts}')\n                    self._zookeeper_hosts = zookeeper_hosts\n                    return True\n        except Exception as e:\n            logger.warning(f'Error polling exhibitor: {e}')\n\n    return False\n\n", "CodeBleu_score": 43.190111300187176}, {"code": "def poll(self) -> bool\n\tif not self._next_poll:\n        return False\n    if time.time() < self._next_poll:\n        return False\n    self._next_poll = None\n    try:\n        response = requests_get(\n            uri.build(\n                self._exhibitors[0],\n                self._exhibitor_port,\n                self._uri_path\n            ),\n            timeout=self.TIMEOUT\n        )\n    except Exception as e:\n        logger.warning('Failed to poll exhibitor: %s', e)\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if response.status_code != 200:\n        logger.warning('Failed to poll exhibitor: %s', response.text)\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    data = json.loads(response.text)\n    if not isinstance(data, dict):\n        logger.warning('Failed to poll exhibitor: invalid response')\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if 'servers' not in data:\n        logger.warning('Failed to poll exhibitor: missing servers')\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if 'port' not in data:\n        logger.warning('Failed to poll exhibitor: missing port')\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if 'backup' not in data:\n        logger.warning('Failed to poll exhibitor: missing backup')\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if 'leader' not in data:\n        logger.warning('Failed to poll exhibitor: missing leader')\n        self._next_poll = time.time() + self._poll_interval\n        return False\n    if 'election' not in data:\n        logger.warning('Failed to poll exhibitor: missing election')\n        self._next_poll = time.time() + self._p", "CodeBleu_score": 35.260418748945035}]}
{"instance_id": "022ed3721913", "generation_result": [{"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider.poll():\n        self._hosts = self._ensemble_provider.zookeeper_hosts\n    return super()._load_cluster(path, loader)\n\n", "CodeBleu_score": 64.36674429501167}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif not self._ensemble_provider:\n        raise RuntimeError('No ensemble provider available')\n    if self._ensemble_provider.poll():\n        self.hosts = self._ensemble_provider.zookeeper_hosts\n    return super()._load_cluster(path, loader)\n\n", "CodeBleu_score": 58.95362316803744}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider and self._ensemble_provider.poll():\n        logger.info('ZooKeeper connection string has changed: %s => %s', self._hosts, self._ensemble_provider.zookeeper_hosts)\n        self._hosts = self._ensemble_provider.zookeeper_hosts\n    return super()._load_cluster(path, loader)\n\n", "CodeBleu_score": 56.35129434904091}]}
{"instance_id": "c7d2e1db0cda", "generation_result": [{"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}]}
{"instance_id": "c779f567d3e2", "generation_result": [{"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n\n    logGetStarted();\n\n    try {\n        String config = zookeeperOperation.getConfig(group, dataId);\n        if (config != null) {\n            callbackConfig(config);\n        } else {\n            logNotFound();\n        }\n    } catch (Exception e) {\n        logGetFailed(e);\n    }\n}", "CodeBleu_score": 78.11050138326428}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}]}
{"instance_id": "7f0ce4db487b", "generation_result": [{"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload();\n    }\n\n    return null;\n}", "CodeBleu_score": 62.81737738018012}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload();\n    }\n\n    return new HashMap<String, String>();\n}", "CodeBleu_score": 63.600111496976076}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n        return zookeeperServer.getPayload();\n    }\n\n    return null;\n}", "CodeBleu_score": 62.81737738018012}]}
{"instance_id": "7365d194b7ee", "generation_result": [{"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            byte[] bytes = childData.getData();\n            if (bytes == null) {\n                return;\n            }\n\n            String config = new String(bytes, \"UTF-8\");\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n    nodeCache.getListenable().addListener(zookeeperListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 68.772915998895}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData != null) {\n                String config = convertConfig(childData);\n                zookeeperSubscribeCallback.callback(config);\n            }\n        }\n    };\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n    nodeCache.getListenable().addListener(zookeeperListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 72.36626941308216}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start();\n\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData);\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n    nodeCache.getListenable().addListener(zookeeperListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 72.24967876674305}]}
{"instance_id": "6f492670162b", "generation_result": [{"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}]}
{"instance_id": "17c01a5f8ef8", "generation_result": [{"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = (RegisterListenerExecutor) applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 78.69919076873295}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = (RegisterListenerExecutor) applicationContext.getBean(\"registerListenerExecutor\");\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Can't find RegisterListenerExecutor bean\");\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 78.45543953235267}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor listenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        listenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.error(\"RegisterListenerExecutor bean not found, ignore it\");\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 80.34852586573037}]}
{"instance_id": "99392dfa2335", "generation_result": [{"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n            RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n            registerListenerExecutor.onDeregister(registration);\n        } catch (BeansException e) {\n            // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n        }\n\n        serviceRegistry.deregister(registration);\n    }", "CodeBleu_score": 89.69690765234188}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}]}
{"instance_id": "02a835946e90", "generation_result": [{"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}]}
{"instance_id": "64cf817f5945", "generation_result": [{"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId();\n    String config = getConfig(group, dataId);\n    if (config == null) {\n        return null;\n    }\n\n    logger.info(\"Subscribe config, group={}, dataId={}, config={}\", group, dataId, config);\n\n    try {\n        ZookeeperSubscribeCallback callback = new ZookeeperSubscribeCallback(this, group, dataId, ConfigType.fromBoolean(globalConfig));\n        ZookeeperListener zookeeperListener = zookeeperOperation.subscribeConfig(group, dataId, callback);\n        callback.setZookeeperListener(zookeeperListener);\n        return zookeeperListener;\n    } catch (Exception e) {\n        logger.error(\"Subscribe config fail, group={}, dataId={}, config={}\", group, dataId, config, e);\n        return null;\n    }\n}", "CodeBleu_score": 44.165814164326264}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\ttry {\n        String group = getGroup();\n        String dataId = getDataId();\n\n        if (globalConfig) {\n            logger.info(\"Start to subscribe global config, group={}, dataId={}\", group, dataId);\n        } else {\n            logger.info(\"Start to subscribe partial config, group={}, dataId={}\", group, dataId);\n        }\n\n        ZookeeperSubscribeCallback callback = new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                try {\n                    logger.info(\"Receive config, group={}, dataId={}, config={}\", group, dataId, config);\n                    updateConfig(config);\n                } catch (Exception e) {\n                    logger.error(\"Update config error\", e);\n                }\n            }\n        };\n\n        return zookeeperOperation.subscribeConfig(group, dataId, globalConfig, callback);\n    } catch (Exception e) {\n        logger.error(\"Subscribe config error\", e);\n\n        return null;\n    }\n}", "CodeBleu_score": 50.87443729228709}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId();\n    String config = getConfig(group, dataId);\n    if (config == null) {\n        return null;\n    }\n\n    log.info(\"Subscribe config, group={}, dataId={}, config={}\", group, dataId, config);\n\n    try {\n        ZookeeperSubscribeCallback callback = new ZookeeperSubscribeCallback(ConfigType.getConfigType(globalConfig), group, dataId);\n        ZookeeperListener listener = zookeeperOperation.subscribe(group, dataId, callback);\n        log.info(\"Subscribe config success, group={}, dataId={}\", group, dataId);\n\n        return listener;\n    } catch (Exception e) {\n        log.error(\"Subscribe config failed, group={}, dataId={}, config={}\", group, dataId, config, e);\n\n        return null;\n    }\n}", "CodeBleu_score": 45.2983994751106}]}
{"instance_id": "be408acc71f6", "generation_result": [{"code": "public CuratorFramework curatorFramework(){\n\tString zookeeperConnectionString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION_STRING);\n    if (StringUtils.isEmpty(zookeeperConnectionString)) {\n        throw new DiscoveryException(\"Zookeeper connection string is null\");\n    }\n\n    int zookeeperRetryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, int.class, 3);\n    int zookeeperSleepTimeMs = environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME_MS, int.class, 500);\n\n    return CuratorFrameworkFactory.builder()\n            .connectString(zookeeperConnectionString)\n            .retryPolicy(new ExponentialBackoffRetry(zookeeperSleepTimeMs, zookeeperRetryCount))\n            .build();\n}", "CodeBleu_score": 43.37456808676148}, {"code": "public CuratorFramework curatorFramework(){\n\tString connectionString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION_STRING);\n    if (StringUtils.isEmpty(connectionString)) {\n        throw new DiscoveryException(\"Zookeeper connection string is null\");\n    }\n\n    int retryCount = Integer.parseInt(environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, ZookeeperConstant.ZOOKEEPER_RETRY_COUNT_DEFAULT));\n    int sleepTime = Integer.parseInt(environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, ZookeeperConstant.ZOOKEEPER_SLEEP_TIME_DEFAULT));\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.newClient(connectionString, new ExponentialBackoffRetry(sleepTime, retryCount));\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 55.01991105424799}, {"code": "public CuratorFramework curatorFramework(){\n\tString connectionString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION_STRING);\n    if (StringUtils.isEmpty(connectionString)) {\n        throw new DiscoveryException(\"Zookeeper connection string is empty\");\n    }\n\n    int retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class, ZookeeperConstant.ZOOKEEPER_RETRY_COUNT_DEFAULT);\n    int retrySleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_SLEEP_TIME, Integer.class, ZookeeperConstant.ZOOKEEPER_RETRY_SLEEP_TIME_DEFAULT);\n\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder().connectString(connectionString);\n    ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(retrySleepTime, retryCount);\n    builder.retryPolicy(retryPolicy);\n\n    CuratorFramework curatorFramework = builder.build();\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 65.82043140092121}]}
{"instance_id": "3628d7b6f0a5", "generation_result": [{"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}]}
{"instance_id": "68a13b54ad7c", "generation_result": [{"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(\n            zooKeeperClientProperties.getBaseSleepTimeMs(),\n            zooKeeperClientProperties.getMaxRetries(),\n            zooKeeperClientProperties.getMaxSleepMs());\n    curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 61.540835474677046}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries(), zooKeeperClientProperties.getMaxSleepMs());\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .connectionTimeoutMs(zooKeeperClientProperties.getConnectionTimeoutMs())\n            .retryPolicy(retryPolicy)\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 60.34337413483616}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(),\n            zooKeeperClientProperties.getMaxRetries(),\n            zooKeeperClientProperties.getMaxSleepMs());\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder();\n    builder.connectString(zooKeeperClientProperties.getHosts())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .connectionTimeoutMs(zooKeeperClientProperties.getConnectionTimeoutMs())\n            .retryPolicy(retryPolicy)\n            .namespace(zooKeeperClientProperties.getNamespace());\n    if (zooKeeperClientProperties.getDigest() != null) {\n        builder.authorization(\"digest\", zooKeeperClientProperties.getDigest().getBytes());\n    }\n    curatorFramework = builder.build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 48.008904449399616}]}
{"instance_id": "018a3d7b5f59", "generation_result": [{"code": "public void lock(String path) throws DistributedLockException{\n\ttry {\n        InterProcessLock lock = locksMap.get(Thread.currentThread());\n        if (lock == null) {\n            lock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\n            if (!lock.acquire(30, TimeUnit.SECONDS)) {\n                throw new DistributedLockException(\"acquire lock failed\");\n            }\n            locksMap.put(Thread.currentThread(), (InterProcessMutex) lock);\n        } else {\n            if (!lock.acquire(30, TimeUnit.SECONDS)) {\n                throw new DistributedLockException(\"acquire lock failed\");\n            }\n        }\n    } catch (Exception e) {\n        throw new DistributedLockException(\"acquire lock failed\", e);\n    }\n}", "CodeBleu_score": 51.09083309594751}, {"code": "public void lock(String path) throws DistributedLockException{\n\tInterProcessLock lock = locksMap.get(Thread.currentThread());\n    if (lock == null) {\n        lock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework());\n        try {\n            lock.acquire(path);\n        } catch (Exception e) {\n            throw new DistributedLockException(\"failed to acquire lock on path \" + path, e);\n        }\n        locksMap.put(Thread.currentThread(), lock);\n    } else {\n        try {\n            lock.acquire(path);\n        } catch (Exception e) {\n            throw new DistributedLockException(\"failed to acquire lock on path \" + path, e);\n        }\n    }\n}", "CodeBleu_score": 46.86464721093888}, {"code": "public void lock(String path) throws DistributedLockException{\n\ttry {\n        Thread currentThread = Thread.currentThread();\n        InterProcessMutex lock = locksMap.get(currentThread);\n        if (lock == null) {\n            lock = new InterProcessMutex(curatorFrameworkClient.getClient(), path);\n            if (!lock.acquire(curatorFrameworkClient.getSessionTimeout(), TimeUnit.MILLISECONDS)) {\n                throw new DistributedLockException(\"Failed to acquire lock \" + path + \" in \" + curatorFrameworkClient.getSessionTimeout() + \" milliseconds\");\n            }\n            locksMap.put(currentThread, lock);\n        } else {\n            if (!lock.acquire(curatorFrameworkClient.getSessionTimeout(), TimeUnit.MILLISECONDS)) {\n                throw new DistributedLockException(\"Failed to acquire lock \" + path + \" in \" + curatorFrameworkClient.getSessionTimeout() + \" milliseconds\");\n            }\n        }\n    } catch (Exception e) {\n        throw new DistributedLockException(e);\n    }\n}", "CodeBleu_score": 44.08967247932849}]}
{"instance_id": "2068af09c973", "generation_result": [{"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    GetDataBuilder getDataBuilder = curatorFramework.getData();\n    byte[] data = getDataBuilder.forPath(nodePath);\n    if (data == null || data.length == 0) {\n        return Collections.emptyMap();\n    }\n    if (dtpProperties.getConfigFileType() == PROPERTIES) {\n        return genPropertiesTypeMap(data);\n    } else if (dtpProperties.getConfigFileType() == JSON) {\n        String configKey = dtpProperties.getConfigKey();\n        String config = new String(data, StandardCharsets.UTF_8);\n        return ConfigHandler.parse(config, configKey);\n    }\n    return Collections.emptyMap();\n}", "CodeBleu_score": 45.75596383198249}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tMap<Object, Object> propertiesMap = Maps.newHashMap();\n    if (PROPERTIES.equals(dtpProperties.getConfigType())) {\n        propertiesMap = genPropertiesTypeMap(dtpProperties);\n    } else if (JSON.equals(dtpProperties.getConfigType())) {\n        String nodePath = CuratorUtil.nodePath(dtpProperties);\n        String configKey = dtpProperties.getConfigKey();\n        String configValue = CuratorUtil.getData(nodePath, configKey);\n        propertiesMap = ConfigHandler.parseToMap(configValue);\n    }\n    return propertiesMap;\n}", "CodeBleu_score": 41.23858360588376}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    List<String> children = curatorFramework.getChildren().forPath(nodePath);\n    if (children.isEmpty()) {\n        return Collections.emptyMap();\n    }\n    String configKey = children.get(0);\n    String configValue = curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey));\n    ConfigHandler configHandler = new ConfigHandler();\n    Map<Object, Object> propertiesMap = configHandler.toMap(configValue.getBytes(StandardCharsets.UTF_8), dtpProperties.getConfigType());\n    return propertiesMap;\n}", "CodeBleu_score": 49.29278675253662}]}
{"instance_id": "3d3c9020d299", "generation_result": [{"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tlog.info(\"postProcessEnvironment\");\n    MutablePropertySources propertySources = environment.getPropertySources();\n    if (propertySources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n        propertySources.remove(ZK_PROPERTY_SOURCE_NAME);\n    }\n    DtpProperties dtpProperties = BinderHelper.getDtpProperties(environment);\n    if (dtpProperties.getZookeeper() != null) {\n        Map<String, Object> zkProperties = CuratorUtil.getZookeeperProperties(dtpProperties);\n        if (zkProperties != null && zkProperties.size() > 0) {\n            log.info(\"Zookeeper properties: {}\", zkProperties);\n            OriginTrackedMapPropertySource zkPropertySource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, zkProperties);\n            propertySources.addFirst(zkPropertySource);\n        }\n    }\n}", "CodeBleu_score": 42.76161186848017}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = BinderHelper.bindProperties(environment, DtpProperties.class);\n    if (dtpProperties.getZookeeper() != null) {\n        Map<String, Object> properties = BinderHelper.bindProperties(environment, dtpProperties.getZookeeper().getProperties());\n        if (properties.containsKey(ZookeeperProperty.ZOOKEEPER_ENABLED.getProperty())) {\n            MutablePropertySources propertySources = environment.getPropertySources();\n            OriginTrackedMapPropertySource mapPropertySource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties);\n            propertySources.addFirst(mapPropertySource);\n            CuratorUtil.setZookeeperEnabled(Boolean.valueOf(properties.get(ZookeeperProperty.ZOOKEEPER_ENABLED.getProperty()).toString()));\n        }\n    }\n}", "CodeBleu_score": 48.36460042738885}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = application.getBean(DtpProperties.class);\n    Map<String, Object> properties = BinderHelper.getProperties(dtpProperties);\n    if (properties.isEmpty()) {\n        return;\n    }\n    MutablePropertySources sources = environment.getPropertySources();\n    if (sources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n        sources.remove(ZK_PROPERTY_SOURCE_NAME);\n    }\n    OriginTrackedMapPropertySource propertySource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties);\n    sources.addFirst(propertySource);\n    CuratorUtil.initCuratorClient(dtpProperties.getZookeeper().getAddress());\n}", "CodeBleu_score": 46.45167150048135}]}
{"instance_id": "0a934e7885be", "generation_result": [{"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    return this;\n}", "CodeBleu_score": 14.1953736628044}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterType<ZooKeeperRouteManager>().As<IServiceRouteManager>().InstancePerLifetimeScope();\n    builder.RegisterType<ZooKeeperRouteFactory>().As<IServiceRouteFactory>().InstancePerLifetimeScope();\n    builder.RegisterType<ZookeeperClientProvider>().As<IZookeeperClientProvider>().InstancePerLifetimeScope();\n    builder.RegisterType<ZookeeperSerializer>().As<ISerializer>().InstancePerLifetimeScope();\n    builder.RegisterType<ZookeeperLogger>().As<ILogger<ZooKeeperRouteManager>>().InstancePerLifetimeScope();\n    builder.RegisterType<ZookeeperSerializer>().As<ISerializer>().InstancePerLifetimeScope();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().InstancePerLifetimeScope();\n    return this;\n}", "CodeBleu_score": 39.40109655181154}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteManager>().As<IServiceRouteManager>().SingleInstance();\n    builder.RegisterType<ZookeeperServiceRouteFactory>().As<IServiceRouteFactory>", "CodeBleu_score": 34.84813834244454}]}
{"instance_id": "c30a17587331", "generation_result": [{"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n            new ZooKeeperServiceCommandManager(\n                GetConfigInfo(configInfo),\n                provider.GetRequiredService<ISerializer<byte[]>>(),\n                provider.GetRequiredService<ISerializer<string>>(),\n                provider.GetRequiredService<IServiceCommandFactory>(),\n                provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                provider.GetRequiredService<IZookeeperClientProvider>()));\n            return this;\n        }", "CodeBleu_score": 69.161495106684}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n                new ZooKeeperServiceCommandManager(\n                    GetConfigInfo(configInfo),\n                    provider.GetRequiredService<ISerializer<byte[]>>(),\n                    provider.GetRequiredService<ISerializer<string>>(),\n                    provider.GetRequiredService<IServiceCommandFactory>(),\n                    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                    provider.GetRequiredService<IZookeeperClientProvider>()));\n            return this;\n        }", "CodeBleu_score": 69.23586879009477}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n              new ZooKeeperServiceCommandManager(\n                 GetConfigInfo(configInfo),\n               provider.GetRequiredService<ISerializer<byte[]>>(),\n               provider.GetRequiredService<ISerializer<string>>(),\n               provider.GetRequiredService<IServiceCommandFactory>(),\n               provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                  provider.GetRequiredService<IZookeeperClientProvider>()));\n            return this;\n        }", "CodeBleu_score": 69.16520933358572}]}
{"instance_id": "213cc4aa9b19", "generation_result": [{"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(nameof(path));\n    }\n\n    provider = provider ?? new PhysicalFileProvider(Path.GetDirectoryName(path));\n\n    var source = new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 58.093579442450036}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\t// Validate arguments\n    if (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(\"File path must be a non-empty string.\", nameof(path));\n    }\n    // Set defaults\n    provider = provider ?? new PhysicalFileProvider(Path.GetDirectoryName(path));\n    // Add the file source and build the configuration\n    return builder.AddZookeeper(new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    });\n}", "CodeBleu_score": 52.20727634742283}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\t// Validate arguments\n    if (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(Resources.Error_InvalidFilePath, nameof(path));\n    }\n\n    // Assign a default file provider if none was specified.\n    if (provider == null && Path.IsPathRooted(path)) {\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n        path = Path.GetFileName(path);\n    }\n\n    // Configure the source.\n    var source = new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange,\n        ReloadDelay = TimeSpan.FromSeconds(1)\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 66.37658627581789}]}
{"instance_id": "15bbffa82a13", "generation_result": [{"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tvar routes = await GetRoutesAsync();\n    var routeDescriptors = routes.Select(route => new ServiceRouteDescriptor(route, route.Address.Except(Address)));\n    await SetRoutesAsync(routeDescriptors);\n}", "CodeBleu_score": 39.57478897499582}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tvar routes = await GetRoutesAsync();\n    foreach (var route in routes) {\n        var addresses = route.Address.Where(p => !Address.Any(p2 => p2.Equals(p))).ToList();\n        route.Address = addresses;\n    }\n    await SetRoutesAsync(routes);\n}", "CodeBleu_score": 56.589191276116324}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u79fb\u9664\u5730\u5740\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.RoutePath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        var routes = await GetRoutesAsync();\n        foreach (var serviceRoute in routes) {\n            var addresses = serviceRoute.Address.ToList();\n            foreach (var address in Address) {\n                if (addresses.Contains(address)) {\n                    addresses.Remove(address);\n                }\n            }\n            serviceRoute.Address = addresses.ToArray();\n        }\n        await SetRoutesAsync(routes);\n    }\n}", "CodeBleu_score": 50.9794746272999}]}
{"instance_id": "125d23de2a60", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}]}
{"instance_id": "b020a235d57f", "generation_result": [{"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 17.564066808828123}]}
{"instance_id": "169ed4cc2ad4", "generation_result": [{"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 42.35116309446582}]}
{"instance_id": "af01261d3199", "generation_result": [{"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (logger.IsEnabled(LogLevel.Information))\n        logger.LogInformation(\"Ready to update service subscribers.\");\n    var zooKeepers = await zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, configInfo.SubscriberPath);\n\n        var path = configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"The node {nodePath} does not exist and will be created.\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            } else {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"The node {nodePath} will be updated.\");\n\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if (!DataEquals", "CodeBleu_score": 39.01034330704442}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (logger.IsEnabled(LogLevel.Information))\n        logger.LogInformation(\"Preparing to update service subscribers.\");\n    var zooKeepers = await zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, configInfo.SubscriberPath);\n\n        var path = configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"Node {nodePath} does not exist. Will create it.\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            } else {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"Will update data of node {nodePath}.\");\n\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if (!Data", "CodeBleu_score": 39.03826377610407}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (logger.IsEnabled(LogLevel.Information))\n        logger.LogInformation(\"Preparing to update service subscribers.\");\n    var zooKeepers = await zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, configInfo.SubscriberPath);\n\n        var path = configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"Node {nodePath} does not exist and will be created.\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"Updating data of node {nodePath}.\");\n\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if", "CodeBleu_score": 39.02082643294722}]}
{"instance_id": "fda02cec3296", "generation_result": [{"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}]}
{"instance_id": "6c3018ff0c0d", "generation_result": [{"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}]}
{"instance_id": "60ff973ba60a", "generation_result": [{"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}]}
{"instance_id": "4597b75f2b3d", "generation_result": [{"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (evt, zk) = await _zooKeeperCall();\n    var currentData = _currentData;\n    var data = await zk.getDataAsync(_path, this, null);\n    _currentData = data.Data;\n    if (data.Data != null && data.Data.Length > 0)\n    {\n        _action(currentData, data.Data);\n    }\n    evt.Set();\n}", "CodeBleu_score": 50.465029446374764}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tif (watchedEvent.get_Type() == Event.EventType.NodeDataChanged)\n    {\n        var (resetEvent, zooKeeper) = await _zooKeeperCall().ConfigureAwait(false);\n        var data = await zooKeeper.getDataAsync(watchedEvent.get_Path(), false, null).ConfigureAwait(false);\n        _action(_currentData, data.get_Data());\n        _currentData = data.get_Data();\n        resetEvent.Set();\n    }\n}", "CodeBleu_score": 52.461534958564634}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (manualResetEvent, zooKeeper) = await _zooKeeperCall();\n    var data = await zooKeeper.getDataAsync(_path, this);\n    if (data.Data != null)\n    {\n        _action(data.Data, _currentData);\n        _currentData = data.Data;\n        zooKeeper.setDataAsync(_path, _currentData, -1);\n    }\n    manualResetEvent.Set();\n}", "CodeBleu_score": 50.778627634833796}]}
{"instance_id": "cb4f0e22a0d5", "generation_result": [{"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.RegisterType<DefaultZookeeperClientProvider>().As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 27.100168259518796}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.RegisterType<DefaultZookeeperClientProvider>().As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 27.100168259518796}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.RegisterType<DefaultZookeeperClientProvider>().As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 27.100168259518796}]}
{"instance_id": "80709a4b2b69", "generation_result": [{"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"Start to setup mqtt service route in zookeeper.\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\"Start to delete node: {childPath}.\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"Start to delete node: {nodePath}.\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"Route config clear complete.\");\n    }\n}", "CodeBleu_score": 59.41394130458784}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (logger.IsEnabled(LogLevel.Information))\n        logger.LogInformation(\"Starting to add mqtt routes.\");\n    var zooKeepers = await zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = $\"/{string.Join(\"/\", childrens)}\";\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (logger.IsEnabled(LogLevel.Debug))\n                            logger.LogDebug($\"Starting to delete: {childPath}.\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"Starting to delete: {nodePath}.\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (logger.IsEnabled(LogLevel.Information))\n            logger.LogInformation(\"Route configuration clear completed.\");\n    }\n}", "CodeBleu_score": 56.68204828078629}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (logger.IsEnabled(LogLevel.Information))\n        logger.LogInformation($\"\u51c6\u5907\u8bbe\u7f6emqtt\u8def\u7531\u914d\u7f6e\u3002\");\n    var zooKeepers = await zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = $\"/{string.Join(\"/\", childrens)}\";\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (logger.IsEnabled(LogLevel.Debug))\n                            logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (logger.IsEnabled(LogLevel.Debug))\n                    logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (logger.IsEnabled(LogLevel.Information))\n            logger.LogInformation($\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\");\n\n        foreach (var route in routes) {\n            var routePath = $\"{path}/{route.Id}\";\n            var nodePath = $\"/{string.Join(\"/\", routePath.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries))}\";\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) ==", "CodeBleu_score": 57.666057988870946}]}
{"instance_id": "69ff4d3427cd", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}]}
{"instance_id": "8856055e4872", "generation_result": [{"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar serviceCaches = await GetCaches(cacheDescriptors.Select(p => p.Id));\n    await RemoveCachesAsync(cacheDescriptors);\n    await base.SetCachesAsync(cacheDescriptors);\n}", "CodeBleu_score": 9.21169061077405}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.CachePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = $\"/{string.Join(\"/\", childrens)}\";\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\");\n    }\n}", "CodeBleu_score": 51.71436927923519}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar serviceCaches = await GetCaches(cacheDescriptors.Select(p => p.Id));\n    await RemoveCachesAsync(cacheDescriptors);\n    await base.SetCachesAsync(cacheDescriptors);\n}", "CodeBleu_score": 9.21169061077405}]}
{"instance_id": "57d92c17de83", "generation_result": [{"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}]}
{"instance_id": "b276a07da453", "generation_result": [{"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar zooKeepers = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses) {\n        if (!await _healthCheckService.IsHealth(address)) {\n            continue;\n        }\n        var connectionWait = new ManualResetEvent(false);\n        var zooKeeper = new ZooKeeper($\"{address.Ip}:{address.Port}\", (int)_config.SessionTimeout.TotalMilliseconds, new ReconnectionWatcher(() => {\n            connectionWait.Set();\n        }, () => {\n            connectionWait.Close();\n        }, async () => {\n            connectionWait.Reset();\n            if (_zookeeperClients.TryRemove(address, out (ManualResetEvent, ZooKeeper) value)) {\n                await value.Item2.closeAsync();\n                value.Item1.Close();\n            }\n            CreateZooKeeper(address);\n        }));\n        zooKeepers.Add((connectionWait, zooKeeper));\n    }\n    if (zooKeepers.Count == 0) {\n        if (_logger.IsEnabled(Level.Warning)) {\n            _logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n        }\n        return zooKeepers;\n    }\n    return zooKeepers;\n}", "CodeBleu_score": 58.64542143204369}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n\tforeach (var address in _config.Addresses) {\n\t\tif (!await _healthCheckService.IsHealth(address)) {\n\t\t\tcontinue;\n\t\t}\n\t\tvar connectionWait = new ManualResetEvent(false);\n\t\tvar zooKeeper = new ZooKeeper($\"{address.Ip}:{address.Port}\", (int)_config.SessionTimeout.TotalMilliseconds, new ReconnectionWatcher(() => connectionWait.Set(), () => connectionWait.Close(), async () => {\n\t\t\tconnectionWait.Reset();\n\t\t\tif (_zookeeperClients.TryRemove(address, out (ManualResetEvent, ZooKeeper) value)) {\n\t\t\t\tawait value.Item2.closeAsync();\n\t\t\t\tvalue.Item1.Close();\n\t\t\t}\n\t\t\tCreateZooKeeper(address);\n\t\t}));\n\t\tresult.Add((connectionWait, zooKeeper));\n\t}\n\treturn result;\n}", "CodeBleu_score": 63.70456834044952}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses) {\n        if (!await _healthCheckService.IsHealth(address)) {\n            throw new RegisterConnectionException(string.Format(\"\u6ce8\u518c\u4e2d\u5fc3{0}\u8fde\u63a5\u5f02\u5e38\uff0c\u8bf7\u8054\u7cfb\u7ba1\u7406\u56ed\", address.ToString()));\n        }\n    }\n    foreach (var address in _config.Addresses) {\n        _healthCheckService.Monitor(address);\n        var task = _healthCheckService.IsHealth(address);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        result.Add(await CreateZooKeeper(address));\n    }\n    if (result.Count == 0) {\n        if (_logger.IsEnabled(Level.Warning))\n            _logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n        return result;\n    }\n    return result;\n}", "CodeBleu_score": 65.59934632117292}]}
{"instance_id": "dbb6463e7ee8", "generation_result": [{"code": "ZooKeeper getZooKeeper() throws Exception{\n\tif (helper == null) {\n        synchronized (this) {\n            if (helper == null) {\n                Helper.Data data = new Helper.Data(); // data shared between initial Helper and the un-synchronized Helper\n                // first helper is synchronized when getZooKeeper is called. Subsequent calls\n                // are not synchronized.\n                //noinspection NonAtomicOperationOnVolatileField\n                helper = new Helper(data) {\n                    @Override\n                    ZooKeeper newZooKeeper(String connectionString, int sessionTimeout, Watcher watcher) throws Exception {\n                        return zookeeperFactory.newZooKeeper(connectionString, sessionTimeout, watcher, canBeReadOnly);\n                    }\n\n                    @Override\n                    void handleNewConnection(ZooKeeper zooKeeper, String connectionString) {\n                        data.setZooKeeper(zooKeeper);\n                        data.setConnectionString(connectionString);\n                    }\n                };\n            }\n        }\n    }\n    return helper.getZooKeeper();\n}", "CodeBleu_score": 38.69983483130399}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\tif (helper == null) {\n        synchronized (this) {\n            if (helper == null) {\n                Helper.Data data = new Helper.Data();\n                //noinspection NonAtomicOperationOnVolatileField\n                helper = new Helper(data);\n                return helper.getZooKeeper();\n            }\n        }\n    }\n    return helper.getZooKeeper();\n}", "CodeBleu_score": 42.0234611933354}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\tsynchronized (this) {\n        if (helper == null) {\n            Helper.Data data = new Helper.Data(); // data shared between initial Helper and the un-synchronized Helper\n            // first helper is synchronized when getZooKeeper is called. Subsequent calls\n            // are not synchronized.\n            //noinspection NonAtomicOperationOnVolatileField\n            helper = new Helper(data);\n        }\n        return helper.getZooKeeper();\n    }\n}", "CodeBleu_score": 37.72218040232204}]}
{"instance_id": "0d9f311aa482", "generation_result": [{"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tif (helper != null) {\n        ZooKeeper zooKeeper = helper.getZooKeeper();\n        if (zooKeeper != null) {\n            Watcher dummyWatcher = new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                }\n            };\n            zooKeeper.register(dummyWatcher);\n            if (waitForShutdownTimeoutMs > 0) {\n                zooKeeper.sync(new byte[0], dummyWatcher, waitForShutdownTimeoutMs);\n            }\n            zooKeeper.close();\n        }\n    }\n}", "CodeBleu_score": 52.285305916846646}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = null;\n    try {\n        synchronized (this) {\n            zooKeeper = helper.getZooKeeper();\n        }\n        if (zooKeeper != null) {\n            Watcher dummyWatcher = new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                }\n            };\n            zooKeeper.register(dummyWatcher);\n            if (waitForShutdownTimeoutMs == 0) {\n                zooKeeper.close();\n            } else {\n                zooKeeper.close(waitForShutdownTimeoutMs);\n            }\n        }\n    } finally {\n        if (zooKeeper != null) {\n            try {\n                zooKeeper.close();\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n    }\n}", "CodeBleu_score": 72.38730819747568}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\t//noinspection SynchronizationOnLocalVariableOrMethodParameter\n    synchronized (helper) {\n        if (helper.getZooKeeper() != null) {\n            //noinspection SynchronizationOnLocalVariableOrMethodParameter\n            synchronized (this) {\n                ZooKeeper zk = helper.getZooKeeper();\n                zk.register(new Watcher() {\n                    @Override\n                    public void process(WatchedEvent event) {\n                    }\n                });\n                zk.close();\n            }\n        }\n    }\n    if (waitForShutdownTimeoutMs > 0) {\n        try {\n            Thread.sleep(waitForShutdownTimeoutMs);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        }\n    }\n}", "CodeBleu_score": 47.57041963681601}]}
{"instance_id": "d57ef4592303", "generation_result": [{"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}]}
{"instance_id": "d722f6d0beb6", "generation_result": [{"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}]}
{"instance_id": "8492e37ce46e", "generation_result": [{"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}]}
{"instance_id": "4a01f5365731", "generation_result": [{"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId == 0) && createMode.isEphemeral()) {\n        sessionId = client.getZooKeeper().getSessionId();\n    }\n    if (doProtected() && foundNode != null) {\n        String oldSessionId = client.getZooKeeper().getSessionId();\n        if (sessionId != oldSessionId) {\n            log.warn(\"Protected mode session changed from {} to {}\", oldSessionId, sessionId);\n            client.delete().guaranteed().forPath(foundNode);\n            sessionId = oldSessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 43.268754971194035}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId == 0) && createMode.isEphemeral()) {\n        sessionId = client.getZooKeeper().getSessionId();\n    }\n    if (doProtected() && createMode.isEphemeral() && (sessionId != 0) && (foundNode != null)) {\n        long foundSessionId = client.getZooKeeper().getSessionId();\n        if (foundSessionId != sessionId) {\n            log.info(\"Protected mode detected session change from {} to {}\", sessionId, foundSessionId);\n            String path = client.getZooKeeper().getCreateExtended().getRealPath(foundNode);\n            client.delete().guaranteed().forPath(path);\n            sessionId = foundSessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 50.80507875513253}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long sessionId = client.getZooKeeper().getSessionId();\n        if (sessionId != this.sessionId) {\n            log.warn(\"Session ID changed from {} to {} during protected mode operation. Deleting old node {}.\", this.sessionId, sessionId, foundNode);\n            client.delete().forPath(foundNode);\n            this.sessionId = sessionId;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 46.27030374659328}]}
{"instance_id": "126d0922a699", "generation_result": [{"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tfinal OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = null;\n    RetryLoop loop = client.getZookeeperClient().getRetryLoop();\n    loop.withDelayOf(client.getZookeeperClient().getBaseSleepTimeMs(), client.getZookeeperClient().getMaxRetries()).run(new Callable<Object>() {\n        @Override\n        public Object call() throws Exception {\n            resultStat = client.getZooKeeper().setACL(path, aclList, version);\n            trace.setStat(resultStat).commit();\n            return null;\n        }\n    });\n    return resultStat;\n}", "CodeBleu_score": 59.027122535989704}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tfinal OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = null;\n    RetryLoop loop = client.getConnectionStateErrorPolicy().getRetryLoop();\n    loop.loop(new Callable<Object>() {\n        @Override\n        public Object call() throws Exception {\n            resultStat = client.getZooKeeper().setACL(path, aclList, version);\n            trace.setResult(resultStat).commit();\n            return null;\n        }\n    });\n    return resultStat;\n}", "CodeBleu_score": 61.164070472041544}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tfinal OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    final RetryLoop loop = client.getZookeeperClient().newRetryLoop();\n    Stat resultStat = loop.withDelay(client.getZookeeperClient().getRetryWaitMs(), Integer.MAX_VALUE).callWithRetry(new Callable<Stat>() {\n\n        @Override\n        public Stat call() throws Exception {\n            Stat stat = client.getZooKeeper().setACL(path, aclList, version);\n            trace.setStat(stat);\n            return stat;\n        }\n    });\n    trace.commit();\n    return resultStat;\n}", "CodeBleu_score": 67.19758501619928}]}
{"instance_id": "e07b079ff61f", "generation_result": [{"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkArgument(!membershipPath.endsWith(\"/\"), \"membershipPath cannot end with /\");\n    Preconditions.checkArgument(!thisId.startsWith(\"/\"), \"thisId cannot start with /\");\n\n    this.membershipPath = membershipPath;\n    this.thisId = thisId;\n\n    // check if thisId is valid\n    try {\n        ZKPaths.validatePath(thisId);\n    } catch (IllegalArgumentException e) {\n        throw new IllegalArgumentException(\"Invalid thisId: \" + thisId);\n    }\n\n    // setup CuratorCache\n    cache = CuratorCacheBridge.bridge(client, membershipPath, parentPathFilter(membershipPath));\n    cache.start();\n\n    // create PersistentNode\n    pen = new PersistentNode(client, CreateMode.EPHEMERAL, membershipPath + \"/\" + thisId, payload);\n    pen.start();\n}", "CodeBleu_score": 45.29518943208584}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkArgument(!thisId.contains(\"/\"), \"thisId cannot contain /\");\n    Preconditions.checkNotNull(payload, \"payload cannot be null\");\n\n    this.membershipPath = membershipPath;\n    this.thisId = thisId;\n\n    // check if the ID is already taken\n    if (client.checkExists().forPath(ZKPaths.makePath(membershipPath, thisId)) != null) {\n        throw new IllegalArgumentException(\"Member ID already exists: \" + thisId);\n    }\n\n    // create the node\n    pen = new PersistentNode(client,\n            ZKPaths.makePath(membershipPath, thisId),\n            payload,\n            CreateMode.EPHEMERAL);\n    pen.start();\n\n    // create the cache\n    cache = new CuratorCacheBridge(client, membershipPath, parentPathFilter(membershipPath));\n    cache.start();\n}", "CodeBleu_score": 49.156737276225726}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkArgument(thisId.length() > 0, \"thisId cannot be empty\");\n    Preconditions.checkNotNull(payload, \"payload cannot be null\");\n    Preconditions.checkArgument(payload.length > 0, \"payload cannot be empty\");\n\n    this.membershipPath = membershipPath;\n    this.thisId = thisId;\n    pen = new PersistentNode(client, true, membershipPath, payload);\n    cache = new CuratorCacheBridge(client, membershipPath, parentPathFilter(membershipPath));\n    cache.start();\n    pen.start();\n}", "CodeBleu_score": 42.79609965052511}]}
{"instance_id": "d11c76b2b912", "generation_result": [{"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path with the given payload asynchronously\n    async.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            // set up a watch on the node\n            async.checkExists().usingWatcher((WatchedEvent event) -> {\n                if (event.getType() == EventType.NodeCreated) {\n                    System.out.println(\"Node created\");\n                } else if (event.getType() == EventType.NodeDeleted) {\n                    System.out.println(\"Node deleted\");\n                }\n            }).forPath(path).whenComplete((exists, exception) -> {\n                if (exception != null) {\n                    // there was a problem\n                    exception.printStackTrace();\n                } else {\n                    if (exists != null) {\n                        System.out.println(\"Node exists\");\n                    } else {\n                        System.out.println(\"Node does not exist\");\n                    }\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 67.11950154816357}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path with the given payload asynchronously\n    async.create().creatingParentsIfNeeded().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            // set up a watch on the node to handle future events\n            async.checkExists().usingWatcher((client, event) -> {\n                if (event.getType() == WatchedEvent.EventType.NodeCreated) {\n                    System.out.println(\"Node created: \" + event.getPath());\n                } else if (event.getType() == WatchedEvent.EventType.NodeDeleted) {\n                    System.out.println(\"Node deleted: \" + event.getPath());\n                } else if (event.getType() == WatchedEvent.EventType.NodeDataChanged) {\n                    System.out.println(\"Node data changed: \" + event.getPath());\n                } else if (event.getType() == WatchedEvent.EventType.NodeChildrenChanged) {\n                    System.out.println(\"Node children changed: \" + event.getPath());\n                } else if (event.getType() == WatchedEvent.EventType.NodeCreated) {\n                    System.out.println(\"Node created: \" + event.getPath());\n                } else {\n                    System.out.println(\"Unknown event: \" + event.getType());\n                }\n            }).forPath(path).whenComplete((exists, exception) -> {\n                if (exception != null) {\n                    // there was a problem\n                    exception.printStackTrace();\n                } else {\n                    System.out.println(\"Watch set on node: \" + exists);\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 65.21963881002655}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path with the given payload asynchronously\n    async.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            System.out.println(\"Created node name is: \" + name);\n\n            // set up a watch on the node to handle future events\n            async.checkExists().usingWatcher((WatchedEvent event) -> {\n                if (event.getType() == EventType.NodeDeleted) {\n                    System.out.println(\"Node deleted\");\n                } else {\n                    System.out.println(\"Node data changed\");\n                }\n            }).forPath(path).whenComplete((exists, exception) -> {\n                if (exception != null) {\n                    // there was a problem\n                    exception.printStackTrace();\n                } else {\n                    System.out.println(\"Node exists: \" + exists);\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 68.21747857650982}]}
{"instance_id": "4571de52ad5d", "generation_result": [{"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZookeeperClient().getZooKeeper().multi(transaction.getOps());\n    if (resultList.size() == 0) {\n        throw new KeeperException.NoNodeException();\n    }\n    OpResult firstResult = resultList.get(0);\n    if (firstResult.getType() == ZooDefs.OpCode.error) {\n        OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n        throw new KeeperException(errorResult.getErr());\n    }\n    return resultList;\n}", "CodeBleu_score": 52.15270593019296}, {"code": "private List<OpResult> doOperation() throws Exception{\n\treturn client.getZooKeeper().multi(transaction.getOperations());\n}", "CodeBleu_score": 6.9495968069065635}, {"code": "private List<OpResult> doOperation() throws Exception{\n\treturn client.getZooKeeper().multi(transaction.getOperations());\n}", "CodeBleu_score": 6.9495968069065635}]}
{"instance_id": "99263700ce6d", "generation_result": [{"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\t// this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    Collection<String> serviceNames = serviceDiscovery.queryForNames();\n    System.out.println(\"Services: \" + serviceNames);\n    for (String serviceName : serviceNames) {\n        Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n        System.out.println(\"Instances for service: \" + serviceName);\n        for (ServiceInstance<InstanceDetails> instance : instances) {\n            outputInstance(instance);\n        }\n    }\n}", "CodeBleu_score": 60.920842451861986}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\t// this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    Collection<String> services = serviceDiscovery.queryForNames();\n    for (String serviceName : services) {\n        Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n        System.out.println(serviceName + \" has \" + instances.size() + \" instances:\");\n        for (ServiceInstance<InstanceDetails> instance : instances) {\n            outputInstance(instance);\n        }\n    }\n}", "CodeBleu_score": 47.85396963291977}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\t// this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you'd create the ServiceProvider early for the service(s) you're interested in\n\n    // this shows how to use a ServiceProvider\n    // in a real application you", "CodeBleu_score": 6.513438527896189}]}
{"instance_id": "891a390fa04d", "generation_result": [{"code": "private List<String> filterAndSortChildren(List<String> children){\n\tList<String> filtered = new ArrayList<String>();\n  for (String child : children) {\n    if (!READY_NODE.equals(child)) {\n      filtered.add(child);\n    }\n  }\n  Collections.sort(filtered);\n  return filtered;\n}", "CodeBleu_score": 28.408929598538368}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\tList<String> filtered = new ArrayList<String>();\n  for (String child : children) {\n    if (!child.equals(READY_NODE)) {\n      filtered.add(child);\n    }\n  }\n  Collections.sort(filtered);\n  return filtered;\n}", "CodeBleu_score": 29.163975852434472}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\tList<String> filteredChildren = Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String input) {\n            return !input.equals(READY_NODE);\n        }\n    }));\n    Collections.sort(filteredChildren);\n    return filteredChildren;\n}", "CodeBleu_score": 65.72296143480487}]}
{"instance_id": "f25af95a13b6", "generation_result": [{"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}]}
{"instance_id": "269eaac5cec6", "generation_result": [{"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}]}
{"instance_id": "c25db637a176", "generation_result": [{"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    if (pathList == null || pathList.isEmpty()) {\n        throw new IllegalArgumentException(\"pathList is null or empty\");\n    }\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).deleteAsync(path);\n        }\n    }\n}", "CodeBleu_score": 79.85908200399199}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    if (pathList == null) {\n        throw new IllegalArgumentException(\"pathList cannot be null\");\n    }\n    if (pathList.size() <= 20) {\n        for (String path : pathList) {\n            delete(serverId, path);\n        }\n    } else {\n        List<Future<Stat>> futureList = new ArrayList<>();\n        for (String path : pathList) {\n            futureList.add(zookeeperMap.get(serverId).deleteAsync(path));\n        }\n        for (Future<Stat> future : futureList) {\n            future.get();\n        }\n    }\n}", "CodeBleu_score": 59.645947536690855}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    if (pathList == null || pathList.isEmpty()) {\n        return;\n    }\n    if (pathList.size() < 20) {\n        pathList.forEach(path -> {\n            try {\n                zookeeperMap.get(serverId).delete(path);\n            } catch (Exception e) {\n                log.error(\"delete path error \" + serverId + \" -> \" + path, e);\n                throw new IllegalStateException(e);\n            }\n        });\n    } else {\n        pathList.forEach(path -> {\n            try {\n                zookeeperMap.get(serverId).deleteAsync(path);\n            } catch (Exception e) {\n                log.error(\"delete path error \" + serverId + \" -> \" + path, e);\n                throw new IllegalStateException(e);\n            }\n        });\n    }\n}", "CodeBleu_score": 52.56279922488275}]}
{"instance_id": "3ecb1ce78f90", "generation_result": [{"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to Zookeeper...\");\n    CuratorZookeeperConnectionFactory factory = new CuratorZookeeperConnectionFactory();\n    ZookeeperParams params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .build();\n    ZookeeperClient client = factory.create(params);\n    writer.write(\"Connected to Zookeeper\");\n    ZooKeeperMain main = new ZooKeeperMain(client);\n    return new Terminal(id, url, client, main);\n}", "CodeBleu_score": 51.54098788657604}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to Zookeeper...\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .aclList(List.of())\n            .maxRetries(3)\n            .connectionTimeout(3000)\n            .retryIntervalTime(1000)\n            .sessionTimeout(3000)\n            .build();\n    var connection = factory.createAsync(params, List.of());\n    writer.write(\"Successfully connected to Zookeeper\");\n    var zookeeper = connection.get();\n    var main = new ZooKeeperMain(zookeeper);\n    return new Terminal(id, url, connection, null, main);\n}", "CodeBleu_score": 62.022083753712664}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to Zookeeper...\");\n    CuratorZookeeperConnectionFactory factory = new CuratorZookeeperConnectionFactory();\n    ZookeeperParams params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .build();\n    ZooKeeperMain main = factory.create(params);\n    writer.write(\"Successfully connected to Zookeeper.\");\n    return new Terminal(id, url, main);\n}", "CodeBleu_score": 47.0418036822145}]}
{"instance_id": "a3606e777eb5", "generation_result": [{"code": "public void blockUntilConnected(){\n\tint attempt = 0;\n    while (attempt < 6) {\n        try {\n            createAsync();\n            return;\n        } catch (IllegalStateException e) {\n            log.error(\"SSH tunnel connection failed\", e);\n        }\n        attempt++;\n        try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            log.error(\"SSH tunnel connection interrupted\", e);\n        }\n    }\n    throw new IllegalStateException(\"SSH tunnel connection failed\");\n}", "CodeBleu_score": 46.92382546720245}, {"code": "public void blockUntilConnected(){\n\tint retries = 0;\n    while (retries < 6) {\n        try {\n            if (sshClient.isConnected()) {\n                return;\n            }\n            sshClient.connect(getSshHost(), getSshPort());\n            if (getSshPassword() != null && !getSshPassword().isBlank()) {\n                log.info(\"use password auth to create ssh-tunnel\");\n                sshClient.authPassword(getSshUsername(), getSshPassword());\n            } else if (getSshKeyFilePath() != null && !getSshKeyFilePath().isBlank()) {\n                log.info(\"use key file {} auth to create ssh-tunnel\", getSshKeyFilePath());\n                sshClient.authPublickey(getSshUsername(), getSshKeyFilePath());\n            } else {\n                log.info(\"use default key auth to create ssh-tunnel\");\n                sshClient.authPublickey(getSshUsername());\n            }\n            proxySocket = new ServerSocket();\n            proxySocket.setReuseAddress(true);\n            proxySocket.bind(new InetSocketAddress(localhost, localPort));\n            new Thread(() -> {\n                try {\n                    Parameters param = new Parameters(localhost, localPort, remoteHost, remotePort);\n                    sshClient.newLocalPortForwarder(param, proxySocket).listen();\n                } catch (IOException e) {\n                    throw new IllegalStateException(e.getMessage(), e);\n                }\n            }).start();\n            return;\n        } catch (IOException e) {\n            if (e.getClass().getSimpleName().contains(\"Timeout\")) {\n                throw new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n            }\n            if (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n                throw new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n            }\n            log.error(\"create ssh-tunnel failed\", e);\n            this.close();\n            throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n        }\n        retries++;\n        try {\n            Thread", "CodeBleu_score": 30.25199749859022}, {"code": "public void blockUntilConnected(){\n\ttry {\n        log.info(\"attempting to create ssh-tunnel\");\n        createAsync();\n        log.info(\"ssh-tunnel created\");\n    } catch (Exception e) {\n        log.error(\"create ssh-tunnel failed\", e);\n        this.close();\n        throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n    }\n}", "CodeBleu_score": 27.817720359458164}]}
{"instance_id": "a4e5b886b72b", "generation_result": [{"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"deleteNode failed\", e);\n        throw new IllegalStateException(\"deleteNode failed\");\n    }\n}", "CodeBleu_score": 76.84451733225598}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(\"delete node failed\");\n    }\n}", "CodeBleu_score": 82.25539154320573}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\tCompletableFuture.runAsync(() -> {\n        try {\n            zookeeperDomainService.delete(serverId, pathList);\n        } catch (Exception e) {\n            log.error(\"delete node failed\", e);\n            throw new IllegalStateException(e);\n        }\n    });\n}", "CodeBleu_score": 87.62991397116079}]}
{"instance_id": "4fdbc9ec2e75", "generation_result": [{"code": "public void disconnect(String id){\n\ttry {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        treeItemCache.remove(id);\n    } catch (Exception e) {\n        log.error(\"disconnect failed\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 47.749875994685986}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        treeItemCache.remove(id);\n    });\n}", "CodeBleu_score": 59.43696483466169}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        treeItemCache.remove(id);\n    });\n}", "CodeBleu_score": 59.43696483466169}]}
{"instance_id": "6bf0d7ac9390", "generation_result": [{"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}]}
{"instance_id": "635dea120e55", "generation_result": [{"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}]}
{"instance_id": "8627d161f5fc", "generation_result": [{"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}]}
{"instance_id": "dce20e143e04", "generation_result": [{"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}]}
{"instance_id": "d89f8e6bfb31", "generation_result": [{"code": "private String response(Socket client){\n\tfinal StringBuilder sb = new StringBuilder();\n    final BufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n    String line;\n    try {\n        while ((line = reader.readLine()) != null) {\n            sb.append(line);\n        }\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    } finally {\n        try {\n            client.close();\n        } catch (IOException e) {\n            throw new IllegalStateException(e);\n        }\n    }\n    return sb.toString();\n}", "CodeBleu_score": 56.48364464572199}, {"code": "private String response(Socket client){\n\tStringBuilder sb = new StringBuilder();\n    try (BufferedReader in = new BufferedReader(new InputStreamReader(client.getInputStream()))) {\n        String line;\n        while ((line = in.readLine()) != null) {\n            sb.append(line);\n        }\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    } finally {\n        try {\n            client.close();\n        } catch (IOException e) {\n            throw new IllegalStateException(e);\n        }\n    }\n    return sb.toString();\n}", "CodeBleu_score": 44.97843409812411}, {"code": "private String response(Socket client){\n\tStringBuilder response = new StringBuilder();\n    try {\n        BufferedReader in = new BufferedReader(new InputStreamReader(client.getInputStream()));\n        String line;\n        while ((line = in.readLine()) != null) {\n            response.append(line);\n        }\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    } finally {\n        try {\n            client.close();\n        } catch (IOException e) {\n            throw new IllegalStateException(e);\n        }\n    }\n    return response.toString();\n}", "CodeBleu_score": 54.04288081072932}]}
{"instance_id": "b0a277c5e3c0", "generation_result": [{"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tCuratorFramework client = createClient(params);\n\n    try {\n        client.start();\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n    } catch (Exception e) {\n        log.error(\"Failed to connect to Zookeeper\", e);\n        client.close();\n        throw new RuntimeException(\"Failed to connect to Zookeeper\", e);\n    }\n\n    return new CuratorZookeeperConnection(client, params);\n}", "CodeBleu_score": 61.66371374603452}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tlog.info(\"Creating connection to Zookeeper at {}\", params.getConnectionString());\n    CuratorFramework client = CuratorFrameworkFactory.builder()\n            .connectString(params.getConnectionString())\n            .sessionTimeoutMs(params.getSessionTimeout().toMillis())\n            .connectionTimeoutMs(params.getConnectionTimeout().toMillis())\n            .retryPolicy(new ExponentialBackoffRetry(\n                    params.getRetryBaseSleepTime().toMillis(),\n                    params.getRetryMaxRetries(),\n                    params.getRetryMaxSleepTime().toMillis()\n            ))\n            .authorization(\"digest\", params.getAuth().getBytes())\n            .aclProvider(new ACLProvider() {\n                @Override\n                public List<ACL> getDefaultAcl() {\n                    return ZooDefs.Ids.CREATOR_ALL_ACL;\n                }\n\n                @Override\n                public List<ACL> getAclForPath(String path) {\n                    return ZooDefs.Ids.CREATOR_ALL_ACL;\n                }\n            })\n            .build();\n    client.start();\n    try {\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n    } catch (InterruptedException e) {\n        log.error(\"Connection to Zookeeper failed\", e);\n        client.close();\n        throw new RuntimeException(\"Connection to Zookeeper failed\", e);\n    }\n    return new CuratorZookeeperConnection(client, params);\n}", "CodeBleu_score": 43.48818902053626}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tlog.info(\"Establishing connection to Zookeeper: {}\", params.getConnectionString());\n\n    // create a CuratorFramework instance\n    CuratorFramework client = CuratorFrameworkFactory.builder()\n            .connectString(params.getConnectionString())\n            .sessionTimeoutMs(params.getSessionTimeout().toMillis())\n            .connectionTimeoutMs(params.getConnectionTimeout().toMillis())\n            .retryPolicy(new ExponentialBackoffRetry(\n                    params.getBaseSleepTime().toMillis(),\n                    params.getMaxRetries(),\n                    params.getMaxSleepTime().toMillis()))\n            .namespace(params.getNamespace())\n            .aclProvider(new ACLProvider() {\n                @Override\n                public List<ACL> getDefaultAcl() {\n                    return ZooDefs.Ids.CREATOR_ALL_ACL;\n                }\n\n                @Override\n                public List<ACL> getAclForPath(String path) {\n                    return ZooDefs.Ids.CREATOR_ALL_ACL;\n                }\n            })\n            .authorization(AuthInfo.SASL(\"digest\", params.getUsername() + \":\" + params.getPassword()))\n            .build();\n\n    // start the client\n    client.start();\n\n    // wait up to 5 seconds for the connection to be established\n    try {\n        if (!client.blockUntilConnected(5, TimeUnit.SECONDS)) {\n            client.close();\n            throw new IllegalStateException(\"Connection to Zookeeper failed\");\n        }\n    } catch (InterruptedException e) {\n        client.close();\n        Thread.currentThread().interrupt();\n        throw new IllegalStateException(\"Connection to Zookeeper interrupted\", e);\n    }\n\n    // create a ZookeeperConnection\n    return new CuratorZookeeperConnection(client, params);\n}", "CodeBleu_score": 56.23936967581865}]}
{"instance_id": "b144bcd67462", "generation_result": [{"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tif (isSync.get()) {\n        log.info(\"Synchronization already in progress\");\n        return;\n    }\n    isSync.set(true);\n    log.info(\"Starting synchronization\");\n    treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n    try {\n        treeCache.start();\n    } catch (Exception e) {\n        log.error(\"Error while starting TreeCache\", e);\n        treeCache.close();\n        isSync.set(false);\n    }\n}", "CodeBleu_score": 39.98149747854004}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tif (!isSync.compareAndSet(false, true)) {\n        log.info(\"Synchronization is already in progress\");\n        return;\n    }\n    log.info(\"Starting synchronization\");\n    treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n    try {\n        treeCache.start();\n    } catch (Exception e) {\n        log.error(\"Error starting TreeCache\", e);\n        treeCache.close();\n        isSync.set(false);\n    }\n}", "CodeBleu_score": 36.10529931995784}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tlog.info(\"Syncing Zookeeper nodes for {}\", id);\n    if (isSync.getAndSet(true)) {\n        log.info(\"Synchronization already in progress for {}\", id);\n        return;\n    }\n    try {\n        treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n        treeCache.start();\n    } catch (Exception e) {\n        log.error(\"Error starting TreeCache for {}\", id, e);\n        treeCache.close();\n    }\n}", "CodeBleu_score": 39.63200417691542}]}
{"instance_id": "6452f272f2d7", "generation_result": [{"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache cache = new PathChildrenCache(client, path, true);\n    cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n    cache.getListenable().addListener(listener);\n}", "CodeBleu_score": 73.3925166751341}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache cache = new PathChildrenCache(client, path, true);\n    cache.getListenable().addListener(listener);\n    cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n}", "CodeBleu_score": 69.9834257660432}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache cache = new PathChildrenCache(client, path, true);\n    cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n    cache.getListenable().addListener(listener);\n}", "CodeBleu_score": 73.3925166751341}]}
{"instance_id": "17f786e147c3", "generation_result": [{"code": "public void unregisterService(){\n\tlogger.info(\"Unregister all service\");\n    for (String path : pathList) {\n        try {\n            curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n}", "CodeBleu_score": 68.79978973860483}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister all service\");\n    for (String path : pathList) {\n        try {\n            this.curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n    logger.info(\"Unregister all service success\");\n}", "CodeBleu_score": 65.43078799400485}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister all services\");\n    for (String path : pathList) {\n        try {\n            this.curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    this.curatorClient.close();\n    logger.info(\"Unregister all services finished\");\n}", "CodeBleu_score": 64.80588384501846}]}
{"instance_id": "32fec6cfa353", "generation_result": [{"code": "public void addListener(IZkStateListener listener){\n\tcompletableFuture.whenComplete((client, throwable) -> makeClientReady(client, throwable, listener));\n}", "CodeBleu_score": 32.350764363817845}, {"code": "public void addListener(IZkStateListener listener){\n\tif (client != null) {\n        client.subscribeStateChanges(listener);\n    }\n}", "CodeBleu_score": 20.20434458476078}, {"code": "public void addListener(IZkStateListener listener){\n\tif (client != null) {\n        client.subscribeStateChanges(listener);\n    } else {\n        logger.warn(\"Zkclient has not been started!\");\n    }\n}", "CodeBleu_score": 25.374597015265298}]}
{"instance_id": "619d57562a76", "generation_result": [{"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tthis(serverAddr, authority, 5000);\n}", "CodeBleu_score": 3.410257367309555}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tsuper(serverAddr, authority);\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder();\n    builder.connectString(serverAddr);\n    if (authority != null && !authority.isEmpty()) {\n        builder.authorization(\"digest\", (authority + \":\" + authority).getBytes());\n    }\n    builder.retryPolicy(new RetryNTimes(10, 1000));\n    builder.connectionTimeoutMs(5000);\n    client = builder.build();\n    client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n        @Override\n        public void stateChanged(CuratorFramework client, ConnectionState state) {\n            if (state == ConnectionState.LOST) {\n                stateChanged(StateListener.DISCONNECTED);\n            } else if (state == ConnectionState.CONNECTED) {\n                stateChanged(StateListener.CONNECTED);\n            } else if (state == ConnectionState.RECONNECTED) {\n                stateChanged(StateListener.RECONNECTED);\n            }\n        }\n    });\n    try {\n        client.start();\n    } catch (Exception e) {\n        throw new IllegalStateException(\"Could not start CuratorZookeeperClient\", e);\n    }\n}", "CodeBleu_score": 57.00342382628344}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tthis(serverAddr, authority, new RetryNTimes(1, 1000));\n}", "CodeBleu_score": 3.691058543727123}]}
{"instance_id": "9e7fb18946e4", "generation_result": [{"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n\tif (listeners == null) {\n\t\treturn;\n\t}\n\tTargetChildListener targetListener = listeners.remove(listener);\n\tif (targetListener != null) {\n\t\tremoveTargetChildListener(path, targetListener);\n\t}\n}", "CodeBleu_score": 78.87595434664948}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}]}
{"instance_id": "2c3fae642654", "generation_result": [{"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}]}
{"instance_id": "0b5ab34e1dd9", "generation_result": [{"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}]}
{"instance_id": "19fc5847a7c5", "generation_result": [{"code": "def make_digest_acl_credential(username, password)\n\thash = hashlib.sha1(username + ':' + password).digest()\n    return '{}:{}'.format(username, b64encode(hash).decode())\n\n", "CodeBleu_score": 17.195099052602274}, {"code": "def make_digest_acl_credential(username, password)\n\thashed = hashlib.sha1(username + \":\" + password).digest()\n    return \":\".join([username, b64encode(hashed)])\n\n", "CodeBleu_score": 16.934310988947104}, {"code": "def make_digest_acl_credential(username, password)\n\treturn \"%s:%s\" % (username, b64encode(hashlib.sha1(\"%s:%s\" % (username, password)).digest()))\n\n", "CodeBleu_score": 15.39008308331897}]}
{"instance_id": "4a062391deb0", "generation_result": [{"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}]}
{"instance_id": "aaac7523d2b5", "generation_result": [{"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}]}
{"instance_id": "bed0c998ac4c", "generation_result": [{"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}]}
{"instance_id": "b10e2c2c410b", "generation_result": [{"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}]}
{"instance_id": "42b3d26a3540", "generation_result": [{"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}]}
{"instance_id": "738974d4e9c0", "generation_result": [{"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n", "CodeBleu_score": 62.52363801861767}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n", "CodeBleu_score": 62.52363801861767}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n", "CodeBleu_score": 62.52363801861767}]}
{"instance_id": "a321fef7d100", "generation_result": [{"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.startServer();\n        map.put(\"is_running\", status);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 53.453734361962546}, {"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.startServer();\n        map.put(\"is_running\", status);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 53.453734361962546}, {"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.startServer();\n        map.put(\"is_running\", status);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 53.453734361962546}]}
{"instance_id": "24b088689114", "generation_result": [{"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 97.12561037093238}, {"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 97.12561037093238}, {"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 97.12561037093238}]}
{"instance_id": "af0884ffda66", "generation_result": [{"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n\t\treturn null;\n\t}\n\tfor (Authority authority : authorities) {\n\t\tif (authority.authorize(request)) {\n\t\t\treturn request;\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 26.046025381116948}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n        return null;\n    }\n    AuthorizationRequest updatedRequest = null;\n    for (Authority authority : authorities) {\n        updatedRequest = authority.authorize(request);\n        if (updatedRequest != null) {\n            break;\n        }\n    }\n    return updatedRequest;\n}", "CodeBleu_score": 33.95334643660946}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\t// if no authorities, return null\n\t\tif (authorities == null) {\n\t\t\treturn null;\n\t\t}\n\t\t// iterate through each authority\n\t\tfor (Authority authority : authorities) {\n\t\t\t// if the authority can authorize the request\n\t\t\tif (authority.canAuthorize(request)) {\n\t\t\t\t// update the request\n\t\t\t\tauthority.authorize(request);\n\t\t\t} else {\n\t\t\t\t// return null if authorization fails\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\t// return the updated request\n\t\treturn request;\n\t}", "CodeBleu_score": 32.49319851241166}]}
{"instance_id": "920d28a0fe76", "generation_result": [{"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 88.99806245204184}]}
{"instance_id": "a2b04232d387", "generation_result": [{"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}]}
{"instance_id": "3407087c9af6", "generation_result": [{"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}]}
{"instance_id": "91227000a139", "generation_result": [{"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.9972916890323145}]}
{"instance_id": "aec4623134b5", "generation_result": [{"code": "public void configure(){\n\ttry {\n\t\tisConfigured = true;\n\t\tuserDataProp = new BaseProperties();\n\t\tif (userDataFile.exists()) {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t}\n\t\tconvertDeprecatedProperties();\n\t} catch (IOException e) {\n\t\tthrow new FtpServerConfigurationException(\"Error configuring user manager\", e, userDataFile.getAbsolutePath());\n\t}\n}", "CodeBleu_score": 42.1606347720507}, {"code": "public void configure(){\n\tisConfigured = true;\n\t\tuserDataProp = new BaseProperties();\n\t\ttry {\n\t\t\tif (userDataFile.exists()) {\n\t\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\"Could not load users from file \" + userDataFile, e);\n\t\t}\n\t\tconvertDeprecatedProperties();\n\t}", "CodeBleu_score": 41.469244635040056}, {"code": "public void configure(){\n\tisConfigured = true;\n\t\tuserDataProp = new BaseProperties();\n\t\ttry {\n\t\t\tif (userDataFile.exists()) {\n\t\t\t\tFileInputStream in = new FileInputStream(userDataFile);\n\t\t\t\ttry {\n\t\t\t\t\tuserDataProp.load(in);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(in);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\"Could not load user data file: \" + userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\t// convert any deprecated property names\n\t\tfor (Iterator<String> iter = userDataProp.getPropertyNames(); iter.hasNext();) {\n\t\t\tString name = iter.next();\n\t\t\tif (name.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newName = PREFIX + name.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newName, userDataProp.getProperty(name));\n\t\t\t\tuserDataProp.remove(name);\n\t\t\t}\n\t\t}\n\t}", "CodeBleu_score": 55.12895163407985}]}
{"instance_id": "b5a2587f14d0", "generation_result": [{"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}]}
{"instance_id": "5cf2989cf513", "generation_result": [{"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.setPasswordEncryptor(new Md5PasswordEncryptor());\n\n        UserFactory userFactory = new UserFactory();\n        userFactory.setName(user);\n        userFactory.setPassword(user);\n        userFactory.setEnabled(true);\n        userFactory.setMaxIdleTime(0);\n\n        User user1 = userFactory.createUser();\n        userManagerFactory.delete(user1);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 66.21438710192379}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.setPasswordEncryptor(new Md5PasswordEncryptor());\n\n        UserFactory userFactory = new UserFactory();\n        userFactory.setHomeDirectory(ftpUser.getHomeDirectory());\n        userFactory.setName(ftpUser.getName());\n        userFactory.setPassword(ftpUser.getPassword());\n        userFactory.setEnabled(ftpUser.isEnabled());\n        userFactory.setMaxIdleTime(ftpUser.getMaxIdleTime());\n\n        User user = userFactory.createUser();\n        userManagerFactory.save(user, ftpUser.isRenamePush());\n        return new Result(true, \"\u65b0\u5efa\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u65b0\u5efa\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 60.56550078900179}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.setPasswordEncryptor(new Md5PasswordEncryptor());\n\n        UserFactory userFactory = new UserFactory();\n        userFactory.setName(user);\n        userFactory.setPassword(\"\");\n        userFactory.setEnabled(false);\n        userFactory.setMaxIdleTime(0);\n\n        User user1 = userFactory.createUser();\n        userManagerFactory.delete(user1);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 66.2332608368789}]}
{"instance_id": "ce383a9a6497", "generation_result": [{"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.info(\\\"open: \\\" + path + \\\" bufferSize: \\\" + bufferSize);\n    path = makeQualified(path);\n    FSDataInputStream fsDataInputStream = null;\n    try {\n        fsDataInputStream = seaweedFileSystemStore.open(path, bufferSize);\n    } catch (Exception e) {\n        LOG.warn(\\\"Exception while opening file: \\\" + path + \\\" bufferSize: \\\" + bufferSize, e);\n    }\n    return fsDataInputStream;\n}", "CodeBleu_score": 19.535395912440656}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.info(\"opening {} with buffer size {}\", path, bufferSize);\n    Path qualifiedPath = makeQualified(path);\n    try {\n        return new FSDataInputStream(new BufferedFSInputStream(seaweedFileSystemStore.openFile(qualifiedPath, bufferSize)));\n    } catch (FileNotFoundException e) {\n        LOG.warn(\"file not found: {}\", qualifiedPath);\n        return null;\n    } catch (IOException e) {\n        LOG.warn(\"error opening file: {}\", qualifiedPath, e);\n        return null;\n    }\n}", "CodeBleu_score": 37.57622640178138}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.info(\\\"Opening file: \\\" + path + \\\" with buffer size: \\\" + bufferSize);\n    Path qualifiedPath = makeQualified(path);\n    LOG.info(\\\"Opening file: \\\" + qualifiedPath + \\\" with buffer size: \\\" + bufferSize);\n    try {\n        return new FSDataInputStream(seaweedFileSystemStore.open(qualifiedPath, bufferSize));\n    } catch (Exception e) {\n        LOG.warn(\\\"Error opening file: \\\" + qualifiedPath + \\\" with buffer size: \\\" + bufferSize + \\\"\\\n\\\" + e.getMessage());\n        return null;\n    }\n}", "CodeBleu_score": 22.895802519028138}]}
{"instance_id": "f0fc12072c02", "generation_result": [{"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}]}
{"instance_id": "5104c071af46", "generation_result": [{"code": "public boolean hasCapability(String capability){\n\treturn \"hsync\".equalsIgnoreCase(capability) || \"hflush\".equalsIgnoreCase(capability)\n            ? supportFlush : false;\n}", "CodeBleu_score": 35.29811547277883}, {"code": "public boolean hasCapability(String capability){\n\tcapability = capability.toLowerCase(Locale.ROOT);\n    return capability.equals(\"hsync\") || capability.equals(\"hflush\") ? supportFlush : false;\n}", "CodeBleu_score": 30.873783081345508}, {"code": "public boolean hasCapability(String capability){\n\treturn capability.toLowerCase(Locale.ROOT).equals(\"hsync\") || capability.toLowerCase(Locale.ROOT).equals(\"hflush\") ? supportFlush : false;\n}", "CodeBleu_score": 42.129526583145726}]}
{"instance_id": "6989c860c79b", "generation_result": [{"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (offset == lastFlushOffset) {\n        return;\n    }\n    if (offset < lastFlushOffset) {\n        throw new IOException(\"Flushing bytes to service in the wrong order: \" + offset + \" < \" + lastFlushOffset);\n    }\n    if (lastError != null) {\n        throw lastError;\n    }\n    if (entry.getChunksCount() == 0) {\n        entry.setChunks(FilerProto.Chunk.newBuilder().setOffset(0).setSize(0).setMtime(System.currentTimeMillis() / 1000L).build());\n    }\n    long writeOffset = offset;\n    long writeSize = 0;\n    int chunkCount = entry.getChunksCount();\n    for (int i = 0; i < chunkCount; i++) {\n        FilerProto.Chunk chunk = entry.getChunks(i);\n        if (chunk.getOffset() + chunk.getSize() > writeOffset) {\n            break;\n        }\n        writeSize += chunk.getSize();\n    }\n    if (writeSize > 0) {\n        filerClient.writeData(entry, writeOffset - writeSize, writeSize);\n    }\n    lastFlushOffset = offset;\n    if (shouldSaveMetadata) {\n        filerClient.setEntry(path, entry);\n    }\n}", "CodeBleu_score": 29.94565617571796}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (offset == lastFlushOffset) {\n        return;\n    }\n\n    if (lastError != null) {\n        throw lastError;\n    }\n\n    if (offset < lastFlushOffset) {\n        throw new IllegalArgumentException(\"offset is smaller than the last flush offset.\");\n    }\n\n    if (offset > buffer.position()) {\n        throw new IllegalArgumentException(\"offset is larger than the buffer position.\");\n    }\n\n    if (offset > lastFlushOffset) {\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Flushing bytes from offset {} to {}\", lastFlushOffset, offset);\n        }\n\n        // If the flush offset is greater than the last flush offset, we should flush the data to the service.\n        // This is done by creating a WriteOperation and submitting it to the thread pool.\n        // The WriteOperation will be completed when the write operation is done.\n        // The completion will be signaled by the completion service.\n        // The completion service will be signaled when the write operation is done.\n        // The write operation will be submitted to the thread pool.\n        // The thread pool will be responsible for writing the data to the service.\n        // The thread pool will be responsible for writing the metadata to the filer client.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        // The thread pool will be responsible for updating the last total append offset.\n        // The thread pool will be responsible for updating the last flush offset.\n        //", "CodeBleu_score": 29.21119232684579}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (offset <= lastFlushOffset) {\n        return;\n    }\n    if (entry == null) {\n        throw new IOException(\"entry is null\");\n    }\n    if (entry.getChunksCount() == 0) {\n        throw new IOException(\"entry chunks count is 0\");\n    }\n    if (entry.getChunks(0).getOffset() != offset) {\n        throw new IOException(\"entry chunks offset is not equal to offset\");\n    }\n    if (entry.getChunks(0).getSize() != buffer.position()) {\n        throw new IOException(\"entry chunks size is not equal to buffer position\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getOffset() + entry.getChunks(entry.getChunksCount() - 1).getSize() != offset + buffer.position()) {\n        throw new IOException(\"entry chunks last offset + size is not equal to offset + buffer position\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getMtime() != entry.getAttributes().getMtime()) {\n        throw new IOException(\"entry chunks last mtime is not equal to entry attributes mtime\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getMd5().equals(entry.getAttributes().getMd5())) {\n        throw new IOException(\"entry chunks last md5 is equal to entry attributes md5\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getCipherKey().equals(entry.getAttributes().getCipherKey())) {\n        throw new IOException(\"entry chunks last cipher key is equal to entry attributes cipher key\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getIsCompressed() != entry.getAttributes().getIsCompressed()) {\n        throw new IOException(\"entry chunks last is compressed is not equal to entry attributes is compressed\");\n    }\n    if (entry.getChunks(entry.getChunksCount() - 1).getIsGzipped() != entry.", "CodeBleu_score": 25.887020130720817}]}
{"instance_id": "56e8fb9de5da", "generation_result": [{"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.086537036686305}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.086537036686305}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 14.086537036686305}]}
{"instance_id": "bf27d955c38b", "generation_result": [{"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}]}
{"instance_id": "5c7a311273ae", "generation_result": [{"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 22.22906271170882}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 22.22906271170882}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 22.22906271170882}]}
{"instance_id": "d8f07e66f690", "generation_result": [{"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"user is null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"user's home directory is null\");\n\t}\n\tthis.user = user;\n\tthis.homePath = user.getHomeDirectory().getAbsolutePath();\n}", "CodeBleu_score": 65.38766360820453}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null || user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"user or user's home directory is null\");\n\t}\n\tthis.homePath = user.getHomeDirectory().getAbsolutePath();\n\tthis.user = user;\n}", "CodeBleu_score": 54.046396093211214}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\t\tthrow new IllegalArgumentException(\"user cannot be null\");\n\t\t}\n\t\tif (user.getHomeDirectory() == null) {\n\t\t\tthrow new IllegalArgumentException(\"user's home directory cannot be null\");\n\t\t}\n\t\tthis.user = user;\n\t\tthis.homePath = user.getHomeDirectory().getAbsolutePath();\n\t}", "CodeBleu_score": 66.47574072295414}]}
{"instance_id": "27807db0eb0f", "generation_result": [{"code": "public boolean changeWorkingDirectory(String dir){\n\tPath newPath = new Path(dir);\n\tif (newPath.isAbsolute()) {\n\t\tif (newPath.toString().equals(homePath)) {\n\t\t\tcurrPath = homePath;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (newPath.toString().equals(homePath)) {\n\t\t\tcurrPath = homePath;\n\t\t\treturn true;\n\t\t} else {\n\t\t\tif (newPath.toString().equals(\"..\")) {\n\t\t\t\tif (currPath.equals(homePath)) {\n\t\t\t\t\treturn false;\n\t\t\t\t} else {\n\t\t\t\t\tcurrPath = currPath.substring(0, currPath.lastIndexOf(File.separator));\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (newPath.toString().equals(\".\")) {\n\t\t\t\t\treturn true;\n\t\t\t\t} else {\n\t\t\t\t\tcurrPath = currPath + File.separator + newPath.toString();\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 37.56167701752251}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tPath newPath = null;\n\tif (dir.equals(\"..\")) {\n\t\tif (currPath.equals(File.separator)) {\n\t\t\treturn false;\n\t\t}\n\t\tnewPath = new Path(currPath).getParent();\n\t} else {\n\t\tif (dir.startsWith(File.separator)) {\n\t\t\tnewPath = new Path(dir);\n\t\t} else {\n\t\t\tnewPath = new Path(currPath + File.separator + dir);\n\t\t}\n\t}\n\tif (newPath == null) {\n\t\treturn false;\n\t}\n\tif (!newPath.isAbsolute()) {\n\t\tnewPath = new Path(homePath + File.separator + newPath);\n\t}\n\tif (newPath.getParent() != null && newPath.getParent().equals(new Path(homePath))) {\n\t\tcurrPath = newPath.toString();\n\t\treturn true;\n\t}\n\treturn false;\n}", "CodeBleu_score": 49.38146950031786}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tPath newPath = new Path(dir);\n\tif (newPath.isAbsolute()) {\n\t\tcurrPath = newPath.toString();\n\t} else {\n\t\tcurrPath = new Path(homePath, newPath).toString();\n\t}\n\tFtpFile fileObj = getWorkingDirectory();\n\tif (fileObj.isDirectory()) {\n\t\treturn true;\n\t}\n\treturn false;\n}", "CodeBleu_score": 19.717669013698686}]}
{"instance_id": "be7aba23d184", "generation_result": [{"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    filerClient = new FilerClient(host, port, grpcPort, conf);\n    filerClient.setAccessMode(conf.get(\"fs.seaweedfs.filer.access.mode\", \"direct\"));\n}", "CodeBleu_score": 29.60839406863769}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    try {\n        filerClient = new FilerClient(host, port, grpcPort);\n        filerClient.withFilerClient(conf);\n        String mode = conf.get(SeaweedFileSystem.FS_SEAWEEDFS_VOLUME_SERVER_ACCESS_KEY);\n        if (mode != null) {\n            switch (mode.toLowerCase()) {\n                case \"direct\":\n                    filerClient.withDirectFilerAccess();\n                    break;\n                case \"publicurl\":\n                    filerClient.withPublicUrl();\n                    break;\n                case \"filerproxy\":\n                    filerClient.withFilerProxy();\n                    break;\n                default:\n                    throw new IllegalArgumentException(\"Invalid access mode: \" + mode);\n            }\n        }\n    } catch (Exception e) {\n        LOG.error(\"Failed to initialize SeaweedFileSystemStore\", e);\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 46.99794494490109}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    filerClient = new FilerClient(host, port, grpcPort, conf);\n    if (conf.get(\"fs.seaweedfs.filer.access.mode\", \"direct\").equals(\"publicUrl\")) {\n        filerClient.setPublicUrl(conf.get(\"fs.seaweedfs.filer.public.url\", \"http://localhost:8888\"));\n    }\n    if (conf.get(\"fs.seaweedfs.filer.access.mode\", \"direct\").equals(\"filerProxy\")) {\n        filerClient.setFilerProxy(conf.get(\"fs.seaweedfs.filer.proxy.url\", \"http://localhost:8888\"));\n    }\n}", "CodeBleu_score": 42.87818528160429}]}
{"instance_id": "7943a1723c23", "generation_result": [{"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}]}
{"instance_id": "36634cee9ed3", "generation_result": [{"code": "public void configure(JobConf conf){\n\tsuper.configure(conf);\n    hostName = \"localhost\";\n    try {\n      hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n      LOG.warn(\"Failed to get host name, using default value: \" + hostName);\n    }\n    threadsPerMap = conf.getInt(\"mapred.map.tasks\", 1);\n    filesPerThread = conf.getInt(\"io.mapper.files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap, new DaemonThreadFactory());\n  }", "CodeBleu_score": 53.21337987533739}, {"code": "public void configure(JobConf conf){\n\tsuper.configure(conf);\n    hostName = \"localhost\";\n    try {\n        hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n        LOG.warn(\"Could not determine local host name: \" + e.getMessage());\n    }\n    threadsPerMap = conf.getInt(\"mapred.map.tasks\", 1);\n    filesPerThread = conf.getInt(\"files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap, new ThreadFactory() {\n        private final AtomicLong threadNumber = new AtomicLong(1);\n\n        public Thread newThread(Runnable r) {\n            Thread t = new Thread(r);\n            t.setDaemon(true);\n            t.setName(\"IOMapper-\" + threadNumber.getAndIncrement());\n            return t;\n        }\n    });\n}", "CodeBleu_score": 59.92417431094166}, {"code": "public void configure(JobConf conf){\n\tsuper.configure(conf);\n    hostName = \"localhost\";\n    try {\n        hostName = InetAddress.getLocalHost().getHostName();\n    } catch (Exception e) {\n        LOG.info(\"Failed to get host name: \" + e);\n    }\n    threadsPerMap = conf.getInt(\"juicefs.threads.per.map\", 1);\n    filesPerThread = conf.getInt(\"juicefs.files.per.thread\", 1);\n    pool = Executors.newFixedThreadPool(threadsPerMap, new DaemonThreadFactory());\n}", "CodeBleu_score": 55.007610534670285}]}
{"instance_id": "53cd9065153d", "generation_result": [{"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n  if (host == null) {\n    throw new IOException(\"Invalid URI: \" + uri);\n  }\n  final String superuser = getConf(conf, \"superuser\", \"hadoop\");\n  final String supergroup = getConf(conf, \"supergroup\", \"supergroup\");\n  final String trashPath = getConf(conf, \"trash-path\", \"/tmp/juicefs/.Trash\");\n  final String trashInterval = getConf(conf, \"trash-interval\", \"10\");\n  final String trashExpiry = getConf(conf, \"trash-expiry\", \"604800\");\n  final String trashCheckInterval = getConf(conf, \"trash-check-interval\", \"10\");\n  final String trashCheckExpiry = getConf(conf, \"trash-check-expiry\", \"30\");\n  final String trashCheckThreshold = getConf(conf, \"trash-check-threshold\", \"100\");\n  final String trashCheckPartition = getConf(conf, \"trash-check-partition\", \"1\");\n  final String trashCheckThreads = getConf(conf, \"trash-check-threads\", \"1\");\n  final String trashCheckBufferSize = getConf(conf, \"trash-check-buffer-size\", \"1000000\");\n  final String trashCheckBufferTimeout = getConf(conf, \"trash-check-buffer-timeout\", \"10\");\n  final String trashCheckBufferMaxSize = getConf(conf, \"trash-check-buffer-max-size\", \"1000000000\");\n  final String trashCheckBufferMaxTimeout = getConf(conf, \"trash-check-buffer-max-timeout\", \"30\");\n  final String trashCheckBufferMaxRetry = getConf(conf, \"trash-check-buffer-max-retry\", \"10\");\n  final String trashCheckBufferMaxRetryInterval = getConf(conf, \"trash-check-buffer-max-retry-interval\", \"30\");\n  final String trashCheckBufferMaxRetryTimeout = getConf(", "CodeBleu_score": 25.823966005266264}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tString host = uri.getHost();\n    if (BgTaskUtil.isTrashEmptierRunning(host)) {\n      return;\n    }\n    try {\n      UserGroupInformation.createUserForTesting(\"juicefs\", new String[]{\"supergroup\"}).doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          final Configuration confCopy = new Configuration(conf);\n          emptierFs = new JuiceFileSystemImpl();\n          emptierFs.initialize(uri, confCopy);\n          BgTaskUtil.startTrashEmptier(host, new Runnable() {\n            @Override\n            public void run() {\n              try {\n                emptierFs.deleteTrash();\n              } catch (IOException e) {\n                LOG.error(\"Failed to delete trash\", e);\n              }\n            }\n          }, confCopy);\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      throw new IOException(\"Failed to start trash emptier\", e);\n    }\n  }", "CodeBleu_score": 38.9741533565184}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n  if (BgTaskUtil.isTrashEmptierRunning(host)) {\n    return;\n  }\n  UserGroupInformation ugi = UserGroupInformation.createRemoteUser(getConf(conf, \"trash-emptier-user\", \"hdfs\"));\n  ugi.doAs(new PrivilegedExceptionAction<Void>() {\n    @Override\n    public Void run() throws Exception {\n      Configuration fsConf = new Configuration(conf);\n      fsConf.set(JFS_URI, uri.toString());\n      fsConf.set(JFS_SUPERUSER, getConf(conf, \"trash-emptier-user\", \"hdfs\"));\n      emptierFs = FileSystem.get(fsConf);\n      BgTaskUtil.startTrashEmptier(host, new TrashEmptier(conf, emptierFs));\n      return null;\n    }\n  });\n}", "CodeBleu_score": 41.582037619258806}]}
{"instance_id": "ede797f6a8f2", "generation_result": [{"code": "public static void setContext(String context) throws Exception{\n\tCallerContext callerContext = CallerContext.getCurrent();\n    if (callerContext == null) {\n        callerContext = new CallerContext(context);\n        CallerContext.setCurrent(callerContext);\n    } else {\n        String currentContext = callerContext.getContext();\n        if (currentContext == null || !currentContext.contains(context)) {\n            callerContext = new CallerContext(currentContext + \" \" + context);\n            CallerContext.setCurrent(callerContext);\n        }\n    }\n}", "CodeBleu_score": 41.08646474556892}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext callerContext = CallerContext.getCurrent();\n  if (callerContext == null) {\n    callerContext = new CallerContext(context);\n  } else {\n    String currentContext = callerContext.getContext();\n    if (!currentContext.equals(context)) {\n      if (currentContext.length() > 0 && !currentContext.endsWith(\"|\")) {\n        currentContext += \"|\";\n      }\n      currentContext += context;\n      callerContext = new CallerContext(currentContext);\n    }\n  }\n  CallerContext.setCurrent(callerContext);\n}", "CodeBleu_score": 40.77677690411073}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext callerContext = CallerContext.getCurrent();\n    if (callerContext == null) {\n        callerContext = new CallerContext(context);\n    } else {\n        String callerContextString = callerContext.toString();\n        if (callerContextString.isEmpty()) {\n            callerContext = new CallerContext(context);\n        } else if (!callerContextString.equals(context)) {\n            callerContext = new CallerContext(callerContextString + \",\" + context);\n        }\n    }\n    CallerContext.setCurrent(callerContext);\n}", "CodeBleu_score": 38.09028056925437}]}
{"instance_id": "56968a526f6a", "generation_result": [{"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}]}
{"instance_id": "f33adb3e8cb6", "generation_result": [{"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}]}
{"instance_id": "893c46ab9675", "generation_result": [{"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}]}
{"instance_id": "3da0b4d8ae16", "generation_result": [{"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tString[] hbaseCfgArray = hbaseCfgString.split(\";\");\n    String quorum = null;\n    String znodeParent = null;\n    for (String hbaseCfg : hbaseCfgArray) {\n        if (hbaseCfg.startsWith(HBaseConstant.HBASE_CONFIG_KEY_QUORUM)) {\n            quorum = hbaseCfg.substring(hbaseCfg.indexOf(\"=\") + 1);\n        } else if (hbaseCfg.startsWith(HBaseConstant.HBASE_CONFIG_KEY_ZNODE_PARENT)) {\n            znodeParent = hbaseCfg.substring(hbaseCfg.indexOf(\"=\") + 1);\n        }\n    }\n    if (StringUtils.isEmpty(quorum)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                String.format(\"hbase.config\u4e2d\u4e0d\u80fd\u6ca1\u6709\u914d\u7f6e%s\", HBaseConstant.HBASE_CONFIG_KEY_QUORUM));\n    }\n    if (StringUtils.isEmpty(znodeParent)) {\n        znodeParent = HBaseConstant.HBASE_DEFAULT_ZNODE_PARENT;\n    }\n    return Pair.of(quorum, znodeParent);\n}", "CodeBleu_score": 31.303977349494115}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tString zkQuorum = null;\n    String znodeParent = null;\n\n    if (StringUtils.isNotBlank(hbaseCfgString)) {\n        try {\n            Map<String, String> hbaseCfg = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, String>>() {\n            });\n            zkQuorum = hbaseCfg.get(HBaseConstant.HBASE_ZOOKEEPER_QUORUM);\n            znodeParent = hbaseCfg.get(HBaseConstant.HBASE_ZNODE_PARENT);\n        } catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE, e);\n        }\n    }\n\n    if (StringUtils.isBlank(zkQuorum)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE, \"HBase\u914d\u7f6e\u4e2d\u5fc5\u987b\u6307\u5b9aZookeeper quorum\");\n    }\n\n    if (StringUtils.isBlank(znodeParent)) {\n        znodeParent = HBaseConstant.DEFAULT_ZNODE_PARENT;\n    }\n\n    return new Pair<String, String>(zkQuorum, znodeParent);\n}", "CodeBleu_score": 48.461423900772914}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tif (StringUtils.isBlank(hbaseCfgString)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The parameter of HBase configuration is not set.\");\n    }\n\n    String quorum = null;\n    String znodeParent = null;\n\n    try {\n        Map<String, String> hbaseConfig = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, String>>() {});\n        if (hbaseConfig.containsKey(\"hbase.zookeeper.quorum\")) {\n            quorum = hbaseConfig.get(\"hbase.zookeeper.quorum\");\n        }\n        if (hbaseConfig.containsKey(\"hbase.zookeeper.znode.parent\")) {\n            znodeParent = hbaseConfig.get(\"hbase.zookeeper.znode.parent\");\n        }\n    } catch (Exception e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The parameter of HBase configuration is not set.\");\n    }\n\n    if (StringUtils.isBlank(quorum)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The parameter of HBase configuration is not set.\");\n    }\n\n    if (StringUtils.isBlank(znodeParent)) {\n        znodeParent = HBaseConstant.DEFAULT_ZNODE_PARENT;\n    }\n\n    return new Pair<>(quorum, znodeParent);\n}", "CodeBleu_score": 45.836940464826}]}
{"instance_id": "860cae2bb7f4", "generation_result": [{"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\tString zkQuorum = cfg.getHbaseConfig().get(HConstants.ZOOKEEPER_QUORUM);\n    // If no Zookeeper port is specified, use the default port\n    if (!zkQuorum.contains(\":\")) {\n        zkQuorum = zkQuorum + \":2181\";\n    }\n    String znode = cfg.getHbaseConfig().get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (znode == null || znode.isEmpty()) {\n        znode = HBaseConstant.DEFAULT_ZNODE;\n    }\n    // Kerberos\n    haveKerberos = cfg.getHbaseConfig().get(HBaseConstant.HAVE_KERBEROS) != null && cfg.getHbaseConfig().get(HBaseConstant.HAVE_KERBEROS).equals(\"true\");\n    if (haveKerberos) {\n        kerberosKeytabFilePath = cfg.getHbaseConfig().get(HBaseConstant.KERBEROS_KEYTAB_FILE_PATH);\n        kerberosPrincipal = cfg.getHbaseConfig().get(HBaseConstant.KERBEROS_PRINCIPAL);\n    }\n    try {\n        Class.forName(\"org.apache.phoenix.jdbc.PhoenixDriver\");\n        String url = \"jdbc:phoenix:\" + zkQuorum + znode;\n        if (haveKerberos) {\n            url = url + \"?\" + HBaseConstant.KERBEROS_PRINCIPAL + \"=\" + kerberosPrincipal + \"&\" + HBaseConstant.KERBEROS_KEYTAB_FILE_PATH + \"=\" + kerberosKeytabFilePath;\n        }\n        Connection conn = DriverManager.getConnection(url);\n        if (haveKerberos) {\n            UserGroupInformation ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(kerberosPrincipal, kerberosKeytabFilePath);\n            ugi.doAs(new PrivilegedExceptionAction<Void>() {", "CodeBleu_score": 29.854330917029337}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\ttry {\n        // 1. \u6821\u9a8cHbase\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getHbaseConfig())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.HBASE_CONFIG);\n        }\n        Pair<String, String> hbaseConfig = getHbaseConfig(cfg.getHbaseConfig());\n        // 2. \u6821\u9a8cjdbc\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getJdbcConfig())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.JDBC_CONFIG);\n        }\n        // 3. \u6821\u9a8c\u8868\u540d\n        if (StringUtils.isBlank(cfg.getTableName())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.TABLE_NAME);\n        }\n        // 4. \u6821\u9a8ccolumn\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getColumns())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.COLUMNS);\n        }\n        // 5. \u6821\u9a8ccolumn\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getColumns())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.COLUMNS);\n        }\n        // 6. \u6821\u9a8ccolumn\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getColumns())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, HbaseSQLConstant.COLUMNS);\n        }\n        // 7. \u6821\u9a8ccolumn\u914d\u7f6e\n        if (StringUtils.isBlank(cfg.getColumns())) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE,", "CodeBleu_score": 30.716550015392784}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\tLOG.info(\"Begin to validate configuration.\");\n    try {\n        Class.forName(\"org.apache.phoenix.jdbc.PhoenixDriver\");\n    } catch (ClassNotFoundException e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.HBASE_SQL_CONNECT_ERROR, e);\n    }\n    String tableName = cfg.getTable();\n    String[] tableNameParts = tableName.split(\"\\\\.\");\n    if (tableNameParts.length != 2) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                String.format(\"The format of table name is illegal. The correct format is \\\"namespace.tableName\\\", but the configured table name is \\\"%s\\\".\", tableName));\n    }\n    String namespace = tableNameParts[0];\n    String table = tableNameParts[1];\n    String zkQuorum = cfg.getHbaseConfig().get(HConstants.ZOOKEEPER_QUORUM);\n    String znode = cfg.getHbaseConfig().get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (StringUtils.isBlank(zkQuorum) || StringUtils.isBlank(znode)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                String.format(\"The format of hbase configuration is illegal. The correct format is \\\"%s=xxx, %s=xxx\\\", but the configured hbase configuration is \\\"%s\\\".\",\n                        HConstants.ZOOKEEPER_QUORUM, HConstants.ZOOKEEPER_ZNODE_PARENT, cfg.getHbaseConfig()));\n    }\n    if (cfg.getHbaseConfig().containsKey(HBaseConstant.HBASE_SECURITY_KERBEROS_ENABLED)) {\n        String enableKerberos = cfg.getHbaseConfig().get(HBaseConstant.HBASE_SECURITY_KERBEROS_ENABLED);\n        if (Boolean.parseBoolean(enableKerber", "CodeBleu_score": 35.6152579553156}]}
{"instance_id": "61721c327cf6", "generation_result": [{"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}]}
{"instance_id": "9ceb7f0bb717", "generation_result": [{"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}]}
{"instance_id": "247fc8882f6f", "generation_result": [{"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}]}
{"instance_id": "ac87e1b9a54e", "generation_result": [{"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}]}
{"instance_id": "5e6a66d06162", "generation_result": [{"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}]}
{"instance_id": "c450f959616c", "generation_result": [{"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}]}
{"instance_id": "1bf487d2cc64", "generation_result": [{"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}]}
{"instance_id": "680728cbc3ec", "generation_result": [{"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}]}
{"instance_id": "ce50bd53abef", "generation_result": [{"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tString rowKey = Bytes.toString(CellUtil.cloneRow(cell));\n        String family = Bytes.toString(CellUtil.cloneFamily(cell));\n        String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));\n        long timestamp = cell.getTimestamp();\n        String value = Bytes.toString(CellUtil.cloneValue(cell));\n\n        // add row key\n        record.addColumn(new LongColumn(timestamp), HBaseConstant.ROWKEY_COLUMN_NAME, HBaseConstant.ROWKEY_COLUMN_FAMILY, HBaseConstant.ROWKEY_COLUMN_QUALIFIER, rowKey);\n\n        // add family and qualifier\n        record.addColumn(new LongColumn(timestamp), HBaseConstant.COLUMN_FAMILY_COLUMN_NAME, HBaseConstant.COLUMN_FAMILY_COLUMN_FAMILY, HBaseConstant.COLUMN_FAMILY_COLUMN_QUALIFIER, family);\n        record.addColumn(new LongColumn(timestamp), HBaseConstant.COLUMN_QUALIFIER_COLUMN_NAME, HBaseConstant.COLUMN_QUALIFIER_COLUMN_FAMILY, HBaseConstant.COLUMN_QUALIFIER_COLUMN_QUALIFIER, qualifier);\n\n        // add timestamp\n        record.addColumn(new LongColumn(timestamp), HBaseConstant.TIMESTAMP_COLUMN_NAME, HBaseConstant.TIMESTAMP_COLUMN_FAMILY, HBaseConstant.TIMESTAMP_COLUMN_QUALIFIER, String.valueOf(timestamp));\n\n        // add value\n        record.addColumn(new LongColumn(timestamp), HBaseConstant.VALUE_COLUMN_NAME, HBaseConstant.VALUE_COLUMN_FAMILY, HBaseConstant.VALUE_COLUMN_QUALIFIER, value);\n\n        // add column family and qualifier to map\n        String familyQualifier = family + StringUtils.join(colonByte, qualifier);\n        if (!this.familyQualifierMap.containsKey(family)) {\n            this.familyQualifier", "CodeBleu_score": 30.953130654951316}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tbyte[] row = CellUtil.cloneRow(cell);\n        byte[] family = CellUtil.cloneFamily(cell);\n        byte[] qualifier = CellUtil.cloneQualifier(cell);\n        byte[] value = CellUtil.cloneValue(cell);\n        long timestamp = cell.getTimestamp();\n\n        // add row key\n        String rowKey = Bytes.toString(row);\n        record.addColumn(new StringColumn(HBaseConstant.ROWKEY, rowKey));\n\n        // add family and qualifier\n        String familyName = Bytes.toString(family);\n        String qualifierName = Bytes.toString(qualifier);\n        record.addColumn(new StringColumn(HBaseConstant.FAMILY, familyName));\n        record.addColumn(new StringColumn(HBaseConstant.QUALIFIER, qualifierName));\n\n        // add timestamp\n        record.addColumn(new LongColumn(HBaseConstant.TIMESTAMP, timestamp));\n\n        // add value\n        String valueStr = Bytes.toString(value);\n        String type = this.familyQualifierMap.get(familyName).get(qualifierName);\n        if (StringUtils.isBlank(type)) {\n            type = HBaseConstant.STRING;\n        }\n        record.addColumn(Hbase20xHelper.convertToType(type, valueStr));\n    }", "CodeBleu_score": 40.62453103233393}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tString rowKey = Bytes.toString(CellUtil.cloneRow(cell));\n        String family = Bytes.toString(CellUtil.cloneFamily(cell));\n        String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));\n        String value = Bytes.toString(CellUtil.cloneValue(cell));\n        String timestamp = String.valueOf(cell.getTimestamp());\n\n        record.setColumn(HBaseConstant.ROWKEY_COLUMN_NAME, new LongColumn(rowKey));\n        record.setColumn(HBaseConstant.FAMILY_COLUMN_NAME, new LongColumn(family));\n        record.setColumn(HBaseConstant.QUALIFIER_COLUMN_NAME, new LongColumn(qualifier));\n        record.setColumn(HBaseConstant.TIMESTAMP_COLUMN_NAME, new LongColumn(timestamp));\n        record.setColumn(HBaseConstant.VALUE_COLUMN_NAME, new LongColumn(value));\n\n        // add the family and qualifier to the map\n        String familyQualifier = family + colonByte + qualifier;\n        if (this.familyQualifierMap.containsKey(familyQualifier)) {\n            Map<String, String> familyQualifierMap = this.familyQualifierMap.get(familyQualifier);\n            for (Map.Entry<String, String> entry : familyQualifierMap.entrySet()) {\n                String key = entry.getKey();\n                String type = entry.getValue();\n                String columnValue = null;\n                switch (type) {\n                    case HBaseConstant.STRING_TYPE:\n                        columnValue = value;\n                        break;\n                    case HBaseConstant.LONG_TYPE:\n                        columnValue = String.valueOf(Long.parseLong(value));\n                        break;\n                    case HBaseConstant.DOUBLE_TYPE:\n                        columnValue = String.valueOf(Double.parseDouble(value));\n                        break;\n                    case HBaseConstant.BOOLEAN_TYPE:\n                        columnValue = String.valueOf(Boolean.parseBoolean(value));\n                        break;\n                    case HBaseConstant.DATE_TYPE:\n                        columnValue =", "CodeBleu_score": 33.20054981928221}]}
{"instance_id": "29e8f3a99f0c", "generation_result": [{"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}]}
{"instance_id": "f5ad51545b10", "generation_result": [{"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}]}
{"instance_id": "8618eef8b0e9", "generation_result": [{"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 44.553271507959366}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 44.553271507959366}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 44.553271507959366}]}
{"instance_id": "46484df184dd", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}]}
{"instance_id": "4ac56ef9f89a", "generation_result": [{"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}]}
{"instance_id": "e13a82b26338", "generation_result": [{"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"Unsubscribing from all topics\");\n        consumer.unsubscribe();\n    } else {\n        log.info(\"Subscribing to topics: {}\", topicNames);\n        consumer.subscribe(topicNames);\n    }\n}", "CodeBleu_score": 61.75250901808999}, {"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"Unsubscribing consumer from all topics\");\n        consumer.unsubscribe();\n    } else {\n        log.info(\"Subscribing consumer to topics {}\", topicNames);\n        topicNames.forEach(topic -> {\n            try {\n                admin.createTopicIfNotExists(topic);\n            } catch (IOException e) {\n                log.error(\"Failed to create topic: {}\", topic, e);\n            }\n        });\n        consumer.subscribe(topicNames);\n    }\n}", "CodeBleu_score": 54.162084965292536}, {"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n    log.info(\"Unsubscribing consumer from all topics\");\n    consumer.unsubscribe();\n  } else {\n    topicNames.forEach(topic -> {\n      try {\n        admin.createTopicIfNotExists(topic);\n      } catch (IOException e) {\n        log.error(\"Failed to create topic [{}]\", topic, e);\n      }\n    });\n    log.info(\"Subscribing consumer to topics {}\", topicNames);\n    consumer.subscribe(topicNames);\n  }\n}", "CodeBleu_score": 55.76560181435903}]}
{"instance_id": "8a20d0b48703", "generation_result": [{"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"Polling records from Kafka topic {}\", topic);\n    List<ConsumerRecord<String, byte[]>> records = new ArrayList<>();\n    ConsumerRecords<String, byte[]> consumerRecords = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    log.trace(\"Polled {} records from Kafka topic {} in {} ms\", consumerRecords.count(), topic, stopWatch.getTotalTimeMillis());\n    if (consumerRecords.count() > 0) {\n        records.addAll(consumerRecords.records(topic));\n    }\n    return records;\n}", "CodeBleu_score": 56.74402157369077}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    if (records.isEmpty()) {\n        if (statsService != null) {\n            statsService.onNoRecords(groupId);\n        }\n        return Collections.emptyList();\n    } else {\n        if (statsService != null) {\n            statsService.onRecords(groupId, records.count());\n        }\n        List<ConsumerRecord<String, byte[]>> result = new ArrayList<>(records.count());\n        records.forEach(record -> result.add(record));\n        return result;\n    }\n}", "CodeBleu_score": 60.09622539065579}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"[{}] Polling for {}ms\", topic, durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    log.trace(\"[{}] Polling took {}ms\", topic, stopWatch.getTotalTimeMillis());\n    if (records.isEmpty()) {\n        return Collections.emptyList();\n    }\n    List<ConsumerRecord<String, byte[]>> result = new ArrayList<>();\n    for (ConsumerRecord<String, byte[]> record : records) {\n        result.add(record);\n    }\n    return result;\n}", "CodeBleu_score": 71.32906758619471}]}
{"instance_id": "328e4c39a8e1", "generation_result": [{"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}]}
{"instance_id": "35f835e8c387", "generation_result": [{"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}]}
{"instance_id": "111a11189434", "generation_result": [{"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}]}
{"instance_id": "f5d2487100a9", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}]}
{"instance_id": "886264f5ca27", "generation_result": [{"code": "private void startLogScheduling(){\n\tDuration timeout = Duration.ofMillis(kafkaSettings.getResponseTimeout());\n    statsPrintScheduler.scheduleAtFixedRate(this::logStats, timeout.toMillis(), timeout.toMillis(), TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 10.24958743945016}, {"code": "private void startLogScheduling(){\n\tDuration timeoutDuration = Duration.ofMillis(kafkaSettings.getResponseTimeoutMs());\n    statsPrintScheduler.scheduleAtFixedRate(() -> {\n        try {\n            if (statsConfig.getEnabled()) {\n                List<TbKafkaConsumerGroupStats> stats = getConsumerGroupStats();\n                for (TbKafkaConsumerGroupStats groupStats : stats) {\n                    if (monitoredGroups.contains(groupStats.getGroupId())) {\n                        logConsumerGroupStats(groupStats);\n                    }\n                }\n            }\n        } catch (Exception e) {\n            log.warn(\"[{}] Failed to print consumer group statistics: {}\", groupStats.getGroupId(), e.getMessage(), e);\n        }\n    }, timeoutDuration.toMillis(), timeoutDuration.toMillis(), TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 25.290948905755133}, {"code": "private void startLogScheduling(){\n\tDuration timeout = Duration.ofMillis(kafkaSettings.getResponseTimeout());\n    this.statsPrintScheduler.scheduleWithFixedDelay(() -> {\n        try {\n            if (shouldPrintStats()) {\n                logStats();\n            }\n        } catch (Exception e) {\n            log.warn(\"[{}] Failed to print Kafka consumer stats: {}\", \"consumer-stats-loader-client-group\", e.getMessage(), e);\n        }\n    }, timeout.toMillis(), timeout.toMillis(), TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 16.617584520437173}]}
{"instance_id": "81553181cdc8", "generation_result": [{"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}]}
{"instance_id": "3c8bf3ff31b7", "generation_result": [{"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tif (metric instanceof Counter) {\n\t\tCounter counter = (Counter) metric;\n\t\tMetricEvent event = new MetricEvent(metricName, group, counter);\n\t\tcounters.put(counter, event);\n\t} else if (metric instanceof Gauge) {\n\t\tGauge gauge = (Gauge) metric;\n\t\tMetricEvent event = new MetricEvent(metricName, group, gauge);\n\t\tgauges.put(gauge, event);\n\t} else if (metric instanceof Histogram) {\n\t\tHistogram histogram = (Histogram) metric;\n\t\tMetricEvent event = new MetricEvent(metricName, group, histogram);\n\t\thistograms.put(histogram, event);\n\t} else if (metric instanceof Meter) {\n\t\tMeter meter = (Meter) metric;\n\t\tMetricEvent event = new MetricEvent(metricName, group, meter);\n\t\tmeters.put(meter, event);\n\t} else {\n\t\tLOG.warn(\"Metric type {} is not supported\", metric.getClass().getSimpleName());\n\t}\n}", "CodeBleu_score": 45.330181072232435}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tif (metric instanceof Gauge) {\n\t\t\tGauge<?> gauge = (Gauge<?>) metric;\n\t\t\tMetricEvent event = new MetricEvent(metricName, group.getAllVariables(), gauge.getValue());\n\t\t\tgauges.put(gauge, event);\n\t\t} else if (metric instanceof Counter) {\n\t\t\tCounter counter = (Counter) metric;\n\t\t\tMetricEvent event = new MetricEvent(metricName, group.getAllVariables(), counter.getCount());\n\t\t\tcounters.put(counter, event);\n\t\t} else if (metric instanceof Histogram) {\n\t\t\tHistogram histogram = (Histogram) metric;\n\t\t\tMetricEvent event = new MetricEvent(metricName, group.getAllVariables(), histogram.getCount(), histogram.getMean(), histogram.getMin(), histogram.getMax(), histogram.getStdDev(), histogram.getMedian(), histogram.getPercentile(0.75), histogram.getPercentile(0.95), histogram.getPercentile(0.98), histogram.getPercentile(0.99), histogram.getPercentile(0.999));\n\t\t\thistograms.put(histogram, event);\n\t\t} else if (metric instanceof Meter) {\n\t\t\tMeter meter = (Meter) metric;\n\t\t\tMetricEvent event = new MetricEvent(metricName, group.getAllVariables(), meter.getCount(), meter.getMeanRate(), meter.getOneMinuteRate(), meter.getFiveMinuteRate(), meter.getFifteenMinuteRate());\n\t\t\tmeters.put(meter, event);\n\t\t} else {\n\t\t\tLOG.warn(\"Metric type {} is not supported by the Kafka reporter\", metric.getClass().getName());\n\t\t}\n\t}", "CodeBleu_score": 42.52414312248873}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tif (metric instanceof Counter) {\n\t\tCounter counter = (Counter) metric;\n\t\tString scopedName = getScopedName(metricName, group);\n\t\tMap<String, String> tags = getTags(metricName, group);\n\t\tMetricEvent event = new MetricEvent(scopedName, tags, counter.getCount());\n\t\tcounters.put(counter, event);\n\t} else if (metric instanceof Gauge) {\n\t\tGauge gauge = (Gauge) metric;\n\t\tString scopedName = getScopedName(metricName, group);\n\t\tMap<String, String> tags = getTags(metricName, group);\n\t\tMetricEvent event = new MetricEvent(scopedName, tags, gauge.getValue());\n\t\tgauges.put(gauge, event);\n\t} else if (metric instanceof Histogram) {\n\t\tHistogram histogram = (Histogram) metric;\n\t\tString scopedName = getScopedName(metricName, group);\n\t\tMap<String, String> tags = getTags(metricName, group);\n\t\tMetricEvent event = new MetricEvent(scopedName, tags, histogram.getCount(), histogram.getMin(), histogram.getMax(), histogram.getMean(), histogram.getStdDev());\n\t\thistograms.put(histogram, event);\n\t} else if (metric instanceof Meter) {\n\t\tMeter meter = (Meter) metric;\n\t\tString scopedName = getScopedName(metricName, group);\n\t\tMap<String, String> tags = getTags(metricName, group);\n\t\tMetricEvent event = new MetricEvent(scopedName, tags, meter.getCount(), meter.getMeanRate(), meter.getOneMinuteRate(), meter.getFiveMinuteRate(), meter.getFifteenMinuteRate());\n\t\tmeters.put(meter, event);\n\t} else {\n\t\tLOG.warn(\"Unsupported metric type: {}\", metric.getClass().getName());\n\t}\n}", "CodeBleu_score": 41.23597992248855}]}
{"instance_id": "77c6addc9b99", "generation_result": [{"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}]}
{"instance_id": "a48971ffb3e9", "generation_result": [{"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}]}
{"instance_id": "c2080c299bb0", "generation_result": [{"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time > 0) {\n        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(props);\n        List<PartitionInfo> partitionInfoList = kafkaConsumer.partitionsFor(topic);\n        for (PartitionInfo partitionInfo : partitionInfoList) {\n            TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n            OffsetAndTimestamp offsetAndTimestamp = kafkaConsumer.offsetsForTimes(\n                    new HashMap<TopicPartition, Long>() {{\n                        put(topicPartition, time);\n                    }}\n            );\n            if (offsetAndTimestamp != null) {\n                consumer.assignTimestampsAndWatermarks(new KafkaTimestampAssigner(topicPartition, offsetAndTimestamp.offset()));\n            }\n        }\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 62.290449581467364}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time != 0L) {\n        consumer.setStartFromTimestamp(time);\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 63.3866988166491}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time != 0L) {\n        Map<TopicPartition, Long> offsets = new HashMap<>();\n        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(props);\n        List<PartitionInfo> partitionInfos = kafkaConsumer.partitionsFor(topic);\n        for (PartitionInfo partitionInfo : partitionInfos) {\n            TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n            OffsetAndTimestamp offsetAndTimestamp = kafkaConsumer.offsetsForTimes(\n                    new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n            offsets.put(topicPartition, offsetAndTimestamp.offset());\n        }\n        consumer.setStartFromSpecificOffsets(offsets);\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 66.14487089149311}]}
{"instance_id": "5e828afbe8e4", "generation_result": [{"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\t// Set the consumer group ID using the provided time\n    props.put(\"group.id\", parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time);\n    // Create a KafkaConsumer with the properties\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    // Retrieve partition information for the specified topic\n    List<PartitionInfo> partitionInfoList = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    // Create a map associating each partition with the timestamp\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfoList) {\n        KafkaTopicPartition ktp = new KafkaTopicPartition(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC), partitionInfo.partition());\n        partitionOffset.put(ktp, time);\n    }\n    // The consumer fetches offsets for the given times\n    Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap = consumer.offsetsForTimes(partitionOffset);\n    // Close the consumer\n    consumer.close();\n    // Build a map of KafkaTopicPartition to offsets\n    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetAndTimestampMap.entrySet()) {\n        partitionOffset.put(new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), entry.getValue().offset());\n    }\n    return partitionOffset;\n}", "CodeBleu_score": 56.35410817565072}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tString topic = parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC);\n    String groupId = parameterTool.getRequired(PropertiesConstants.KAFKA_GROUP_ID);\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    List<PartitionInfo> partitionInfoList = consumer.partitionsFor(topic);\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfoList) {\n        TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n        long offset = 0L;\n        try {\n            OffsetAndTimestamp offsetAndTimestamp = consumer.offsetsForTimes(\n                    new HashMap<TopicPartition, Long>() {{\n                        put(topicPartition, time);\n                    }}).get(topicPartition);\n            if (offsetAndTimestamp != null) {\n                offset = offsetAndTimestamp.offset();\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n        partitionOffset.put(new KafkaTopicPartition(topic, partitionInfo.partition()), offset);\n    }\n    consumer.close();\n    return partitionOffset;\n}", "CodeBleu_score": 50.59724437539151}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\t// Set the consumer group ID using the provided time\n    props.put(\"group.id\", parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time);\n    // Create a KafkaConsumer with the properties\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    // Retrieve partition information for the specified topic\n    List<PartitionInfo> partitionInfos = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    // Create a map associating each partition with the timestamp\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfos) {\n        KafkaTopicPartition kafkaTopicPartition = new KafkaTopicPartition(partitionInfo.topic(), partitionInfo.partition());\n        partitionOffset.put(kafkaTopicPartition, time);\n    }\n    // Fetch offsets for the given times\n    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(partitionOffset);\n    // Close the consumer\n    consumer.close();\n    // Build a map of KafkaTopicPartition to offsets\n    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n        partitionOffset.put(new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), entry.getValue().offset());\n    }\n    return partitionOffset;\n}", "CodeBleu_score": 58.19208581356968}]}
{"instance_id": "28da89f56f67", "generation_result": [{"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\\\"bootstrap.servers\\\", broker_list);\n    props.put(\\\"key.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n    props.put(\\\"value.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n    MetricEvent metricEvent = new MetricEvent();\n    metricEvent.setTimestamp(System.currentTimeMillis());\n    metricEvent.setName(\\\"test\\\");\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\\\"tag1\\\", \\\"value1\\\");\n    metricEvent.setTags(tags);\n    Map<String, Double> fields = new HashMap<>();\n    fields.put(\\\"field1\\\", 1.0);\n    metricEvent.setFields(fields);\n    String metricEventJson = GsonUtil.toJson(metricEvent);\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, metricEventJson);\n    producer.send(record);\n    producer.flush();\n    System.out.println(metricEventJson);\n    producer.close();\n}", "CodeBleu_score": 31.19283736780571}, {"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\\\"bootstrap.servers\\\", broker_list);\n    props.put(\\\"key.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n    props.put(\\\"value.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n    for (int i = 0; i < 10; i++) {\n        MetricEvent metricEvent = new MetricEvent();\n        metricEvent.setTimestamp(System.currentTimeMillis());\n        metricEvent.setName(\\\"metric-name-\\\" + i);\n        Map<String, String> tags = new HashMap<>();\n        tags.put(\\\"tag-key-1\\\", \\\"tag-value-1\\\");\n        metricEvent.setTags(tags);\n        Map<String, Double> fields = new HashMap<>();\n        fields.put(\\\"field-key-1\\\", 1.0);\n        fields.put(\\\"field-key-2\\\", 2.0);\n        metricEvent.setFields(fields);\n        String metricEventStr = GsonUtil.GSON.toJson(metricEvent);\n        System.out.println(metricEventStr);\n        ProducerRecord<String, String> record = new ProducerRecord<>(topic, metricEventStr);\n        producer.send(record);\n    }\n    producer.flush();\n    producer.close();\n}", "CodeBleu_score": 31.746230374794344}, {"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties properties = new Properties();\n    properties.put(\\\"bootstrap.servers\\\", broker_list);\n    properties.put(\\\"key.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n    properties.put(\\\"value.serializer\\\", \\\"org.apache.kafka.common.serialization.StringSerializer\\\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(properties);\n\n    MetricEvent metricEvent = new MetricEvent();\n    metricEvent.setTimestamp(System.currentTimeMillis());\n    metricEvent.setName(\\\"metric\\\");\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\\\"tag1\\\", \\\"value1\\\");\n    tags.put(\\\"tag2\\\", \\\"value2\\\");\n    metricEvent.setTags(tags);\n    Map<String, String> fields = new HashMap<>();\n    fields.put(\\\"field1\\\", \\\"value1\\\");\n    fields.put(\\\"field2\\\", \\\"value2\\\");\n    metricEvent.setFields(fields);\n\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, GsonUtil.GSON.toJson(metricEvent));\n    producer.send(record);\n    producer.flush();\n    System.out.println(\\\"Sent: \\\" + GsonUtil.GSON.toJson(metricEvent));\n    producer.close();\n}", "CodeBleu_score": 30.86095887185452}]}
{"instance_id": "14bffe09c25a", "generation_result": [{"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().isMoreSpecificThan(org.apache.logging.log4j.Level.toLevel(level))) {\n        if (event.getLoggerName() != null && !event.getLoggerName().contains(\"xxx\")) {\n            try {\n                String logEvent = JacksonUtil.toJson(new LogEvent(event));\n                ProducerRecord<String, String> record = new ProducerRecord<>(topic, logEvent);\n                producer.send(record);\n            } catch (JsonProcessingException e) {\n                log.error(\"Log4j2 appender exception\", e);\n            }\n        }\n    }\n}", "CodeBleu_score": 49.990503972258935}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tString loggerName = event.getLoggerName();\n    if (loggerName != null && !loggerName.contains(\"xxx\")) {\n        String level = event.getLevel().name();\n        if (this.level != null && this.level.contains(level)) {\n            LogEvent logEvent = new LogEvent(event);\n            String log = JacksonUtil.toJson(logEvent);\n            try {\n                ProducerRecord<String, String> record = new ProducerRecord<>(topic, log);\n                producer.send(record);\n            } catch (Exception e) {\n                log.error(\"Error sending log event to Kafka\", e);\n            }\n        }\n    }\n}", "CodeBleu_score": 46.512794766587284}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().isLessSpecificThan(org.apache.logging.log4j.Level.toLevel(level))) {\n        return;\n    }\n    if (event.getLoggerName().contains(\"xxx\")) {\n        return;\n    }\n    String msg = event.getMessage().getFormattedMessage();\n    LogEvent logEvent = new LogEvent();\n    logEvent.setSource(source);\n    logEvent.setLevel(event.getLevel().name());\n    logEvent.setTimestamp(event.getTimeMillis());\n    logEvent.setMessage(msg);\n    logEvent.setAppId(appId);\n    logEvent.setContainerId(containerId);\n    logEvent.setContainerType(containerType);\n    logEvent.setTaskName(taskName);\n    logEvent.setTaskId(taskId);\n    logEvent.setNodeIp(nodeIp);\n    logEvent.setThreadName(event.getThreadName());\n    logEvent.setLoggerName(event.getLoggerName());\n    logEvent.setThreadId(event.getThreadId());\n    logEvent.setThreadPriority(event.getThreadPriority());\n    logEvent.setSource(source);\n    logEvent.setStackTrace(event.getThrown() != null ? ExceptionUtil.getStackTrace(event.getThrown()) : null);\n    logEvent.setMarker(event.getMarker() != null ? event.getMarker().toString() : null);\n    logEvent.setLoggerFqcn(event.getLoggerFqcn());\n    logEvent.setEndOfBatch(event.isEndOfBatch());\n    logEvent.setNanoTime(event.getNanoTime());\n    logEvent.setThreadPriority(event.getThreadPriority());\n    logEvent.setThreadId(event.getThreadId());\n    logEvent.setThreadName(event.getThreadName());\n    logEvent.setLoggerName(event.getLoggerName());\n    logEvent.setMarker(event.getMarker() != null ? event.getMarker().toString() : null);\n    logEvent.setLoggerFqcn(event.getLoggerFqcn());\n    logEvent.setEndOfBatch(", "CodeBleu_score": 34.11308890829303}]}
{"instance_id": "a9e0786fb5c1", "generation_result": [{"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"unknown topic suffix method: %s\", method)\n}", "CodeBleu_score": 92.40633567441876}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"unknown topic suffix method: %s\", method)\n}", "CodeBleu_score": 92.40633567441876}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"unknown topic suffix method: %s\", method)\n}", "CodeBleu_score": 92.40633567441876}]}
{"instance_id": "9b88d720d48a", "generation_result": [{"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingKey == \"random\" {\n\t\treturn uuid.NewV5(uuid.NamespaceOID, metric.Name()).String(), nil\n\t}\n\tif k.RoutingTag != \"\" {\n\t\tif v, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn v, nil\n\t\t}\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 55.783269542427675}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif routingKey, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn routingKey, nil\n\t\t}\n\t}\n\tif k.RoutingKey == \"random\" {\n\t\treturn uuid.NewV5(uuid.NamespaceDNS, metric.Name()).String(), nil\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 61.90688449448126}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\t// If a routing tag is specified, return the value of that tag as the key.\n\tif k.RoutingTag != \"\" {\n\t\tif value, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn value, nil\n\t\t}\n\t}\n\n\t// If a routing key is specified, return it.\n\tif k.RoutingKey != \"\" {\n\t\treturn k.RoutingKey, nil\n\t}\n\n\t// If no routing key is specified, return a random UUID.\n\treturn uuid.NewV5(uuid.NamespaceOID, metric.Name()).String(), nil\n}", "CodeBleu_score": 58.099850475296755}]}
{"instance_id": "dc35e80e7f8d", "generation_result": [{"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}]}
{"instance_id": "daf09ba144b2", "generation_result": [{"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}]}
{"instance_id": "8ecf2a1efead", "generation_result": [{"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}]}
{"instance_id": "eba9dafa99ab", "generation_result": [{"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id.length <= maxNameLength, LogkafkaIdTooLong)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 71.01320112835893}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 74.29902955557642}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id != \".\", LogkafkaIdIsDot)\n  checkCondition(logkafka_id != \"..\", LogkafkaIdIsDotDot)\n  checkCondition(logkafka_id.length <= maxNameLength, LogkafkaIdTooLong)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) => checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None => checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 78.4664649844876}]}
{"instance_id": "45d5aefd3f6c", "generation_result": [{"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}]}
{"instance_id": "a535ce861222", "generation_result": [{"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}]}
{"instance_id": "ad37d3914acc", "generation_result": [{"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 19.669529430642616}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 19.669529430642616}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 19.669529430642616}]}
{"instance_id": "3ecdb2c1534f", "generation_result": [{"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}]}
{"instance_id": "5955ade118b9", "generation_result": [{"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}]}
{"instance_id": "26fbc2ba1d28", "generation_result": [{"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}]}
{"instance_id": "adf40b68c93e", "generation_result": [{"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}]}
{"instance_id": "07ff1ca41c93", "generation_result": [{"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tString kafkaVersion = AppInfoParser.getVersion();\n    Version kafkaVersionObj = parseVersion(kafkaVersion);\n    if (kafkaVersionObj.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0) {\n        return (Boolean) config.getOrDefault(\"topic.creation.enable\", true);\n    } else {\n        return false;\n    }\n}", "CodeBleu_score": 31.081422990953023}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tString kafkaConnectVersion = AppInfoParser.getVersion();\n    if (null == kafkaConnectVersion) {\n        throw new DebeziumException(\"Unable to determine Kafka Connect version\");\n    }\n    Version kafkaVersion = parseVersion(kafkaConnectVersion);\n    if (kafkaVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) < 0) {\n        return false;\n    }\n    Object topicCreationEnabled = config.get(\"topic.creation.enable\");\n    if (null == topicCreationEnabled) {\n        return true;\n    }\n    if (topicCreationEnabled instanceof Boolean) {\n        return (Boolean) topicCreationEnabled;\n    }\n    if (topicCreationEnabled instanceof String) {\n        return Boolean.parseBoolean((String) topicCreationEnabled);\n    }\n    throw new DebeziumException(\"Unexpected type for topic.creation.enable: \" + topicCreationEnabled.getClass().getName());\n}", "CodeBleu_score": 45.93605381550199}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tif (null == this.isTopicCreationEnabled) {\n        final String version = AppInfoParser.getVersion();\n        final Version kafkaVersion = parseVersion(version);\n        final String topicCreationEnabled = (String) config.get(\"topic.creation.enable\");\n        this.isTopicCreationEnabled = (null != topicCreationEnabled && Boolean.parseBoolean(topicCreationEnabled)) && (kafkaVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0);\n    }\n    return this.isTopicCreationEnabled;\n}", "CodeBleu_score": 50.71715510704475}]}
{"instance_id": "e2cc420e1e63", "generation_result": [{"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tif (!config.containsKey(DistributedConfig.GROUP_ID_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.GROUP_ID_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.BOOTSTRAP_SERVERS_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.BOOTSTRAP_SERVERS_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.CONFIG_STORAGE_TOPIC_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.CONFIG_STORAGE_TOPIC_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.CONFIG_TOPIC_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.CONFIG_TOPIC_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.STATUS_STORAGE_TOPIC_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.STATUS_STORAGE_TOPIC_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.KEY_CONVERTER_CLASS_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.KEY_CONVERTER_CLASS_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.VALUE_CONVERTER_CLASS_CONFIG)) {\n        throw new DebeziumException(\"Missing mandatory configuration option: \" + DistributedConfig.VALUE_CONVERTER_CLASS_CONFIG);\n    }\n    if (!config.containsKey(DistributedConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG)) {\n        throw new Debez", "CodeBleu_score": 27.628727287762633}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal Map<String, Object> adminConfig = new HashMap<>(config);\n    final String clientId = config.getOrDefault(CLIENT_ID_CONFIG, \"connect-offset-backing-store\");\n    final String adminClientId = clientId + \"-admin\";\n    adminConfig.put(CLIENT_ID_CONFIG, adminClientId);\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminConfig);\n    final Converter converter = converterForOffsetStore();\n    return new KafkaOffsetBackingStore(admin, clientId, converter);\n}", "CodeBleu_score": 33.127713196168344}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal Map<String, Object> props = new HashMap<>(config);\n    final String clientId = props.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(\"Kafka client ID must be specified\");\n    }\n    final Map<String, String> adminProps = new HashMap<>(props);\n    adminProps.put(CLIENT_ID_CONFIG, clientId + \"-admin\");\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n    final Converter converter = converterForOffsetStore();\n    return new KafkaOffsetBackingStore(admin, clientId, converter);\n}", "CodeBleu_score": 37.257344300872774}]}
{"instance_id": "4ecb2279231e", "generation_result": [{"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        JsonNode payload = node.get(PAYLOAD_FIELD);\n        if (payload != null) {\n            node = payload;\n        }\n        if (node.size() == 1) {\n            Iterator<String> fields = node.fieldNames();\n            String field = fields.next();\n            return reader.readValue(node.get(field));\n        } else {\n            return reader.readValue(node);\n        }\n    } else {\n        return reader.readValue(node);\n    }\n}", "CodeBleu_score": 48.74577661170366}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        JsonNode payload = node.get(PAYLOAD_FIELD);\n\n        // Schema + payload format\n        if (payload != null) {\n            node = payload;\n        }\n        // Debezium envelope\n        if (config.asEnvelope()) {\n            return reader.readValue(node);\n        }\n        else if (node.has(Envelope.FieldName.SOURCE) && node.has(config.sourceField())) {\n            return reader.readValue(node.get(config.sourceField()));\n        }\n        // Extracted format\n        else {\n            return reader.readValue(node);\n        }\n    }\n    else {\n        return reader.readValue(node);\n    }\n}", "CodeBleu_score": 41.16073773136249}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (!node.isObject()) {\n        return reader.readValue(node);\n    }\n    JsonNode payload = node.get(PAYLOAD_FIELD);\n    if (payload != null) {\n        node = payload;\n    }\n    Iterator<String> fieldNames = node.fieldNames();\n    if (fieldNames.hasNext()) {\n        String fieldName = fieldNames.next();\n        if (!fieldNames.hasNext()) {\n            return reader.readValue(node.get(fieldName));\n        } else {\n            return reader.readValue(node);\n        }\n    } else {\n        return null;\n    }\n}", "CodeBleu_score": 57.43566822259019}]}
{"instance_id": "b8ba1bfc7b5e", "generation_result": [{"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sanitized = new StringBuilder(topicName.length());\n    for (int i = 0; i < topicName.length(); i++) {\n        char c = topicName.charAt(i);\n        if (c == delimiter.charAt(0)) {\n            sanitized.append(delimiter);\n        } else if (c == '.') {\n            sanitized.append(\"_\");\n        } else if (c == '-') {\n            sanitized.append(\"_\");\n        } else if (c == '+') {\n            sanitized.append(\"_\");\n        } else if (c == '=') {\n            sanitized.append(\"_\");\n        } else if (c == ' ' || c == ',' || c == '[' || c == ']') {\n            sanitized.append(\"_\");\n        } else if (c == '/') {\n            sanitized.append(\"_\");\n        } else if (c == '\\\\') {\n            sanitized.append(\"_\");\n        } else if (c == '*') {\n            sanitized.append(\"_\");\n        } else if (c == '?') {\n            sanitized.append(\"_\");\n        } else if (c == ':') {\n            sanitized.append(\"_\");\n        } else if (c == '|') {\n            sanitized.append(\"_\");\n        } else if (c == '&') {\n            sanitized.append(\"_\");\n        } else if (c == '%') {\n            sanitized.append(\"_\");\n        } else if (c == '#') {\n            sanitized.append(\"_\");\n        } else if (c == '@') {\n            sanitized.append(\"_\");\n        } else if (c == '!') {\n            sanitized.append(\"_\");\n        } else if (c == '\\'') {\n            sanitized.append(\"_\");\n        } else if (c == '\\\"') {\n            sanitized.append(\"_\");\n        } else if (c == '(') {\n            sanitized", "CodeBleu_score": 44.33881698455993}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder();\n    for (char c : topicName.toCharArray()) {\n        if (TopicName.isValidTopicNameCharacter(c)) {\n            sb.append(c);\n        } else {\n            sb.append(REPLACEMENT_CHAR);\n            LOGGER.warn(\"Topic name '{}' contains invalid characters and will be sanitized to '{}'\", topicName, sb.toString());\n        }\n    }\n    return sb.toString();\n}", "CodeBleu_score": 36.35025734882969}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder(topicName.length());\n    for (char c : topicName.toCharArray()) {\n        if (c == '.' || c == '_' || c == '-') {\n            sb.append(c);\n        } else if (Character.isLetterOrDigit(c)) {\n            sb.append(c);\n        } else {\n            sb.append(REPLACEMENT_CHAR);\n        }\n    }\n    if (sb.toString().equals(topicName)) {\n        return topicName;\n    } else {\n        LOGGER.warn(\"Topic name \\\"{}\\\" contains invalid characters. Replacing with \\\"{}\\\"\", topicName, sb.toString());\n        return sb.toString();\n    }\n}", "CodeBleu_score": 47.73673393674087}]}
{"instance_id": "9cf0d74c098d", "generation_result": [{"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n    this.queue = queue;\n    this.maxRetries = replacedErrorHandler != null ? replacedErrorHandler.maxRetries : connectorConfig.getRetryMaxRetries();\n    this.retries = replacedErrorHandler != null ? replacedErrorHandler.retries : 0;\n    this.producerThrowable = new AtomicReference<>();\n}", "CodeBleu_score": 55.61916408294546}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n    this.queue = queue;\n    this.producerThrowable = new AtomicReference<>();\n    this.maxRetries = replacedErrorHandler != null ? replacedErrorHandler.maxRetries : connectorConfig.maxRetries();\n    this.retries = replacedErrorHandler != null ? replacedErrorHandler.retries : 0;\n}", "CodeBleu_score": 60.389538493787335}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n        this.connectorConfig = connectorConfig;\n        this.maxRetries = connectorConfig.getMaxRetries() == null ? RETRIES_UNLIMITED : connectorConfig.getMaxRetries();\n        this.retries = replacedErrorHandler != null ? replacedErrorHandler.retries : 0;\n    }", "CodeBleu_score": 56.264929066193794}]}
{"instance_id": "8ab4bf3a4dd8", "generation_result": [{"code": "public void validate(Configuration configuration, Field.Set fields){\n\tList<String> errors = new ArrayList<>();\n    for (Field field : fields) {\n        ConfigValue configValue = configuration.validate(field);\n        if (configValue.errorMessages().size() > 0) {\n            errors.add(configValue.errorMessages().get(0));\n        }\n    }\n    if (errors.size() > 0) {\n        throw new ConfigException(errors.get(0));\n    }\n}", "CodeBleu_score": 41.365985440283616}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tfor (Map.Entry<String, ConfigValue> entry : configuration.asMap().entrySet()) {\n        if (fields.contains(entry.getKey())) {\n            if (entry.getValue().errorMessages().size() > 0) {\n                throw new ConfigException(\"Invalid value for configuration '\" + entry.getKey() + \"': \" + entry.getValue().errorMessages().get(0));\n            }\n        }\n    }\n}", "CodeBleu_score": 46.19405897516125}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tfor (Field field : fields) {\n        ConfigValue configValue = configuration.validate(field);\n        if (configValue.errorMessages().size() > 0) {\n            throw new ConfigException(configValue.name(), configValue.value(), configValue.errorMessages().get(0));\n        }\n    }\n}", "CodeBleu_score": 34.65969530277374}]}
{"instance_id": "860405dc9abb", "generation_result": [{"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}]}
{"instance_id": "681a0415df83", "generation_result": [{"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n    if (source.schema().field(SNAPSHOT_KEY) == null) {\n        return null;\n    }\n    String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 37.45476512539193}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source.schema().field(SNAPSHOT_KEY) != null) {\n        String snapshotString = source.getString(SNAPSHOT_KEY);\n        if (snapshotString != null) {\n            return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n        }\n    }\n    return null;\n}", "CodeBleu_score": 44.62395815845043}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n    if (!source.schema().field(SNAPSHOT_KEY).schema().name().equals(SnapshotRecord.class.getName())) {\n        return null;\n    }\n    String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 39.67616658159431}]}
{"instance_id": "85ef1162e38a", "generation_result": [{"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tString[] parts = version.split(\"\\\\.\");\n    if (parts.length > 2) {\n        parts = Arrays.copyOfRange(parts, 0, 2);\n    }\n    String[] subParts = parts[0].split(\"-\");\n    if (subParts.length > 1) {\n        parts[0] = subParts[0];\n    }\n    return Optional.ofNullable(Float.parseFloat(parts[0]));\n}", "CodeBleu_score": 55.10861626526663}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tif (version == null || version.isEmpty()) {\n        return Optional.empty();\n    }\n    String[] parts = version.split(\"\\\\.\");\n    if (parts.length > 2) {\n        parts = new String[] {parts[0], parts[1]};\n    }\n    String[] versionParts = parts[0].split(\"-\");\n    if (versionParts.length > 1) {\n        versionParts = new String[] {versionParts[0]};\n    }\n    return Optional.ofNullable(Float.parseFloat(versionParts[0]));\n}", "CodeBleu_score": 51.793173444605046}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tif (version == null || version.isEmpty()) {\n    return Optional.empty();\n  }\n  String[] parts = version.split(\"\\\\.\");\n  if (parts.length > 2) {\n    parts = new String[] { parts[0], parts[1] };\n  }\n  String[] versionParts = parts[0].split(\"-\");\n  if (versionParts.length > 1) {\n    versionParts = new String[] { versionParts[0] };\n  }\n  return Optional.ofNullable(Float.parseFloat(versionParts[0]));\n}", "CodeBleu_score": 52.21858521495588}]}
{"instance_id": "64346d69eba0", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(kafkaConnectService.createConnector(getCluster(clusterName), connectName, connector))\n        .flatMap(result -> Mono.just(ResponseEntity.ok(result)))\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 68.74723123755643}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(kafkaConnectService.createConnector(getCluster(clusterName), connectName, connector))\n        .flatMap(connectorDTO -> ResponseEntity.status(201).body(connectorDTO))\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 68.23674862720283}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector)\n        .flatMap(c -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, c))\n        .flatMap(c -> Mono.just(ResponseEntity.ok(c)))\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 69.5321309265037}]}
{"instance_id": "4c446fd64db6", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n            .cluster(clusterName)\n            .connect(connectName)\n            .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n            .connector(connectorName)\n            .operationName(\"deleteConnector\")\n            .build();\n\n    return validateAccess(context)\n            .then(kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName))\n            .map(ResponseEntity::ok)\n            .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 68.48877635589592}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 69.87433531773667}]}
{"instance_id": "d34a49f3ca1b", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}]}
{"instance_id": "199cfe41b374", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}]}
{"instance_id": "88380ae300c4", "generation_result": [{"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n      .initializers(new DynamicConfigOperations())\n      .run(args);\n}", "CodeBleu_score": 40.8226072471737}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n        .initializers(new DynamicConfigOperations.DynamicConfigInitializer())\n        .run(args);\n}", "CodeBleu_score": 45.10427576420186}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n      .initializers(new DynamicConfigOperations())\n      .run(args);\n}", "CodeBleu_score": 40.8226072471737}]}
{"instance_id": "3f022876f737", "generation_result": [{"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tif (search == null) {\n        return x -> true;\n    }\n    return x -> Stream.of(x.getName(), x.getConnectorClass(), x.getTopics())\n        .anyMatch(y -> StringUtils.containsIgnoreCase(y, search));\n}", "CodeBleu_score": 61.811488696251715}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\treturn search == null ?\n        (connector) -> true :\n        (connector) -> connector.getConnector().getConnectorName().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getConnectorClass().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getType().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getState().getState().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getState().getWorkerId().toLowerCase().contains(search.toLowerCase()) ||\n            connector.getConnector().getConfig().entrySet().stream().anyMatch(entry -> entry.getKey().toLowerCase().contains(search.toLowerCase()) || entry.getValue().toLowerCase().contains(search.toLowerCase())) ||\n            connector.getTasks().stream().anyMatch(task -> task.getTaskId().toLowerCase().contains(search.toLowerCase()) || task.getState().toLowerCase().contains(search.toLowerCase()) || task.getWorkerId().toLowerCase().contains(search.toLowerCase())) ||\n            connector.getTopics().getTopics().stream().anyMatch(topic -> topic.getTopic().toLowerCase().contains(search.toLowerCase()));\n}", "CodeBleu_score": 27.668818165044435}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tif (search == null) {\n    return (connector) -> true;\n  }\n  return (connector) -> connector.getConnector().getConnector().stream().anyMatch(connectorName -> StringUtils.containsIgnoreCase(connectorName, search));\n}", "CodeBleu_score": 57.141855973978515}]}
{"instance_id": "9754aa1ec96c", "generation_result": [{"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return new HashMap<>();\n    }\n    Map<String, Object> flattenedMap = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      String newKey = prefix == null ? key : prefix + \".\" + key;\n      if (value instanceof Map) {\n        flattenedMap.putAll(flattenClusterProperties(newKey, (Map<String, Object>) value));\n      } else {\n        flattenedMap.put(newKey, value);\n      }\n    }\n    return flattenedMap;\n  }", "CodeBleu_score": 52.00735805742836}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return new HashMap<>();\n    }\n    Map<String, Object> flattened = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      if (value instanceof Map) {\n        flattened.putAll(flattenClusterProperties(key, (Map<String, Object>) value));\n      } else {\n        flattened.put(prefix == null ? key : prefix + \".\" + key, value);\n      }\n    }\n    return flattened;\n  }", "CodeBleu_score": 53.48656429134774}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return new HashMap<>();\n    }\n    Map<String, Object> flattenedProperties = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      if (value instanceof Map) {\n        Map<String, Object> nestedProperties = (Map<String, Object>) value;\n        flattenedProperties.putAll(flattenClusterProperties(key, nestedProperties));\n      } else {\n        String newKey = prefix == null ? key : prefix + \".\" + key;\n        flattenedProperties.put(newKey, value);\n      }\n    }\n    return flattenedProperties;\n  }", "CodeBleu_score": 47.06466351262372}]}
{"instance_id": "99266677c846", "generation_result": [{"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n  configDef.define(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SslConfigs.SSL_CLIENT_AUTH_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SaslConfigs.SASL_KERBEROS_SERVICE_NAME, ConfigDef.Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Importance.MEDIUM, \"\");\n  configDef.define(SaslConfigs.SASL_MECHANISM, ConfigDef.Type.PASS", "CodeBleu_score": 43.07868101637502}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\treturn new ConfigDef()\n      .define(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The path to the keystore file\")\n      .define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, \"The password for the keystore file\")\n      .define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, \"The password for the private key in the keystore file\")\n      .define(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The type of keystore file\")\n      .define(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The path to the truststore file\")\n      .define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, \"The password for the truststore file\")\n      .define(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The type of truststore file\")\n      .define(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The endpoint identification algorithm to validate server hostname\")\n      .define(SslConfigs.SSL_PROTOCOL_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, \"The SSL protocol used to generate the SSLContext\")\n      .define(SslConfigs.SSL_CIPHER_SUITES_CONFIG, ConfigDef.Type.LIST, null, ConfigDef.Importance.MEDIUM,", "CodeBleu_score": 33.191977231546325}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_PROTOCOL_CONFIG, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_PROVIDER_CONFIG, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_CIPHER_SUITES_CONFIG, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, ConfigDef.Type.LIST, ConfigDef.Importance.HIGH, \"\");\n    configDef.define(SslConfigs.SSL_ENABLED_CIPHER_SUITES_", "CodeBleu_score": 43.28105695845785}]}
{"instance_id": "deebb6fcb0a8", "generation_result": [{"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n    return adminClient.isAclEditable()\n        ? authorizedOperations.containsAll(Set.of(AclOperation.ALL, AclOperation.ALTER))\n            ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT)\n            : Mono.empty()\n        : Mono.empty();\n}", "CodeBleu_score": 37.34968946546438}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\treturn adminClient.getAclOperations()\n            .filter(Set::isEmpty)\n            .map(operations -> ClusterFeature.KAFKA_ACL_EDIT)\n            .switchIfEmpty(Mono.empty());\n}", "CodeBleu_score": 18.699418045342785}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n    boolean isAclEditAllowed = authorizedOperations.contains(AclOperation.DESCRIBE) &&\n            (authorizedOperations.contains(AclOperation.ALTER) || authorizedOperations.contains(AclOperation.ALL));\n    return isAclEditAllowed ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT) : Mono.empty();\n}", "CodeBleu_score": 50.91550679745455}]}
{"instance_id": "a5d8cc77955f", "generation_result": [{"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tTreeMap<TopicPartition, FromToOffset> nextRange = new TreeMap<>();\n  for (Map.Entry<TopicPartition, FromToOffset> entry : prevRange.entrySet()) {\n    TopicPartition partition = entry.getKey();\n    FromToOffset prev = entry.getValue();\n    if (prev.isEmpty()) {\n      // If the previous range is empty, we can use the offsets from seek operations.\n      FromToOffset fromTo = seekOperations.get(partition);\n      if (fromTo != null) {\n        nextRange.put(partition, fromTo);\n      }\n    } else {\n      // If the previous range is not empty, we can use the offsets from the previous range.\n      nextRange.put(partition, prev);\n    }\n  }\n  for (Map.Entry<TopicPartition, FromToOffset> entry : nextRange.entrySet()) {\n    TopicPartition partition = entry.getKey();\n    FromToOffset fromTo = entry.getValue();\n    long from = fromTo.from();\n    long to = fromTo.to();\n    long messagesToPoll = Math.min(to - from, messagesPerPartition);\n    long newFrom = Math.max(from - messagesToPoll, 0);\n    nextRange.put(partition, new FromToOffset(newFrom, fromTo.to()));\n  }\n  return nextRange;\n}", "CodeBleu_score": 34.506147267298346}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tTreeMap<TopicPartition, FromToOffset> newRange = new TreeMap<>(prevRange);\n  Map<TopicPartition, Long> offsetsToReadFrom = new TreeMap<>(prevRange);\n  Map<TopicPartition, Long> offsetsToSeekTo = new TreeMap<>(seekOperations.getSeekToOffsets());\n  Map<TopicPartition, Long> offsetsToSeekToEnd = new TreeMap<>(seekOperations.getSeekToEndOffsets());\n\n  offsetsToReadFrom.putAll(offsetsToSeekTo);\n  offsetsToReadFrom.putAll(offsetsToSeekToEnd);\n\n  if (offsetsToReadFrom.isEmpty()) {\n    return newRange;\n  }\n\n  for (Map.Entry<TopicPartition, Long> entry : offsetsToReadFrom.entrySet()) {\n    TopicPartition partition = entry.getKey();\n    long offset = entry.getValue();\n    long startOffset = offset;\n    long endOffset = getEndOffset(partition);\n\n    if (startOffset < 0) {\n      startOffset = 0;\n    }\n\n    if (startOffset > endOffset) {\n      startOffset = endOffset;\n    }\n\n    if (startOffset == endOffset) {\n      newRange.remove(partition);\n    } else {\n      newRange.put(partition, new FromToOffset(startOffset, endOffset));\n    }\n  }\n\n  return newRange;\n}", "CodeBleu_score": 36.37769893511942}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tMap<TopicPartition, Long> offsetsToReadFrom = prevRange.isEmpty() ?\n        seekOperations.getSeekToOffsets() :\n        prevRange.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().from));\n    int messagesPerPartition = messagesPerPage / offsetsToReadFrom.size();\n    return offsetsToReadFrom.entrySet().stream()\n        .collect(Collectors.toMap(\n            Map.Entry::getKey,\n            e -> new FromToOffset(Math.max(e.getValue() - messagesPerPartition, 0), e.getValue()),\n            (a, b) -> a,\n            TreeMap::new\n        ));\n}", "CodeBleu_score": 34.18565372606728}]}
{"instance_id": "c4c657caeba5", "generation_result": [{"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}]}
{"instance_id": "dffe85dbb47b", "generation_result": [{"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}]}
{"instance_id": "ea146a7bc30c", "generation_result": [{"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries == null) {\n        return;\n    }\n\n    List<RawMessageEvent> validEntries = entries.stream().filter(Objects::nonNull).collect(Collectors.toList());\n    long totalBytes = validEntries.stream().mapToLong(RawMessageEvent::getSize).sum();\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(validEntries);\n            return null;\n        });\n        metrics.writtenMessages().mark(validEntries.size());\n        metrics.writtenBytes().mark(totalBytes);\n    } catch (ExecutionException | RetryException e) {\n        LOG.error(\"Unable to write to journal - retrying with exponential back-off\", e);\n        metrics.failedWriteAttempts().mark();\n        throw new MessageQueueException(e);\n    }\n}", "CodeBleu_score": 45.790717925647925}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries == null || entries.isEmpty()) {\n        return;\n    }\n\n    try {\n        List<Journal.Entry> journalEntries = entries.stream().filter(Objects::nonNull).map(Journal.Entry::new).collect(Collectors.toList());\n        metrics.totalMessages().inc(journalEntries.size());\n        metrics.totalMessageBytes().inc(journalEntries.stream().mapToLong(e -> e.getMessage().length).sum());\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n}", "CodeBleu_score": 44.781709794180856}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries == null) {\n        return;\n    }\n\n    List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(entry -> new Journal.Entry(entry.getBytes()))\n            .collect(Collectors.toList());\n\n    long totalBytes = journalEntries.stream()\n            .mapToLong(Journal.Entry::getBytes)\n            .sum();\n\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(totalBytes);\n\n    try {\n        writeRetryer.call(() -> kafkaJournal.write(journalEntries));\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n\n    journalFilled.release(journalEntries.size());\n}", "CodeBleu_score": 52.360008910936614}]}
{"instance_id": "944e77614042", "generation_result": [{"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxMessageQueueId = messages.stream().map(Message::getMessageQueueId).max(Long::compareTo);\n    maxMessageQueueId.ifPresent(messageQueueId -> {\n        kafkaJournal.acknowledge(messageQueueId);\n        metrics.incrementAcknowledgedMessages(messages.size());\n    });\n}", "CodeBleu_score": 36.17540733555909}, {"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxValidId = messages.stream().map(Message::getId).max(Long::compareTo);\n    maxValidId.ifPresent(id -> kafkaJournal.acknowledge(id));\n    metrics.incrementAcknowledgedMessages(messages.size());\n}", "CodeBleu_score": 32.1865370893152}, {"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxId = messages.stream().map(Message::getId).max(Long::compareTo);\n    if (maxId.isPresent()) {\n        kafkaJournal.acknowledge(maxId.get());\n        metrics.incrementAcknowledgedMessages(1);\n    }\n}", "CodeBleu_score": 32.84798665711174}]}
{"instance_id": "981280879490", "generation_result": [{"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.04351090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.04351090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.04351090250743}]}
{"instance_id": "3ca34bfd2d64", "generation_result": [{"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}]}
{"instance_id": "e1d07a1fa986", "generation_result": [{"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}]}
{"instance_id": "8b30a387069f", "generation_result": [{"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}]}
{"instance_id": "ee338b61e75d", "generation_result": [{"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\t// \u83b7\u53d6\u9501\n    List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n    if (adminClientList == null) {\n        return;\n    }\n    synchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n        LOGGER.info(\"start to close kafka admin-client, cluster-phy-id: {}\", clusterPhyId);\n        List<AdminClient> closedAdminClientList = new ArrayList<>();\n        for (AdminClient adminClient : adminClientList) {\n            try {\n                adminClient.close();\n                closedAdminClientList.add(adminClient);\n            } catch (Exception e) {\n                LOGGER.error(\"close kafka admin-client failed, cluster-phy-id: {}, admin-client: {}\", clusterPhyId, adminClient, e);\n            }\n        }\n        if (closedAdminClientList.size() == adminClientList.size()) {\n            KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n            LOGGER.info(\"close kafka admin-client successfully, cluster-phy-id: {}\", clusterPhyId);\n        } else {\n            LOGGER.error(\"close kafka admin-client failed, cluster-phy-id: {}\", clusterPhyId);\n        }\n    }\n}", "CodeBleu_score": 56.19168712722542}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\t// \u52a0\u9501\n    List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n    if (adminClientList == null) {\n        return;\n    }\n\n    synchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        // \u518d\u6b21\u83b7\u53d6\u4e00\u6b21\uff0c\u9632\u6b62\u5728\u9501\u5b9a\u671f\u95f4\uff0c\u6709\u5176\u4ed6\u7ebf\u7a0b\u5df2\u7ecf\u5173\u95ed\u4e86\n        adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        LOGGER.info(\"start to close kafka admin-client, cluster-phy-id: {}\", clusterPhyId);\n        try {\n            for (AdminClient adminClient : adminClientList) {\n                adminClient.close();\n            }\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka admin-client failed, cluster-phy-id: {}, exception: {}\", clusterPhyId, e);\n        }\n        LOGGER.info(\"finish to close kafka admin-client, cluster-phy-id: {}\", clusterPhyId);\n        KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n    }\n}", "CodeBleu_score": 53.62341044638096}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tList<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n    if (adminClientList == null) {\n        return;\n    }\n    LOGGER.info(\"start to close kafka admin-client for cluster-phy-id: {}\", clusterPhyId);\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close();\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka admin-client failed for cluster-phy-id: {}, reason: {}\", clusterPhyId, e.getMessage());\n        }\n    }\n    LOGGER.info(\"close kafka admin-client successfully for cluster-phy-id: {}\", clusterPhyId);\n}", "CodeBleu_score": 46.08235954345452}]}
{"instance_id": "6f0b935b971c", "generation_result": [{"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            return false;\n        }\n    }\n    return true;\n}", "CodeBleu_score": 59.83187674491934}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n    boolean allSuccess = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            allSuccess = false;\n        }\n    }\n    return allSuccess;\n}", "CodeBleu_score": 79.9206672992389}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean allSuccess = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n            allSuccess = false;\n        }\n    }\n\n    return allSuccess;\n}", "CodeBleu_score": 92.12390159845636}]}
{"instance_id": "b5475aa94c1a", "generation_result": [{"code": "public void stop() {\n\tif (app != null) {\n\t\tapp.stop();\n\t\tint attempts = 0;\n\t\twhile (app.isRunning() && attempts < 5) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t}\n\t\t\tcatch (InterruptedException ex) {\n\t\t\t\tthrow new IllegalStateException(\"Interrupted while waiting for app to stop\");\n\t\t\t}\n\t\t\tattempts++;\n\t\t}\n\t\tif (app.isRunning()) {\n\t\t\tthrow new IllegalStateException(\"App did not stop\");\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 53.49720059944395}, {"code": "public void stop() {\n\tif (app != null) {\n\t\tapp.stop();\n\t\ttry {\n\t\t\tapp.close();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\t// ignore\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 23.695715934565587}, {"code": "public void stop() {\n\tif (app != null) {\n\t\t\tapp.stop();\n\t\t\ttry {\n\t\t\t\tfor (int i = 0; i < 5; i++) {\n\t\t\t\t\tif (app.isRunning()) {\n\t\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (InterruptedException ex) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t\tthrow new IllegalStateException(\"Interrupted\", ex);\n\t\t\t}\n\t\t\tapp = null;\n\t\t}\n\t}", "CodeBleu_score": 47.78527499150955}]}
{"instance_id": "5da405de9e3d", "generation_result": [{"code": "static EurekaServerRunner startEurekaServer(Class config){\n\treturn startEurekaServer(config, new Properties());\n\t}", "CodeBleu_score": 9.605385771572584}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\treturn startEurekaServer(config, null, null, null);\n}", "CodeBleu_score": 7.380509170017271}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner runner = new EurekaServerRunner(config);\n\t\trunner.setServerPort(0);\n\t\trunner.setEnableExposure(true);\n\t\trunner.setEnableSelfPreservation(true);\n\t\trunner.setEnablePeerToPeer(true);\n\t\trunner.setEnableSecurePort(true);\n\t\trunner.setKeyStore(caCert);\n\t\trunner.setKeyStorePassword(KEY_STORE_PASSWORD);\n\t\trunner.setKeyManagerPassword(KEY_PASSWORD);\n\t\trunner.setTrustStore(caCert);\n\t\trunner.setTrustStorePassword(KEY_STORE_PASSWORD);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);\n\t\trunner.setTlsPort(0);\n\t\trunner.setTlsPortEnabled(true);", "CodeBleu_score": 27.57210936293959}]}
{"instance_id": "19cbf70751b2", "generation_result": [{"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config);\n\tclient.setEurekaServer(server.getEurekaServerUrl());\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.setServiceName(\"testservice\");\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 34.34843773759493}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config, server);\n\tclient.setServiceName(\"testservice\");\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 40.78297810303896}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config);\n\t\tclient.setEurekaServer(server.getEurekaServerUrl());\n\t\tclient.enableTls();\n\t\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\t\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\t\tclient.start();\n\t\treturn client;\n\t}", "CodeBleu_score": 33.957475113614954}]}
{"instance_id": "d6ccff5c0536", "generation_result": [{"code": "static void createCertificates() throws Exception{\n\t// Create a CA certificate and save it\n\tKeyTool ca = new KeyTool();\n\tcaCert = ca.createSelfSignedCert(\"CA\", \"CN=Test CA,OU=Test CA,O=Test CA,L=Test CA,ST=Test CA,C=Test CA\", 3650);\n\tsaveCert(caCert, \"ca\");\n\n\t// Create a server certificate and save it\n\tKeyTool server = new KeyTool();\n\tserverCert = server.createCert(\"server\", \"CN=Test Server,OU=Test Server,O=Test Server,L=Test Server,ST=Test Server,C=Test Server\", ca, \"Test CA\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tsaveCert(serverCert, \"server\");\n\n\t// Create a client certificate and save it\n\tKeyTool client = new KeyTool();\n\tclientCert = client.createCert(\"client\", \"CN=Test Client,OU=Test Client,O=Test Client,L=Test Client,ST=Test Client,C=Test Client\", ca, \"Test CA\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tsaveCert(clientCert, \"client\");\n\n\t// Create a wrong CA certificate and save it\n\twrongCaCert = ca.createSelfSignedCert(\"CA\", \"CN=Wrong Test CA,OU=Wrong Test CA,O=Wrong Test CA,L=Wrong Test CA,ST=Wrong Test CA,C=Wrong Test CA\", 3650);\n\tsaveCert(wrongCaCert, \"wrong-ca\");\n\n\t// Create a wrong client certificate and save it\n\twrongClientCert = client.createCert(\"wrong-client\", \"CN=Wrong Test Client,OU=Wrong Test Client,O=Wrong Test Client,L=Wrong Test Client,ST=Wrong Test Client,C=Wrong Test Client\", ca, \"Wrong Test CA\", KEY_STORE_PASSWORD, WRONG_PASSWORD);\n\tsaveCert(wrongClientCert, \"wrong-client\");\n\n\t// Create a server certificate and save it\n\tKeyTool wrongServer = new KeyTool();\n\twrongServerCert = wrongServer.create", "CodeBleu_score": 25.230895215677187}, {"code": "static void createCertificates() throws Exception{\n\tKeyTool keyTool = new KeyTool();\n\t// Generate CA certificate\n\tFile caKey = new File(\"target/ca.key\");\n\tFile caCert = new File(\"target/ca.crt\");\n\tkeyTool.genKey(caKey, \"ca\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tkeyTool.genCert(caCert, caKey, \"ca\", \"ca\", \"cn=ca\", 365);\n\n\t// Generate server certificate\n\tFile serverKey = new File(\"target/server.key\");\n\tFile serverCert = new File(\"target/server.crt\");\n\tkeyTool.genKey(serverKey, \"server\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tkeyTool.genCert(serverCert, caKey, serverKey, \"server\", \"cn=server\", 365);\n\n\t// Generate client certificate\n\tFile clientKey = new File(\"target/client.key\");\n\tFile clientCert = new File(\"target/client.crt\");\n\tkeyTool.genKey(clientKey, \"client\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tkeyTool.genCert(clientCert, caKey, clientKey, \"client\", \"cn=client\", 365);\n\n\t// Generate wrong client certificate\n\tFile wrongClientKey = new File(\"target/wrong-client.key\");\n\tFile wrongClientCert = new File(\"target/wrong-client.crt\");\n\tkeyTool.genKey(wrongClientKey, \"wrong-client\", KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tkeyTool.genCert(wrongClientCert, caKey, wrongClientKey, \"wrong-client\", \"cn=wrong-client\", 365);\n\n\t// Save CA certificate\n\tsaveCert(caCert, \"ca.crt\");\n\n\t// Save server certificate\n\tsaveCert(serverCert, \"server.crt\");\n\n\t// Save client certificate\n\tsaveCert(clientCert, \"client.crt\");\n\n\t// Save wrong client certificate\n\tsaveCert(wrongClientCert, \"wrong-client.crt\");", "CodeBleu_score": 26.75004848376789}, {"code": "static void createCertificates() throws Exception{\n\t// Create CA certificate and save it\n\t\tKeyTool keyTool = new KeyTool();\n\t\tFile caCertFile = new File(\"src/test/resources/ca.crt\");\n\t\tFile caKeyFile = new File(\"src/test/resources/ca.key\");\n\t\tkeyTool.genKeyPair(caCertFile, caKeyFile, \"test-ca\", \"test-ca-password\");\n\t\tkeyTool.genCert(caCertFile, caKeyFile, caCertFile, \"test-ca\", \"test-ca-password\", 365, \"CN=Test CA\");\n\t\tcaCert = caCertFile;\n\t\t// Create server certificate and save it\n\t\tFile serverCertFile = new File(\"src/test/resources/server.crt\");\n\t\tFile serverKeyFile = new File(\"src/test/resources/server.key\");\n\t\tkeyTool.genKeyPair(serverCertFile, serverKeyFile, \"test-server\", \"test-server-password\");\n\t\tkeyTool.genCert(serverCertFile, serverKeyFile, caCertFile, \"test-server\", \"test-server-password\", 365, \"CN=Test Server\");\n\t\tserverCert = serverCertFile;\n\t\t// Create client certificate and save it\n\t\tFile clientCertFile = new File(\"src/test/resources/client.crt\");\n\t\tFile clientKeyFile = new File(\"src/test/resources/client.key\");\n\t\tkeyTool.genKeyPair(clientCertFile, clientKeyFile, \"test-client\", \"test-client-password\");\n\t\tkeyTool.genCert(clientCertFile, clientKeyFile, caCertFile, \"test-client\", \"test-client-password\", 365, \"CN=Test Client\");\n\t\tclientCert = clientCertFile;\n\t\t// Create wrong client certificate and save it\n\t\tFile wrongClientCertFile = new File(\"src/test/resources/wrong-client.crt\");\n\t\tFile wrongClientKeyFile = new File(\"src/test/resources/wrong-client.key\");\n\t\tkeyTool.genKeyPair(wrongClientCertFile, wrongClientKeyFile, \"test-wrong-client", "CodeBleu_score": 26.132193078803535}]}
{"instance_id": "d2eeaa6a6bbe", "generation_result": [{"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong limit = System.currentTimeMillis() + seconds * 1000;\n    while (System.currentTimeMillis() < limit) {\n        if (assertion.getAsBoolean()) {\n            return;\n        }\n        try {\n            Thread.sleep(100);\n        }\n        catch (InterruptedException e) {\n            throw new RuntimeException(e);\n        }\n    }\n    throw new RuntimeException(\"Assertion not satisfied within \" + seconds + \" seconds\");\n}", "CodeBleu_score": 43.63833375151729}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong endTime = System.currentTimeMillis() + seconds * 1000;\n\twhile (System.currentTimeMillis() < endTime) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t\ttry {\n\t\t\tThread.sleep(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tThread.currentThread().interrupt();\n\t\t\tthrow new RuntimeException(\"interrupted\");\n\t\t}\n\t}\n\tthrow new RuntimeException(\"assertion timed out\");\n}", "CodeBleu_score": 41.50998207480325}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong start = System.currentTimeMillis();\n\tlong end = start + seconds * 1000;\n\twhile (System.currentTimeMillis() < end) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t\ttry {\n\t\t\tThread.sleep(100);\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\tthrow new RuntimeException(\"Assertion not met within \" + seconds + \" seconds\");\n}", "CodeBleu_score": 44.764464816460524}]}
{"instance_id": "6bb261b0e4e1", "generation_result": [{"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX500Name caX500Name = new X500Name(ca);\n\treturn createCert(keyPair.getPublic(), caX500Name);\n}", "CodeBleu_score": 18.778655987610872}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\treturn createCert(keyPair.getPublic(), ca);\n\t}", "CodeBleu_score": 12.950294484666703}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX500Name caName = new X500Name(ca);\n\tX509CertificateHolder certHolder = createCertHolder(keyPair.getPublic(), caName);\n\tContentSigner signer = new JcaContentSignerBuilder(\"SHA1withRSA\").setProvider(\"BC\").build(keyPair.getPrivate());\n\treturn new JcaX509CertificateConverter().getCertificate(certHolder.sign(signer));\n}", "CodeBleu_score": 36.150531736041934}]}
{"instance_id": "e4f9c32a7c9e", "generation_result": [{"code": "EurekaHttpClient getEurekaHttpClient(){\n\tif (this.eurekaHttpClient.get() == null) {\n\t\ttry {\n\t\t\tObject eurekaTransport = ReflectionUtils.getField(this.eurekaTransportField, this);\n\t\t\tthis.eurekaHttpClient.set((EurekaHttpClient) ReflectionUtils.getField(ReflectionUtils.findField(eurekaTransport.getClass(), \"registrationClient\"), eurekaTransport));\n\t\t} catch (IllegalAccessException e) {\n\t\t\tlog.error(\"Error retrieving EurekaHttpClient\", e);\n\t\t}\n\t}\n\treturn this.eurekaHttpClient.get();\n}", "CodeBleu_score": 55.99944221407351}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tif (eurekaHttpClient.get() == null) {\n\t\ttry {\n\t\t\tObject eurekaTransport = ReflectionUtils.getField(eurekaTransportField, this);\n\t\t\teurekaHttpClient.set((EurekaHttpClient) ReflectionUtils.getField(ReflectionUtils.findField(eurekaTransport.getClass(), \"registrationClient\"), eurekaTransport));\n\t\t} catch (IllegalAccessException e) {\n\t\t\tlog.error(\"Error retrieving EurekaHttpClient\", e);\n\t\t}\n\t}\n\treturn eurekaHttpClient.get();\n}", "CodeBleu_score": 45.921531651849115}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tif (this.eurekaHttpClient.get() == null) {\n\t\ttry {\n\t\t\tObject registrationClient = ReflectionUtils.getField(this.eurekaTransportField, this);\n\t\t\tReflectionUtils.makeAccessible(this.eurekaTransportField);\n\t\t\tthis.eurekaHttpClient.set((EurekaHttpClient) ReflectionUtils.getField(this.eurekaTransportField, registrationClient));\n\t\t} catch (IllegalAccessException e) {\n\t\t\tlog.error(\"Could not access EurekaHttpClient\", e);\n\t\t}\n\t}\n\treturn this.eurekaHttpClient.get();\n}", "CodeBleu_score": 53.02971889250958}]}
{"instance_id": "5c404f04e34c", "generation_result": [{"code": "public List<String> getServices(){\n\tApplications applications = eurekaClient.getApplications();\n    if (applications == null) {\n        return Collections.emptyList();\n    }\n    List<String> services = new ArrayList<>();\n    for (Application application : applications.getRegisteredApplications()) {\n        if (application.getInstances() != null && !application.getInstances().isEmpty()) {\n            services.add(application.getName().toLowerCase());\n        }\n    }\n    return services;\n}", "CodeBleu_score": 60.205514913407455}, {"code": "public List<String> getServices(){\n\tList<Application> applications = this.eurekaClient.getApplications();\n\tif (applications == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications) {\n\t\tif (!application.getInstances().isEmpty()) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 60.998524473627114}, {"code": "public List<String> getServices(){\n\tApplications applications = eurekaClient.getApplications();\n    if (applications == null) {\n        return Collections.emptyList();\n    }\n    List<String> services = new ArrayList<>();\n    for (Application application : applications.getRegisteredApplications()) {\n        if (application.getInstances() != null && !application.getInstances().isEmpty()) {\n            services.add(application.getName().toLowerCase());\n        }\n    }\n    return services;\n}", "CodeBleu_score": 60.205514913407455}]}
{"instance_id": "a21251337ade", "generation_result": [{"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\t\tHealthContributor contributor = entry.getValue();\n\t\t\tif (contributor instanceof DiscoveryCompositeHealthContributor) {\n\t\t\t\tDiscoveryCompositeHealthContributor compositeContributor = (DiscoveryCompositeHealthContributor) contributor;\n\t\t\t\tfor (HealthContributor hc : compositeContributor.getContributors()) {\n\t\t\t\t\tif (!(hc instanceof EurekaHealthIndicator)) {\n\t\t\t\t\t\tthis.healthContributors.put(entry.getKey(), hc);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthis.healthContributors.put(entry.getKey(), contributor);\n\t\t\t}\n\t\t}\n\t}", "CodeBleu_score": 41.4395934768312}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\t\tHealthContributor healthContributor = entry.getValue();\n\t\t\tif (healthContributor instanceof DiscoveryCompositeHealthContributor) {\n\t\t\t\tDiscoveryCompositeHealthContributor compositeHealthContributor = (DiscoveryCompositeHealthContributor) healthContributor;\n\t\t\t\tfor (NamedContributor<HealthContributor> contributor : compositeHealthContributor.getContributors()) {\n\t\t\t\t\tHealthContributor namedContributor = contributor.getContributor();\n\t\t\t\t\tif (!(namedContributor instanceof EurekaHealthIndicator)) {\n\t\t\t\t\t\tthis.healthContributors.put(contributor.getName(), namedContributor);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthis.healthContributors.put(entry.getKey(), healthContributor);\n\t\t\t}\n\t\t}\n\t}", "CodeBleu_score": 39.20555958889354}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\t\tHealthContributor contributor = entry.getValue();\n\t\t\tif (contributor instanceof DiscoveryCompositeHealthContributor) {\n\t\t\t\tDiscoveryCompositeHealthContributor compositeHealthContributor = (DiscoveryCompositeHealthContributor) contributor;\n\t\t\t\tfor (HealthContributor healthContributor : compositeHealthContributor.getContributors()) {\n\t\t\t\t\tif (!(healthContributor instanceof EurekaHealthIndicator)) {\n\t\t\t\t\t\tthis.healthContributors.put(entry.getKey(), healthContributor);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthis.healthContributors.put(entry.getKey(), contributor);\n\t\t\t}\n\t\t}\n\t}", "CodeBleu_score": 40.36730627414453}]}
{"instance_id": "998fee829929", "generation_result": [{"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No instances found for provider-application\");\n    }\n    ServiceInstance instance = instances.get(0);\n    URI uri = instance.getUri();\n    String response = restTemplate.postForObject(uri + \"/echo\", \"Hello from consumer-application!\", String.class);\n    return \"Hello from consumer-application! I got a response from provider-application: \" + response;\n}", "CodeBleu_score": 58.27157112145643}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No instances found for provider-application\");\n    }\n    URI uri = instances.get(0).getUri();\n    String result = restTemplate.postForObject(uri + \"/echo\", \"Hello from consumer-application\", String.class);\n    return \"Hello from consumer-application: \" + result;\n}", "CodeBleu_score": 50.41346119531549}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No instances found for provider-application\");\n    }\n    ServiceInstance instance = instances.get(0);\n    URI uri = instance.getUri();\n    String response = restTemplate.postForObject(uri + \"/echo\", \"Hello, World!\", String.class);\n    return response;\n}", "CodeBleu_score": 61.55615539480739}]}
{"instance_id": "31166ae7d3dc", "generation_result": [{"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 77.66941561934959}, {"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 75.43732908608489}, {"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 77.66941561934959}]}
{"instance_id": "d7714f9cf0e9", "generation_result": [{"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service: \" + serviceName + \" at \" + ip + \":\" + port);\n    Set<Server> servers = serverMap.get(serviceName);\n    if (servers == null) {\n        servers = Collections.synchronizedSet(new HashSet<>());\n        serverMap.put(serviceName, servers);\n    }\n    servers.add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 54.180305078382986}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service \" + serviceName + \" at \" + ip + \":\" + port);\n    Set<Server> servers = serverMap.get(serviceName);\n    if (servers == null) {\n        servers = Collections.synchronizedSet(new HashSet<>());\n        serverMap.put(serviceName, servers);\n    }\n    servers.add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 54.16020053323256}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering serviceName: \" + serviceName + \" with IP: \" + ip + \" and port: \" + port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.putIfAbsent(serviceName, Collections.synchronizedSet(new HashSet<Server>()));\n    }\n    serverMap.get(serviceName).add(new Server(ip, port));\n    return true;\n}", "CodeBleu_score": 50.42467279735463}]}
{"instance_id": "45f91ed1c560", "generation_result": [{"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    if (serverMap.containsKey(serviceName)) {\n        Server server = new Server(ip, port);\n        if (serverMap.get(serviceName).remove(server)) {\n            logger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n            return true;\n        }\n    }\n    return false;\n}", "CodeBleu_score": 65.10897293523588}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Server server = new Server(ip, port);\n    if (serverMap.containsKey(serviceName.toLowerCase())) {\n        serverMap.get(serviceName.toLowerCase()).remove(server);\n    }\n    return true;\n}", "CodeBleu_score": 68.90368517997241}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Server server = new Server(ip, port);\n    Set<Server> serverSet = serverMap.get(serviceName.toLowerCase());\n    if (serverSet == null) {\n        return false;\n    }\n    boolean result = serverSet.remove(server);\n    if (serverSet.isEmpty()) {\n        serverMap.remove(serviceName.toLowerCase());\n    }\n    return result;\n}", "CodeBleu_score": 77.38387519704068}]}
{"instance_id": "f671d9564e40", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes == null) {\n        return;\n    }\n    Object defaultConfiguration = attributes.get(\"defaultConfiguration\");\n    if (defaultConfiguration != null) {\n        String name = defaultConfiguration.toString();\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addConstructorArgValue(name);\n        registry.registerBeanDefinition(name, builder.getBeanDefinition());\n    }\n    Object[] clients = (Object[]) attributes.get(\"value\");\n    if (clients == null) {\n        return;\n    }\n    for (Object client : clients) {\n        String name = client.toString();\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addConstructorArgValue(name);\n        registry.registerBeanDefinition(name, builder.getBeanDefinition());\n    }\n}", "CodeBleu_score": 42.5981410547687}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null) {\n        String defaultConfiguration = (String) attributes.get(\"defaultConfiguration\");\n        if (defaultConfiguration != null) {\n            String beanName = defaultConfiguration.substring(0, 1).toUpperCase() + defaultConfiguration.substring(1) + \"ClientConfiguration\";\n            BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(ClientConfiguration.class);\n            builder.addPropertyValue(\"defaultConfiguration\", defaultConfiguration);\n            registry.registerBeanDefinition(beanName, builder.getBeanDefinition());\n        }\n    }\n}", "CodeBleu_score": 42.7453637585971}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    String defaultConfiguration = (String) attributes.get(\"defaultConfiguration\");\n    if (defaultConfiguration != null) {\n        String clientConfiguration = \"ribbonClientConfiguration\";\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n        builder.addPropertyValue(\"defaultConfiguration\", defaultConfiguration);\n        registry.registerBeanDefinition(clientConfiguration, builder.getBeanDefinition());\n    }\n}", "CodeBleu_score": 43.25663967933945}]}
{"instance_id": "dbc92d8f146f", "generation_result": [{"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\t// \u83b7\u53d6\u670d\u52a1\u540d\u79f0\n    String serviceName = serviceId;\n    // \u83b7\u53d6\u8d1f\u8f7d\u5747\u8861\u5668\n    ILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceName);\n    // \u83b7\u53d6\u670d\u52a1\u5b9e\u4f8b\n    Server server = loadBalancer.chooseServer();\n    // \u5224\u65ad\u670d\u52a1\u5b9e\u4f8b\u662f\u5426\u5b58\u5728\n    if (server == null) {\n        return null;\n    }\n    // \u83b7\u53d6\u670d\u52a1\u5b9e\u4f8b\u7684\u5730\u5740\n    String host = server.getHost();\n    // \u83b7\u53d6\u670d\u52a1\u5b9e\u4f8b\u7684\u7aef\u53e3\n    int port = server.getPort();\n    // \u521b\u5efa\u670d\u52a1\u5b9e\u4f8b\n    return new TutuServiceInstance(serviceName, host, port);\n}", "CodeBleu_score": 42.28929435475235}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n    if (loadBalancer == null) {\n        return null;\n    }\n    Server server = loadBalancer.chooseServer();\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort(), server.getMetaInfo());\n}", "CodeBleu_score": 54.69534094864763}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer lb = clientFactory.getLoadBalancer(serviceId);\n    if (lb == null) {\n        return null;\n    }\n    Server server = lb.chooseServer(request);\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort(), server.isSecure());\n}", "CodeBleu_score": 52.90586375878309}]}
{"instance_id": "362eadbd3c5e", "generation_result": [{"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n        String scheme = original.getScheme() == null ? \"http\" : original.getScheme();\n        String host = server.getHost();\n        int port = server.getPort();\n        String path = original.getPath();\n        String query = original.getQuery();\n        String fragment = original.getFragment();\n        return new URI(scheme, null, host, port, path, query, fragment);\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 40.54524120745114}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\tString originalURI = original.toString();\n    String originalPath = original.getPath();\n    String originalQuery = original.getQuery();\n    String originalScheme = original.getScheme();\n    String originalHost = original.getHost();\n    int originalPort = original.getPort();\n    String originalUserInfo = original.getUserInfo();\n    String originalFragment = original.getFragment();\n\n    String serverHost = server.getHost();\n    int serverPort = server.getPort();\n\n    String reconstructedURI;\n    if (originalQuery != null) {\n        reconstructedURI = String.format(\"%s://%s:%s%s?%s\", originalScheme, serverHost, serverPort, originalPath, originalQuery);\n    } else {\n        reconstructedURI = String.format(\"%s://%s:%s%s\", originalScheme, serverHost, serverPort, originalPath);\n    }\n\n    URI reconstructedURIObj = null;\n    try {\n        reconstructedURIObj = new URI(reconstructedURI);\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(\"Error reconstructing URI\", e);\n    }\n\n    return reconstructedURIObj;\n}", "CodeBleu_score": 35.6169605285512}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n        String scheme = original.getScheme();\n        String host = server.getHost();\n        int port = server.getPort();\n        String path = original.getPath();\n        String query = original.getQuery();\n        String fragment = original.getFragment();\n        StringBuilder sb = new StringBuilder();\n        sb.append(scheme).append(\"://\").append(host);\n        if (port != -1) {\n            sb.append(\":\").append(port);\n        }\n        sb.append(path);\n        if (StrUtil.isNotEmpty(query)) {\n            sb.append(\"?\").append(query);\n        }\n        if (StrUtil.isNotEmpty(fragment)) {\n            sb.append(\"#\").append(fragment);\n        }\n        return new URI(sb.toString());\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 57.06936573678198}]}
{"instance_id": "6801d2b2e4c8", "generation_result": [{"code": "private List<TutuServer> getServer(){\n\t// \u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    List<TutuServer> serverList = new ArrayList<>();\n    // \u6784\u5efa\u8bf7\u6c42\u53c2\u6570\n    Map<String, String> params = new HashMap<>();\n    params.put(\"serviceName\", serviceId);\n    // \u53d1\u9001\u8bf7\u6c42\n    String response = HttpUtil.get(discoveryProperties.getServerAddress() + \"/list\", params);\n    // \u89e3\u6790\u54cd\u5e94\n    JSONObject jsonObject = JSON.parseObject(response);\n    JSONArray jsonArray = jsonObject.getJSONArray(\"list\");\n    // \u89e3\u6790\u6570\u636e\n    List<TutuServer> serverListTmp = jsonArray.stream().map(obj -> JSON.parseObject(JSON.toJSONString(obj), TutuServer.class)).collect(Collectors.toList());\n    // \u6dfb\u52a0\u5230\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    serverList.addAll(serverListTmp);\n    // \u8fd4\u56de\u670d\u52a1\u5b9e\u4f8b\u5217\u8868\n    return serverList;\n}", "CodeBleu_score": 46.748724852511195}, {"code": "private List<TutuServer> getServer(){\n\tMap<String, String> params = new HashMap<>();\n    params.put(\"serviceName\", serviceId);\n    String result = HttpUtil.get(discoveryProperties.getServer() + \"/list\", params);\n    JSONObject jsonObject = JSON.parseObject(result);\n    return jsonObject.getJSONArray(\"list\").stream().map(item -> {\n        JSONObject json = (JSONObject) item;\n        return new TutuServer(json.getString(\"ip\"), json.getInteger(\"port\"));\n    }).collect(Collectors.toList());\n}", "CodeBleu_score": 48.39088828922059}, {"code": "private List<TutuServer> getServer(){\n\tString url = discoveryProperties.getServer() + \"/list\";\n    Map<String, String> params = new HashMap<>();\n    params.put(\"serviceName\", serviceId);\n    String result = HttpUtil.get(url, params);\n    JSONObject jsonObject = JSON.parseObject(result);\n    List<TutuServer> tutuServers = jsonObject.getJSONArray(\"data\").stream().map(o -> {\n        JSONObject json = (JSONObject) o;\n        return new TutuServer(json.getString(\"ip\"), json.getInteger(\"port\"));\n    }).collect(Collectors.toList());\n    return tutuServers;\n}", "CodeBleu_score": 48.06383365610335}]}
{"instance_id": "ab2fb77484fd", "generation_result": [{"code": "public Route getMatchingRoute(String path){\n\tif (path == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n\t\t\tString routePath = entry.getKey();\n\t\t\tZuulProperties.ZuulRoute route = entry.getValue();\n\t\t\tif (pathMatcher.match(routePath, path)) {\n\t\t\t\tString extractedPath = pathMatcher.extractPathWithinPattern(routePath, path);\n\t\t\t\treturn new Route(extractedPath, route.getServiceId());\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}", "CodeBleu_score": 53.398673434965204}, {"code": "public Route getMatchingRoute(String path){\n\t// Iterate through the configured Zuul routes to find a matching route based on the given path.\n\t\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : this.zuulProperties.getRoutes().entrySet()) {\n\n\t\t\t// Extract the Zuul route pattern and the corresponding service ID.\n\t\t\tString pattern = entry.getKey();\n\t\t\tString id = entry.getValue().getId();\n\n\t\t\t// Use the Ant-style path matching provided by pathMatcher to compare the incoming path with the route pattern.\n\t\t\tif (pathMatcher.match(pattern, path)) {\n\n\t\t\t\t// Extract the target path from the original path.\n\t\t\t\tString target = pathMatcher.extractPathWithinPattern(pattern, path);\n\n\t\t\t\t// Create a new Route object with the extracted path and the corresponding service ID.\n\t\t\t\treturn new Route(id, target);\n\t\t\t}\n\t\t}\n\n\t\t// If no match is found among the configured routes, return null.\n\t\treturn null;\n\t}", "CodeBleu_score": 52.846604595696654}, {"code": "public Route getMatchingRoute(String path){\n\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n        String pattern = entry.getKey();\n        ZuulProperties.ZuulRoute route = entry.getValue();\n        if (pathMatcher.match(pattern, path)) {\n            String extractedPath = pathMatcher.extractPathWithinPattern(pattern, path);\n            return new Route(extractedPath, route.getServiceId());\n        }\n    }\n    return null;\n}", "CodeBleu_score": 57.338469580274634}]}
{"instance_id": "822071925513", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tSet<String> basePackages = AnnotationAttributes.fromMap(importingClassMetadata.getAnnotationAttributes(EnableFeignClients.class.getName())).getStringArray(\"value\");\n    if (basePackages.isEmpty()) {\n        basePackages = AnnotationAttributes.fromMap(importingClassMetadata.getAnnotationAttributes(EnableFeignClients.class.getName())).getStringArray(\"basePackages\");\n    }\n    for (String basePackage : basePackages) {\n        Set<Class<?>> classes = ClassUtils.getClassesWithAnnotation(basePackage, FeignClient.class);\n        for (Class<?> clazz : classes) {\n            GenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n            beanDefinition.setBeanClass(FeignClientFactoryBean.class);\n            beanDefinition.getPropertyValues().add(\"contextId\", clazz.getSimpleName());\n            beanDefinition.getPropertyValues().add(\"type\", clazz);\n            beanDefinition.setAutowireMode(GenericBeanDefinition.AUTOWIRE_BY_TYPE);\n            beanDefinition.setLazyInit(false);\n            registry.registerBeanDefinition(clazz.getSimpleName(), beanDefinition);\n        }\n    }\n}", "CodeBleu_score": 58.3336426683693}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(false);\n    scanner.addIncludeFilter(new AnnotationTypeFilter(FeignClient.class));\n    Set<BeanDefinition> candidates = scanner.findCandidateComponents(importingClassMetadata.getClassName());\n    for (BeanDefinition candidate : candidates) {\n        GenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n        beanDefinition.setBeanClass(FeignClientFactoryBean.class);\n        beanDefinition.getPropertyValues().add(\"contextId\", candidate.getBeanClassName());\n        beanDefinition.getPropertyValues().add(\"type\", ClassUtils.resolveClassName(candidate.getBeanClassName(), ClassUtils.getDefaultClassLoader()));\n        beanDefinition.setAutowireMode(GenericBeanDefinition.AUTOWIRE_BY_TYPE);\n        registry.registerBeanDefinition(ClassUtil.getSimpleName(candidate.getBeanClassName()), beanDefinition);\n    }\n}", "CodeBleu_score": 40.555605533354026}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tSet<String> basePackages = ClassUtils.getPackageName(importingClassMetadata.getClassName());\n    basePackages.forEach(basePackage -> {\n        // \u83b7\u53d6\u6240\u6709\u7684FeignClient\u7c7b\n        Set<Class<?>> feignClientClasses = ClassUtil.scanPackageByAnnotation(basePackage, FeignClient.class);\n        feignClientClasses.forEach(feignClientClass -> {\n            // \u83b7\u53d6FeignClient\u6ce8\u89e3\n            FeignClient feignClient = feignClientClass.getAnnotation(FeignClient.class);\n            // \u83b7\u53d6FeignClient\u6ce8\u89e3\u7684value\u5c5e\u6027\n            String contextId = feignClient.contextId();\n            // \u83b7\u53d6FeignClient\u6ce8\u89e3\u7684value\u5c5e\u6027\n            Class<?> feignClientType = feignClient.value();\n            // \u521b\u5efaGenericBeanDefinition\n            GenericBeanDefinition genericBeanDefinition = new GenericBeanDefinition();\n            // \u8bbe\u7f6eBean\u7684\u7c7b\u578b\n            genericBeanDefinition.setBeanClass(FeignClientFactoryBean.class);\n            // \u8bbe\u7f6eBean\u7684\u5c5e\u6027\n            genericBeanDefinition.getPropertyValues().addPropertyValue(\"contextId\", contextId);\n            genericBeanDefinition.getPropertyValues().addPropertyValue(\"type\", feignClientType);\n            // \u6ce8\u518cBean\n            registry.registerBeanDefinition(feignClientClass.getSimpleName(), genericBeanDefinition);\n        });\n    });\n}", "CodeBleu_score": 51.888747761201245}]}
{"instance_id": "3d678e3bf3c8", "generation_result": [{"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}]}
