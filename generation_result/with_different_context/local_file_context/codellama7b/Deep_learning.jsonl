{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\tif self.config.model.cache:\n        if cached_ldsr_model is not None:\n            if ismap(cached_ldsr_model):\n                cached_ldsr_model = cached_ldsr_model[0]\n            if cached_ldsr_model is not None:\n                if self.config.model.cache_check and cached_ldsr_model.config.model.path == self.config.model.path:\n                    print(\"Loading cached model from\", self.config.model.path)\n                    return cached_ldsr_model\n        if self.config.model.path.endswith(\".safetensors\"):\n            if self.config.model.path.startswith(\"s3://\"):\n                from safetensors.torch import S3Tensor\n                cached_ldsr_model = S3Tensor(self.config.model.path, map_location=devices.get_device())\n            else:\n                cached_ldsr_model = safetensors.torch.load(self.config.model.path, map_location=devices.get_device())\n            if ismap(cached_ldsr_model):\n                cached_ldsr_model = cached_ldsr_model[0]\n            if self.config.model.cache:\n                cached_ldsr_model.eval()\n                cached_ldsr_model.config.model.cache_check = self.config.model.cache_check\n                cached_ldsr_model.config.model.path = self.config.model.path\n                cached_ldsr_model.config.model.half_attention = self.config.model.half_attention\n                cached_ldsr_model.config.model.optimize_memory = self.config.model.optimize_memory\n                cached_ldsr_model.config.model.optimize_memory_half_precision = self.config.model.optimize_memory_half_precision\n                cached_ldsr_model.config.model.optimize_memory_recompute = self.config.model.optimize_memory_recompute\n                cached_ldsr_model.config.model.optimize_memory_recompute_", "CodeBleu_score": 32.410158175092505}, {"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    if cached_ldsr_model is not None:\n        if self.config.cache_model:\n            if os.path.exists(self.config.cache_model_path):\n                if os.path.getmtime(self.config.cache_model_path) > os.path.getmtime(self.config.model_path):\n                    print(f'Loading cached model from {self.config.cache_model_path}')\n                    self.ldsr_model = cached_ldsr_model\n                    self.ldsr_model.eval()\n                    return\n\n    # Load the model\n    print(f'Loading model from {self.config.model_path}')\n    self.ldsr_model = instantiate_from_config(self.config.model_config)\n\n    # Move the model to the appropriate device\n    self.ldsr_model = self.ldsr_model.to(self.device)\n\n    # Set the model in evaluation mode\n    self.ldsr_model.eval()\n\n    # Cache the model if desired\n    if self.config.cache_model:\n        print(f'Caching model to {self.config.cache_model_path}')\n        torch.save(self.ldsr_model, self.config.cache_model_path)\n        cached_ldsr_model = self.ldsr_model\n\n    # Half-precision attention\n    if half_attention:\n        self.ldsr_model.half()\n\n    # Memory format optimization\n    if self.config.memory_format == 'channels_last':\n        self.ldsr_model = sd_hijack.sd_hijack(self.ldsr_model, self.config.memory_format)\n\n", "CodeBleu_score": 32.73823627468523}, {"code": "def load_model_from_config(self, half_attention)\n\t# Load model from cache if available\n    if cached_ldsr_model is not None:\n        print(\"Loading LDSR model from cache\")\n        model = cached_ldsr_model\n    else:\n        # Load model from path\n        print(\"Loading LDSR model from path\")\n        model = self.load_model_from_path(self.model_path)\n        if self.cache_model:\n            cached_ldsr_model = model\n\n    # Move model to device\n    model = model.to(self.device)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Set half-precision attention\n    if half_attention:\n        model = sd_hijack.sd_hijack(model)\n\n    # Set memory format\n    if self.memory_format == \"channels_last\":\n        model = sd_hijack.sd_hijack(model, memory_format=\"channels_last\")\n\n    # Return model\n    return model\n\n", "CodeBleu_score": 27.23142797512824}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is None:\n            context = \"EMA\"\n        if self.model_ema.enabled:\n            if self.model_ema.context != context:\n                print(f\"{context}: Switching to EMA weights.\")\n                self.model_ema.enable(context)\n        yield\n        if self.model_ema.enabled:\n            if self.model_ema.context == context:\n                print(f\"{context}: Restoring original weights.\")\n                self.model_ema.disable()\n\n", "CodeBleu_score": 51.331057660763314}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema and context is not None:\n        context.print(f\"Using EMA weights.\")\n    if self.use_ema:\n        self.model_ema.apply_shadow()\n    try:\n        yield\n    finally:\n        if self.use_ema:\n            self.model_ema.swap_ema_buffers()\n            if context is not None:\n                context.print(f\"Restored original weights.\")\n\n", "CodeBleu_score": 52.76281901412049}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is None:\n            context = self.__class__.__name__\n        print(f\"{context}: Using EMA weights.\")\n        self.model_ema.apply_shadow()\n        yield\n        print(f\"{context}: Restoring original weights.\")\n        self.model_ema.restore()\n    else:\n        yield\n\n", "CodeBleu_score": 42.181210203160035}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif ignore_keys is None:\n        ignore_keys = []\n    if isinstance(ignore_keys, str):\n        ignore_keys = [ignore_keys]\n    ckpt = torch.load(path, map_location=lambda s, l: s)\n    state_dict = ckpt[\"state_dict\"]\n    for k in list(state_dict.keys()):\n        if any(ki in k for ki in ignore_keys):\n            state_dict.pop(k)\n    self.load_state_dict(state_dict, strict=False)\n    print(f\"{self.__class__.__name__}: Loaded checkpoint at {path}\")\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        module._load_from_state_dict(\n            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n        )\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    load(self)\n    print(f\"{self.__class__.__name__}: Missing keys: {missing_keys}\")\n    print(f\"{self.__class__.__name__}: Unexpected keys: {unexpected_keys}\")\n    print(f\"{self.__class__.__name__}: Error msgs: {error_msgs}\")\n\n", "CodeBleu_score": 34.82879336909785}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif ignore_keys is None:\n        ignore_keys = []\n    ckpt = torch.load(path, map_location=lambda storage, loc: storage)\n    state_dict = ckpt[\\\"model\\\"]\n    for k in list(state_dict.keys()):\n        if any(ki in k for ki in ignore_keys):\n            del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    print(f\\\"Initialized from {path}. Missing/unexpected:\\\")\n    missing = set()\n    unexpected = set()\n    for k in self.state_dict().keys():\n        if k not in state_dict:\n            missing.add(k)\n        elif state_dict[k].shape != self.state_dict()[k].shape:\n            unexpected.add(k)\n    print(f\\\"Missing:\\\", missing)\n    print(f\\\"Unexpected:\\\", unexpected)\n\n", "CodeBleu_score": 39.36168658721352}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    for k in list(sd.keys()):\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(self.state_dict(), \"_metadata\", None)\n    state_dict = self.state_dict()\n    for k, v in state_dict.items():\n        if k not in sd:\n            missing_keys.append(k)\n        elif v.shape != sd[k].shape:\n            unexpected_keys.append(k)\n        elif v.dtype != sd[k].dtype:\n            error_msgs.append(\n                \"For {} all params loaded but with wrong dtype: {} vs {}\".format(\n                    k, sd[k].dtype, v.dtype\n                )\n            )\n        else:\n            state_dict[k] = sd[k]\n    self.load_state_dict(state_dict, strict=False)\n    if len(unexpected_keys) > 0:\n        print(\n            f\"{self.__class__.__name__}: Unexpected key(s) in state_dict: {unexpected_keys}", "CodeBleu_score": 41.1255653635053}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\tif self.training:\n        quant, emb_loss, info = self.encode(input)\n        dec = self.decode(quant)\n        loss = self.loss(input, dec)\n        loss += emb_loss\n        if return_pred_indices:\n            pred_indices = info[\\\"pred_indices\\\"]\n            return dec, loss, pred_indices\n        else:\n            return dec, loss\n    else:\n        quant = self.encode_to_prequant(input)\n        dec = self.decode_code(quant)\n        return dec\n\n", "CodeBleu_score": 47.04122524922523}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, emb_loss, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, emb_loss, info, self.quantize.embed_code(quant)\n    else:\n        return dec, emb_loss, info\n\n", "CodeBleu_score": 42.09735211547722}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, emb_loss, info = self.encode(input)\n    if self.training:\n        dec = self.decode(quant)\n        loss = self.loss(input, dec, emb_loss, info)\n        if return_pred_indices:\n            return dec, loss, info, quant\n        else:\n            return dec, loss, info\n    else:\n        dec = self.decode(quant)\n        return dec, info\n\n", "CodeBleu_score": 26.615502242340423}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\tinput = batch[k]\n    if self.batch_resize_range is not None:\n        # Resize the input if enabled and if it's an image.\n        # We do this here instead of in the training loop so that the input is always the same size.\n        if input.shape[1] == 3 and input.shape[2] == 256:\n            input = input.unsqueeze(0)\n            input = F.interpolate(input, size=self.batch_resize_range, mode=\\\"bicubic\\\", align_corners=False)\n            input = input.squeeze(0)\n        input = input.permute(0, 2, 3, 1)\n        input = input.contiguous().float()\n        input.detach_()\n        return input\n    else:\n        return input\n\n", "CodeBleu_score": 33.89163331766198}, {"code": "def get_input(self, batch, k)\n\tinput = batch[k]\n\n    if self.batch_resize_range is not None and self.training:\n\n        resize_factor = self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * (self.trainer.global_step / self.trainer.max_steps)\n\n        input = F.interpolate(input, scale_factor=resize_factor, mode=\\\"bicubic\\\")\n\n    if len(input.shape) == 3:\n\n        input = input.unsqueeze(1)\n\n    if len(input.shape) == 2:\n\n        input = input.unsqueeze(1).unsqueeze(1)\n\n    input = input.permute(0, 3, 1, 2)\n\n    input = input.contiguous().float()\n\n    if not self.training:\n\n        input = input.detach()\n\n    return input\n\n\n", "CodeBleu_score": 43.71899456964874}, {"code": "def get_input(self, batch, k)\n\tx = batch[self.image_key]\n    if self.batch_resize_range is not None:\n        if self.batch_resize_range[0] == self.batch_resize_range[1]:\n            x = F.interpolate(x, size=self.batch_resize_range[0], mode=\\\"bicubic\\\", align_corners=False)\n        else:\n            x = F.interpolate(x, size=int(self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * self.current_epoch / self.trainer.max_epochs), mode=\\\"bicubic\\\", align_corners=False)\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 45.71045445899984}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t# Get the input data and predictions\n    x = self.get_input(batch, self.image_key)\n    x_hat, diff, _ = self(x, return_pred_indices=True)\n    # Compute the autoencoding loss\n    if optimizer_idx == 0:\n        loss = self.loss(x_hat, x, diff, self.monitor)\n        # Log the autoencoding loss\n        self.log(f\\\"train_loss_{optimizer_idx}\\\", loss, prog_bar=True, logger=True)\n    # Compute the discriminator loss\n    elif optimizer_idx == 1:\n        # Get the label for the discriminator\n        y = batch[\\\"label\\\"]\n        # Get the prediction for the discriminator\n        y_hat = self.discriminator(x_hat)\n        # Compute the discriminator loss\n        loss = self.loss(y_hat, y)\n        # Log the discriminator loss\n        self.log(f\\\"train_loss_{optimizer_idx}\\\", loss, prog_bar=True, logger=True)\n    return loss\n", "CodeBleu_score": 32.08047987914113}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tif optimizer_idx == 0:\n        x = self.get_input(batch, self.image_key)\n        x_hat, _, _ = self(x, return_pred_indices=True)\n        loss = self.loss(x, x_hat, self.monitor)\n        self.log_dict({f\\\"train_{self.monitor}_loss\\\": loss}, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return loss\n    elif optimizer_idx == 1:\n        x = self.get_input(batch, self.image_key)\n        x_hat, _, _ = self(x, return_pred_indices=True)\n        loss = self.loss(x, x_hat, self.monitor)\n        self.log_dict({f\\\"train_{self.monitor}_loss\\\": loss}, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return loss\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 42.77753916855179}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tif optimizer_idx == 0:\n        x = self.get_input(batch, self.image_key)\n        with self.ema_scope(\\\"AE\\\"):\n            pred, loss, _ = self.forward(x)\n            if self.monitor is not None:\n                self.log(f\\\"{self.monitor}_ae_loss\\\", loss, prog_bar=True)\n\n        return loss\n    elif optimizer_idx == 1:\n        x = self.get_input(batch, self.image_key)\n        with self.ema_scope(\\\"D\\\"):\n            pred, loss, _ = self.forward(x)\n            if self.monitor is not None:\n                self.log(f\\\"{self.monitor}_dis_loss\\\", loss, prog_bar=True)\n\n        return loss\n    else:\n        raise ValueError(f\\\"Unknown optimizer_idx: {optimizer_idx}\\\")\n\n\n\n", "CodeBleu_score": 34.73255182720173}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        factor = dimension\n    while factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        factor -= 1\n    raise ValueError(f\"{dimension} is not a multiple of any number less than {dimension}\")\n\n", "CodeBleu_score": 21.0729945256318}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor == -1:\n        factor = dimension\n    if factor > 0:\n        m = min(factor, dimension)\n        n = dimension // m\n        return m, n\n    if factor < 0:\n        m = 1\n        n = dimension\n        while m < n:\n            m += 1\n            n = dimension // m\n        return m, n\n\n", "CodeBleu_score": 33.1580207974738}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        factors = []\n        for i in range(1, dimension + 1):\n            if dimension % i == 0:\n                factors.append(i)\n        factors.sort()\n        if factor > 0:\n            for i in range(len(factors)):\n                if factors[i] >= factor:\n                    return factors[i], dimension // factors[i]\n        else:\n            return factors[-1], dimension // factors[-1]\n\n", "CodeBleu_score": 32.30818034287086}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tweight = orig_weight\n    if updown == 'up':\n        weight = weight.transpose(-1, -2)\n    if updown == 'down':\n        weight = weight.transpose(-2, -1)\n    weight = weight.to(self.device)\n    weight = weight.type(self.dtype)\n    weight = weight.detach().requires_grad_(True)\n    weight = weight.reshape(-1)\n    if self.scale is not None:\n        weight = weight * self.scale\n    if self.alpha is not None:\n        weight = weight * self.alpha\n    if self.dim is not None:\n        weight = weight / self.dim\n    weight = weight.detach().requires_grad_(True)\n    weight = weight.reshape(self.shape)\n    if updown == 'up':\n        weight = weight.transpose(-1, -2)\n    if updown == 'down':\n        weight = weight.transpose(-2, -1)\n    if self.dora_scale is not None:\n        weight = weight * self.dora_scale\n    weight = weight.detach().requires_grad_(True)\n    weight = weight.reshape(self.shape)\n    if updown == 'up':\n        weight = weight.transpose(-1, -2)\n    if updown == 'down':\n        weight = weight.transpose(-2, -1)\n    weight = weight.detach().requires_grad_(True)\n    weight = weight.reshape(-1)\n    if self.dora_norm_dims is not None:\n        weight = F.layer_norm(weight, self.dora_norm_dims, self.bias, self.scale)\n    weight = weight.detach().requires_grad_(True)\n    weight = weight.reshape(self.shape)\n    if updown == 'up':\n        weight = weight.transpose(-1, -2)\n    if updown == 'down':\n        weight = weight.transpose(-2, -1)\n    weight = weight.detach().requires_grad_(True)\n    weight =", "CodeBleu_score": 32.08436402498121}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tweight = updown.weight\n    if weight.device != orig_weight.device:\n        weight = weight.to(orig_weight.device)\n    if weight.dtype != orig_weight.dtype:\n        weight = weight.to(orig_weight.dtype)\n    weight = weight.detach().clone()\n    orig_weight = orig_weight.detach().clone()\n    weight = weight * updown.multiplier()\n    orig_weight = orig_weight * updown.multiplier()\n    weight = weight / weight.norm()\n    orig_weight = orig_weight / orig_weight.norm()\n    weight = weight * self.calc_scale()\n    orig_weight = orig_weight * self.calc_scale()\n    weight = weight - orig_weight\n    return weight\n\n", "CodeBleu_score": 30.362994747263972}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif orig_weight is None:\n        return None\n    if updown == \"down\":\n        return orig_weight / self.scale\n    if updown == \"up\":\n        return orig_weight * self.scale\n    return orig_weight\n\n", "CodeBleu_score": 8.735324816518874}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.02647780864708}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.02647780864708}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 25.02647780864708}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\torig_weight = orig_weight.to(self.weight.device)\n    return self.calc_updown_final(orig_weight)\n\n", "CodeBleu_score": 6.101785908036981}, {"code": "def calc_updown(self, orig_weight)\n\tif orig_weight is None:\n        return None\n\n    orig_weight = orig_weight.to(self.weight.device)\n\n    return self.calc_updown_final(orig_weight, self.weight, self.ex_bias)\n\n", "CodeBleu_score": 26.761245611138907}, {"code": "def calc_updown(self, orig_weight)\n\t# check if the current weight tensor is on the same device as the original weight tensor\n    if orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n        if self.ex_bias is not None:\n            self.ex_bias = self.ex_bias.to(orig_weight.device)\n    # check if the current bias is on the same device as the original weight tensor\n    if orig_weight.device != self.ex_bias.device if self.ex_bias is not None else None:\n        self.ex_bias = self.ex_bias.to(orig_weight.device) if self.ex_bias is not None else None\n    # call the finalize function\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 50.71234962847513}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\treturn [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.194787406674239}, {"code": "def trainables(self)\n\treturn [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.194787406674239}, {"code": "def trainables(self)\n\ttrainables = []\n    for module in self.modules():\n        if isinstance(module, torch.nn.Linear):\n            trainables.extend([module.weight, module.bias])\n        if isinstance(module, torch.nn.LayerNorm):\n            trainables.extend([module.weight, module.bias])\n    return trainables\n\n", "CodeBleu_score": 32.77585492853396}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tif subsection not in caches:\n        caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, min_file_size=256 * 1024)\n    return caches[subsection]\n\n", "CodeBleu_score": 55.725692195684395}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\treturn diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, min_file_size=256 * 1024)\n\n", "CodeBleu_score": 51.0073635520631}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\treturn diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, min_file_size=256 * 1024)\n\n", "CodeBleu_score": 51.47916098771129}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        cache_lock.acquire()\n        cache_obj = caches.get(subsection)\n        if cache_obj is None:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n        cache_lock.release()\n    return cache_obj\n\n", "CodeBleu_score": 46.50019755839806}, {"code": "def cache(subsection)\n\tglobal caches\n    global cache_lock\n\n    if subsection in caches:\n        return caches[subsection]\n\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n\n        # Check if cache directory exists and if cache filename is a valid file.\n        # If these conditions are not met, call convert_old_cached_data() to convert legacy cached data.\n        if not os.path.exists(cache_dir):\n            os.mkdir(cache_dir)\n        elif not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n\n        # Re-check the caches dictionary for the subsection.\n        # If the cache object still does not exist, create a new cache object using make_cache(subsection) and store it in the caches dictionary.\n        cache_obj = caches.get(subsection)\n        if cache_obj is None:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n\n        return cache_obj\n\n", "CodeBleu_score": 60.63757194790743}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        cache_lock.acquire()\n        cache_obj = caches.get(subsection)\n        if cache_obj is None:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n        cache_lock.release()\n    return cache_obj\n\n", "CodeBleu_score": 46.50019755839806}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        if mtime > cache_obj[title][0]:\n            del cache_obj[title]\n    if title not in cache_obj:\n        cache_obj[title] = (mtime, func())\n    dump_cache()\n    return cache_obj[title][1]\n\n", "CodeBleu_score": 17.695390521001205}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime = cache_obj[title]\n        if mtime > cached_mtime:\n            cache_obj.pop(title)\n            dump_cache()\n    if title not in cache_obj:\n        cache_obj[title] = mtime\n        dump_cache()\n        data = func()\n        cache_obj[title] = data\n        dump_cache()\n        return data\n    else:\n        return cache_obj[title]\n\n", "CodeBleu_score": 28.75786618763486}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        if mtime > cache_obj[title][\\\"mtime\\\"]:\n            del cache_obj[title]\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = {\n            \\\"mtime\\\": mtime,\n            \\\"data\\\": data,\n        }\n        dump_cache()\n    return cache_obj[title][\\\"data\\\"]\n\n", "CodeBleu_score": 23.28963968458745}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\tif exc_info:\n        record_exception()\n        traceback.print_exc()\n    print(\"***\", message, file=sys.stderr)\n    print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 43.11219280942336}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\tif exc_info:\n        record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in textwrap.indent(format_traceback(traceback.format_exc()), \"    \").split(\"", "CodeBleu_score": 48.99700886328209}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\tif exc_info:\n        record_exception()\n        print(\"***\", message, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n        print(traceback.format_exc(), file=sys.stderr)\n    else:\n        record_exception()\n        print(\"***\", message, file=sys.stderr)\n    print(\"***\", file=sys.stderr)\n\n", "CodeBleu_score": 47.30112023491071}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    if style.prompt:\n        prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    else:\n        prompt_extracted = True\n\n    if style.negative_prompt:\n        negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    else:\n        negative_prompt_extracted = True\n\n    if prompt_extracted and negative_prompt_extracted:\n        return True, prompt, negative_prompt\n\n    return False, prompt, negative_prompt\n\n", "CodeBleu_score": 61.185464828524516}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted and not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 67.36760897420169}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted and not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 61.10407972299466}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 63.43035982242717}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = [u.random_partial(n_frames) for u in utterances]\n    return [(u, p) for u, p in zip(utterances, partials)]\n\n", "CodeBleu_score": 66.94989238363577}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = [\n        (u, u.random_partial(n_frames)) for u in utterances\n    ]\n    return partials\n\n", "CodeBleu_score": 69.28562342459851}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set, which is not possible.\")\n    if increase_only:\n        change_in_dBFS = target_dBFS - safe_rms_dBFS(wav)\n        if change_in_dBFS <= 0:\n            return wav\n        return wav * (10 ** (change_in_dBFS / 20))\n    elif decrease_only:\n        change_in_dBFS = safe_rms_dBFS(wav) - target_dBFS\n        if change_in_dBFS <= 0:\n            return wav\n        return wav * (10 ** (change_in_dBFS / 20))\n    else:\n        change_in_dBFS = target_dBFS - safe_rms_dBFS(wav)\n        return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 39.70730976979239}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set to True. Only one of these may be set to True.\")\n    change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    if change_in_dBFS == 0:\n        return wav\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    if decrease_only and change_in_dBFS > 0:\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 44.529528159098874}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set to True.\")\n    change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    if change_in_dBFS == 0:\n        return wav\n    elif change_in_dBFS < 0 and increase_only:\n        return wav\n    elif change_in_dBFS > 0 and decrease_only:\n        return wav\n    else:\n        return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 46.343297545083395}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model\n    global _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.to(device).eval()\n    print(\"Loaded model {} trained to step {}\".format(weights_fpath, _model.step))\n\n", "CodeBleu_score": 46.55324685644069}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model, _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded speaker encoder model from {weights_fpath} (step {_model.step}).\")\n\n", "CodeBleu_score": 48.287128465289086}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model, _device\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _device = device\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f'Loaded speaker encoder model from {weights_fpath}, trained to step {_model.step}.')\n\n", "CodeBleu_score": 46.93662634397042}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\tif _model is None:\n        raise Exception(\"No model loaded. Please call encoder.embed_frames.load_model() before calling embed_frames().\")\n    # Convert frames_batch from a NumPy array to a PyTorch tensor\n    frames_batch_torch = torch.tensor(frames_batch, dtype=torch.float32).to(_device)\n    # Perform a forward pass through the model to compute the embeddings\n    embeddings_batch = _model(frames_batch_torch)\n    # Detach the result from the computation graph\n    embeddings_batch = embeddings_batch.detach()\n    # Move the result to the CPU\n    embeddings_batch = embeddings_batch.cpu()\n    # Convert the result back to a NumPy array\n    embeddings_batch = embeddings_batch.numpy()\n    return embeddings_batch\n\n", "CodeBleu_score": 46.626293865447636}, {"code": "def embed_frames_batch(frames_batch)\n\tif _model is None:\n        raise Exception(\"You must load the model using load_model() before calling embed_frames_batch().\")\n    if isinstance(frames_batch, np.ndarray):\n        frames_batch = torch.from_numpy(frames_batch).float()\n    frames_batch = frames_batch.to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 55.37955510579181}, {"code": "def embed_frames_batch(frames_batch)\n\tif _model is None:\n        raise Exception(\"Model is not loaded. Please call load_model() first.\")\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model.forward(frames_batch_tensor).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 72.13632345012459}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage < 1\n    assert partial_utterance_n_frames > 0\n    assert n_samples > 0\n    # Compute the number of frames and the frame step based on the sampling rate and overlap.\n    n_frames = int(np.ceil(n_samples / sampling_rate * (1 - overlap)))\n    frame_step = int(np.ceil(sampling_rate * overlap))\n    # Compute wav and mel slices using a loop, considering the steps required based on the frames and frame step.\n    wav_slices = []\n    mel_slices = []\n    for start_frame in range(0, n_frames, partial_utterance_n_frames - frame_step):\n        wav_slices.append(start_frame)\n        mel_slices.append(start_frame)\n    # Evaluate the need for extra padding by checking the coverage of the last slice.\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    last_slice_coverage = (last_wav_slice + partial_utterance_n_frames) / n_samples\n    if last_slice_coverage < min_pad_coverage and len(wav_slices) > 1:\n        # If the coverage is below the minimum pad coverage and there is more than one slice, it removes the last slice from both wav and mel slices.\n        wav_slices.pop()\n        mel_slices.pop()\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.22765576010083}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0 and 1\"\n    assert 0.0 <= overlap <= 1.0, \"overlap must be between 0 and 1\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be an even number\"\n    assert partial_utterance_n_frames >= 2, \"partial_utterance_n_frames must be at least 2\"\n    assert partial_utterance_n_frames <= 50, \"partial_utterance_n_frames must be at most 50\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be an even number\"\n    assert partial_utterance_n_frames >= 2, \"partial_utterance_n_frames must be at least 2\"\n    assert partial_utterance_n_frames <= 50, \"partial_utterance_n_frames must be at most 50\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be an even number\"\n    assert partial_utterance_n_frames >= 2, \"partial_utterance_n_frames must be at least 2\"\n    assert partial_utterance_n_frames <= 50, \"partial_utterance_n_frames must be at most 50\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be an even number\"\n    assert partial_utterance_n_frames >= 2, \"partial_utterance_n_frames must be at least 2\"\n    assert partial_utterance_n_frames <= 50, \"partial_utterance_n_frames must be at most 50\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be an even number\"\n    assert partial_utterance_n_frames >= 2, \"partial_utterance_n_frames must be at least", "CodeBleu_score": 9.01766023759504}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert partial_utterance_n_frames >= 0, \"partial_utterance_n_frames must be greater than or equal to 0\"\n    assert min_pad_coverage >= 0 and min_pad_coverage <= 1, \"min_pad_coverage must be between 0 and 1\"\n    assert overlap >= 0 and overlap <= 1, \"overlap must be between 0 and 1\"\n    assert n_samples > 0, \"n_samples must be greater than 0\"\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be greater than 0\"\n    # Compute the frame step and number of frames for the partial utterance\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    n_frames = int(np.ceil((n_samples - partial_utterance_n_frames) / frame_step)) + 1\n    # Compute the wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        wav_slices.append(i * frame_step)\n        mel_slices.append(i * (frame_step + 1))\n    # Compute the coverage of the last slice\n    coverage = (wav_slices[-1] + partial_utterance_n_frames) / n_samples\n    # If the coverage is below the minimum pad coverage and there is more than one slice, remove the last slice\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 37.42722252420824}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t# Get the number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Initialize the similarity matrix and mask matrix\n    similarity_matrix = torch.zeros(n_speakers, n_speakers).to(self.loss_device)\n    mask = torch.ones(n_speakers, n_speakers).to(self.loss_device)\n\n    # Compute the centroids\n    centroids_incl = torch.zeros(n_speakers, embeds.shape[2]).to(self.loss_device)\n    for i in range(n_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-5)\n\n    centroids_excl = torch.zeros(n_speakers, embeds.shape[2]).to(self.loss_device)\n    for i in range(n_speakers):\n        centroids_excl[i] = torch.mean(embeds[i, 1:], dim=0)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-5)\n\n    # Compute the similarity scores\n    similarity_scores_incl = torch.mm(embeds, centroids_incl.T)\n    similarity_scores_excl = torch.mm(embeds, centroids_excl.T)\n\n    # Compute the similarity matrix\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            if i == j:\n                mask[i, j] = 0\n            else:\n                similarity_matrix[i, j] = self.similarity_weight * torch.cosine_similarity(embeds[i], embed", "CodeBleu_score": 38.65844516176924}, {"code": "def similarity_matrix(self, embeds)\n\t# Determine the number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n    # Initialize similarity matrix\n    similarity_matrix = torch.zeros(n_speakers, n_speakers).to(self.loss_device)\n    # Initialize mask matrix\n    mask = torch.ones(n_speakers, n_speakers).to(self.loss_device)\n    # Compute centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_excl = torch.zeros(n_speakers, model_embedding_size).to(self.loss_device)\n    for i in range(n_speakers):\n        centroids_excl[i] = torch.mean(embeds[i:n_speakers, :], dim=0)\n    # Compute similarity scores\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            # Exclude self-similarity\n            if i == j:\n                mask[i, j] = 0\n            # Compute similarity scores\n            similarity_matrix[i, j] = self.similarity_score(embeds[i], centroids_incl[i], centroids_excl[i], centroids_excl[j])\n    # Scale and bias the similarity matrix\n    similarity_matrix = similarity_matrix * self.similarity_weight + self.similarity_bias\n    return similarity_matrix, mask\n\n", "CodeBleu_score": 29.960005778956916}, {"code": "def similarity_matrix(self, embeds)\n\t# Get the number of speakers and utterances per speaker from the shape of the input embeddings\n    n_speakers, n_utterances, n_dims = embeds.shape\n\n    # Initialize the similarity matrix and mask matrix\n    similarity_matrix = torch.zeros((n_speakers, n_speakers), device=self.loss_device)\n    mask = torch.ones((n_speakers, n_speakers), device=self.loss_device)\n\n    # Compute the centroids for each speaker\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_excl = torch.zeros_like(centroids_incl)\n    for i in range(n_speakers):\n        # Exclude the current speaker from the mean calculation\n        centroids_excl[i] = torch.mean(embeds[i], dim=0)\n        # Average the remaining embeddings\n        centroids_excl[i] = torch.mean(embeds[i] != i, dim=0)\n    # Normalize the centroids\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-5)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-5)\n\n    # Compute the similarity scores for each speaker\n    for i in range(n_speakers):\n        # Compute the similarity scores using the respective centroids\n        similarity_matrix[i, i] = (\n            (centroids_incl[i] * centroids_incl[i]).sum()\n            + (centroids_excl[i] * centroids_excl[i]).sum()\n        )\n        for j in range(n_speakers):\n            if i != j:\n                similarity_matrix[i, j] = (\n                    (centroids_incl[i] * centroids_incl[", "CodeBleu_score": 37.71594031772404}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances", "CodeBleu_score": 34.83811323532684}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utter", "CodeBleu_score": 34.83811323532684}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_", "CodeBleu_score": 34.83811323532684}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t# If visdom is disabled TODO: use a better paradigm for that\n    if self.disabled:\n        return\n\n    # Get the parameters\n    params = self.get_params()\n\n    # Log the parameters\n    if params is not None:\n        # Create the string\n        string = \\\"\\\"\\\"\n        Parameters\n        ----------\n        \\\"\\\"\\\"\n        for name, value in params.items():\n            string += \\\"\\\"\\\"\n        %s: %s\n\n        \\\"\\\"\\\" % (name, value)\n        string += \\\"\\\"\\\"\n\n        \\\"\\\"\\\"\n        # Log the string\n        self.log_string(string)\n\n", "CodeBleu_score": 12.889088963382287}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n    # Get the parameter configurations\n    params = self.get_params()\n    # Create the parameter string\n    param_string = \\\"\\\"\\\"\n    # Model parameters\n    %s\n    # Data parameters\n    %s\n    \\\"\\\"\\\" % (params[0], params[1])\n    # Log the parameter string\n    self.vis.text(param_string, win=\\\"Parameters\\\")\n\n", "CodeBleu_score": 6.43826988391341}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n    params = self.get_params()\n    param_string = \\\"\\\"\\\"\n    Parameters:\n    \\\"\\\"\\\"\n    for name, value in params.items():\n        param_string += \\\"\\\"\\\"\n    %s: %s\n    \\\"\\\"\\\" % (name, value)\n    self.vis.text(param_string, win=self.implementation_win)\n\n", "CodeBleu_score": 10.730115598247734}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    from encoder import params_data\n    from encoder import params_model\n    from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset\n    from encoder.data_objects.speaker import Speaker\n    import numpy as np\n    import umap\n    import visdom\n    import webbrowser\n    from datetime import datetime\n    from time import perf_counter as timer\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib.colors as colors\n    import matplotlib.patches as mpatches\n    import matplotlib.lines as mlines\n    import matplotlib.ticker as ticker\n    import matplotlib.animation as animation\n    from matplotlib.collections import LineCollection\n    import matplotlib.ticker as ticker\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib.colors as colors\n    import matplotlib.patches as mpatches\n    import matplotlib.lines as mlines\n    import matplotlib.ticker as ticker\n    import matplotlib.animation as animation\n    from matplotlib.collections import LineCollection\n    import matplotlib.ticker as ticker\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib.colors as colors\n    import matplotlib.patches as mpatches\n    import matplotlib.lines as mlines\n    import matplotlib.ticker as ticker\n    import matplotlib.animation as animation\n    from matplotlib.collections import LineCollection\n    import matplotlib.ticker as ticker\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib.colors as colors\n    import matplotlib.patches as mpatches\n    import matplotlib.lines as mlines\n    import matplotlib.ticker as ticker\n    import matplotlib.animation as animation\n    from matplotlib.collections import LineCollection\n    import matplotlib.ticker as ticker\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib.colors as colors\n    import matplotlib.patches as mpatches\n    import matplotlib.lines as mlines\n    import matplotlib.ticker as ticker\n    import matplotlib.animation as animation\n    from matplotlib.collections import LineCollection\n    import matplotlib.ticker", "CodeBleu_score": 13.957865559664254}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    # UMAP dimensionality reduction\n    embeds = embeds.cpu().numpy()\n    embeds = umap.UMAP(n_components=2, random_state=42).fit_transform(embeds)\n    # Plotting\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colormap[np.arange(len(embeds)) % len(colormap)])\n    # Assign labels\n    labels = []\n    for i, speakers in enumerate(utterances_per_speaker):\n        labels.append(speakers[0].speaker_id)\n        for j in range(1, len(speakers)):\n            labels.append(None)\n    # Assign labels\n    labels = np.array(labels)\n    labels = labels[np.argsort(labels)]\n    # Assign colors\n    cmap = ListedColormap(colormap)\n    bounds = np.arange(len(labels))\n    norm = BoundaryNorm(bounds, len(bounds))\n    # Assign labels\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=labels, cmap=cmap, norm=norm)\n    # Assign labels\n    plt.legend(handles=np.arange(max_speakers))\n    # Assign labels\n    plt.title(\"Step %d\" % step)\n    # Assign labels\n    plt.axis(\"off\")\n    # Assign labels\n    plt.tight_layout()\n    # Assign labels\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath, bbox_inches=\"tight\")\n        plt.close()\n\n", "CodeBleu_score": 43.774150458965295}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    # UMAP reduces the dimensionality of the embeddings\n    embeds = embeds.cpu().numpy()\n    embeds = umap.UMAP(n_neighbors=2, min_dist=0.1, random_state=42).fit_transform(embeds)\n    # Flatten the embeddings, limit the number of speakers, and assign colors\n    embeds = embeds[:, :2]\n    embeds = np.vstack([embeds[speaker_id] for speaker_id in range(min(len(embeds), max_speakers))])\n    colors = colormap[np.mod(np.arange(len(embeds)), len(colormap))].tolist()\n    # Plot and display the visualization\n    self.projection_win = self.vis.scatter(\n        X=embeds,\n        win=self.projection_win,\n        opts=dict(\n            legend=[\\\"Speaker %d\\\" % speaker_id for speaker_id in range(len(embeds))],\n            markersize=10,\n            markercolor=colors,\n            xlabel=\\\"UMAP 1\\\",\n            ylabel=\\\"UMAP 2\\\",\n            title=\\\"UMAP projection (step %d)\\\" % step,\n        )\n    )\n    if out_fpath is not None:\n        self.vis.save([self.projection_win], out_fpath)\n\n", "CodeBleu_score": 43.5883880183059}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, _ = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    embedding_dim = speaker_embedding.size(1)\n    if embedding_dim == 1:\n        embedding_dim = -1\n    elif embedding_dim == 2:\n        embedding_dim = -2\n    else:\n        raise ValueError(f\"Speaker embedding should have dimension 1 or 2, but found {embedding_dim}.\")\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(1, seq_len, 1)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.view(batch_size, seq_len, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=embedding_dim)\n    return x\n\n", "CodeBleu_score": 32.216739608519504}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tx_shape = x.shape\n    speaker_embedding_shape = speaker_embedding.shape\n    if len(x_shape) == 3:\n        batch_size, sequence_length, embedding_size = x_shape\n        speaker_dim = 1\n    elif len(x_shape) == 2:\n        sequence_length, embedding_size = x_shape\n        speaker_dim = 0\n    else:\n        raise ValueError(\"Unsupported input shape: %s\" % str(x_shape))\n    if len(speaker_embedding_shape) == 1:\n        speaker_embedding_size = speaker_embedding_shape[0]\n    elif len(speaker_embedding_shape) == 2:\n        speaker_embedding_size = speaker_embedding_shape[1]\n    else:\n        raise ValueError(\"Unsupported speaker embedding shape: %s\" % str(speaker_embedding_shape))\n    if speaker_embedding_size != embedding_size:\n        raise ValueError(\"Speaker embedding size (%d) does not match embedding size (%d)\" % (speaker_embedding_size, embedding_size))\n    speaker_embedding = speaker_embedding.unsqueeze(speaker_dim)\n    speaker_embedding = speaker_embedding.repeat(1, sequence_length, 1)\n    speaker_embedding = speaker_embedding.reshape(x_shape)\n    speaker_embedding = speaker_embedding.transpose(speaker_dim, -1)\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n\n", "CodeBleu_score": 31.41476655342195}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t(batch, seq_len, emb_dim) = x.size()\n    (batch, emb_dim) = speaker_embedding.size()\n    x = x.transpose(1, 2)\n    x = x.contiguous().view(batch, emb_dim, seq_len)\n    x = torch.cat((x, speaker_embedding.repeat(1, 1, seq_len).transpose(1, 2)), dim=1)\n    return x\n\n", "CodeBleu_score": 20.294952351210767}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token:\n\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 78.6126294722131}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n    sequence.append(_symbols_to_sequence(_clean_text(\"eos\", cleaner_names)))\n    return sequence\n\n", "CodeBleu_score": 73.04908354373376}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    sequence.append(_symbols_to_sequence(_clean_text(text, cleaner_names)))\n    sequence.append([_symbol_to_id[\"~\"]])\n    return sequence\n\n\n\n\n", "CodeBleu_score": 73.90491157287883}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\tresult = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{\" + s[1:] + \"}\"\n            result.append(s)\n    return \"\".join(result)\n\n", "CodeBleu_score": 59.0313185630588}, {"code": "def sequence_to_text(sequence)\n\tresult = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{\" + s[1:] + \"}\"\n            result.append(s)\n    return \"\".join(result)\n\n", "CodeBleu_score": 59.0313185630588}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{\" + s[1:] + \"}\"\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 68.38075744698607}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + [i for i in range(attn.shape[0])], fontdict={'fontsize': 12})\n    ax.set_yticklabels([''] + [i for i in range(attn.shape[1])], fontdict={'fontsize': 12})\n    fig.tight_layout()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 47.432876510352045}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    import matplotlib.ticker as ticker\n    import numpy as np\n    import seaborn as sns\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(attn, cmap=\"Blues\")\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    for (i, j), z in np.ndenumerate(attn):\n        ax.text(j, i, round(z, 2), ha=\"center\", va=\"center\")\n    fig.tight_layout()\n    sns.set(font_scale=1.5)\n    fig.savefig(path, dpi=200)\n    plt.close()\n\n", "CodeBleu_score": 46.333069389230104}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    plt.xticks(range(attn.shape[1]), range(attn.shape[1]), fontsize=12, rotation=90)\n    plt.yticks(range(attn.shape[0]), range(attn.shape[0]), fontsize=12)\n    fig.tight_layout()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 44.30997816327539}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 1, 1)\n    plt.xlabel(\"Time\", fontsize=18, color=\"black\")\n    plt.ylabel(\"Frequency\", fontsize=18, color=\"black\")\n    plt.xticks(fontsize=16, color=\"black\")\n    plt.yticks(fontsize=16, color=\"black\")\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 33.51802813421814}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(array.T, interpolation='nearest', aspect='auto')\n    plt.xlabel('Time', fontsize=16, color='#222222')\n    plt.ylabel('Frequency', fontsize=16, color='#222222')\n    plt.yticks(fontsize=14, color='#222222')\n    plt.xticks(fontsize=14, color='#222222')\n    plt.show()\n\n", "CodeBleu_score": 25.756762098245822}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"Time\", fontsize=18, color=\"black\")\n    ax.set_ylabel(\"Frequency\", fontsize=18, color=\"black\")\n    ax.tick_params(axis='both', which='major', labelsize=18, color='black')\n    ax.tick_params(axis='both', which='minor', labelsize=18, color='black')\n    plt.plot(array)\n    plt.savefig(\"plot.png\", bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 37.35872330271858}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 29.144819895472864}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 29.144819895472864}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 29.144819895472864}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\twith torch.enable_grad():\n        taus = taus.clone().detach().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        grad_log_alpha = torch.autograd.grad(log_alpha, taus, create_graph=True)[0]\n        integrand = torch.sum(grad_log_alpha * cal_poly(taus, 0, taus), dim=0)\n    return integrand\n\n", "CodeBleu_score": 50.32400943406125}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus = taus.clone().detach()\n    taus.requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    alpha_log = alpha.log()\n    alpha_log.backward(torch.ones_like(taus), retain_graph=True)\n    integrand = taus.grad * alpha\n    return integrand\n\n", "CodeBleu_score": 20.439067506089682}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\twith torch.enable_grad():\n        taus_clone = taus.clone().detach().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n        log_alpha = alpha.log()\n        grad_log_alpha = torch.autograd.grad(outputs=log_alpha, inputs=taus_clone, create_graph=True)[0]\n        integrand = torch.sum(grad_log_alpha * cal_poly(prev_t=taus, j=0, taus=taus), dim=0)\n    return integrand\n\n", "CodeBleu_score": 47.45740202317326}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.95002017820207}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.95002017820207}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.95002017820207}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\tif x.dim() >= target_dims:\n        raise ValueError(f\"Target dimensions {target_dims} should be greater than current dimensions {x.dim()}.\")\n    diff = target_dims - x.dim()\n    if diff == 1:\n        return x.unsqueeze(1)\n    elif diff == 2:\n        return x.unsqueeze(1).unsqueeze(2)\n    elif diff == 3:\n        return x.unsqueeze(1).unsqueeze(2).unsqueeze(3)\n    elif diff == 4:\n        return x.unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)\n    else:\n        raise ValueError(f\"Target dimensions {target_dims} is too large.\")\n\n", "CodeBleu_score": 24.881567531846464}, {"code": "def append_dims(x, target_dims)\n\tif x.dim() >= target_dims:\n        raise ValueError(\"The target dimensions must be greater than the current dimensions.\")\n    num_dims_to_append = target_dims - x.dim()\n    x = x.unsqueeze(-1)\n    for _ in range(num_dims_to_append):\n        x = x.unsqueeze(-1)\n    return x.detach() if x.is_mps else x\n\n", "CodeBleu_score": 35.8399311126116}, {"code": "def append_dims(x, target_dims)\n\tif x.dim() >= target_dims:\n        raise ValueError(f\"The target dimensions {target_dims} are fewer than the current dimensions {x.dim()}.\")\n    diff = target_dims - x.dim()\n    for _ in range(diff):\n        x = x.unsqueeze(-1)\n    x = x.detach() if x.is_mps else x\n    return x\n\n", "CodeBleu_score": 32.48797548777517}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\tpath.parent.mkdir(exist_ok=True, parents=True)\n    if path.exists() and (digest is None or hash_file(path) == digest):\n        return path\n    with urllib.request.urlopen(url) as response, path.open('wb') as out_file:\n        shutil.copyfileobj(response, out_file)\n    if digest is not None and hash_file(path) != digest:\n        raise RuntimeError(f'File {path} has wrong hash. Expected {digest}, got {hash_file(path)}.')\n    return path\n\n", "CodeBleu_score": 40.899674417618925}, {"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if path.exists() and (digest is None or hashlib.sha256(path.read_bytes()).hexdigest() == digest):\n        return path\n    with urllib.request.urlopen(url) as response, path.open('wb') as out_file:\n        shutil.copyfileobj(response, out_file)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest() != digest:\n            raise RuntimeError(f'failed to verify hash of downloaded file {path}')\n    return path\n\n", "CodeBleu_score": 45.725221091007754}, {"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            if not verify_digest(path, digest):\n                raise RuntimeError(f'File exists but does not have expected SHA-256 hash: {path}')\n        return path\n    with urllib.request.urlopen(url) as response, path.open('wb') as f:\n        shutil.copyfileobj(response, f)\n    if digest is not None:\n        if not verify_digest(path, digest):\n            raise RuntimeError(f'File exists but does not have expected SHA-256 hash: {path}')\n    return path\n\n", "CodeBleu_score": 47.04577746849476}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\tmode = mode if isinstance(mode, bool) else True\n    training_modes = [module.training for module in model.modules()]\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), training_modes):\n            module.train(mode)\n\n", "CodeBleu_score": 41.303710726185}, {"code": "def train_mode(model, mode=True)\n\twas_training = model.training\n    model.train(mode=mode)\n    try:\n        yield\n    finally:\n        model.train(mode=was_training)\n\n", "CodeBleu_score": 20.92456183491921}, {"code": "def train_mode(model, mode=True)\n\ttraining_modes = [module.training for module in model.modules()]\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), training_modes):\n            module.train(mode)\n\n", "CodeBleu_score": 43.22240148883828}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\tfor param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.copy_(param * decay + averaged_param * (1 - decay))\n    for key, value in model.state_dict().items():\n        if key in averaged_model.state_dict():\n            averaged_model.state_dict()[key].copy_(value)\n\n", "CodeBleu_score": 27.748708567829084}, {"code": "def ema_update(model, averaged_model, decay)\n\tfor param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(1 - decay, param.data)\n    for key, value in model.state_dict().items():\n        if key in averaged_model.state_dict():\n            averaged_model.state_dict()[key].copy_(value)\n\n", "CodeBleu_score": 26.737228830854942}, {"code": "def ema_update(model, averaged_model, decay)\n\tparams = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    for name in params:\n        averaged_params[name].data.mul_(decay).add_(params[name].data, alpha=1 - decay)\n    averaged_model.load_state_dict(averaged_params)\n    for name in averaged_model.buffers():\n        if name in model.buffers():\n            averaged_model.__setattr__(name, model.__getattr__(name))\n\n", "CodeBleu_score": 38.703180578634125}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation type not recognized.\")\n\n", "CodeBleu_score": 38.76769752834476}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Invalid activation type.\")\n\n", "CodeBleu_score": 38.709650973221265}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Invalid activation type: {}\".format(activation))\n\n", "CodeBleu_score": 40.03283200140622}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\tif dim != -1:\n        idx = idx.unsqueeze(dim)\n        idx = idx.transpose(dim, -1)\n    return self.codebook(idx)\n\n", "CodeBleu_score": 55.469345922511636}, {"code": "def idx2vq(self, idx, dim=-1)\n\treturn self.codebook(idx.unsqueeze(-1)).squeeze(dim)\n\n", "CodeBleu_score": 16.575015420874607}, {"code": "def idx2vq(self, idx, dim=-1)\n\treturn self.codebook(idx.long()).permute(idx.size(0), idx.size(1), *[f'dim_{i}' for i in range(idx.dim() - 2)], 'dim_-1', 'dim_-2')\n\n", "CodeBleu_score": 30.652594229845132}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self\n    original_weights = {}\n    for name, param in context.named_parameters():\n        if param.requires_grad:\n            original_weights[name] = param.data.clone()\n    try:\n        yield\n    finally:\n        for name, param in context.named_parameters():\n            if param.requires_grad:\n                param.data.copy_(original_weights[name])\n        if context is self:\n            logpy.info(f\"Restored original parameters for {self}\")\n\n", "CodeBleu_score": 33.538394463723456}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        self.model_ema.apply_ema()\n        if context is not None:\n            logpy.info(f\"Applying EMA weights for {context}.\")\n    try:\n        yield\n    finally:\n        if self.use_ema:\n            self.model_ema.restore_params()\n            if context is not None:\n                logpy.info(f\"Restoring original weights for {context}.\")\n\n", "CodeBleu_score": 53.210893874091084}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        with torch.no_grad():\n            if context is not None:\n                logpy.info(f\"Switching to EMA weights for {context}.\")\n            self.model_ema.apply_ema()\n        yield\n        with torch.no_grad():\n            if context is not None:\n                logpy.info(f\"Switching to original weights for {context}.\")\n            self.model_ema.restore_original_weights()\n    else:\n        yield\n\n", "CodeBleu_score": 54.10528264484048}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.400594082726926}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    timestep_range = torch.arange(timesteps.shape[-1], dtype=torch.float32)\n    dim_range = torch.arange(embedding_dim, dtype=torch.float32)\n    dim_range = dim_range.unsqueeze(0)\n    dim_range = dim_range.expand(timesteps.shape[-1], embedding_dim)\n    dim_range = torch.pow(10000.0, -dim_range / half_dim)\n    dim_range = dim_range.float()\n    # Create the sinusoidal embedding.\n    timestep_range = timestep_range.unsqueeze(1)\n    sin_inp = torch.sin(timestep_range * dim_range)\n    cos_inp = torch.cos(timestep_range * dim_range)\n    emb = torch.cat([sin_inp, cos_inp], dim=1)\n    if embedding_dim % 2 == 1:\n        # Zero pad the final dimension if the embedding dimension is odd.\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, dtype=emb.dtype, device=emb.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 50.40984180102638}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, \"timestep embedding should be a one-dimensional tensor\"\n    half_dim = embedding_dim // 2\n    timestep_range = torch.arange(timesteps.shape[0], dtype=torch.float32)\n    dim_range = torch.arange(half_dim, dtype=torch.float32)\n    dim_range = torch.pow(10000.0, dim_range / embedding_dim)\n    dim_range = dim_range.view(1, -1)\n    log_timestep_range = torch.log(timestep_range.view(-1, 1))\n    log_dim_range = torch.log(dim_range)\n    log_dim_range = torch.pow(log_timestep_range, log_dim_range)\n    log_dim_range = log_dim_range.view(-1, 1)\n    log_dim_range = log_dim_range.repeat(1, half_dim)\n    sin_log_dim_range = torch.sin(log_dim_range)\n    cos_log_dim_range = torch.cos(log_dim_range)\n    sin_log_timestep_range = torch.sin(log_timestep_range)\n    cos_log_timestep_range = torch.cos(log_timestep_range)\n    dim_range_mat = torch.cat((sin_log_dim_range, cos_log_dim_range), 1)\n    timestep_range_mat = torch.cat((sin_log_timestep_range, cos_log_timestep_range), 1)\n    timestep_range_mat = timestep_range_mat.view(1, -1)\n    timestep_embedding = torch.matmul(timestep_range_mat, dim_range_mat)\n    if embedding_dim % 2 == 1:\n        timestep_embedding = torch.cat((timestep_embedding, torch.zeros(1", "CodeBleu_score": 39.1128916185859}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, \"timestep input should be one-dimensional\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = xformers.ops.combine_dims(timesteps.float() @ emb.unsqueeze(0))\n    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, device=emb.device)], dim=-1)\n    return emb\n\n", "CodeBleu_score": 65.87715099422368}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t# reshape indices to match embedding dim\n    indices = indices.view(-1, 1)\n    # create one-hot encoding\n    one_hot = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    one_hot.scatter_(1, indices, 1)\n    # compute quantized vectors\n    quantized = torch.matmul(one_hot, self.embedding.weight).view(shape)\n    return quantized\n\n", "CodeBleu_score": 39.68493671770888}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# reshape indices to match the shape of the embedding weights\n    indices = indices.view(-1, 1)\n\n    # create a one-hot encoding of the indices and multiply it with the embedding weights to get the quantized vectors\n    quantized_vectors = torch.matmul(indices, self.embedding.weight.t())\n\n    # reshape back to match the original input shape\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(shape)\n\n    return quantized_vectors\n\n", "CodeBleu_score": 34.210822432243674}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# reshape indices to match the shape of the embedding weights\n    shape_indices = (1,) * (len(shape) - 1) + (self.n_e,)\n    indices = indices.view(*shape_indices)\n    # create one-hot encoding of the indices and multiply with the embedding weights\n    embeddings = torch.zeros(indices.shape + self.embedding.weight.shape, device=indices.device).scatter_(1, indices, 1)\n    quantized = torch.matmul(embeddings, self.embedding.weight.T)\n    # reshape back to match the original input shape\n    if shape is not None:\n        quantized = quantized.permute(0, 2, 3, 1).contiguous()\n        quantized = quantized.view(*shape)\n    return quantized\n\n", "CodeBleu_score": 39.84396000142892}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.47591846139633}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.47591846139633}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 23.47591846139633}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\tshift = np.array(shift)\n    shift = np.random.uniform(-shift, shift)\n    img = img + shift\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 50.104640837729086}, {"code": "def color_jitter(img, shift)\n\timg = img.copy()\n    img = img.astype(np.float32) / 255.\n    img = img.transpose(2, 0, 1)\n    img = torch.from_numpy(img)\n    img = img.float()\n    if shift == 0:\n        return img\n    else:\n        shift = torch.FloatTensor(shift)\n        shift = shift.view(-1, 1, 1)\n        img = img + shift\n        img = torch.clamp(img, 0, 1)\n        img = img.numpy()\n        img = img.transpose(1, 2, 0)\n        img = (img * 255).astype(np.uint8)\n        return img\n\n", "CodeBleu_score": 42.29568692772594}, {"code": "def color_jitter(img, shift)\n\tif not isinstance(img, torch.Tensor):\n        raise TypeError(\"img should be Tensor Image. Got {}\".format(type(img)))\n\n    if not isinstance(shift, (list, tuple)):\n        raise TypeError(\"shift should be a sequence. Got {}\".format(type(shift)))\n\n    if len(shift) != 3:\n        raise ValueError(\"shift should be a sequence of length 3. Got {}\".format(len(shift)))\n\n    if img.size(0) == 1:\n        raise ValueError(\"img should have more than one channel. Got {}\".format(img.size(0)))\n\n    shift = torch.tensor(shift)\n    shift = shift.to(img)\n    shift = shift.type(img.type())\n\n    img_jittered = img + shift\n    img_jittered = torch.clamp(img_jittered, min=0, max=1)\n\n    return img_jittered\n\n", "CodeBleu_score": 32.15721146371256}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\tsample_size = min(sample_size, len(self._exps))\n    if sample_size == 0:\n        return []\n    batch = random.sample(self._exps, sample_size)\n    self._exps = [exp for exp in self._exps if exp not in batch]\n    return batch\n\n", "CodeBleu_score": 45.70776017768097}, {"code": "def next_batch(self, sample_size=1)\n\tsample_size = min(sample_size, len(self.experiences))\n    sample = random.sample(self.experiences, sample_size)\n    self.experiences = [e for e in self.experiences if e not in sample]\n    return sample\n\n", "CodeBleu_score": 28.90899938039091}, {"code": "def next_batch(self, sample_size=1)\n\tbatch = []\n    sample_size = min(sample_size, len(self.exps))\n    for _ in range(sample_size):\n        idx = random.randint(0, len(self.exps) - 1)\n        batch.append(self.exps.pop(idx))\n    return batch\n\n\n\n", "CodeBleu_score": 35.846890552403515}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t# TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested\n    # TODO: this function is not tested", "CodeBleu_score": 0.36231884057971014}, {"code": "def find_estimated_top_configs(self)\n\tlogger.info(f\"Finding estimated top configurations\")\n    self.train_ct += 1\n\n    # Collect all numerical values from all_configs\n    all_configs = []\n    for config in self.all_configs:\n        config_dict = {}\n        for key, value in config.items():\n            if isinstance(value, numbers.Number):\n                config_dict[key] = value\n        all_configs.append(config_dict)\n\n    # Convert all_configs to a numpy array\n    all_configs = np.array(all_configs)\n\n    # Get the predicted values from the cost model\n    predicted_values = self.cost_model.predict(all_configs)\n\n    # Sort the predicted values and return the indices of the top configurations\n    sorted_indices = np.argsort(predicted_values)\n    return sorted_indices\n\n", "CodeBleu_score": 38.57201350668088}, {"code": "def find_estimated_top_configs(self)\n\t# TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost model to estimate the top configurations\n    # TODO: use the cost", "CodeBleu_score": 0.36231884057971014}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for key, value in d.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.abc.MutableMapping):\n            items.extend(flatten(value, new_key, sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n\n", "CodeBleu_score": 67.09492725127706}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t# TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to use the new reservation system\n    # TODO: update this to", "CodeBleu_score": 1.8529411764705885}, {"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp[\"exp_id\"]\n    exp_name = exp[\"name\"]\n    exp_path = exp[\"exp_path\"]\n    exp_args = exp[\"args\"]\n    exp_ds_config = exp[\"ds_config\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_script = exp[\"script\"]\n    exp_script_args = exp[\"script_args\"]\n    exp_script_env = exp[\"script_env\"]\n    exp_script_cwd = exp[\"script_cwd\"]\n    exp_script_timeout = exp[\"script_timeout\"]\n    exp_script_env_file = exp[\"script_env_file\"]\n\n    # set master port\n    exp_ds_config[\"master_port\"] = self.get_master_port()\n\n    # set result directory\n    exp_ds_config[\"result_dir\"] = exp_result_dir\n\n    # update user arguments\n    for k, v in self.arg_mappings.items():\n        exp_args[k] = v\n\n    # set environment variables\n    exp_script_env[\"MASTER_PORT\"] = str(exp_ds_config[\"master_port\"])\n    exp_script_env[\"WORLD_SIZE\"] = str(exp_ds_config[\"world_size\"])\n    exp_script_env[\"RANK\"] = str(exp_ds_config[\"rank\"])\n    exp_script_env[\"LOCAL_RANK\"] = str(exp_ds_config[\"local_rank\"])\n    exp_script_env[\"NODE_RANK\"] = str(exp_ds_config[\"node_rank\"])\n    exp_script_env[\"MASTER_ADDR\"] = exp_ds_config[\"master_addr\"]\n    exp_script_env[\"NCCL_SOCKET_IFNAME\"] = exp_ds_config[\"nccl_socket_ifname\"]\n    exp_script_env[\"NCCL_IB_DISABLE\"] = str(exp_ds_config[\"nccl_ib_disable\"])\n    exp_script_env[\"NCCL_IB_GPU_TRANSPORT\"] = exp_ds_config[\"nccl_ib", "CodeBleu_score": 15.178004383137662}, {"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp[\"exp_id\"]\n    exp[\"master_port\"] = self.nodes[0].get_next_port()\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp[\"name\"])\n    for k, v in self.arg_mappings.items():\n        exp[k] = v\n    if \"ds_config\" in exp:\n        if AUTOTUNING in exp[\"ds_config\"]:\n            metric_file = os.path.join(exp[\"result_dir\"], \"metrics.json\")\n            exp[\"ds_config\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n    stderr_file = os.path.join(exp[\"result_dir\"], \"stderr.log\")\n    model_info_file = os.path.join(exp[\"result_dir\"], \"model_info.json\")\n    metric_file = os.path.join(exp[\"result_dir\"], \"metrics.json\")\n    # skip existing experiments (except for the ones that were interrupted)\n    if os.path.exists(exp[\"result_dir\"]) and os.path.exists(stderr_file):\n        if not was_interruptted(stderr_file):\n            err = search_error(stderr_file)\n            self.finished_experiments[exp_id] = (exp, err)\n            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                logger.info(f\"Skipping exp {exp['name']} whose result already exists\")\n                return\n    self.running_experiments[exp_id] = (exp, time.time())\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n", "CodeBleu_score": 35.198822465065604}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 19.084247617080297}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 19.084247617080297}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 19.084247617080297}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.985856982637525}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.985856982637525}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.985856982637525}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\tsystem_info = {\n        \"OS\": platform.system(),\n        \"Architecture\": platform.architecture()[0],\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python\": sys.version\n    }\n    return system_info\n\n", "CodeBleu_score": 53.40425669334279}, {"code": "def system_info()\n\tsystem_info = {}\n    system_info[\"OS\"] = platform.system()\n    system_info[\"Architecture\"] = platform.architecture()[0]\n    system_info[\"Version\"] = platform.version()\n    system_info[\"Processor\"] = platform.processor()\n    system_info[\"Python Version\"] = platform.python_version()\n    system_info[\"Numpy Version\"] = numpy.__version__\n    system_info[\"PyTorch Version\"] = torch.__version__\n    return system_info\n\n", "CodeBleu_score": 37.7068200397677}, {"code": "def system_info()\n\tsystem_info = {}\n    system_info[\"operating_system\"] = platform.system()\n    system_info[\"architecture\"] = platform.architecture()[0]\n    system_info[\"version\"] = platform.version()\n    system_info[\"processor\"] = platform.processor()\n    system_info[\"python_version\"] = platform.python_version()\n    system_info[\"numpy_version\"] = numpy.__version__\n    system_info[\"torch_version\"] = torch.__version__\n    system_info[\"cuda_version\"] = torch.version.cuda\n    return system_info\n\n", "CodeBleu_score": 36.60435964449902}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\tcuda_info = {}\n    cuda_info['GPUs'] = []\n    cuda_info['CUDA'] = torch.cuda.is_available()\n    cuda_info['CUDA version'] = torch.version.cuda\n    cuda_info['GPUs'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    return cuda_info\n\n", "CodeBleu_score": 51.36688555702016}, {"code": "def cuda_info()\n\treturn {\n        \"GPU devices\": torch.cuda.get_device_name(0),\n        \"CUDA available\": torch.cuda.is_available(),\n        \"CUDA version\": torch.version.cuda,\n    }\n\n", "CodeBleu_score": 40.638515822774075}, {"code": "def cuda_info()\n\timport torch\n    return {\n        \"CUDA\": torch.cuda.is_available(),\n        \"CUDA version\": torch.version.cuda,\n        \"GPUs\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    }\n\n", "CodeBleu_score": 66.78095468231793}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Get the encoder class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n\n    # Initialize the accuracy counters\n    class_accuracy_counters = {}\n    total_samples = 0\n\n    # Iterate over the dataset items\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item[0]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key is not None and class_id_to_class_name is not None:\n            class_label = encoder_manager.predict_class_label(embedding)\n        else:\n            class_label = None\n\n        # Record the accuracy for each class\n        if class_label is not None:\n            if class_label in class_accuracy_counters:\n                class_accuracy_counters[class_label] += 1\n            else:\n                class_accuracy_counters[class_label] = 1\n\n        # Increment the total number of samples\n        total_samples += 1\n\n    # Print the accuracy for each class\n    for class_label, class_accuracy_counter in class_accuracy_counters.items():\n        class_name = class_id_to_class_name[class_label] if class_id_to_class_name is not None else class_label\n        print(\"{}: {:.2f}% ({}/{})\".format(class_name, 100.0 * class_accuracy_counter / total_samples, class_accuracy_counter, total_samples))\n\n    # Print the average accuracy across all classes\n    print(\"Average: {:.2f}% ({}/{})\".format(100.0 * sum(", "CodeBleu_score": 32.70927758910005}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Get the encoder configuration\n    encoder_config = encoder_manager.config\n    # Get the class name key and optional class ID to class name mapping\n    class_name_key = encoder_config['class_name_key']\n    class_id_to_class_name = encoder_config.get('class_id_to_class_name', None)\n    # Initialize the accuracy counters\n    total_num_samples = 0\n    total_num_correct = 0\n    # Initialize the class counters\n    class_counters = {}\n    # Iterate over the dataset items\n    for item in tqdm(dataset_items):\n        # Get the audio file\n        audio_file = item[class_name_key]\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # Predict the class label\n        predicted_class_label = encoder_manager.predict_class_label(embedding)\n        # Get the true class name\n        true_class_name = item['class_name']\n        # Get the true class ID\n        true_class_id = item['class_id']\n        # Get the true class name from the class ID\n        if class_id_to_class_name is not None:\n            true_class_name = class_id_to_class_name[true_class_id]\n        # Increment the total number of samples\n        total_num_samples += 1\n        # Increment the total number of correct samples if the true class name matches the predicted class label\n        if true_class_name == predicted_class_label:\n            total_num_correct += 1\n        # Increment the class counter for the true class name\n        if true_class_name in class_counters:\n            class_counters[true_class_name] += 1\n        else:\n            class_counters[true_class_name] = 1\n    # Calculate the accuracy for each class\n    class_accuracies = {}\n    for class_name in class_counters:\n        class_accuracy = class_counters[class_name] / class_c", "CodeBleu_score": 27.49135264561734}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tencoder_manager.model.eval()\n    encoder_manager.model.to(encoder_manager.device)\n    encoder_manager.model.zero_grad()\n    encoder_manager.model.reset_parameters()\n    encoder_manager.model.reset_states()\n    encoder_manager.model.reset_hidden()\n    encoder_manager.model.reset_cell()\n    encoder_manager.model.reset_output()\n    encoder_manager.model.reset_output_hidden()\n    encoder_manager.model.reset_output_cell()\n    encoder_manager.model.reset_output_hidden_cell()\n    encoder_manager.model.reset_attention()\n    encoder_manager.model.reset_attention_hidden()\n    encoder_manager.model.reset_attention_cell()\n    encoder_manager.model.reset_attention_hidden_cell()\n    encoder_manager.model.reset_attention_weights()\n    encoder_manager.model.reset_attention_hidden_weights()\n    encoder_manager.model.reset_attention_cell_weights()\n    encoder_manager.model.reset_attention_hidden_cell_weights()\n    encoder_manager.model.reset_attention_weights_cumulative()\n    encoder_manager.model.reset_attention_hidden_weights_cumulative()\n    encoder_manager.model.reset_attention_cell_weights_cumulative()\n    encoder_manager.model.reset_attention_hidden_cell_weights_cumulative()\n    encoder_manager.model.reset_attention_context()\n    encoder_manager.model.reset_attention_hidden_context()\n    encoder_manager.model.reset_attention_cell_context()\n    encoder_manager.model.reset_attention_hidden_cell_context()\n    encoder_manager.model.reset_attention_weights_cumulative_context()\n    encoder_manager.model.reset_attention_hidden_weights_cumulative_context()\n    encoder_manager", "CodeBleu_score": 6.863140782174939}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\toutput_path = audio_path.replace(args.input_dir, args.output_dir)\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    else:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        audio, speech_detected = remove_silence(audio_path, vad_model, vad_preproc, vad_postproc, vad_threshold, vad_min_silence, vad_min_speech)\n        if not speech_detected:\n            return output_path, False\n        torchaudio.save(output_path, audio, args.sample_rate)\n        return output_path, True\n\n", "CodeBleu_score": 41.31955186034921}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\toutput_path = audio_path.replace(args.input_dir, args.output_dir)\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    speech_detected = remove_silence(audio_path, output_path, args.vad_model, args.vad_threshold, args.vad_context, args.vad_overlap, args.vad_min_duration, args.vad_max_duration)\n    return output_path, speech_detected\n\n", "CodeBleu_score": 44.329500139492296}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\taudio_path = str(audio_path)\n    audio_path = audio_path.replace(input_dir, output_dir)\n    audio_path = pathlib.Path(audio_path)\n    audio_path.parent.mkdir(parents=True, exist_ok=True)\n    if audio_path.exists() and not force:\n        return str(audio_path), False\n    audio, fs = librosa.load(audio_path, sr=sample_rate)\n    audio = remove_silence(audio, sample_rate, silence_threshold=silence_threshold, silence_duration_threshold=silence_duration_threshold, min_silence_duration=min_silence_duration, trim=True)\n    if len(audio) == 0:\n        return str(audio_path), False\n    librosa.output.write_wav(audio_path, audio, fs)\n    return str(audio_path), True\n\n", "CodeBleu_score": 36.39570203187053}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    file_list = glob.glob(os.path.join(input_dir, \"**\", \"*.{}\".format(file_ext)), recursive=True)\n    pool = Pool(n_jobs)\n    pool.map(resample_file, [(filename, output_sr) for filename in tqdm(file_list)])\n    pool.close()\n    pool.join()\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 35.206336726079826}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is not None:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(input_dir, \"**\", \"*.{}\".format(file_ext)), recursive=True)\n    if n_jobs == 1:\n        for filename in tqdm(files, desc=\"Resampling files\"):\n            resample_file((filename, output_sr))\n    else:\n        with Pool(n_jobs) as pool:\n            pool.map(resample_file, [(filename, output_sr) for filename in tqdm(files, desc=\"Resampling files\")])\n\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 42.50091649242588}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    file_paths = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    pool = Pool(n_jobs)\n    func_args = [(file_path, output_sr) for file_path in tqdm(file_paths)]\n    pool.map(resample_file, func_args)\n    pool.close()\n    pool.join()\n    print(\"Done!\")\n\n", "CodeBleu_score": 33.00911068433197}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    criterion.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(data_loader):\n            if use_cuda:\n                batch = batch.cuda()\n            # reshape input and label to group samples of each class\n            input = batch[\"input\"].view(-1, batch[\"input\"].size(-1))\n            label = batch[\"label\"].view(-1)\n            # compute the loss\n            output = model(input)\n            loss = criterion(output, label)\n            # sum the loss\n            total_loss += loss.item()\n            # log the loss\n            if batch_idx % c.eval_log_interval == 0:\n                print(\n                    f\"Evaluation: [{batch_idx * len(batch['input'])}/{len(data_loader.dataset)} ({100. * batch_idx / len(data_loader):.0f}%)] Loss: {loss.item():.6f}\"\n                )\n    # calculate the average loss\n    avg_loss = total_loss / len(data_loader)\n    # log the evaluation loss\n    print(f\"Evaluation: Average loss: {avg_loss:.6f}\")\n    # plot the UMAP plot of the embeddings\n    if c.plot_umap:\n        plot_embeddings(\n            model,\n            data_loader,\n            c.model_params[\"encoder_params\"][\"embedding_size\"],\n            global_step,\n        )\n    # return the average loss\n    return avg_loss\n\n", "CodeBleu_score": 32.41095740717509}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t# initialize the loss and number of batches\n    loss = 0.0\n    num_batches = 0\n    # initialize the embeddings\n    embeddings = None\n    # initialize the labels\n    labels = None\n    # loop over the batches\n    for batch in data_loader:\n        # get the batch size\n        batch_size = batch[0].size(0)\n        # get the labels\n        labels = batch[1]\n        # get the embeddings\n        embeddings = batch[2]\n        # reshape the embeddings\n        embeddings = embeddings.view(batch_size, -1)\n        # reshape the labels\n        labels = labels.view(batch_size, -1)\n        # get the model predictions\n        model_outputs = model(embeddings)\n        # get the loss\n        loss += criterion(model_outputs, labels).item() * batch_size\n        # increment the number of batches\n        num_batches += 1\n    # compute the average loss\n    loss = loss / num_batches\n    # log the evaluation loss\n    log.info(f\"Evaluation Loss: {loss}\")\n    # get the embeddings\n    embeddings = embeddings.cpu().detach().numpy()\n    # get the labels\n    labels = labels.cpu().detach().numpy()\n    # get the model predictions\n    model_outputs = model_outputs.cpu().detach().numpy()\n    # get the class names\n    class_names = data_loader.dataset.get_map_classid_to_classname()\n    # get the class ids\n    class_ids = data_loader.dataset.get_map_classname_to_classid()\n    # get the class names\n    class_names = data_loader.dataset.get_map_classid_to_classname()\n    # get the class ids\n    class_ids = data_loader.dataset.get_map_classname_to_classid()\n    # get the class names\n    class_names = data_loader.dataset.get_map_classid_to_classname()\n    # get the class ids", "CodeBleu_score": 28.150231152475335}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    loss_avg = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(data_loader):\n            batch = batch.to(device)\n            batch_size = batch.shape[0]\n            # reshape input and target\n            input_reshape = batch.reshape(batch_size, -1, c.input_dim)\n            target_reshape = batch.reshape(batch_size, -1, c.output_dim)\n            # get model output\n            output = model(input_reshape)\n            # get loss\n            loss = criterion(output, target_reshape)\n            loss_avg += loss.item()\n    loss_avg /= batch_idx + 1\n    if c.model_params.get(\"use_torch_spec\", False):\n        loss_avg /= 100\n    # log statistics\n    print(\n        f\" > Evaluation > Step: {global_step} > Loss: {loss_avg:.4f}\"\n    )\n    writer.add_scalar(\"loss/eval\", loss_avg, global_step)\n    return loss_avg\n\n", "CodeBleu_score": 24.97251637332599}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t# get model name\n    if model_name.startswith(\"xtts\"):\n        model_name = model_name[4:]\n    # get config class name\n    config_class_name = model_name.capitalize() + \"Config\"\n    # try to import config class\n    try:\n        config_class = find_module(config_class_name, [\n            \"TTS.configs\",\n            \"TTS.configs.models\",\n            \"TTS.configs.models.\" + model_name,\n            \"TTS.configs.models.xtts\",\n        ])\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            f\"Could not find config class {config_class_name} for model {model_name}.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 34.76355186692676}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = model_name + \"Config\"\n    config_class = None\n    if model_name == \"xtts\":\n        config_class = XttsConfig\n    else:\n        config_class = find_module(config_class_name)\n    if config_class is None:\n        raise ModuleNotFoundError(f\"No configuration class found for {model_name}\")\n    return config_class\n\n", "CodeBleu_score": 30.76752186913575}, {"code": "def register_config(model_name: str) -> Coqpit\n\tif model_name == \"xtts\":\n        model_name = \"xtts_tts\"\n    config_class_name = model_name + \"_config\"\n    try:\n        config_class = find_module(config_class_name, config_paths)\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(f\"{config_class_name} configuration class not found. Please make sure that the model name is correct.\") from e\n    return config_class\n\n", "CodeBleu_score": 32.47647375859932}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = read_config(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n", "CodeBleu_score": 9.54283197261314}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_path = os.path.abspath(config_path)\n    _, ext = os.path.splitext(config_path)\n    if ext == \".yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n    elif ext == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(\"Unknown config file extension: {}\".format(ext))\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.populate(config_dict)\n    return config\n\n", "CodeBleu_score": 43.24582935058878}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = {}\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown file type: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 44.59485659944104}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t# Check if the basePath is a valid directory\n    if not os.path.isdir(basePath):\n        raise ValueError(\"Base path is not a directory\")\n\n    # Process the base directory\n    for root, dirs, files in os.walk(basePath):\n        for name in files:\n            if contains is not None:\n                if contains not in name:\n                    continue\n            if validExts is not None:\n                ext = os.path.splitext(name)[1]\n                if ext.lower() not in validExts:\n                    continue\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 34.9442418499041}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# walk through the directory structure, spawning off processes as we go\n    for root, dirs, files in os.walk(basePath):\n        for name in files:\n            if contains is not None and contains not in name:\n                continue\n            if validExts is not None:\n                ext = os.path.splitext(name)[1]\n                if not ext.lower() in validExts:\n                    continue\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 39.74328953120533}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# Walk through the directory structure\n\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            # Check if the file extension is in the set of valid extensions\n\n            if validExts is not None:\n                file_ext = os.path.splitext(file)[1]\n                if file_ext.lower() not in validExts:\n                    continue\n            # Check if the file name contains the 'contains' string\n\n            if contains is not None:\n                if contains not in file:\n                    continue\n            # Construct the full file path\n\n            file_path = os.path.join(root, file)\n            yield file_path\n\n", "CodeBleu_score": 37.6564384939868}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if not xtts_checkpoint:\n        print(\"XTTS checkpoint path not provided.\")\n        return\n    if not xtts_config:\n        print(\"XTTS config path not provided.\")\n        return\n    if not xtts_vocab:\n        print(\"XTTS vocab path not provided.\")\n        return\n    print(\"Loading XTTS configuration...\")\n    config = XttsConfig.from_pretrained(xtts_config)\n    print(\"Initializing XTTS model...\")\n    model = Xtts(config)\n    print(\"Loading XTTS model checkpoint...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    print(\"Loading XTTS vocabulary...\")\n    model.vocab.load_vocabulary(xtts_vocab)\n    if torch.cuda.is_available():\n        print(\"Moving XTTS model to GPU...\")\n        model.to(\"cuda\")\n    print(\"XTTS model loaded successfully!\")\n    global XTTS_MODEL\n    XTTS_MODEL = model\n\n", "CodeBleu_score": 46.77746706811957}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Please provide all the required arguments\"\n    if not os.path.exists(xtts_checkpoint) or not os.path.exists(xtts_config) or not os.path.exists(xtts_vocab):\n        return \"Please provide valid paths for the checkpoint, config, and vocab files\"\n    print(\"Loading configuration...\")\n    config = XttsConfig.from_pretrained(xtts_config)\n    print(\"Loading model...\")\n    global XTTS_MODEL\n    XTTS_MODEL = Xtts(config)\n    XTTS_MODEL.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    if torch.cuda.is_available():\n        XTTS_MODEL.to(torch.device(\"cuda\"))\n    print(\"Loading vocabulary...\")\n    XTTS_MODEL.set_vocabulary(xtts_vocab)\n    print(\"Model loaded successfully!\")\n\n", "CodeBleu_score": 55.058263173911826}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if not xtts_checkpoint:\n        raise ValueError(\"XTTS checkpoint path not provided\")\n    if not xtts_config:\n        raise ValueError(\"XTTS config path not provided\")\n    if not xtts_vocab:\n        raise ValueError(\"XTTS vocab path not provided\")\n    print(\"Loading XTTS model\")\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config)\n    model.to(config.device)\n    checkpoint = torch.load(xtts_checkpoint, map_location=config.device)\n    model.load_state_dict(checkpoint[\"model\"])\n    model.eval()\n    print(\"XTTS model loaded\")\n    return model\n\n", "CodeBleu_score": 46.22547291107667}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tx = x.cpu().numpy()\n    if self.torch_mel_spectrogram is not None:\n        x = self.torch_mel_spectrogram(x)\n    x = x.transpose(1, 2)\n    num_frames = min(num_frames, x.shape[1])\n    offsets = np.arange(0, num_frames, num_frames // num_eval)\n    embeddings = []\n    for offset in offsets:\n        embedding = self.forward(x[:, offset:offset + num_frames, :], l2_norm)\n        embeddings.append(embedding)\n    embeddings = np.concatenate(embeddings, axis=0)\n    if return_mean:\n        embeddings = embeddings.mean(axis=0)\n    if l2_norm:\n        embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 43.98569445560869}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif self.audio_config[\"hop_length\"] is not None:\n        num_frames = num_frames // self.audio_config[\"hop_length\"]\n    assert num_frames * self.audio_config[\"hop_length\"] <= x.shape[1]\n    assert num_frames > 0\n    num_eval = min(num_eval, num_frames)\n    assert num_eval > 0\n    offset_positions = np.arange(0, num_frames, num_frames // num_eval)\n    embeddings = []\n    for i in range(num_eval):\n        offset = offset_positions[i]\n        batch = x[:, offset : offset + num_frames]\n        batch = self.get_torch_mel_spectrogram_class(self.audio_config)(batch)\n        batch = batch.unsqueeze(0)\n        batch = batch.transpose(1, 2)\n        embeddings.append(self.inference(batch, l2_norm=l2_norm).squeeze(0))\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n    return embeddings\n\n", "CodeBleu_score": 42.28219220283184}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# pylint: disable=W0212\n    if self.torch_mel_spectrogram is not None:\n        x = self.torch_mel_spectrogram(x)\n    else:\n        x = self.mel_spectrogram(x)\n    if self.hop_length is None:\n        num_frames = min(num_frames, x.shape[1])\n    else:\n        num_frames = min(num_frames, x.shape[1] // self.hop_length)\n    if num_frames == 0:\n        return x\n    offset = np.arange(num_frames) * self.hop_length\n    x = x[:, offset]\n    x = x.transpose(1, 2)\n    x = x.reshape(x.shape[0], -1, x.shape[-1])\n    x = x.transpose(0, 2, 1)\n    x = x.reshape(num_frames, -1)\n    x = x.transpose(1, 0)\n    x = x.reshape(num_frames, num_eval, -1)\n    x = x.transpose(1, 0)\n    x = x.reshape(num_eval, -1)\n    x = self.forward(x, l2_norm)\n    if return_mean:\n        x = x.mean(0)\n        if l2_norm:\n            x = x / np.linalg.norm(x)\n    return x\n\n", "CodeBleu_score": 34.06356289638819}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = [block(self.inplanes, planes, stride, downsample)]\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.58613991692006}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = [block(self.inplanes, planes, stride, downsample)]\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 75.37897351338722}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.96342696046426}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate the number of utterances per class\n    utterances_per_class = np.array([len(embeddings[embeddings[:, -1] == class_id]) for class_id in range(num_classes_in_batch)])\n    # If the number of utterances per class is greater than 10, limit the number of classes to 10\n    if np.max(utterances_per_class) > 10:\n        num_classes_in_batch = 10\n        utterances_per_class = utterances_per_class[:num_classes_in_batch]\n    # Perform UMAP dimensionality reduction\n    reducer = umap.UMAP(random_state=42)\n    embeddings = reducer.fit_transform(embeddings[:, :-1])\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeddings[:, 0], embeddings[:, 1], c=colormap[embeddings[:, -1]], s=20)\n    # Set the aspect ratio to 1\n    ax.set(aspect=\"equal\", xlim=(-1, 1), ylim=(-1, 1))\n    # Set the title to \"UMAP projection\"\n    ax.set_title(\"UMAP projection\", fontsize=18)\n    # Save the plot as \"umap\"\n    fig.savefig(\"umap.png\", dpi=200)\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 38.564274384667755}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate number of utterances per class\n    num_utterances_per_class = np.zeros(num_classes_in_batch)\n    for i in range(num_classes_in_batch):\n        num_utterances_per_class[i] = len(embeddings[i])\n    # Limit number of classes to 10 if necessary\n    num_classes_to_plot = 10\n    if num_classes_in_batch < num_classes_to_plot:\n        num_classes_to_plot = num_classes_in_batch\n    # UMAP transforms the embeddings\n    embeddings = umap.UMAP(\n        n_neighbors=10,\n        min_dist=0.1,\n        n_components=2,\n        metric=\"euclidean\",\n        random_state=42,\n    ).fit_transform(embeddings)\n    # Scatter plot is created with colors representing different classes\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(\n        embeddings[:, 0],\n        embeddings[:, 1],\n        c=colormap[np.arange(num_classes_to_plot)],\n        s=100,\n        edgecolor=\"black\",\n    )\n    # Title of the plot is \"UMAP projection\", adjusted for equal aspect ratio\n    plt.title(\"UMAP projection\", fontsize=20)\n    plt.axis(\"equal\")\n    # Plot is saved as \"umap\" and returned as a figure\n    plt.savefig(\"umap.png\")\n    plt.close(fig)\n    return fig\n\n", "CodeBleu_score": 38.72044548894531}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate the number of utterances per class\n    num_utterances_per_class = [\n        len(embeddings[embeddings[:, 1] == class_id]) for class_id in range(num_classes_in_batch)\n    ]\n    # Limit the number of classes to 10 if necessary\n    num_classes_to_plot = min(num_classes_in_batch, 10)\n    # Plot the embeddings using UMAP\n    reducer = umap.UMAP(\n        n_components=2,\n        n_neighbors=10,\n        min_dist=0.0,\n        metric=\"euclidean\",\n        random_state=42,\n    )\n    embeddings = reducer.fit_transform(embeddings[:, 2:])\n    fig, ax = plt.subplots(figsize=(10, 10))\n    # Create a scatter plot with colors representing different classes\n    for class_id in range(num_classes_to_plot):\n        # Select the embeddings of the current class\n        embeddings_current_class = embeddings[embeddings[:, 1] == class_id]\n        # Calculate the number of utterances of the current class\n        num_utterances_current_class = len(embeddings_current_class)\n        # Select the colors for the current class\n        colors_current_class = colormap[\n            : num_utterances_current_class, :\n        ]  # Use the first num_utterances_current_class colors\n        # Plot the embeddings of the current class\n        ax.scatter(\n            embeddings_current_class[:, 0],\n            embeddings_current_class[:, 1],\n            c=colors_current_class,\n            s=100,\n            alpha=0.5,\n        )\n    # Set the title of the plot\n    fig.suptitle(\n        \"UMAP projection\",\n        fontsize=20,\n        fontweight=\"bold\",\n        y=0.95,\n        x=0.5,\n        horizontalalignment=\"center\",", "CodeBleu_score": 27.986778584484984}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tsim_matrix = []\n    for spkr in range(dvecs.size(0)):\n        for utt in range(dvecs.size(1)):\n            if utt == 0:\n                new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            else:\n                new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = torch.mm(dvecs[spkr, utt], new_centroids.t())\n            sim = torch.clamp(sim, min=-1.0, max=1.0)\n            sim_matrix.append(sim)\n    sim_matrix = torch.stack(sim_matrix)\n    return sim_matrix\n\n", "CodeBleu_score": 33.2064673296296}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcosine_sim = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cosine_sim_utt = torch.mm(dvecs[spkr, utt].unsqueeze(0), new_centroids.t())\n            cosine_sim_utt = torch.clamp(cosine_sim_utt, min=0.0, max=1.0)\n            cosine_sim.append(cosine_sim_utt)\n    cosine_sim = torch.stack(cosine_sim)\n    return cosine_sim\n\n", "CodeBleu_score": 29.17114644920136}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcosine_sim = torch.zeros((dvecs.shape[0], dvecs.shape[1]))\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cosine_sim[spkr, utt] = torch.clamp(F.cosine_similarity(dvecs[spkr, utt].unsqueeze(0), new_centroids), min=1e-6)\n    return cosine_sim\n\n", "CodeBleu_score": 18.17804821426963}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for speaker in dvecs:\n        speaker_losses = []\n        for utterance in speaker:\n            speaker_losses.append(\n                F.log_softmax(cos_sim_matrix, dim=2)[:, :, speaker.index(utterance)].sum(dim=1)\n            )\n        losses.append(torch.stack(speaker_losses))\n    return torch.stack(losses)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 31.429513743541083}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        losses.append(F.log_softmax(cos_sim_matrix[spkr_idx], dim=0))\n    return torch.stack(losses, dim=0)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 22.942867500720222}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for embedding_idx, embedding in enumerate(dvecs):\n        loss = F.log_softmax(cos_sim_matrix[embedding_idx], dim=0)\n        losses.append(loss)\n    return torch.stack(losses)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 29.555962505652346}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-torch.log(torch.sigmoid(cos_sim_matrix[j, i])))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 42.84805643467687}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-torch.sigmoid(cos_sim_matrix[j, i]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 41.91937157564955}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(\n                torch.sigmoid(cos_sim_matrix[j, i]) - torch.sigmoid(cos_sim_matrix[j, i, j])\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 49.94314695507008}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t# pylint: disable=E1102\n    N, M, _ = x.shape\n    assert N >= 2\n    assert M >= 2\n    dvecs = x.clone()\n    # calculate centroids\n    centroids = torch.mean(dvecs, 1)\n    # calculate cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n    # scale and shift\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # calculate embedding loss\n    if self.loss_method == \"softmax\":\n        embed_loss = self.embed_loss_softmax(dvecs, cos_sim_matrix)\n    if self.loss_method == \"contrast\":\n        embed_loss = self.embed_loss_contrast(dvecs, cos_sim_matrix)\n    # average loss\n    loss = torch.mean(embed_loss)\n    return loss\n\n", "CodeBleu_score": 53.03154696170983}, {"code": "def forward(self, x, _label=None)\n\tnum_speakers, num_utts_per_speaker, _ = x.shape\n    assert num_utts_per_speaker >= 2, \"GE2E requires at least two utterances per speaker\"\n    # compute centroids\n    centroids = torch.mean(x, 1)\n    # compute cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    cos_sim_matrix = cos_sim_matrix * self.w + self.b\n    # compute the embedding loss\n    if self.loss_method == \"softmax\":\n        embed_loss = self.embed_loss_softmax(x, cos_sim_matrix)\n    elif self.loss_method == \"contrast\":\n        embed_loss = self.embed_loss_contrast(x, cos_sim_matrix)\n    else:\n        raise ValueError(\"Unknown loss method: {}\".format(self.loss_method))\n    # average the embedding loss\n    loss = torch.mean(torch.sum(embed_loss, 1))\n    return loss\n\n", "CodeBleu_score": 47.14926344787297}, {"code": "def forward(self, x, _label=None)\n\tif x.shape[1] < 2:\n        raise ValueError(\"Input must have at least two utterances per speaker\")\n    num_speakers = x.shape[0]\n    num_utts_per_speaker = x.shape[1]\n    dvec_feats = x.shape[2]\n    # compute centroids\n    centroids = torch.mean(x, 1)\n    # compute cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # calculate embedding loss\n    if self.loss_method == \"softmax\":\n        embed_loss = self.embed_loss_softmax(x, cos_sim_matrix)\n    if self.loss_method == \"contrast\":\n        embed_loss = self.embed_loss_contrast(x, cos_sim_matrix)\n    # average over utterances\n    embed_loss = torch.mean(embed_loss, 1)\n    # average over speakers\n    embed_loss = torch.mean(embed_loss, 0)\n    # average over utterances and speakers\n    loss = torch.mean(embed_loss)\n    return loss\n\n", "CodeBleu_score": 51.60171751620759}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoice_dirs = []\n    voice_dirs.extend(extra_voice_dirs)\n    voice_dirs.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\"))\n    voices = {}\n    for voice_dir in voice_dirs:\n        logger.info(f\"Loading voices from {voice_dir}\")\n        for subdir in tqdm.tqdm(glob(os.path.join(voice_dir, \"*\"))):\n            if not os.path.isdir(subdir):\n                continue\n            subdir_voices = []\n            for file in glob(os.path.join(subdir, \"*.npz\")):\n                subdir_voices.append(file)\n            if len(subdir_voices) == 0:\n                for file in glob(os.path.join(subdir, \"*.wav\")):\n                    subdir_voices.append(file)\n            if len(subdir_voices) == 0:\n                for file in glob(os.path.join(subdir, \"*.mp3\")):\n                    subdir_voices.append(file)\n            if len(subdir_voices) > 0:\n                voices[os.path.basename(subdir)] = subdir_voices\n    return voices\n\n", "CodeBleu_score": 26.882793616068195}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tall_voice_dirs = [\n        \"TTS/tts/layers/bark/hubert/hubert_data/voices\",\n        \"TTS/tts/layers/bark/hubert/hubert_data/voices_extra\",\n    ]\n    all_voice_dirs.extend(extra_voice_dirs)\n    voice_files = {}\n    for voice_dir in all_voice_dirs:\n        if os.path.exists(voice_dir):\n            for root, _, files in os.walk(voice_dir):\n                for file in files:\n                    if file.endswith(\".npz\"):\n                        if root not in voice_files:\n                            voice_files[root] = []\n                        voice_files[root].append(os.path.join(root, file))\n                    elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        if root not in voice_files:\n                            voice_files[root] = []\n                        voice_files[root].append(os.path.join(root, file))\n    return voice_files\n\n", "CodeBleu_score": 25.291844219162307}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoice_dirs = []\n    for extra_voice_dir in extra_voice_dirs:\n        voice_dirs.append(extra_voice_dir)\n    voice_dirs.append(os.path.join(os.path.dirname(__file__), \"voices\"))\n    voice_dirs = list(set(voice_dirs))\n    voices = {}\n    for voice_dir in voice_dirs:\n        voice_dir_name = os.path.basename(voice_dir)\n        voice_files = glob(os.path.join(voice_dir, \"**\", \"*.wav\"), recursive=True) + glob(os.path.join(voice_dir, \"**\", \"*.mp3\"), recursive=True) + glob(os.path.join(voice_dir, \"**\", \"*.npz\"), recursive=True)\n        voice_files = [os.path.abspath(voice_file) for voice_file in voice_files]\n        voices[voice_dir_name] = voice_files\n    return voices\n\n", "CodeBleu_score": 27.64087285740268}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tvoices = get_voices(extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found. Please check the name and try again.\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}. Please check the name and try again.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n        return semantic, coarse, fine\n    else:\n        return load_audio(model, voice_path)\n\n", "CodeBleu_score": 41.31177342460243}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found in {voices.keys()}\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        audio_path = voice_path\n        semantic, coarse, fine = convert_audio(\n            audio_path,\n            model.sample_rate,\n            model.hparams.fft_size,\n            model.hparams.hop_size,\n            model.hparams.win_length,\n            model.hparams.n_mel_channels,\n            model.hparams.mel_fmin,\n            model.hparams.mel_fmax,\n            model.hparams.normalize,\n            model.hparams.clip_norm,\n            model.hparams.clip_norm_type,\n        )\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 48.57744799784226}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found. Please check your spelling or use one of the following: {list(voices.keys())}.\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}. Please use one of the following: {voice_paths}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n        return semantic, coarse, fine\n    else:\n        return load_audio(model, voice_path)\n\n", "CodeBleu_score": 45.97013871443379}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t# pylint: disable=arguments-differ\n    # pylint: disable=unused-argument\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=invalid-name\n    # pylint: disable=no-self-use\n    # pylint: disable=not-callable\n    # pylint: disable=unused-variable\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=invalid-name\n    # pylint: disable=no-self-use\n    # pylint: disable=not-callable\n    # pylint: disable=unused-variable\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=invalid-name\n    # pylint: disable=no-self-use\n    # pylint: disable=not-callable\n    # pylint: disable=unused-variable\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable", "CodeBleu_score": 1.459942084942085}, {"code": "def run_padded_sequence(self, context, lens)\n\t# pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=invalid-name\n    # pylint: disable=too-many-arguments\n    # pylint: disable=no-member\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=consider-using-enumerate\n    # pylint: disable=unused-argument\n    # pylint: disable=redefined-outer-name\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused-argument\n    # pylint: disable=unused", "CodeBleu_score": 1.459942084942085}, {"code": "def run_padded_sequence(self, context, lens)\n\t# pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    #", "CodeBleu_score": 1.459942084942085}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, sorted_indices = lens.sort(0, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context_packed = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context_packed = fn(sorted_context_packed)\n    sorted_context_unpacked, _ = nn.utils.rnn.pad_packed_sequence(sorted_context_packed, batch_first=True)\n    context = sorted_context_unpacked[sorted_indices]\n    return context\n\n", "CodeBleu_score": 17.896700649498698}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tif context.size(0) == 0:\n        return context\n    sorted_lens, indices = lens.sort(descending=True)\n    sorted_context = context[indices]\n    sorted_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context = fn(sorted_context)\n    sorted_context, _ = pad_packed_sequence(sorted_context, batch_first=True)\n    _, unsorted_indices = indices.sort(descending=False)\n    unsorted_context = sorted_context[unsorted_indices]\n    return unsorted_context\n\n", "CodeBleu_score": 31.112123830322265}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t# sort the lengths and indices\n    lens, indices = lens.sort(descending=True)\n    context = context[indices]\n    # pack the context sequences\n    context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n    # apply the function\n    context = fn(context)\n    # unpack the sequences\n    context = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n    # map the results back to the original indices\n    context = context[indices]\n    return context\n\n", "CodeBleu_score": 18.94827399154671}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 23.635053446302113}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se_resnet')\n    mx_name = mx_name.replace('senet_', 'senet')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    mx_name = mx_name.replace('_v2', '_v2_')\n    mx_name = mx_name.replace('_v3', '_v3_')\n    mx_name = mx_name.replace('_v4', '_v4_')\n    mx_name = mx_name.replace('_v5', '_v5_')\n    mx_name = mx_name.replace('_v6', '_v6_')\n    mx_name = mx_name.replace('_v7', '_v7_')\n    mx_name = mx_name.replace('_v8', '_v8_')\n    mx_name = mx_name.replace('_v9', '_v9_')\n    mx_name = mx_name.replace('_v10', '_v10_')\n    mx_name = mx_name.replace('_v11', '_v11_')\n    mx_name = mx_name.replace('_v12', '_v12_')\n    mx_name = mx_name.replace('_v13', '_v13_')\n    mx_name = mx_name.replace('_v14', '_v14_')\n    mx_name = mx_name.replace('_v15', '_v15_')\n    mx_name = mx_name.replace('_v16', '_v16_')\n    mx_name = mx_name.replace('_v17', '_v17_')\n    mx_name = mx_name.replace('_v18', '_v18_')\n    mx_name = mx_name.replace('_v19", "CodeBleu_score": 34.0533293894968}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'se_' + mx_name[3:]\n    if mx_name.startswith('senet_'):\n        mx_name = 'senet_' + mx_name[6:]\n    if mx_name == 'inceptionv3':\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 33.02152646569916}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = 'senet_' + mx_name[6:]\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = 'inceptionv3' + mx_name[13:]\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 39.592483714200895}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\tif self.repeats == 0:\n        return math.ceil(self.num_samples / self.num_workers)\n    else:\n        return math.ceil(self.num_samples / self.repeats)\n\n", "CodeBleu_score": 11.69002814407927}, {"code": "def _num_samples_per_worker(self)\n\t# The number of samples per worker is the number of samples divided by the number of workers.\n    num_samples_per_worker = self.num_samples // self.global_num_workers\n    # If the number of samples is not divisible by the number of workers,\n    # the number of samples per worker is rounded up to the next integer.\n    if self.num_samples % self.global_num_workers != 0:\n        num_samples_per_worker += 1\n    # If training or multiple replicas are used, the number of samples per worker\n    # is rounded up to the next integer.\n    if self.is_training or self.global_num_workers > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n    # If a batch size is specified during training, the number of samples per worker\n    # is adjusted to be a multiple of the batch size.\n    if self.batch_size > 1 and self.is_training:\n        num_samples_per_worker = (\n            num_samples_per_worker - (num_samples_per_worker % self.batch_size)\n        )\n    return num_samples_per_worker\n\n", "CodeBleu_score": 37.34578080526526}, {"code": "def _num_samples_per_worker(self)\n\tnum_samples_per_worker = self.num_samples // self.global_num_workers\n    if self.is_training:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n    if self.batch_size > 1:\n        num_samples_per_worker = (\n            num_samples_per_worker // self.batch_size\n        ) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 32.85311039797102}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    idx_to_class = {}\n    for root, dirs, files in os.walk(folder, topdown=False):\n        if leaf_name_only:\n            dirs[:] = [os.path.basename(d) for d in dirs]\n        for f in files:\n            if f.split('.')[-1] in types:\n                path = os.path.join(root, f)\n                label = os.path.relpath(root, folder)\n                if label not in class_to_idx:\n                    idx = len(class_to_idx)\n                    class_to_idx[label] = idx\n                    idx_to_class[idx] = label\n                yield path, class_to_idx[label]\n    if sort:\n        return sorted(list(zip(*(find_images_and_targets(folder, types, class_to_idx, leaf_name_only, False)))), key=natural_key)\n    return list(zip(*(find_images_and_targets(folder, types, class_to_idx, leaf_name_only, False))))\n\n", "CodeBleu_score": 36.15071242738117}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = load_class_map(folder, leaf_name_only)\n    class_to_idx = {k: idx for k, idx in class_to_idx.items() if idx is not None}\n    files = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in fnames:\n            if os.path.splitext(fname)[1] in types:\n                path = os.path.join(root, fname)\n                relpath = os.path.relpath(path, folder)\n                label = os.path.relpath(root, folder) if leaf_name_only else relpath\n                if label in class_to_idx:\n                    files.append((path, class_to_idx[label]))\n    if sort:\n        files = sorted(files, key=natural_key)\n    return files, class_to_idx\n\n", "CodeBleu_score": 38.47461099594798}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    # If we have a leaf name only, we use the folder name as label\n    if leaf_name_only:\n        label = os.path.basename(folder)\n    else:\n        label = folder\n    if label in class_to_idx:\n        idx = class_to_idx[label]\n    else:\n        idx = len(class_to_idx)\n        class_to_idx[label] = idx\n    # If we have a leaf name only, we use the folder name as label\n    if leaf_name_only:\n        label = os.path.basename(folder)\n    else:\n        label = folder\n    if label in class_to_idx:\n        idx = class_to_idx[label]\n    else:\n        idx = len(class_to_idx)\n        class_to_idx[label] = idx\n    filenames = []\n    for root, dirs, files in os.walk(folder, topdown=True):\n        if leaf_name_only:\n            dirs[:] = [os.path.basename(d) for d in dirs]\n        for f in files:\n            if f.lower().endswith(tuple(types)):\n                filenames.append(os.path.join(root, f))\n    if sort:\n        filenames = sorted(filenames)\n    return filenames, class_to_idx\n\n", "CodeBleu_score": 38.81051440264596}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is None:\n        return torch.initial_seed() + dist.get_rank()\n    else:\n        return worker_info.seed\n\n", "CodeBleu_score": 42.489035230245015}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.initial_seed() + dist.get_rank()\n\n", "CodeBleu_score": 53.21905471056826}, {"code": "def pytorch_worker_seed()\n\ttry:\n        worker_info = get_worker_info()\n        if worker_info is not None:\n            seed = worker_info.seed\n        else:\n            seed = random.randint(0, 10000)\n    except:\n        seed = random.randint(0, 10000)\n    return seed\n\n", "CodeBleu_score": 53.97411838655308}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\tif img.mode not in (\"L\", \"RGB\"):\n        return img\n    if add == 0:\n        return img\n    if img.mode == \"L\":\n        if thresh == 0:\n            thresh = 1\n        lut = [min(255, 255 - 255 * x / thresh) + add for x in range(256)]\n    else:\n        lut = [min(255, 255 - 255 * (x / 255) / thresh) + add for x in range(256)]\n    if img.mode == \"L\":\n        return img.point(lut)\n    else:\n        return img.convert(\"L\").point(lut).convert(img.mode)\n\n", "CodeBleu_score": 31.85123757303429}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\treturn ImageOps.solarize(img, thresh, add)\n\n", "CodeBleu_score": 5.233480815035253}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tif img.mode == 'L':\n        img = img.convert('RGB')\n    img = ImageOps.solarize(img, thresh)\n    img = ImageEnhance.Brightness(img).enhance(1.0 + add)\n    if img.mode == 'RGB':\n        img = img.convert('L')\n    return img\n\n", "CodeBleu_score": 28.373402748369337}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (list, tuple)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple(x)\n        _logger.warning(\n            \"Input tuple x has length %d, but required length is %d. The mean of x is calculated and x is repeated to match the required length.\",\n            len(x),\n            n,\n        )\n        x = (np.mean(x),) * n\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 42.62753115423145}, {"code": "def adapt_to_chs(x, n)\n\tif isinstance(x, (list, tuple)):\n        assert len(x) == n, f\"{len(x)} != {n}\"\n        return x\n    else:\n        x = (x,) * n\n        _logger.warning(f\"{x} is not a tuple or list, repeating it {n} times\")\n        return x\n\n", "CodeBleu_score": 27.983550278824836}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (list, tuple)):\n        x = (x,) * n\n    elif len(x) != n:\n        if len(x) == 1:\n            x = x[0]\n            x = (x,) * n\n        else:\n            x = tuple(x)\n            x = tuple(np.mean(x, axis=0) for _ in range(n))\n            _logger.warning(\n                'The number of channels in the input does not match the number of channels in the model. '\n                'The mean of the input channels is used to adapt the number of channels in the input.')\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 43.195744030873605}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(image_size * tf.cast(tf.minimum(image_height, image_width), tf.float32) / tf.maximum(image_height, image_width), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.decode_and_crop_jpeg(image_bytes, tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]), channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 52.10129475569046}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(\n        image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size],\n                            resize_method)[0]\n    return image\n\n", "CodeBleu_score": 61.96992133247458}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.decode_and_crop_jpeg(\n        image_bytes,\n        tf.stack([offset_height, offset_width,\n                  offset_height + padded_center_crop_size,\n                  offset_width + padded_center_crop_size]),\n        channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 57.195991906877566}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t# Decode and random crop image\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    # Randomly flip image\n    image = _flip(image)\n    # Convert image to target dtype\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 31.14488408418079}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 40.51338026157162}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif use_bfloat16:\n        dtype = tf.bfloat16\n    else:\n        dtype = tf.float32\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.cast(image, dtype)\n    return image\n\n", "CodeBleu_score": 36.11033264496504}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor.predict(model, eval_dataloader, output_file=output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, f\"{output_file}.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 56.0283747308197}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    self.predictor(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, \"merged.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 56.786588455983654}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif self.metric is None:\n        raise ValueError(\"config.metric is\", self.metric)\n    metric_cls = getattr(metric_path, self.metric)\n    self.metric = metric_cls(self)\n    if self.predictor is None:\n        raise ValueError(\"config.predictor is\", self.predictor)\n    predictor_cls = getattr(predictor_path, self.predictor)\n    self.predictor = predictor_cls(self)\n    self.eval_dataloader = eval_dataloader\n    try:\n        print(self.predictor.pred_dir)\n        for pred_file in glob.glob(self.predictor.pred_dir + \"/\" + \"*_merged.npy\"):\n            outputs = np.load(pred_file)\n            results = self.metric.compute_metrics(outputs)\n            self.metric.print_computed_metrics(results)\n\n        outputs = np.load(os.path.join(self.predictor.pred_dir, \"merged.npy\"))\n        results = self.metric.compute_metrics(outputs)\n        return {\"results\": results, \"metric\": self.metric}\n    except FileNotFoundError:\n        print(\"", "CodeBleu_score": 32.54630866030682}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"R@1: {:.4f} - R@5: {:.4f} - mIoU: {:.4f}\".format(rank1, rank5, miou))\n\n", "CodeBleu_score": 36.951942939343425}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\\\"rank1\\\"]\n    rank5 = metrics[\\\"rank5\\\"]\n    miou = metrics[\\\"miou\\\"]\n    print(\n            \\\"R@1: {:.4f} - R@5: {:.4f} - mIOU: {:.4f}\\\".format(\n                rank1, rank5, miou\n            )\n        )\n\n", "CodeBleu_score": 32.467807986782404}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"R@1: {:.4f} - R@5: {:.4f} - mIoU: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )\n\n\n", "CodeBleu_score": 50.188181089416226}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\trank1 = 0\n    rank5 = 0\n    miou = 0\n    for segment, data_ in zip(segments, data):\n        ious = [self._iou(segment, d) for d in data_]\n        ranks = [self._rank(segment, d) for d in data_]\n        rank1 += ranks.count(1)\n        rank5 += ranks.count(5)\n        miou += np.mean(ious)\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n    miou = miou / len(segments)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 28.20979317512468}, {"code": "def _eval_predictions(self, segments, data)\n\trank1 = 0.0\n    rank5 = 0.0\n    miou = 0.0\n    for segment, data_ in zip(segments, data):\n        ious = [self._iou(segment, data_) for data_ in data]\n        ranks = [self._rank(segment, data_) for data_ in data]\n        rank1 += ranks[0] / len(data)\n        rank5 += ranks[4] / len(data)\n        miou += np.mean(ious)\n    return rank1 / len(segments), rank5 / len(segments), miou / len(segments)\n\n", "CodeBleu_score": 25.802223491356642}, {"code": "def _eval_predictions(self, segments, data)\n\tassert len(segments) == len(data)\n    rank1 = 0\n    rank5 = 0\n    miou = 0\n    for segment, data_ in zip(segments, data):\n        ious = []\n        for pred in segment:\n            ious.append(self._iou(pred, data_))\n        iou = np.mean(ious)\n        rank = self._rank(segment, data_)\n        rank1 += 1 if rank == 1 else 0\n        rank5 += 1 if rank <= 5 else 0\n        miou += iou\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n    miou = miou / len(segments)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 31.70666268048467}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# set model to evaluation mode\n    model.eval()\n    # transfer model to GPU 0\n    model = model.to(self.device)\n    # initialize the empty list of scores\n    scores = []\n    # iterate over the dataloader\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(eval_dataloader):\n            # transfer data to GPU 0\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            # make predictions\n            output = model(batch)\n            # update the scores\n            scores.append(output)\n    # finalize and return the results\n    scores = self.finalize_predictions(scores)\n    if output_file is not None:\n        self.save_predictions(output_file, scores)\n    return scores\n\n", "CodeBleu_score": 37.81738169308291}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# Initialize the output\n    outputs = []\n    # Set the model to evaluation mode\n    model.eval()\n    # Transfer the model to GPU 0\n    model = model.cuda()\n    # Iterate over the dataloader\n    for batch in tqdm(eval_dataloader):\n        # Transfer the data to GPU 0\n        batch = {k: v.cuda(non_blocking=True) for k, v in batch.items()}\n        # Make predictions\n        with torch.no_grad():\n            outputs.append(model(batch))\n    # Finalize the outputs\n    outputs = self.finalize_output(outputs)\n    # Optionally save the results\n    if output_file is not None:\n        self.save_output(outputs, output_file)\n    # Return the results\n    return outputs\n\n", "CodeBleu_score": 43.67365189809476}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# TODO: add support for distributed evaluation\n    # TODO: add support for multiple GPUs\n    # TODO: add support for multiple dataloaders\n    # TODO: add support for multiple output files\n    # TODO: add support for multiple models\n    # TODO: add support for multiple metrics\n    # TODO: add support for multiple tasks\n    # TODO: add support for multiple output types\n    # TODO: add support for multiple datasets\n    # TODO: add support for multiple batch sizes\n    # TODO: add support for multiple model types\n    # TODO: add support for multiple loss functions\n    # TODO: add support for multiple optimizers\n    # TODO: add support for multiple learning rates\n    # TODO: add support for multiple learning rate schedulers\n    # TODO: add support for multiple weight decay\n    # TODO: add support for multiple optimizer parameters\n    # TODO: add support for multiple optimizer states\n    # TODO: add support for multiple grad norms\n    # TODO: add support for multiple grad clipping\n    # TODO: add support for multiple grad accumulation\n    # TODO: add support for multiple epochs\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate decay\n    # TODO: add support for multiple learning rate decay\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for multiple learning rate warmup\n    # TODO: add support for", "CodeBleu_score": 4.791666666666666}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t# TODO (huxu): implement this function.\n    pass\n\n", "CodeBleu_score": 0.8333333363857726}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n\n", "CodeBleu_score": 57.12217514941781}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tpass\n\n", "CodeBleu_score": 0.8333333333333389}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tpipeline = cfg.test_dataloader.pipeline\n    if 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    if 'img_id' in pipeline[0]['keys']:\n        pipeline[0]['keys'].remove('img_id')\n    for i, transform in enumerate(pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            break\n    else:\n        raise RuntimeError('The pipeline does not contain the '\n                           '\"LoadImageFromFile\" transform.')\n    pipeline[i]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 30.030660546169013}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tpipeline = cfg.test_pipeline\n    if isinstance(pipeline, list):\n        for i, pipe in enumerate(pipeline):\n            if 'img_id' in pipe['meta_keys']:\n                pipeline[i]['meta_keys'] = [key for key in pipe['meta_keys'] if key != 'img_id']\n            if 'LoadImageFromFile' in pipe['type']:\n                load_index = i\n                break\n        else:\n            raise KeyError('Cannot find the \"LoadImageFromFile\" transform in the test pipeline.')\n        pipeline[load_index]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline)\n    else:\n        raise TypeError('The test pipeline should be a list.')\n\n", "CodeBleu_score": 31.02644769173685}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t# Remove the 'img_id' key from meta_keys if present\n    if 'img_id' in cfg.test_dataloader.dataset.pipeline[0].meta_keys:\n        cfg.test_dataloader.dataset.pipeline[0].meta_keys.remove('img_id')\n    # Find the index of the 'LoadImageFromFile' transform\n    load_image_from_file_idx = -1\n    for i, transform in enumerate(cfg.test_dataloader.dataset.pipeline):\n        if transform.type == 'LoadImageFromFile':\n            load_image_from_file_idx = i\n            break\n    if load_image_from_file_idx == -1:\n        raise RuntimeError('The LoadImageFromFile transform must be present in the pipeline.')\n    # Update the transform type to 'mmdet.InferencerLoader'\n    cfg.test_dataloader.dataset.pipeline[load_image_from_file_idx].type = 'mmdet.InferencerLoader'\n    return Compose(cfg.test_dataloader.dataset.pipeline)\n\n", "CodeBleu_score": 34.351507626236206}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.93286372814753}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.886069757932077}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.886069757932077}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.886069757932077}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 29.064205322652782}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 29.064205322652782}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 29.064205322652782}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 23.245089001911083}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 23.245089001911083}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 23.245089001911083}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 22.896522834605044}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, mode=\"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 70.66356345490536}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, mode=\"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 70.66356345490536}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, mode=\"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 70.66356345490536}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        if item in vocab:\n            output.append(vocab[item])\n        else:\n            output.append(0)\n    return output\n\n", "CodeBleu_score": 62.32414605142138}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        if item in vocab:\n            output.append(vocab[item])\n        else:\n            output.append(vocab[unk_token])\n    return output\n\n", "CodeBleu_score": 61.24947631311275}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.22929475931163}, {"code": "def tokenize(self, text)\n\ttokens = self.basic_tokenizer.tokenize(text)\n    for i, token in enumerate(tokens):\n        tokens[i] = self.wordpiece_tokenizer.tokenize(token)\n    return tokens\n\n", "CodeBleu_score": 36.2335604453432}, {"code": "def tokenize(self, text)\n\traise NotImplementedError()\n\n", "CodeBleu_score": 2.044744036316641}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        return load(path, offset, duration, sample_rate, dtype)\n    except Exception as e:\n        logger.warning(f\"Failed to load audio from {path} at offset {offset} and duration {duration} with sample rate {sample_rate} and dtype {dtype}.\")\n        logger.warning(e)\n        return -1.0, True\n\n", "CodeBleu_score": 24.31398353363622}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        return load(path, offset, duration, sample_rate, dtype)\n    except Exception as e:\n        logger.error(f\"Exception {e} occurred while loading {path}.\")\n        return np.full(1, -1.0, dtype=np.float32), True\n\n", "CodeBleu_score": 20.27720803564257}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(\"Loading audio file %s\", path)\n        logger.info(\"Loading audio file with offset %s and duration %s\", offset, duration)\n        logger.info(\"Loading audio file with sample rate %s\", sample_rate)\n        logger.info(\"Loading audio file with dtype %s\", dtype)\n        return (\n            tf.cast(\n                tf.io.read_file(path),\n                dtype,\n            ),\n            False,\n        )\n    except Exception as e:\n        logger.error(\"Failed to load audio file %s\", path)\n        logger.error(\"Failed to load audio file with offset %s and duration %s\", offset, duration)\n        logger.error(\"Failed to load audio file with sample rate %s\", sample_rate)\n        logger.error(\"Failed to load audio file with dtype %s\", dtype)\n        logger.error(e)\n        return -1.0, True\n\n", "CodeBleu_score": 28.292951248852482}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels <= 0:\n        raise ValueError(\"n_channels must be a positive integer\")\n    if n_channels == 1:\n        return waveform\n    if waveform.shape[1] == n_channels:\n        return waveform\n    if waveform.shape[1] < n_channels:\n        return tf.tile(waveform, [1, n_channels])\n    return waveform[:, :n_channels]\n\n", "CodeBleu_score": 46.6357139372082}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels == waveform.shape[0]:\n        return waveform\n    if n_channels > waveform.shape[0]:\n        waveform = tf.tile(waveform, [n_channels // waveform.shape[0], 1])\n    waveform = waveform[:n_channels]\n    return waveform\n\n", "CodeBleu_score": 44.06432992604887}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels <= 0:\n        raise ValueError(\"n_channels must be positive\")\n    if n_channels == 1:\n        return waveform\n    if n_channels == 2:\n        return tf.stack([waveform, waveform], axis=0)\n    if waveform.shape[0] > n_channels:\n        return waveform[:n_channels]\n    if waveform.shape[0] < n_channels:\n        return tf.tile(waveform, [n_channels, 1])[:n_channels]\n    return waveform\n\n", "CodeBleu_score": 41.566352220344505}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tdB_spectrogram = gain_to_db(spectrogram, **kwargs)\n    dB_spectrogram_clipped = tf.clip_by_value(\n        dB_spectrogram, clip_value_min=-db_range, clip_value_max=0.0\n    )\n    return from_float32_to_uint8(dB_spectrogram_clipped)\n\n", "CodeBleu_score": 37.46917714636763}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tdB_spectrogram = gain_to_db(spectrogram, **kwargs)\n    dB_spectrogram_clipped = tf.clip_by_value(\n        dB_spectrogram,\n        clip_value_min=-db_range,\n        clip_value_max=db_range,\n    )\n    return from_float32_to_uint8(dB_spectrogram_clipped)\n\n", "CodeBleu_score": 38.48064899452459}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = tf.cast(gain_to_db(spectrogram, **kwargs), tf.float32)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, max_db)\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 43.75673996087082}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    # pylint: enable=line-too-long\n    window_fn = lambda length: tf.signal.hann_window(length, dtype=waveform.dtype)\n    window_fn = lambda length: tf.signal.hann_window(length, dtype=waveform.dtype)\n    window = window_fn(frame_length)\n    window = tf.pow(window, window_exponent)\n    window = tf.cast(window, waveform.dtype)\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n    )\n    stfts = tf.abs(stfts)\n    stfts = tf.pow(stfts, spec_exponent)\n    stfts = tf.transpose(stfts, [0, 2, 1])\n    return stfts\n\n", "CodeBleu_score": 41.570195254451114}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_length = frame_length * window_exponent\n    window = hann_window(window_length)\n    window = tf.cast(window, waveform.dtype)\n    window = tf.reshape(window, [1, window_length])\n    waveform = tf.cast(waveform, waveform.dtype)\n    waveform = tf.reshape(waveform, [1, -1])\n    waveform = tf.tile(waveform, [1, frame_length])\n    waveform = waveform * window\n    waveform = waveform[:, : frame_length // 2 + 1]\n    waveform = tf.reshape(waveform, [1, 1, frame_length // 2 + 1])\n    waveform = tf.tile(waveform, [1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, frame_length])\n    waveform = tf.reshape(waveform, [1, 1, frame_length, frame_length])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [1, 1, 1, 1])\n    waveform = tf.tile(waveform, [", "CodeBleu_score": 30.134448454897527}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_length = frame_length * window_exponent\n    window = hann_window(window_length)\n    window = window ** window_exponent\n    spectrogram = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda _: window,\n    )\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = spectrogram ** spec_exponent\n    spectrogram = tf.transpose(spectrogram, perm=[0, 2, 1])\n    return spectrogram\n\n", "CodeBleu_score": 36.81402255619788}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    if factor == 1.0:\n        return spectrogram\n    # pylint: enable=line-too-long\n    original_shape = tf.shape(spectrogram)\n    original_time_dim = original_shape[0]\n    original_freq_dim = original_shape[1]\n    original_channels_dim = original_shape[2]\n    original_time_length = tf.cast(original_time_dim, tf.float32)\n    original_freq_length = tf.cast(original_freq_dim, tf.float32)\n    new_time_length = original_time_length * factor\n    new_time_dim = tf.cast(tf.math.round(new_time_length), tf.int32)\n    new_freq_dim = original_freq_dim\n    new_freq_length = original_freq_length\n    new_shape = tf.stack([new_time_dim, new_freq_dim, original_channels_dim])\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [new_time_dim, new_freq_dim],\n        method=method,\n        antialias=True,\n    )\n    if new_time_length > original_time_length:\n        # Add padding to the end of the resized spectrogram\n        pad_size = new_time_dim - original_time_dim\n        pad_shape = tf.stack([pad_size, 0, 0])\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            paddings=pad_shape,\n            mode=\"CONSTANT\",\n            constant_values=0,\n        )\n    else:\n        # Crop the end of the resized spectrogram\n        crop_size = original_time_dim - new_time_dim\n        crop_shape = tf.stack([crop_size, 0, 0])\n        resized_spectrogram = tf.slice(\n            resized_spectrogram,", "CodeBleu_score": 39.0008255557015}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    \"\"\"\n    Parameters\n    ----------\n    spectrogram : tf.Tensor\n        Input spectrogram as a `(times x frequency)` tensor.\n    factor : float, optional\n        Factor to stretch the spectrogram.\n        Defaults to 1.0.\n    method : tf.image.ResizeMethod, optional\n        Interpolation method used for resizing.\n        Defaults to tf.image.ResizeMethod.BILINEAR.\n\n    Returns\n    -------\n    tf.Tensor\n        Time-stretched spectrogram as a `(times x frequency)` tensor.\n\n    \"\"\"\n    # pylint: enable=line-too-long\n    # pylint: disable=unused-argument\n    # pylint: enable=unused-argument\n    return spectrogram\n\n", "CodeBleu_score": 22.705868091082888}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n\n    if method not in (\n        tf.image.ResizeMethod.AREA,\n        tf.image.ResizeMethod.BICUBIC,\n        tf.image.ResizeMethod.BILINEAR,\n        tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n    ):\n        raise ValueError(\n            \"The method should be one of the following: AREA, BICUBIC, BILINEAR, NEAREST_NEIGHBOR\"\n        )\n\n    # Compute the new dimensions\n    new_time_dim = tf.cast(tf.math.round(spectrogram.shape[0] * factor), tf.int32)\n    new_freq_dim = spectrogram.shape[1]\n\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [new_time_dim, new_freq_dim],\n        method=method,\n        antialias=True,\n        preserve_aspect_ratio=False,\n    )\n\n    # Adjust the spectrogram to the original dimensions by cropping or padding\n    if spectrogram.shape[0] > new_time_dim:\n        spectrogram = spectrogram[:new_time_dim, :]\n    else:\n        spectrogram = tf.pad(\n            spectrogram,\n            paddings=[[0, new_time_dim - spectrogram.shape[0]], [0, 0]],\n            mode=\"CONSTANT\",\n        )\n\n    return spectrogram\n\n", "CodeBleu_score": 37.62465326865022}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tF = tf.shape(spectrogram)[1]\n    F_resized = tf.cast(tf.cast(F, tf.float32) * (2.0 ** semitone_shift), tf.int32)[0]\n    return tf.image.resize_images(\n        spectrogram, [tf.shape(spectrogram)[0], F_resized], method=method, align_corners=True\n    )\n\n", "CodeBleu_score": 31.267024266489322}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_shift = tf.cast(tf.cast(F, tf.float32) * tf.exp(semitone_shift * 12.0 / 12.0), tf.int32)[0]\n    spec_pitch_shifted = tf.image.resize_images(spectrogram, [T, F_shift], method=method, align_corners=True)\n    return tf.image.resize_image_with_crop_or_pad(spec_pitch_shifted, T, F)\n\n", "CodeBleu_score": 37.05433515232848}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif semitone_shift == 0:\n        return spectrogram\n    else:\n        resizing_factor = 2 ** (semitone_shift / 12)\n        resized_spectrogram = tf.image.resize_images(\n            spectrogram,\n            tf.cast(\n                tf.cast(tf.shape(spectrogram)[1], tf.float32) * resizing_factor,\n                tf.int32,\n            ),\n            method=method,\n            align_corners=True,\n        )\n        padded_spectrogram = tf.image.resize_image_with_crop_or_pad(\n            resized_spectrogram,\n            tf.shape(spectrogram)[0],\n            tf.shape(spectrogram)[1],\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 39.87538285147455}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(f\"Unknown convolutional activation function {params['conv_activation']}\")\n\n", "CodeBleu_score": 33.86198544588631}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tconv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    if conv_activation == \"elu\":\n        return ELU()\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\"Unknown conv activation function: {}\".format(conv_activation))\n\n", "CodeBleu_score": 36.44638162384365}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU(name=\"conv_activation\")\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU(name=\"conv_activation\")\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2, name=\"conv_activation\")\n    else:\n        raise ValueError(f\"Invalid conv_activation parameter: {params['conv_activation']}\")\n\n", "CodeBleu_score": 25.772734050232103}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 63.94265490077441}, {"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 63.94265490077441}, {"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 63.94265490077441}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self._release_path}/{self._checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"{url} returned {response.status_code}\")\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"{name} not found in {url}\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 37.562810236128364}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self._release}/{self.RELEASE_PATH}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Unable to retrieve checksum for {name} from {url}\")\n    data = response.json()\n    if name not in data:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return data[name]\n", "CodeBleu_score": 39.331299652438375}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self._release_path}/{self._checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Unable to retrieve checksum index from {url}.\")\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"{name} not found in checksum index.\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 37.56491896367766}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name + \".tar.gz\",\n        )\n    )\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    checksum: str = response.headers[\"X-Checksum\"]\n    with NamedTemporaryFile() as file:\n        logger.info(f\"Downloading {name} from {url}\")\n        with httpx.stream(\"GET\", url) as stream:\n            stream.raise_for_status()\n            for chunk in stream:\n                file.write(chunk)\n        file.flush()\n        file.seek(0)\n        actual_checksum: str = compute_file_checksum(file.name)\n        if actual_checksum != checksum:\n            raise RuntimeError(\n                f\"Checksum mismatch for {name}. Expected {checksum}, got {actual_checksum}\"\n            )\n        logger.info(f\"Extracting {name} to {path}\")\n        with tarfile.open(file.name, \"r:gz\") as archive:\n            archive.extractall(path)\n        logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 47.500392115280206}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    with NamedTemporaryFile() as temp:\n        with tarfile.open(fileobj=temp, mode=\"w:gz\") as archive:\n            archive.addfile(\n                tarfile.TarInfo(name=name),\n                io.BytesIO(response.content),\n            )\n        temp.flush()\n        temp.seek(0)\n        checksum: str = compute_file_checksum(temp.name)\n        if checksum != self.checksum(name):\n            raise ValueError(f\"Checksum mismatch for {name}\")\n        with tarfile.open(fileobj=temp, mode=\"r:gz\") as archive:\n            archive.extractall(path=path)\n    logger.info(f\"{name} extracted to {path}\")\n\n", "CodeBleu_score": 44.95721963399222}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"\".join(\n        (\n            self._host,\n            \"/\",\n            self._repository,\n            \"/\",\n            self.RELEASE_PATH,\n            \"/\",\n            self._release,\n            \"/\",\n            name,\n            \".tar.gz\",\n        )\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    response: httpx.Response = httpx.get(url, stream=True)\n    response.raise_for_status()\n    with NamedTemporaryFile() as temp_file:\n        for chunk in response.iter_bytes():\n            temp_file.write(chunk)\n        temp_file.flush()\n        checksum: str = compute_file_checksum(temp_file.name)\n        if checksum != self.checksum(name):\n            raise RuntimeError(\n                f\"Checksum mismatch for {name} (expected {self.checksum(name)}, got {checksum})\"\n            )\n        with tarfile.open(temp_file.name, \"r:gz\") as tar:\n            tar.extractall(path)\n    logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 46.95733071931361}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            with loader.path(resources, configuration_name) as configuration_file:\n                return json.load(configuration_file)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Embedded configuration {configuration_name} not found\")\n    else:\n        if not exists(descriptor):\n            raise SpleeterError(f\"Configuration file {descriptor} not found\")\n        with open(descriptor, \"r\") as configuration_file:\n            return json.load(configuration_file)\n\n", "CodeBleu_score": 64.33913620492154}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            with loader.path(resources, f\"{configuration_name}.json\") as configuration_file_path:\n                with open(configuration_file_path, \"r\") as configuration_file:\n                    return json.load(configuration_file)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Embedded configuration {configuration_name} not found\")\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as configuration_file:\n            return json.load(configuration_file)\n    else:\n        raise SpleeterError(f\"Configuration file {descriptor} not found\")\n\n", "CodeBleu_score": 63.68413147385421}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \"\")\n        try:\n            with loader.path(resources, f\"{name}.json\") as path:\n                return json.load(path)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Embedded configuration {name} not found\")\n    elif exists(descriptor):\n        with open(descriptor) as path:\n            return json.load(path)\n    else:\n        raise SpleeterError(f\"Configuration file {descriptor} not found\")\n\n", "CodeBleu_score": 64.6559077315438}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, got {}\".format(concat_axis)\n        )\n    if concat_axis == 1:\n        tensor_dict = {\n            key: tf.transpose(tensor, perm=[1, 0] + list(range(2, tensor.shape.rank)))\n            for key, tensor in tensor_dict.items()\n        }\n    tensor_dict = {\n        key: tf.concat([tensor for tensor in tensor_dict.values()], axis=concat_axis)\n        for key, tensor in tensor_dict.items()\n    }\n    tensor_dict = {\n        key: func(tensor) for key, tensor in tensor_dict.items()\n    }\n    if concat_axis == 1:\n        tensor_dict = {\n            key: tf.transpose(tensor, perm=[1, 0] + list(range(2, tensor.shape.rank)))\n            for key, tensor in tensor_dict.items()\n        }\n    tensor_dict = {\n        key: tf.split(tensor, len(tensor_dict), axis=concat_axis)\n        for key, tensor in tensor_dict.items()\n    }\n    return tensor_dict\n\n", "CodeBleu_score": 39.216513085616555}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, but got {}\".format(concat_axis)\n        )\n\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=0)\n    else:\n        tensor = tf.concat(tensor_list, axis=1)\n    processed_tensor = func(tensor)\n    if concat_axis == 0:\n        return {\n            key: processed_tensor[i : i + len(tensor_list[i])]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n    else:\n        return {\n            key: processed_tensor[:, i : i + len(tensor_list[i])]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n\n", "CodeBleu_score": 49.420247153252184}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in (0, 1):\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=0)\n    else:\n        tensor = tf.concat(tensor_list, axis=1)\n    processed_tensor = func(tensor)\n    if concat_axis == 0:\n        return {\n            key: processed_tensor[start : start + size]\n            for key, (start, size) in tensor_dict.items()\n        }\n    else:\n        return {\n            key: processed_tensor[:, start : start + size]\n            for key, (start, size) in tensor_dict.items()\n        }\n\n", "CodeBleu_score": 46.9638546736606}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = (255 * (tensor - min_value)) / (max_value - min_value)\n    scaled_tensor = tf.cast(scaled_tensor, tf.uint8)\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 49.721302300521316}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    normalized_tensor = (\n        (tensor - min_value) / (max_value - min_value) * 255.0\n    )  # type: ignore\n    normalized_tensor = tf.cast(normalized_tensor, tf.uint8)  # type: ignore\n    return {\n        tensor_key: normalized_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.555518237458045}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            tf.round(\n                tf.multiply(\n                    tf.subtract(\n                        tf.cast(tensor, tf.float32), tf.cast(min_value, tf.float32)\n                    ),\n                    255.0 / tf.cast(tf.subtract(max_value, min_value), tf.float32),\n                )\n            ),\n            tf.uint8,\n        ),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 41.94282441354551}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    \"\"\"\n    Parameters\n    ----------\n    tensor: tf.Tensor\n        A tensor of shape [batch_size, time_length, feature_length]\n    segment_len: int\n        The length of each segment\n\n    Returns\n    -------\n    tf.Tensor\n        A tensor of shape [batch_size, segment_len, time_length, feature_length]\n    \"\"\"\n    # pylint: enable=line-too-long\n    batch_size, time_length, feature_length = tf.shape(tensor)[0], tf.shape(tensor)[1], tf.shape(tensor)[2]\n    padding_length = segment_len - (time_length % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, 0], [0, padding_length], [0, 0]])\n    reshaped_tensor = tf.reshape(\n        padded_tensor,\n        [\n            batch_size,\n            segment_len,\n            time_length // segment_len,\n            feature_length,\n        ],\n    )\n    return reshaped_tensor\n\n", "CodeBleu_score": 26.924754603351985}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_shape = tf.shape(tensor)\n    num_frames = tensor_shape[0]\n    padding = tf.math.floormod(num_frames, segment_len)\n    if padding != 0:\n        padding = segment_len - padding\n    tensor = tf.pad(tensor, [[0, padding], [0, 0], [0, 0]])\n    tensor_shape = tf.shape(tensor)\n    num_frames = tensor_shape[0]\n    num_segments = tf.math.floordiv(num_frames, segment_len)\n    tensor = tf.reshape(tensor, [num_segments, segment_len, -1])\n    return tensor\n\n", "CodeBleu_score": 41.197803292901874}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    \"\"\"\n    Parameters\n    ----------\n    tensor : tf.Tensor\n        The tensor to be padded and partitioned.\n    segment_len : int\n        The segment length.\n\n    Returns\n    -------\n    tf.Tensor\n        The padded and partitioned tensor.\n    \"\"\"\n    # pylint: enable=line-too-long\n    tensor_shape = tf.shape(tensor)\n    padded_tensor = tf.pad(tensor, [[0, 0], [0, 0], [0, segment_len - 1]])\n    padded_tensor = tf.reshape(\n        padded_tensor,\n        [\n            tensor_shape[0],\n            tensor_shape[1],\n            tf.math.ceil(tensor_shape[2] / segment_len),\n            segment_len,\n        ],\n    )\n    return padded_tensor\n\n", "CodeBleu_score": 24.235186433270272}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tpad_size = tf.math.floormod(frame_length - instr_spec.shape[1], frame_length)\n    pad = tf.pad(instr_spec, [[0, pad_size]] + [[0, 0]] * (len(instr_spec.shape) - 1))\n    return tf.reshape(pad, tf.concat([[F, frame_length], instr_spec.shape[1:]], axis=0))\n\n", "CodeBleu_score": 13.522130379754241}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tif F > frame_length:\n        raise ValueError(\"frame_length must be greater than F\")\n    if frame_length % F != 0:\n        raise ValueError(\"frame_length must be a multiple of F\")\n    pad_size = frame_length - F\n    padded = tf.pad(instr_spec, [[0, pad_size], [0, 0]])\n    return tf.reshape(padded, [frame_length // F, F, instr_spec.shape[1]])\n\n", "CodeBleu_score": 16.59274840595413}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tpad_length = F - (instr_spec.shape[0] % F)\n    if pad_length != F:\n        instr_spec = tf.pad(instr_spec, [[0, pad_length], [0, 0]], constant_values=0)\n    return tf.reshape(instr_spec, (instr_spec.shape[0] + instr_spec.shape[1] - 1) // frame_length * frame_length, frame_length, instr_spec.shape[2])\n\n", "CodeBleu_score": 18.057729483885204}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices({name: tf.constant(values) for name, values in df.to_dict().items()})\n\n", "CodeBleu_score": 31.4826371839997}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n    return dataset_from_dataframe(df, **kwargs)\n\n", "CodeBleu_score": 17.793066251861937}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(dict(zip(df.columns, df.values)))\n\n", "CodeBleu_score": 21.58057542342904}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for index, target_dimension in enumerate(target_shape):\n        if target_dimension is not None and tensor_tf.shape[index] != target_dimension:\n            result = False\n    return result\n\n", "CodeBleu_score": 32.08049167257121}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for target_dimension, tensor_dimension in zip(target_shape, tensor_tf.shape):\n        if target_dimension is not None:\n            result = result and (tensor_dimension == target_dimension)\n    return result\n\n", "CodeBleu_score": 27.332019541444446}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for target_dimension, tensor_dimension in zip(target_shape, tensor_tf.shape):\n        if target_dimension is not None and target_dimension != tensor_dimension:\n            result = False\n            break\n    return result\n\n", "CodeBleu_score": 28.989476776067676}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tdataset_builder = DatasetBuilder(audio_params, audio_adapter)\n    dataset_builder.build(\n        train_csv=audio_path,\n        cache_dir=None,\n        batch_size=16,\n        num_chunks_per_song=1,\n        cache_dataset=False,\n        augment_data=False,\n        shuffle_buffer_size=1000,\n        shuffle_seed=1234,\n        num_threads=16,\n        prefetch_buffer_size=16,\n    )\n    return dataset_builder.dataset\n\n", "CodeBleu_score": 24.58747412797041}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tlogger.info(\"Building training dataset\")\n    builder = DatasetBuilder(audio_params, audio_adapter)\n    builder.build(\n        train_csv=audio_path + \"train.csv\",\n        cache_dir=audio_path + \"cache\",\n        batch_size=16,\n        chunks_per_song=1,\n        augmentation_options={\n            \"time_stretch\": {\"min_rate\": 0.8, \"max_rate\": 1.2},\n            \"pitch_shift\": {\"min_semitones\": -24, \"max_semitones\": 24},\n        },\n        cache_options={\"enable\": True, \"cache_dir\": audio_path + \"cache\"},\n    )\n    return builder.dataset\n\n", "CodeBleu_score": 35.27409940888243}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tdataset_builder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, audio_path\n    )\n    return dataset_builder.build(\n        train_csv=os.path.join(audio_path, \"train.csv\"),\n        cache_dir=os.path.join(audio_path, \"cache\"),\n        batch_size=8,\n        num_chunks_per_song=10,\n        augmentation_options={\n            \"time_stretch\": True,\n            \"pitch_shift\": True,\n            \"reverb\": True,\n            \"noise\": True,\n            \"background\": True,\n        },\n        cache_options={\"enabled\": True, \"cache_dir\": os.path.join(audio_path, \"cache\")},\n    )\n\n", "CodeBleu_score": 42.753349379608274}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t# pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=line-too-long\n    # pylint: disable=invalid-name\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too", "CodeBleu_score": 7.202380952380952}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_name\"], params[\"model_weights_url\"])\n    model_provider.load_model_directory()\n    params.update(model_provider.get_parameters())\n    params[\"model_dir\"] = model_provider.get_model_directory()\n    params[\"session_config\"] = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n    )\n    params[\"run_config\"] = tf.estimator.RunConfig(\n        session_config=params[\"session_config\"]\n    )\n    model_fn = model_fn if not MWF else model_fn_MWF\n    return tf.estimator.Estimator(model_fn=model_fn, **params)\n\n", "CodeBleu_score": 34.541735148041916}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_dir\"])\n    model_provider.update_params(params)\n    params = model_provider.get_params()\n    params[\"model_dir\"] = model_provider.get_model_dir()\n    model_fn = model_provider.get_model_fn()\n    if MWF:\n        model_fn = model_fn.model_fn_mwf\n    session_conf = tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=False,\n        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5),\n    )\n    run_config = tf.estimator.RunConfig(\n        session_config=session_conf,\n        save_checkpoints_steps=1000,\n        keep_checkpoint_max=100,\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=run_config\n    )\n\n", "CodeBleu_score": 43.54358291519904}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform, audio_descriptor.sample_rate)\n    audio_id = audio_descriptor.audio_id\n    self._data_generator.update_data((waveform, audio_id))\n    prediction = next(self._get_prediction_generator())\n    return {\n        key: value\n        for key, value in prediction.items()\n        if key != \"audio_id\"\n    }\n\n", "CodeBleu_score": 37.830976259169816}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t# pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements", "CodeBleu_score": 12.372448979591837}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t# Ensure waveform is stereo\n    if waveform.ndim == 1:\n        waveform = to_stereo(waveform)\n    # Create prediction generator\n    prediction_generator = self._get_prediction_generator()\n    # Update data generator\n    self._data_generator.update_data((waveform, audio_descriptor.id))\n    # Generate prediction\n    prediction = next(prediction_generator)\n    # Remove audio_id\n    return prediction\n", "CodeBleu_score": 53.90491891702132}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.BACKBONE.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone \\\"{}\\\" not supported\".format(config.BACKBONE)\n        return compute_resnet_backbone_shapes(config, image_shape)\n\n", "CodeBleu_score": 46.081262386153185}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.BACKBONE.compute_backbone_shapes(image_shape)\n    elif config.BACKBONE == \"resnet50\":\n        return resnet_backbone.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES)\n    elif config.BACKBONE == \"resnet101\":\n        return resnet_backbone.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES)\n    else:\n        raise ValueError(\"Unsupported backbone type: {}\".format(config.BACKBONE))\n\n", "CodeBleu_score": 42.10502142889169}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.BACKBONE.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone must be either resnet50 or resnet101\"\n    if config.BACKBONE == \"resnet50\":\n        return ResNet.compute_backbone_shapes(config, image_shape)\n    elif config.BACKBONE == \"resnet101\":\n        return ResNet101.compute_backbone_shapes(config, image_shape)\n    else:\n        raise ValueError(\"Backbone must be either resnet50 or resnet101\")\n\n", "CodeBleu_score": 38.24173785773091}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t# Reset the graph\n    tf.reset_default_graph()\n    # Define the input layer with the shape of the training data\n    inputs = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n    # The model consists of two convolutional layers with ReLU activation, followed by a max-pooling layer\n    x = keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    # The output is flattened and passed through a dense layer with ReLU activation, followed by a dense layer with softmax activation for classification\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    # Compile the model\n    model = keras.models.Model(inputs, outputs)\n    model.compile(optimizer=keras.optimizers.Adam(),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 31.913449240139148}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the graph\n    tf.reset_default_graph()\n\n    # Create the input placeholder\n    x = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='input')\n\n    # Create the model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n\n    # Compile the model\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 30.240053386281225}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the graph and define the input layer with the shape of the training data\n    tf.reset_default_graph()\n    input_layer = Input(shape=x_train.shape[1:])\n\n    # Define the model layers\n    conv_layer1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(input_layer)\n    conv_layer1 = Activation('relu')(conv_layer1)\n    conv_layer2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(conv_layer1)\n    conv_layer2 = Activation('relu')(conv_layer2)\n    pool_layer = MaxPooling2D(pool_size=(2, 2))(conv_layer2)\n    flatten_layer = Flatten()(pool_layer)\n    dense_layer1 = Dense(units=128, activation='relu')(flatten_layer)\n    dense_layer2 = Dense(units=num_classes, activation='softmax')(dense_layer1)\n\n    # Compile the model\n    model = Model(inputs=input_layer, outputs=dense_layer2, name='model')\n    model.compile(optimizer=keras.optimizers.Adam(lr=0.0001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 38.42241176952782}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t# If no masks were found, return an empty list\n    if mask.shape[0] == 0:\n        return []\n\n    # Or if the masks layer is empty for this image\n    if (mask.sum() == 0).all():\n        return []\n\n    # We'll do everything on HSV space to get better results\n    # First convert the image to HSV format\n    image = skimage.color.rgb2hsv(mask[0])\n    # Split the image to 3 different channels\n    # The order is: Hue, Saturation, Value\n    h, s, v = cv2.split(image)\n\n    # Threshold the Saturation channel\n    s_thresh_min = 170\n    s_thresh_max = 255\n    s = (s >= s_thresh_min) & (s <= s_thresh_max)\n\n    # Threshold the Value channel\n    v_thresh_min = 20\n    v_thresh_max = 255\n    v = (v >= v_thresh_min) & (v <= v_thresh_max)\n\n    # Combine the thresholds\n    thresh = s & v\n\n    # Find contours of 'thresh' which will give us the outlines of the shapes\n    # We'll end up with a list of (n, 1, 2) CContours\n    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Initialize an array to store bounding boxes\n    bounding_boxes = []\n\n    # For each contour, find the bounding box\n    for contour in contours:\n        # Find the bounding box for the contour\n        x, y, w, h = cv2.boundingRect(contour)\n        bounding_boxes.append([x, y, x + w, y + h])\n\n    # Return the bounding boxes array\n    return bounding_boxes\n\n", "CodeBleu_score": 15.951140948180853}, {"code": "def extract_bboxes(mask)\n\tmask = np.where(mask, 1, 0)\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.50457163671102}, {"code": "def extract_bboxes(mask)\n\t# Masks are packed into a three dimensional array,\n    # masks[height][width][instance]\n    # We'll flatten this into two dimensional array\n    # masks[height * width * instance][mask_id]\n    mask = mask.permute(1, 2, 0)\n    mask = mask.contiguous().view(mask.shape[0], -1)\n    # Get the index of the first and last 1 in each mask\n    # We use these two indices to determine the bounding box\n    idxs = np.where(np.any(mask, axis=1))[0]\n    if len(idxs) == 0:\n        return np.array([[0, 0, 0, 0]])\n    idxs = np.concatenate(([0], idxs, [mask.shape[1]]))\n    bboxes = np.vstack([mask[:, idxs[i]:idxs[i + 1]].argmax(1) for i in range(len(idxs) - 1)])\n    bboxes = bboxes.transpose()\n    bboxes = np.vstack([np.hstack([bboxes[:, 0] - bboxes[:, 2] // 2, bboxes[:, 0] + bboxes[:, 2] // 2]),\n                        np.hstack([bboxes[:, 1] - bboxes[:, 3] // 2, bboxes[:, 1] + bboxes[:, 3] // 2])]).transpose()\n    bboxes = np.vstack([bboxes.min(1), bboxes.max(1)])\n    return bboxes\n\n", "CodeBleu_score": 33.57035635372883}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Initialize the intersection and union areas to zero\n    iou = np.zeros((boxes.shape[0]))\n    # Calculate the intersection areas\n    for i in range(boxes.shape[0]):\n        # Calculate the intersection area\n        intersection = np.minimum(box[2], boxes[i, 2]) - np.maximum(box[0], boxes[i, 0])\n        intersection = np.maximum(intersection, 0)\n        # Calculate the union area\n        union = box_area + boxes_area[i] - intersection\n        # Calculate the IoU\n        iou[i] = intersection / union\n    return iou\n\n", "CodeBleu_score": 35.5306045012296}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Initialize the intersection and union variables\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n\n    # Calculate the area of the intersection rectangles\n    intersection_area = np.maximum((x2 - x1 + 1), 0) * np.maximum((y2 - y1 + 1), 0)\n\n    # Calculate the area of the union rectangles\n    union_area = box_area + boxes_area[:] - intersection_area[:]\n\n    # Compute the IoU\n    iou = intersection_area / union_area\n\n    return iou\n\n", "CodeBleu_score": 41.999249586790775}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Initialize the intersection and union areas\n    iou = np.zeros((boxes.shape[0]))\n    # Calculate the intersection areas\n    inter = np.zeros((boxes.shape[0]))\n    for i in range(boxes.shape[0]):\n        inter[i] = compute_intersection(box, boxes[i])\n    # Calculate the union areas\n    for i in range(boxes.shape[0]):\n        union = (box_area + boxes_area[i] - inter[i])\n        iou[i] = inter[i] / union\n    return iou\n", "CodeBleu_score": 22.202772870774385}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t# Compute areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    box_area = np.expand_dims(box_area, 1)\n    box_area = np.repeat(box_area, boxes2.shape[0], axis=1)\n    box_area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    box_area2 = np.expand_dims(box_area2, 0)\n    box_area2 = np.repeat(box_area2, boxes1.shape[0], axis=0)\n    # Compute overlaps to generate matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        overlaps[:, i] = compute_iou(boxes1, boxes2[i], box_area, box_area2[i])\n    return overlaps\n\n", "CodeBleu_score": 46.54602532817423}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# Compute areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    box_area = np.expand_dims(box_area, 1)\n    box_area = np.repeat(box_area, boxes2.shape[0], axis=1)\n    box_area = np.reshape(box_area, [boxes1.shape[0], boxes2.shape[0]])\n    box_area = np.transpose(box_area)\n    box_area = np.reshape(box_area, [boxes2.shape[0], boxes1.shape[0]])\n    box_area = np.repeat(box_area, boxes2.shape[0], axis=1)\n    box_area = np.reshape(box_area, [boxes2.shape[0], boxes2.shape[0]])\n    # Compute overlaps to generate matrix\n    overlaps = np.zeros(box_area.shape)\n    for i in range(overlaps.shape[0]):\n        for j in range(overlaps.shape[1]):\n            overlaps[i, j] = compute_iou(boxes2[j], boxes1, box_area[i, j], box_area[j, i])\n    return overlaps\n\n", "CodeBleu_score": 32.66821548927056}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# Calculate areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    box_area = np.expand_dims(box_area, 1)\n    box_area = np.repeat(box_area, boxes2.shape[0], axis=1)\n    box_area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    box_area2 = np.repeat(box_area2, boxes1.shape[0], axis=0)\n    # Initialize overlaps matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # Calc overlaps\n    for i in range(overlaps.shape[1]):\n        overlaps[:, i] = compute_iou(boxes1, boxes2[i], box_area, box_area2[i])\n    return overlaps\n\n", "CodeBleu_score": 47.20595149946663}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\tassert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure(figsize=(1.8 * n_images * cols, 2.4 * cols))\n    for i in range(n_images):\n        fig.add_subplot(cols, np.ceil(n_images / float(cols)), i + 1)\n        if cmap is None:\n            plt.imshow(images[i], cmap=cmap, norm=norm,\n                       interpolation=interpolation)\n        else:\n            plt.imshow(images[i])\n        plt.title(titles[i])\n        plt.axis('off')\n    fig.tight_layout()\n    fig.show()\n\n", "CodeBleu_score": 31.359301478645275}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\tassert ((titles is None) or (len(images) == len(titles)))\n    n_ims = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_ims + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_ims / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        a.set_title(title)\n        if norm is not None:\n            image = (norm(image.astype(np.float32)) * 255).astype(np.uint8)\n        if cmap is not None:\n            plt.imshow(image, cmap=cmap, interpolation=interpolation)\n        else:\n            plt.imshow(image, cmap='gray', interpolation=interpolation)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n    plt.show()\n\n", "CodeBleu_score": 34.76552906376235}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t# Number of images\n    n_images = len(images)\n    # Check if titles are provided\n    if titles:\n        # If titles are provided, check if there are enough titles\n        if len(titles) < n_images:\n            # If there are not enough titles, add blank titles\n            titles += [''] * (n_images - len(titles))\n    else:\n        # If no titles are provided, make a list of blank titles\n        titles = [''] * n_images\n    # Calculate number of rows and columns\n    n_rows = n_images // cols\n    # Create figure\n    fig = plt.figure(figsize=(14, 14))\n    # Generate subplots\n    for i in range(n_images):\n        # Add subplot\n        ax = fig.add_subplot(n_rows, cols, i + 1)\n        # Display image\n        ax.imshow(images[i], cmap=cmap, norm=norm,\n                  interpolation=interpolation)\n        # Add title\n        ax.set_title(titles[i])\n        # Remove ticks\n        ax.set_xticks([])\n        ax.set_yticks([])\n    # Display figure\n    plt.show()\n\n", "CodeBleu_score": 34.65707678103871}]}
