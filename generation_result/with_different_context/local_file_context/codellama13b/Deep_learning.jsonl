{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\tglobal cached_ldsr_model\n    if self.config.model.cache_model and cached_ldsr_model is not None:\n        model = cached_ldsr_model\n        print(\"Using cached model\")\n    else:\n        if self.config.model.path.endswith(\".safetensors\"):\n            model = safetensors.torch.load(self.config.model.path)\n        else:\n            model = torch.load(self.config.model.path, map_location=self.device)\n        model.eval()\n        if half_attention:\n            model.half()\n        if self.config.model.cache_model:\n            cached_ldsr_model = model\n    return model\n\n", "CodeBleu_score": 26.743136165989167}, {"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    if self.cached_ldsr_model is not None:\n        # Check if the cached model is valid\n        if self.cached_ldsr_model.valid:\n            # Load the cached model\n            self.model = self.cached_ldsr_model\n            # Set the model to evaluation mode\n            self.model.eval()\n            # Return the model\n            return self.model\n\n    # Load the model from the specified path\n    if self.model_path.endswith(\".safetensors\"):\n        # Load the model from a safetensors file\n        self.model = torch.load(self.model_path, map_location=self.device)\n    else:\n        # Load the model from a PyTorch-compatible file\n        self.model = torch.load(self.model_path, map_location=self.device)\n\n    # Configure the model using settings from a YAML file\n    self.model = instantiate_from_config(self.model_config)\n\n    # Move the model to the appropriate device\n    self.model.to(self.device)\n\n    # Set the model to evaluation mode\n    self.model.eval()\n\n    # Optionally, set the model to use half-precision attention\n    if half_attention:\n        self.model.half_attention()\n\n    # Optionally, optimize the memory format of the model\n    if self.memory_format == \"channels_last\":\n        self.model = self.model.to_channels_last()\n\n    # Optionally, cache the loaded model for future use\n    if self.cache:\n        self.cached_ldsr_model = self.model\n\n    # Return the model\n    return self.model\n\n", "CodeBleu_score": 27.397874310305646}, {"code": "def load_model_from_config(self, half_attention)\n\tglobal cached_ldsr_model\n    if cached_ldsr_model is not None:\n        if half_attention:\n            cached_ldsr_model.half()\n        return {\"model\": cached_ldsr_model}\n\n    # Load the model\n    model_path = self.config.model_path\n    if model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load(model_path)\n    else:\n        model = torch.load(model_path, map_location=devices.map_location)\n\n    # Set model to eval mode\n    model.eval()\n\n    # Move model to correct device\n    model = model.to(devices.device)\n\n    # Half-precision attention\n    if half_attention:\n        model.half()\n\n    # Memory format optimization\n    if self.config.memory_format_opt:\n        model = shared.MemoryFormatOptimizedModel(model)\n\n    # Cache the model\n    cached_ldsr_model = model\n    return {\"model\": model}\n\n", "CodeBleu_score": 32.7185723010642}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tPath)\n            config.model.target = \\\"ldm.models.diffusion.ddpm.LatentDiffusionV1\\\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\\\"model\\\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\\\"image\\\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\\\"ks\\\": (ks, ks), \\\"stride\\\": (stride, stride),\n                                        \\\"vqf\\\": vqf,\n                                        \\\"patch_distributed_vq\\\": True,\n                                        \\\"tie_braker\\\": False,\n                                        \\\"clip_max_weight\\\": 0.5,\n                                        \\\"clip_min_weight\\\": 0.01,\n                                        \\\"clip_max_tie_weight\\\": 0.5,\n                                        \\\"clip_min_tie_weight\\\": 0.01}\n        else:\n            if hasattr(model, \\\"split_input_params\\\"):\n                delattr(model, \\\"split_input_params\\\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\\\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\\\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\\\"model\\\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\\\"sample\\\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n", "CodeBleu_score": 31.60717998397481}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif self.use_ema and self.model_ema is not None:\n        self.model_ema.update(self.model)\n        if context is not None:\n            print(f\"{context}: Using EMA weights.\")\n        yield\n        if context is not None:\n            print(f\"{context}: Restoring original weights.\")\n        self.model_ema.restore(self.model)\n    else:\n        yield\n\n", "CodeBleu_score": 58.551962600465714}, {"code": "def ema_scope(self, context=None)\n\tif not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = self\n    with self.model_ema.apply_and_restore_weights(context):\n        yield\n\n", "CodeBleu_score": 19.391190688769587}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema and self.model_ema is not None:\n        self.model_ema.update(self.model)\n        if context is not None:\n            print(f\"{context}: Using EMA weights.\")\n        with self.model_ema.apply_shadow_weights():\n            yield\n        if context is not None:\n            print(f\"{context}: Restored original weights.\")\n    else:\n        yield\n\n", "CodeBleu_score": 55.31833670819461}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif not os.path.exists(path):\n        raise FileNotFoundError(f'Missing checkpoint: {path}')\n    print(f'Loading model from {path}')\n    checkpoint = torch.load(path, map_location='cpu')\n    state_dict = checkpoint['state_dict']\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            print(f'Deleting key from state_dict: {k}')\n            del state_dict[k]\n    unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    missing_keys = set(self.state_dict().keys()) - set(state_dict.keys())\n    print(f'Missing keys: {missing_keys}')\n    print(f'Unexpected keys: {unexpected_keys}')\n\n", "CodeBleu_score": 41.00634258626369}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif not os.path.exists(path):\n        raise IOError(f\"{path} does not exist\")\n    state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    unexpected_keys = []\n    missing_keys = []\n    model_dict = self.state_dict()\n    for k, v in state_dict.items():\n        if k in model_dict and v.shape != model_dict[k].shape:\n            unexpected_keys.append(k)\n        elif k not in model_dict:\n            missing_keys.append(k)\n    if unexpected_keys:\n        print(f\"WARNING: Unexpected key(s) in state_dict: {unexpected_keys}\")\n    if missing_keys:\n        print(f\"WARNING: Missing key(s) in state_dict: {missing_keys}\")\n    self.load_state_dict(state_dict, strict=False)\n\n", "CodeBleu_score": 41.51993849202259}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif not os.path.exists(path):\n        raise FileNotFoundError(f'Missing checkpoint at {path}')\n    state_dict = torch.load(path, map_location='cpu')['state_dict']\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            state_dict.pop(k, None)\n    self.load_state_dict(state_dict, strict=False)\n    print('Restored model from {}'.format(path))\n    print('Restored model with {} parameters'.format(sum(p.numel() for p in self.parameters())))\n    print('Restored model with {} buffers'.format(len(list(self.buffers()))))\n    print('Restored model with {} modules'.format(len(list(self.modules()))))\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # compare state_dicts\n    for k, p in self.state_dict().items():\n        if k in state_dict.keys():\n            if p.shape != state_dict[k].shape:\n                error_msgs.append('mismatched key {}: {} vs {}'.format(k, p.shape, state_dict[k].shape))\n                continue\n        else:\n            missing_keys.append(k)\n    for k in state_dict.keys():\n        if k not in self.state_dict().keys():\n            unexpected_keys.append(k)\n    if len(missing_keys) > 0:\n        print('Missing Keys: {}'.format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print('Unexpected Keys: {}'.format(unexpected_keys))\n    if len(error_msgs) > 0:\n        raise RuntimeError('Error(s) in loading state_dict for {}:", "CodeBleu_score": 36.951826946006236}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\tquant, emb_loss, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        pred_indices = info[\\\"pred_indices\\\"]\n        return dec, emb_loss, pred_indices\n    return dec, emb_loss\n\n", "CodeBleu_score": 35.58506106585012}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, quant\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 50.32552376744825}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, self.quantize.pred_indices\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 46.87517563726943}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if self.batch_resize_range is not None:\n        x = F.interpolate(x, size=self.batch_resize_range.get(self.current_epoch, x.shape[-1]), mode=\"bicubic\")\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    return x.detach()\n\n", "CodeBleu_score": 38.30377076501716}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if self.batch_resize_range is not None and self.training:\n        resize_factor = self.batch_resize_range[0] + (\n            self.batch_resize_range[1] - self.batch_resize_range[0]\n        ) * self.trainer.global_step / self.trainer.max_steps\n        x = F.interpolate(x, scale_factor=resize_factor, mode=\"bicubic\")\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    return x.detach()\n\n", "CodeBleu_score": 47.039935304487265}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if self.batch_resize_range is not None:\n        resize_factor = float(\n            self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * self.trainer.global_step / self.trainer.max_steps\n        )\n        x = F.interpolate(x, scale_factor=resize_factor, mode=\"bicubic\", align_corners=False)\n    return x.permute(0, 2, 3, 1).contiguous().float().detach()\n\n", "CodeBleu_score": 38.70375302902585}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    if self.use_ema:\n        self.model_ema.store(self.parameters())\n        self.model_ema.copy_to(self)\n    with torch.enable_grad():\n        pred, diff, *_ = self(x)\n        if optimizer_idx == 0:\n            loss = self.loss(pred, x, diff, self.global_step, optimizer_idx)\n            if self.monitor is not None:\n                self.log(self.monitor, loss, on_step=True, on_epoch=False)\n            return loss\n        else:\n            loss = self.loss(pred, x, diff, self.global_step, optimizer_idx)\n            return loss\n    if self.use_ema:\n        self.model_ema.restore(self.parameters())\n\n\n", "CodeBleu_score": 32.78055581332763}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    if self.current_epoch < self.learn_in_warmup:\n        # do not backprop in warmup\n        xrec = xrec.detach()\n        qloss = qloss.detach()\n    if optimizer_idx == 0:\n        loss = self.loss(qloss, xrec, x)\n    elif optimizer_idx == 1:\n        with torch.no_grad():\n            xrec, qloss = self(x, return_pred_indices=True)\n        loss = self.loss(qloss, xrec, x, disc_loss=True)\n    else:\n        raise ValueError(f\\\"optimizer_idx {optimizer_idx} not recognized\\\")\n    self.log_dict({\n        \\\"train/loss\\\": loss,\n        \\\"train/nll_loss\\\": qloss.mean(),\n        \\\"train/kl_loss\\\": -torch.mean(torch.sum(qloss, dim=1) - torch.logsumexp(torch.sum(qloss, dim=1), dim=1)),\n        \\\"lr\\\": self._optimizers[optimizer_idx].param_groups[0]['lr'],\n    })\n    return loss\n\n", "CodeBleu_score": 40.15299478762107}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, xrec_loss = self(x)\n    if optimizer_idx == 0:\n        # autoencode\n        loss = xrec_loss\n        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train/nll_loss\", -loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train/bpd\", loss / (np.log(2) * x.shape[1] * x.shape[2] * x.shape[3]), on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    else:\n        # discriminator\n        with self.ema_scope(\"train_step\"):\n            xrec_loss = self(xrec)\n        loss = xrec_loss\n        self.log(\"train/disc_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n", "CodeBleu_score": 29.725782711878164}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tn\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\\\"train\\\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\\\"_ema\\\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\\\"\\\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\\\"val\\\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\\\"val\\\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log(f\\\"val{suffix}/rec_loss\\\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\\\"val{suffix}/aeloss\\\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\\\"val{suffix}/rec_loss\\\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\\\"lr_d\\\", lr_d)\n        print(\\\"lr_g\\\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\\\"Setting up LambdaLR scheduler...\\\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n\n\n", "CodeBleu_score": 29.15260645847016}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}, {"code": "def get_loss(self, pred, target, mean=True)\n\tsqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\\\"Deleting key {} from state_dict.\\\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 25.48945547073781}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t               # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \\\"split_input_params\\\"):\n            if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n                ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n\n\n", "CodeBleu_score": 32.2492033205215}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}, {"code": "def get_codebook_entry(self, indices, shape)\n\tand associated documentation files (the \\\"Software\\\"), to deal", "CodeBleu_score": 2.0962640213325776}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}.\")\n    elif factor == 0:\n        raise ValueError(f\"The factor {factor} must be positive.\")\n    else:\n        for i in range(1, dimension//2+1):\n            if dimension % i == 0:\n                return i, dimension//i\n\n", "CodeBleu_score": 24.892663895108587}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        for i in range(1, dimension):\n            if dimension % i == 0 and i <= factor:\n                return i, dimension // i\n        return 1, dimension\n    else:\n        return 1, dimension\n\n", "CodeBleu_score": 25.912343416139482}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}.\")\n    else:\n        if factor == -1:\n            factor = dimension\n        for i in range(1, dimension):\n            if dimension % i == 0:\n                if i <= factor:\n                    return i, dimension // i\n                else:\n                    return factor, dimension // factor\n        return dimension, 1\n\n", "CodeBleu_score": 32.331128533107275}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t# TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same device as the input weights\n    if orig_weight.device != updown.device:\n        orig_weight = orig_weight.to(updown.device)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same data type as the input weights\n    if orig_weight.dtype != updown.dtype:\n        orig_weight = orig_weight.to(updown.dtype)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.device)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.dtype)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.device)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.dtype)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.device)\n\n    # TODO: this is a hack to deal with the fact that the original weights are sometimes not in the same shape as the input weights\n    if orig_weight.shape != updown.shape:\n        orig_weight = orig_weight.to(updown.dtype)\n\n    # TODO: this is a hack to deal with the fact that the", "CodeBleu_score": 23.16118268469776}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t# TODO: This is a bit of a hack, but it's needed because the original weight is sometimes a list, and sometimes a tensor.\n    if isinstance(orig_weight, list):\n        orig_weight = orig_weight[0]\n\n    # Make sure that the weights are on the same device and have the same data type.\n    if orig_weight.device != self.device or orig_weight.dtype != self.dtype:\n        orig_weight = orig_weight.to(self.device, self.dtype)\n\n    # Combine the input weight with the original weight.\n    combined_weight = self.weight_decompose_scale * updown + orig_weight\n\n    # Normalize the combined weight.\n    combined_weight = combined_weight / combined_weight.norm(2, dim=self.weight_decompose_norm_dims, keepdim=True)\n\n    # Scale the combined weight.\n    scaled_combined_weight = combined_weight * self.calc_scale()\n\n    # Calculate the adjusted weight by subtracting the original weight from the scaled and normalized combined weight.\n    adjusted_weight = scaled_combined_weight - orig_weight\n\n    return adjusted_weight\n\n", "CodeBleu_score": 30.590329205704048}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif not self.decompose:\n        return orig_weight\n\n    if self.dora_scale is None:\n        self.dora_scale = self.calc_scale()\n\n    if self.dora_scale == 1.0:\n        return orig_weight\n\n    if self.dora_scale == 0.0:\n        return torch.zeros_like(orig_weight)\n\n    if self.dora_scale < 0.0:\n        raise ValueError(f\"dora_scale cannot be negative: {self.dora_scale}\")\n\n    if self.dora_norm_dims < 1:\n        raise ValueError(f\"dora_norm_dims must be >= 1, got {self.dora_norm_dims}\")\n\n    if updown is None:\n        raise ValueError(\"updown must be specified\")\n\n    if updown not in [0, 1]:\n        raise ValueError(f\"updown must be 0 or 1, got {updown}\")\n\n    if self.dim is None:\n        self.dim = orig_weight.shape[self.dora_norm_dims]\n\n    if self.dim == 0:\n        raise ValueError(f\"dim must be > 0, got {self.dim}\")\n\n    if self.dim != orig_weight.shape[self.dora_norm_dims]:\n        raise ValueError(f\"dim must match weight shape, got {self.dim} != {orig_weight.shape[self.dora_norm_dims]}\")\n\n    if orig_weight.device != self.weight.device:\n        orig_weight = orig_weight.to(self.weight.device)\n\n    if orig_weight.dtype != self.weight.dtype:\n        orig_weight = orig_weight.to(self.weight.dtype)\n\n    if self.weight.shape != orig_weight.shape:\n        raise ValueError(f\"weight shape must match orig_weight shape, got {self.weight.shape} != {orig_weight.shape}\")\n\n    if self.weight.device != orig_weight.device:\n        self.weight", "CodeBleu_score": 19.64146255164565}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 26.150073314265054}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 26.150073314265054}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tfrom_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \\\"lora/\\\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\\\"reading lora {filename}\\\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \\\"\\\")).startswith(\\\"sdxl_\\\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \\\"\\\")) == \\\"True\\\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \\\"lora/\\\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \\\"Filename\\\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n", "CodeBleu_score": 26.150073314265054}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\tif orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n\n    if self.ex_bias is not None:\n        if orig_weight.device != self.ex_bias.device:\n            self.ex_bias = self.ex_bias.to(orig_weight.device)\n\n    return self.calc_updown_on_device(orig_weight)\n\n", "CodeBleu_score": 47.026788405887615}, {"code": "def calc_updown(self, orig_weight)\n\torig_weight = orig_weight.to(self.weight.device)\n\n    if self.ex_bias is not None:\n        self.ex_bias = self.ex_bias.to(self.weight.device)\n\n    return self.calc_updown_impl(orig_weight)\n\n", "CodeBleu_score": 35.95512541760252}, {"code": "def calc_updown(self, orig_weight)\n\torig_weight = orig_weight.to(self.weight.device)\n\n    if self.ex_bias is None:\n        return self.calc_updown_no_bias(orig_weight)\n    else:\n        return self.calc_updown_with_bias(orig_weight)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 22.57204247438955}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}, {"code": "def purge_networks_from_memory()\n\t)_(.+)\\\"):\n        if 'mlp_fc1' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\\\"\n        elif 'mlp_fc2' in m[1]:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\\\"\n        else:\n            return f\\\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\\\"\n\n    return key\n\n", "CodeBleu_score": 6.667427348508603}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\ttext_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"transformer_text_model\\\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \\\"oft_unet\\\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\\\"oft_unet\\\", \\\"diffusion_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \\\"oft_diag\\\" in key:\n            key = key_network_without_network_parts.replace(\\\"lora_unet\\\", \\\"diffusion_model\\\")\n            key = key_network_without_network_parts.replace(\\\"lora_te1_text_model\\\", \\\"0_transformer_text_model\\\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\\\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\\\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \\\"/\\\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\\\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\\\")\n\n    return net\n\n", "CodeBleu_score": 31.715455102291866}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}, {"code": "def network_forward(org_module, input, original_forward)\n\tup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 19.516528715524394}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\treturn [param for param in self.parameters() if param.requires_grad]\n\n", "CodeBleu_score": 6.6137994946136525}, {"code": "def trainables(self)\n\ttrainables = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 58.54993544956477}, {"code": "def trainables(self)\n\treturn [param for layer in self.linear for param in layer.parameters()]\n\n", "CodeBleu_score": 9.446256683618412}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 20.95722778161572}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 16.069674113110185}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\\\"  INFO:\n {self.optional_info}", "CodeBleu_score": 23.255658732656357}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t               if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\\\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\\\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \\\"hypernetwork_loss.csv\\\", hypernetwork.step, steps_per_epoch, {\n                    \\\"loss\\\": f\\\"{loss_step:.7f}\\\",\n                    \\\"learn_rate\\\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\\\"Validation at epoch {epoch_num}\\\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \\\"\\\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\\\", prompt: {preview_text}\\\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\\\"\\\"\\\"", "CodeBleu_score": 25.73345987984148}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}, {"code": "def ema_scope(self, context=None)\n\tn                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\\\"eps\\\", \\\"x0\\\"], 'currently only supporting \\\"eps\\\" and \\\"x0\\\"'\n        self.parameterization = parameterization\n        print(f\\\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\\\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\\\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\\\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\\\"linear\\\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n\n\n", "CodeBleu_score": 21.652309530695256}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tnp.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \\\"eps\\\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \\\"x0\\\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\\\"{context}: Switched to EMA weights\\\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 24.55939764218753}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}, {"code": "def get_loss(self, pred, target, mean=True)\n\t_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\\\"{context}: Restored training weights\\\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\\\"cpu\\\")\n        if \\\"state_dict\\\" in list(sd.keys()):\n            sd = sd[\\\"state_dict\\\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \\\"model.diffusion_model.input_blocks.0.0.weight\\\",\n            \\\"model_ema.diffusion_modelinput_blocks00weight\\\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\\\"Manual init: {input_key}\\\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\\\"Deleting key {k} from state_dict.\\\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\\\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\\\")\n        if missing:\n            print(f\\\"Missing Keys: {missing}\\\")\n        if unexpected:\n            print(f\\\"Unexpected Keys: {unexpected}\\\")\n\n    def q_mean_variance(self, x_start, t):\n        \\\"\\\"\\\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \\\"\\\"\\\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \\\"eps\\\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \\\"x0\\\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n\n\n", "CodeBleu_score": 24.74928848293435}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}, {"code": "def _prior_bpd(self, x_start)\n\t, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \\\"split_input_params\\\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n            stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\\\"image\\\", \\\"LR_image\\\", \\\"segmentation\\\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n\n\n", "CodeBleu_score": 25.406098415317924}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tglobal caches\n    global cache_lock\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(\n                os.path.join(cache_dir, subsection),\n                size_limit=4 * 1024**3,\n                disk_min_file_size=256 * 1024,\n            )\n    return caches[subsection]\n\n", "CodeBleu_score": 59.122943391190084}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tglobal caches\n    global cache_lock\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4294967296, disk_min_file_size=262144)\n    return caches[subsection]\n\n", "CodeBleu_score": 48.82730132881204}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tglobal caches, cache_lock\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(\n                os.path.join(cache_dir, subsection),\n                size_limit=4 * 1024 ** 3,\n                disk_min_file_size=256 * 1024,\n            )\n    return caches[subsection]\n\n", "CodeBleu_score": 60.57832348986389}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                if not os.path.isdir(cache_dir):\n                    os.makedirs(cache_dir)\n                if not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n                cache_obj = caches.get(subsection)\n                if cache_obj is None:\n                    cache_obj = make_cache(subsection)\n                    caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 63.340542337606735}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                if not os.path.exists(cache_dir):\n                    os.makedirs(cache_dir)\n                if not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n                cache_obj = caches.get(subsection)\n                if cache_obj is None:\n                    cache_obj = make_cache(subsection)\n                    caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 63.541857898370615}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                if not os.path.isdir(cache_dir):\n                    os.makedirs(cache_dir)\n                if not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n                cache_obj = caches.get(subsection)\n                if cache_obj is None:\n                    cache_obj = make_cache(subsection)\n                    caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 63.340542337606735}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        if mtime > cache_obj[title][1]:\n            cache_obj.pop(title)\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = (data, mtime)\n        dump_cache()\n    return cache_obj[title][0]\n\n", "CodeBleu_score": 22.615665562010516}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        if cache_obj[title][0] < mtime:\n            cache_obj.pop(title)\n    else:\n        cache_obj[title] = [mtime, None]\n    if cache_obj[title][1] is None:\n        cache_obj[title][1] = func()\n        dump_cache()\n    return cache_obj[title][1]\n\n", "CodeBleu_score": 23.10362419419864}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n\n    if title in cache_obj:\n        cached_data, cached_mtime = cache_obj[title]\n        if os.path.getmtime(filename) > cached_mtime:\n            cache_obj.pop(title)\n    else:\n        cached_data = None\n\n    if cached_data is None:\n        cached_data = func()\n        cached_mtime = os.path.getmtime(filename)\n        cache_obj[title] = (cached_data, cached_mtime)\n        dump_cache()\n\n    return cached_data\n\n", "CodeBleu_score": 32.018249734042456}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    print(\"***\", file=sys.stderr)\n    print(textwrap.dedent(message), file=sys.stderr)\n    if exc_info:\n        print(file=sys.stderr)\n        for line in traceback.format_exc().splitlines():\n            print(\"    \" + line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 49.63688113932447}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    print(\"***\", message, file=sys.stderr, sep=\"", "CodeBleu_score": 19.362018089978257}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    for line in textwrap.wrap(message):\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"***\", file=sys.stderr)\n        for line in format_traceback(sys.exc_info()[2]):\n            print(\"    \", line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 57.90813258821367}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 66.93784359600453}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    positive_prompt_extracted = False\n    negative_prompt_extracted = False\n\n    if style.prompt:\n        positive_prompt_extracted, prompt = extract_style_text_from_prompt(\n            style.prompt, prompt\n        )\n\n    if style.negative_prompt:\n        negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(\n            style.negative_prompt, negative_prompt\n        )\n\n    if not positive_prompt_extracted or not negative_prompt_extracted:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 63.75737123577008}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        res, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not res:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        res, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not res:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 67.35017131486877}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 49.74640122118473}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    partials = [(u, u.random_partial(n_frames)) for u in samples]\n    return partials\n\n", "CodeBleu_score": 68.35465334295921}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in samples]\n\n", "CodeBleu_score": 55.54201333921027}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set to True. Please set either increase_only or decrease_only, or set both to False.\")\n\n    change_in_dBFS = target_dBFS - 20 * np.log10(np.maximum(1e-5, np.mean(np.abs(wav))))\n\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    if decrease_only and change_in_dBFS > 0:\n        return wav\n\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 58.01687295428012}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set to True. Please set either increase_only or decrease_only (or neither) to True.\")\n    rms = np.sqrt(np.mean((wav * np.iinfo(wav.dtype).max) ** 2))\n    wave_dBFS = 20 * np.log10(rms / np.iinfo(wav.dtype).max)\n    dBFS_change = target_dBFS - wave_dBFS\n    if np.isnan(dBFS_change) or (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 69.49196103866176}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    change_in_dBFS = target_dBFS - 20 * np.log10(np.max(np.abs(wav)))\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    if decrease_only and change_in_dBFS > 0:\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 62.45160217406489}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model\n    global _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained to step {_model.step}\")\n\n", "CodeBleu_score": 43.464413781488524}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model\n    global _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model trained to step {_model.step} from {weights_fpath}\")\n\n", "CodeBleu_score": 45.348995128973684}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tglobal _model\n    global _device\n    if device is None:\n        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print('Using device:', device)\n    _device = device\n    _model = SpeakerEncoder(device)\n    _model.eval()\n    step = _model.load(weights_fpath)\n    print('Loaded checkpoint from step {}: {}'.format(step, weights_fpath))\n\n", "CodeBleu_score": 45.48335712226359}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\tif not is_loaded():\n        raise Exception(\"The model has not been loaded. Please call load_model() first.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed_batch = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 61.72418888656688}, {"code": "def embed_frames_batch(frames_batch)\n\tglobal _model, _device\n    if _model is None:\n        raise Exception(\\\"Please call load_model() first!\\\")\n    frames_batch = torch.tensor(frames_batch, dtype=torch.float).to(_device)\n    embed_batch = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 59.77406083011583}, {"code": "def embed_frames_batch(frames_batch)\n\tglobal _model, _device\n    if not is_loaded():\n        raise Exception(\\\"Model not loaded, please call load_model() first.\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embed_batch = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 57.48447350737429}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage < 1\n\n    # Compute the number of frames and the frame step:\n    n_frames = int(np.ceil(n_samples / hop_size))  # Equivalent to the length of the mel spectrogram\n    frame_step = int(hop_size * (1 - overlap))  # Frame step in terms of samples\n    assert 0 < frame_step <= hop_size\n\n    # Compute the slices:\n    wav_slices, mel_slices = [], []\n    for slice_start in range(0, n_frames, partial_utterance_n_frames):\n        slice_end = slice_start + partial_utterance_n_frames\n        wav_slice = slice(slice_start * hop_size, slice_end * hop_size)\n        mel_slice = slice(slice_start, slice_end)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n\n    # Check if last slice is too short and remove it if so:\n    if len(wav_slices) > 1:\n        last_wav_slice, last_mel_slice = wav_slices[-1], mel_slices[-1]\n        last_wav_slice_size = last_wav_slice.stop - last_wav_slice.start\n        last_mel_slice_size = last_mel_slice.stop - last_mel_slice.start\n        total_wav_slice_size = wav_slices[-1].stop - wav_slices[0].start\n        total_mel_slice_size = mel_slices[-1].stop - mel_slices[0].start\n        coverage = (last_wav_slice_size + last_mel_slice_size) / (total_wav_slice_size + total_mel_slice_size)\n        if coverage < min_pad_coverage:\n            wav_slices.pop()\n            mel_slices.pop()\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 38.250471636870884}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames >= 1\n    n_frames = int(np.ceil(n_samples / hop_size))\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    n_slices = 1 + int(np.ceil(1.0 * n_frames / frame_step))\n    wav_slices = []\n    mel_slices = []\n    for slice_idx in range(n_slices):\n        start_frame = slice_idx * frame_step\n        end_frame = min(n_frames, start_frame + partial_utterance_n_frames)\n        wav_slice = slice(int(hop_size * start_frame), int(hop_size * end_frame))\n        mel_slice = slice(start_frame, end_frame)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    # Padding for quantized signal may cause last slice to be too short\n    if len(wav_slices) > 1 and len(wav_slices[-1]) < partial_utterance_n_frames:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    # Ensure that the last slice is at least min_pad_coverage\n    # of partial_utterance_n_frames\n    last_slice_size = len(wav_slices[-1])\n    if last_slice_size < partial_utterance_n_frames * min_pad_coverage:\n        max_pad_size = int(partial_utterance_n_frames * min_pad_coverage - last_slice_size)\n        right_pad_size = int(np.random.uniform(0, max_pad_size))\n        left_pad_size = max_pad_size - right_pad_size\n        wav_slices[-1] = slice(wav_slices[-1].start - left_pad_", "CodeBleu_score": 39.38786001260096}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= overlap <= 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames >= 1\n    # Compute number of slices and frame step\n    n_slices = int(np.ceil(float(n_samples) / partial_utterance_n_frames))\n    n_frames = partial_utterance_n_frames\n    step = int(n_frames * (1 - overlap))\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for slice_idx in range(n_slices):\n        start_idx = slice_idx * step\n        end_idx = min(n_samples, start_idx + n_frames * sampling_rate)\n        wav_slice = wav[start_idx:end_idx]\n        mel_slice = mel[start_idx:end_idx]\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    # Check if last slice is not enough to cover min_pad_coverage\n    last_slice_idx = len(wav_slices) - 1\n    last_slice = wav_slices[last_slice_idx]\n    coverage = float(last_slice.shape[0]) / n_frames\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        # Remove last slice if it is not enough to cover min_pad_coverage\n        wav_slices.pop()\n        mel_slices.pop()\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 36.0427819010002}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tloaded():\n    return _model is not None\n\n", "CodeBleu_score": 4.11162301810453}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t# Determine number of speakers and utterances per speaker from input shape\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Initialize similarity matrix and mask matrix\n    similarity_matrix = torch.zeros(n_speakers, n_speakers).to(self.loss_device)\n    mask_matrix = torch.ones(n_speakers, n_speakers).to(self.loss_device)\n\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl[range(n_speakers), range(n_speakers)] = 0\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-5)\n\n    # Compute similarities\n    for i in range(n_speakers):\n        similarity_matrix[i, :] = torch.matmul(\n            centroids_incl[i, :], centroids_excl.transpose(0, 1)\n        )\n\n    # Scale and bias similarity matrix\n    similarity_matrix = similarity_matrix * self.similarity_weight + self.similarity_bias\n\n    # Exclude self-similarity\n    mask_matrix[range(n_speakers), range(n_speakers)] = 0\n    similarity_matrix = similarity_matrix * mask_matrix\n\n    return similarity_matrix\n\n", "CodeBleu_score": 27.696853240830933}, {"code": "def similarity_matrix(self, embeds)\n\t# Determine number of speakers and utterances per speaker from input shape\n    n_spks, n_utts, _ = embeds.shape\n    # Initialize similarity matrix and mask matrix\n    sim_mat = torch.zeros(n_spks, n_spks, device=self.loss_device)\n    mask = torch.eye(n_spks, device=self.loss_device)\n    # Compute centroids\n    centroids_incl = torch.zeros(n_spks, self.model_embedding_size, device=self.loss_device)\n    centroids_excl = torch.zeros(n_spks, self.model_embedding_size, device=self.loss_device)\n    for spk_idx in range(n_spks):\n        # Compute centroids\n        centroids_incl[spk_idx] = torch.mean(embeds[spk_idx], dim=0)\n        centroids_incl[spk_idx] = centroids_incl[spk_idx] / (torch.norm(centroids_incl[spk_idx], dim=0, keepdim=True) + 1e-5)\n        centroids_excl[spk_idx] = torch.mean(embeds[spk_idx][1:], dim=0)\n        centroids_excl[spk_idx] = centroids_excl[spk_idx] / (torch.norm(centroids_excl[spk_idx], dim=0, keepdim=True) + 1e-5)\n        # Compute similarities\n        sims_incl = torch.matmul(centroids_incl[spk_idx].unsqueeze(0), embeds.transpose(0, 1))\n        sims_excl = torch.matmul(centroids_excl[spk_idx].unsqueeze(0), embeds.transpose(0, 1))\n        # Update similarity matrix\n        sim_mat[spk_idx] = sims_incl", "CodeBleu_score": 31.402338525758672}, {"code": "def similarity_matrix(self, embeds)\n\t# Determine the number of speakers per batch and the number of utterances per speaker\n    n_spks, n_utts, _ = embeds.shape\n\n    # Compute two types of centroids:\n    # 1. \"centroids_incl\" which includes the current embedding in the mean calculation and normalizes it.\n    # 2. \"centroids_excl\" which excludes the current embedding from the mean calculation, averages the remaining embeddings, and normalizes it.\n    centroids_incl = torch.zeros(n_spks, self.model_embedding_size, device=self.loss_device)\n    centroids_excl = torch.zeros(n_spks, self.model_embedding_size, device=self.loss_device)\n    for i in range(n_spks):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_excl[i] = torch.mean(embeds[i][1:], dim=0)\n    centroids_incl = torch.nn.functional.normalize(centroids_incl, dim=1, p=2)\n    centroids_excl = torch.nn.functional.normalize(centroids_excl, dim=1, p=2)\n\n    # Initialize a similarity matrix with zeros on the specified device and a mask matrix to exclude self-similarity\n    sim_matrix = torch.zeros(n_spks, n_spks, device=self.loss_device)\n    mask = torch.ones(n_spks, n_spks, device=self.loss_device)\n    mask = torch.triu(mask, diagonal=1)\n\n    # Compute the similarity scores using the respective centroids and update the similarity matrix accordingly\n    for i in range(n_spks):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_excl.transpose(0, 1))\n\n    # Scale and bias the similarity matrix\n    sim", "CodeBleu_score": 28.75904751011517}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                             speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch", "CodeBleu_score": 35.551237796152236}, {"code": "def loss(self, embeds)\n\t# Determine number of speakers and utterances per speaker from the shape of the input embeddings\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                             speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    #", "CodeBleu_score": 35.62834300782267}, {"code": "def loss(self, embeds)\n\t# Determine the number of speakers and utterances per speaker from the shape of the input embeddings\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape the similarity matrix to have the same shape as the labels\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, speakers_per_batch)\n\n    # Generate ground truth labels\n    labels = np.arange(speakers_per_batch)\n    labels = np.tile(labels, utterances_per_speaker)\n\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Compute the EER\n    with torch.no_grad():\n        # Compute the ROC curve\n        scores, labels = sim_matrix.cpu().numpy(), labels.numpy()\n        fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=None, drop_intermediate=False)\n\n        # Find the threshold where the false positive rate equals the false negative rate\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 50.452128135712194}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\tif self.disabled:\n        return\n    # Import parameter configurations\n    from encoder.params_data import params_data\n    from encoder.params_model import params_model\n\n    # Create the string\n    params_string = \"Parameters:", "CodeBleu_score": 5.928195643209111}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n\n    self.implementation_string += \\\"\\", "CodeBleu_score": 3.5246096468656076}, {"code": "def log_params(self)\n\tif self.disabled:\n        return\n\n    # Import parameter configurations\n    from encoder.params_data import *\n    from encoder.params_model import *\n\n    # Iterate through the model and data parameters\n    params_string = \"Parameters:", "CodeBleu_score": 3.6094644316191116}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t# Limit the number of speakers\n    embeds = embeds[:max_speakers]\n    utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = embeds.reshape(embeds.shape[0], -1)\n\n    # Assign colors based on speaker identity\n    colors = np.array([colormap[i] for i in range(embeds.shape[0])])\n\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(n_neighbors=30, min_dist=0.0, metric=\"euclidean\")\n    embeds = reducer.fit_transform(embeds)\n\n    # Plot the embeddings\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.scatter(embeds[:, 0], embeds[:, 1], c=colors, s=10)\n    ax.set_title(\"Step %d\" % step)\n    ax.set_xlabel(\"UMAP 1\")\n    ax.set_ylabel(\"UMAP 2\")\n    ax.set_aspect(1)\n\n    # Display or save the plot\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath)\n\n    # Update the visualizations\n    if not self.disabled:\n        self.projection_win = self.vis.matplot(\n            plt, win=self.projection_win, opts={\"title\": \"Projections\"}\n        )\n\n", "CodeBleu_score": 49.680077218746234}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t# Limit the number of speakers\n    speaker_ids = list(embeds.keys())\n    speaker_ids = speaker_ids[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = [embeds[speaker_id] for speaker_id in speaker_ids]\n    embeds = np.concatenate(embeds, axis=0)\n\n    # Assign colors based on speaker identity\n    colors = np.array([colormap[int(speaker_id)] for speaker_id in speaker_ids])\n\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(n_neighbors=15, min_dist=0.3, metric=\"cosine\")\n    embeds = reducer.fit_transform(embeds)\n\n    # Plot the embeddings\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    for i, speaker_id in enumerate(speaker_ids):\n        ax.scatter(embeds[i * utterances_per_speaker : (i + 1) * utterances_per_speaker, 0],\n                   embeds[i * utterances_per_speaker : (i + 1) * utterances_per_speaker, 1],\n                   c=colors[i],\n                   s=4,\n                   alpha=0.5)\n\n    # Set the plot title and labels\n    title = \"Step %d\" % step\n    ax.set_title(title)\n    ax.set_xlabel(\"UMAP 1\")\n    ax.set_ylabel(\"UMAP 2\")\n\n    # Save or show the plot\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 45.103384197400594}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif self.disabled:\n        return\n    # Limit the number of speakers to <max_speakers>\n    speakers = list(embeds.keys())\n    if len(speakers) > max_speakers:\n        speakers = np.random.choice(speakers, size=max_speakers, replace=False)\n    # Flatten the embeddings\n    embeds = [np.mean(embeds[speaker], axis=0) for speaker in speakers]\n    # Assign colors based on speaker identity\n    colors = [colormap[speakers.index(speaker)] for speaker in speakers]\n    # Project the embeddings\n    embeds = umap.UMAP(n_components=2).fit_transform(embeds)\n    # Plot the embeddings\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for i, (embed, color) in enumerate(zip(embeds, colors)):\n        ax.scatter(embed[0], embed[1], c=color, label=speakers[i])\n    ax.legend()\n    ax.set_title(\"Step %d\" % step)\n    # Show the plot\n    plt.show()\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath, bbox_inches=\"tight\")\n\n", "CodeBleu_score": 47.055108534051115}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# Save input tensor dimensions as human-readable names\n    batch_size = x.size(0)\n    num_chars = x.size(1)\n    encoder_dims = x.size(2)\n    # Determine the index of the last dimension\n    index = -1\n    # If the input tensor is 3D, the index is 2\n    if len(x.size()) == 3:\n        index = 2\n    # If the input tensor is 4D, the index is 3\n    if len(x.size()) == 4:\n        index = 3\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(batch_size, num_chars, 1)\n    # Reshape the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.view(batch_size, num_chars, encoder_dims)\n    # Transpose the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.transpose(1, index)\n    # Concatenate the speaker embedding with the input tensor\n    x = torch.cat((x, speaker_embedding), dim=index)\n    return x\n\n", "CodeBleu_score": 42.86392612667165}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# Save input tensor dimensions as human-readable names\n    batch_size, seq_len, mel_channels = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.training:\n        # If we are training, we need to pass the speaker embedding to the CBHG module\n        # so it can be used to condition the convolutional layers.\n        # The speaker embedding is passed as a one-hot tensor with the same length as the sequence length\n        # and the same batch size as the input tensor.\n        # The dimension index of the speaker embedding is 2, which corresponds to the sequence length.\n        speaker_dim_idx = 2\n    else:\n        # If we are not training, we do not need to pass the speaker embedding to the CBHG module.\n        # We only need to pass it to this function, so that the speaker embedding can be concatenated\n        # to the input tensor along the last dimension.\n        # The dimension index of the speaker embedding is 1, which corresponds to the batch size.\n        speaker_dim_idx = 1\n    # Repeat speaker embedding along the sequence length and batch size dimensions\n    speaker_embedding = speaker_embedding.repeat(1, seq_len, 1)\n    # Reshape speaker embedding to (batch_size, seq_len, 1, mel_channels)\n    speaker_embedding = speaker_embedding.view(batch_size, seq_len, 1, mel_channels)\n    # Transpose speaker embedding to (batch_size, 1, seq_len, mel_channels)\n    speaker_embedding = speaker_embedding.transpose(1, speaker_dim_idx)\n    # Concatenate speaker embedding with input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 34.54353244635983}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# Save input tensor shape as human-readable names\n    batch_size, seq_length, num_channels = x.size()\n    # Determine which dimension to repeat along\n    if seq_length is None:\n        repeat_dim = 1\n    else:\n        repeat_dim = 2\n    # Repeat speaker embedding along the input tensor dimension\n    speaker_embedding = speaker_embedding.repeat(repeat_dim, 1)\n    speaker_embedding = speaker_embedding.view(batch_size, seq_length, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    # Concatenate along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n\n", "CodeBleu_score": 40.1751630162908}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}, {"code": "def init_attention(self, encoder_seq_proj)\n\tcopy of each speaker embedding to match the input text length\n        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n        speaker_embedding_size = speaker_embedding.size()[idx]\n        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n\n        # Reshape it and transpose\n        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n        e = e.transpose(1, 2)\n\n        # Concatenate the tiled speaker embedding with the encoder output\n        x = torch.cat((x, e), 2)\n        return x\n\n", "CodeBleu_score": 11.967843375981412}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}, {"code": "def load(self, path, optimizer=None)\n\t   @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, speaker_embedding):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n\n\n", "CodeBleu_score": 31.428068306899227}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}, {"code": "def save(self, path, optimizer=None)\n\t_, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        return mel_outputs, linear, attn_scores, stop_outputs\n\n    def generate(self, x, speaker_embedding=None, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size, _  = x.size()\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n\n        # SV2TTS: Run the encoder with the speaker embedding\n        # The projection avoids unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x, speaker_embedding)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores, stop_outputs = [], [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t, x)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            stop_outputs.extend([stop_tokens] * self.r)\n            # Stop the loop when all stop tokens in batch exceed threshold\n            if (stop_tokens > 0.5).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        stop_outputs = torch.cat(stop_outputs, 1)\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n\n\n", "CodeBleu_score": 29.32958082245366}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            clean_text = cleaner_names[0](text)\n            for symbol in clean_text:\n                sequence.append(_symbol_to_id[symbol])\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 62.735824940314444}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            clean_text = cleaner_names[0](text)\n            for sym in clean_text:\n                if sym in _symbol_to_id:\n                    sequence += [_symbol_to_id[sym]]\n            break\n        sequence += _symbols_to_sequence(_curly_re.sub(r\\\"\\g<1> \\g<2> \\g<3>\\\", text))\n        text = _curly_re.sub(r\\\"\\g<3>\\\", text)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\\\"~\\\"])\n    return sequence\n\n", "CodeBleu_score": 43.32656006303558}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 81.15500235356905}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\tresult = \\\"\\\"\\\"\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n        else:\n            s = \\\"<?>\\\"\n        # Enclose ARPAbet back in curly braces:\n        if len(s) > 1 and s[0] == \\\"@\\\" and s[-1] == \\\"@\\\":\n            s = \\\"{%s}\\\" % s[1:-1]\n        result.append(s)\n\n    return \\\" \\\".join(result)", "CodeBleu_score": 49.59093378879212}, {"code": "def sequence_to_text(sequence)\n\tresult = \\\"\\\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \\\"@\\\" and s[-1] == \\\"@\\\":\n                s = \\\"{\\\" + s[1:-1] + \\\"}\\\"\n            result += s\n        else:\n            result += \\\" \\\"\n\n    return result.strip()\n\n\n\n\n", "CodeBleu_score": 49.12608385129688}, {"code": "def sequence_to_text(sequence)\n\tresult = \\\"\\\"\n    for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n        s = _id_to_symbol[symbol_id]\n    else:\n        s = \\\"{}\\\".format(symbol_id)\n    # Enclose ARPAbet back in curly braces:\n    if len(s) > 1 and s[0] == \\\"@\\\" and s[-1] == \\\"@\\\":\n        s = \\\"{\\\" + s[1:-1] + \\\"}\\\"\n    result += s\n    if symbol_id == _symbol_to_id[\\\"~\\\"]:\n        break\n    elif symbol_id == _symbol_to_id[\\\"{\\\"]:\n        result += \\\" \\\"\n    elif symbol_id == _symbol_to_id[\\\"}\\\"]:\n        result += \\\" \\\"\n    else:\n        result += \\\" \\\"\n    return result.strip()\n\n\n\n\n", "CodeBleu_score": 32.43581776418122}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tance(wav, text, out_dir, sub_basename,\n                                                      skip_existing, hparams))\n\n    return [m for m in metadata if m is not None]\n\n", "CodeBleu_score": 9.464176332029675}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == \\\"\\\" and words[-1] == \\\"\\\"\n\n    # Find pauses that are too long\n    mask = (words == \\\"\\\") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n\n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(\\\"inf\\\") if i == 0 else segment_durations[i - 1]\n            right_duration = float(\\\"inf\\\") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n\n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = [\\\" \\\".join(words[start + 1:end]).replace(\\\"  \\\", \\\" \\\") for start, end in segments]\n\n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(\\\"This sentence was split in %d segments:\\\" % len(wavs))\n    # else:\n    #     print(\\\"There are no silences long enough for this sentence to be split:\\\")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn't need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(\\\"\\t%s\\\" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print(\\\"\\\")\n\n    return wavs, texts\n\n", "CodeBleu_score": 25.701075338471956}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\tn\n    else:\n        print(\\\"", "CodeBleu_score": 4.500446030330124}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}, {"code": "def num_params(self)\n\t, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n\n        # Project the prev input\n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n\n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n\n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n\n        # Compute all gates for coarse and fine\n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n\n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n\n        # Compute outputs\n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n\n\n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates\n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input\n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()\n            output = combine_signal(coarse, fine)\n\n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n\n\n\n", "CodeBleu_score": 29.616317262133396}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}, {"code": "def pad_tensor(self, x, pad, side='both')\n\trange(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == 'RAW' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                mels = mels.cuda()\n            else:\n                mels = mels.cpu()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            if torch.cuda.is_available():\n                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n                x = torch.zeros(b_size, 1).cuda()\n            else:\n                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n                x = torch.zeros(b_size, 1).cpu()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == 'MOL':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    if torch.cuda.is_available():\n                        # x = torch.FloatTensor([[sample]]).cuda()\n                        x = sample.transpose(0, 1).cuda()\n                    else:\n                        x = sample.transpose(0, 1)\n\n                elif self.mode == 'RAW' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\\\"Unknown model mode value - \\\", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n\n\n", "CodeBleu_score": 23.019740641218046}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import matplotlib as mpl\n    import numpy as np\n    import sys\n\n    def progbar(i, n, size=16):\n        done = (i * size) // n\n        bar = ''\n        for i in range(size):\n            bar += '\u2588' if i <= done else '\u2591'\n        return bar\n\n    def stream(message):\n        try:\n            sys.stdout.write(\"\\r{%s}\" % message)\n        except:\n            # Remove non-ASCII characters from message\n            message = ''.join(i for i in message if ord(i) < 128)\n            sys.stdout.write(\"\\r{%s}\" % message)\n\n    def simple_table(item_tuples):\n\n        border_pattern = '+---------------------------------------'\n        whitespace = '                                            '\n\n        headings, cells, = [], []\n\n        for item in item_tuples:\n\n            heading, cell = str(item[0]), str(item[1])\n\n            pad_head = True if len(heading) < len(cell) else False\n\n            pad = abs(len(heading) - len(cell))\n            pad = whitespace[:pad]\n\n            pad_left = pad[:len(pad) // 2]\n            pad_right = pad[len(pad) // 2:]\n\n            if pad_head:\n                heading = pad_left + heading + pad_right\n            else:\n                cell = pad_left + cell + pad_right\n\n            headings += [heading]\n            cells += [cell]\n\n        border, head, body = '', '', ''\n\n        for i in range(len(item_tuples)):\n\n            temp_head = f'| {headings[i]} '\n            temp_body = f'| {cells[i]} '\n\n            border += border_pattern[:len(temp_head)]\n            head += temp_head\n            body += temp_body\n\n            if i == len(item_tuples) - 1:\n                head += '|'\n                body += '|'\n                border += '+'", "CodeBleu_score": 30.804455075250026}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    import matplotlib.ticker as ticker\n    import matplotlib.cm as cm\n\n    # Prepare the data\n    attn = attn.squeeze()\n    attn = attn / attn.max()\n\n    # Create a figure with a grid of axes\n    fig, axes = plt.subplots(nrows=1, ncols=attn.shape[1], figsize=(15, 5))\n\n    # Show the attention matrix\n    for i, ax in enumerate(axes):\n        im = ax.imshow(attn[:, i], cmap=cm.Greys)\n\n    # Tweak spacing to prevent clipping of tick labels\n    fig.tight_layout()\n\n    # Show the figure\n    plt.savefig(path, dpi=300)\n    plt.close(fig)\n\n", "CodeBleu_score": 41.473947738168334}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + ['<sos>'] + ['<eos>'] + ['<pad>'] + list(map(str, range(1000))), rotation=90)\n    ax.set_yticklabels([''] + ['<sos>'] + ['<eos>'] + ['<pad>'] + list(map(str, range(1000))))\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 41.10736079942453}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 1, 1)\n    plt.xlabel(\"Time\", color=\"black\", fontsize=16)\n    plt.ylabel(\"Amplitude\", color=\"black\", fontsize=16)\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=16, colors=\"black\")\n    plt.tick_params(axis=\"both\", which=\"minor\", labelsize=16, colors=\"black\")\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 37.69755304924731}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 1, 1)\n    plt.tick_params(axis='both', colors='#666666', labelcolor='#666666', labelsize=18)\n    plt.xlabel('Time', color='#666666', fontsize=18)\n    plt.ylabel('Amplitude', color='#666666', fontsize=18)\n    plt.plot(array, color='#666666')\n    plt.show()\n\n", "CodeBleu_score": 32.25307033432671}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 1, 1)\n    plt.xlabel('Time', color='#000000', fontsize=10)\n    plt.ylabel('Amplitude', color='#000000', fontsize=10)\n    plt.tick_params(axis='both', colors='#000000', labelsize=10)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 35.89777250406826}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t(str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \\\"\\\"\\\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\\\".txt\\\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\\\"rtsp://\\\", \\\"rtmp://\\\", \\\"http://\\\", \\\"https://\\\"))\n    webcam = source.isnumeric() or source.endswith(\\\".streams\\\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\\\"screen\\\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \\\"labels\\\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \\\"predictions.csv\\\"\n\n        # Create or append to the CSV file\n\n\n", "CodeBleu_score": 26.671179679728485}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ts.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,", "CodeBleu_score": 12.902965008309517}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\tP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \\\"OpenVINO:\\\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \\\"\\\"\\\"\n    check_requirements(\\\"openvino-dev>=2023.0\\\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 23.74267358699717}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}, {"code": "def generate_individual(input_ranges, individual_length)\n\tn        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])\n        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])\n\n        # Create gene_ranges list to hold the range of values for each gene in the population\n        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]\n\n        # Initialize the population with initial_values or random values\n        initial_values = []\n\n        # If resuming evolution from a previous checkpoint\n        if opt.resume_evolve is not None:\n            assert os.path.isfile(ROOT / opt.resume_evolve), \\\"evolve population path is wrong!\\\"\n            with open(ROOT / opt.resume_evolve, errors=\\\"ignore\\\") as f:\n                evolve_population = yaml.safe_load(f)\n                for value in evolve_population.values():\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population\n        else:\n            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(\\\".yaml\\\")]\n            for file_name in yaml_files:\n                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:\n                    value = yaml.safe_load(yaml_file)\n                    value = np.array([value[k] for k in hyp_GA.keys()])\n                    initial_values.append(list(value))\n\n        # Generate random values within the search space for the rest of the population\n        if initial_values is None:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]\n        elif pop_size > 1:\n            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]\n            for initial_value in initial_values:\n                population = [initial_value] + population\n\n        # Run the genetic algorithm for a fixed number of generations\n        list_keys = list(hyp_GA.keys())\n        for generation in range(opt.evolve):\n            if generation >= 1:\n                save_dict = {}\n                for i in range(len(population)):\n                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}\n                    save_dict[f\\\"gen{str(generation)}number{str(i)}\\\"] = little_dict\n\n                with open(save_dir / \\\"evolve_population.yaml\\\", \\\"w\\\") as outfile:\n                    yaml.dump(save_dict, outfile, default_flow_style=False)\n\n            # Adaptive elite size\n            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))\n            # Evaluate the fitness of each individual in the population\n            fitness_scores = []\n            for individual in population:\n                for key, value in zip(hyp_GA.keys(), individual):\n                    hyp_GA[key] = value\n                hyp.update(hyp_GA)\n                results = train(hyp.copy(), opt, device, callbacks)\n                callbacks = Callbacks()\n                # Write mutation results\n                keys = (\n                    \\\"metrics/precision\\\",\n                    \\\"metrics/recall\\\",\n                    \\\"metrics/mAP_0.5\\\",\n                    \\\"metrics/mAP_0.5:0.95\\\",\n                    \\\"val/box_loss\\\",\n                    \\\"val/obj_loss\\\",\n                    \\\"val/cls_loss\\\",\n                )\n                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)\n                fitness_scores.append(results[2])\n\n            # Select the fittest individuals for reproduction using adaptive tournament selection\n            selected_indices = []\n            for _ in range(pop_size - elite_size):\n                # Adaptive tournament size\n                tournament_size = max(\n                    max(2, tournament_size_min),\n                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),\n                )\n                # Perform tournament selection to choose the best individual\n                tournament_indices = random.sample(range(pop_size), tournament_size)\n                tournament_fitness = [fitness_scores[j] for j in tournament_indices]\n                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]\n                selected_indices.append(winner_index)\n\n            # Add the elite individuals to the selected indices\n            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]\n            selected_indices.extend(elite_indices)\n            # Create the next generation through crossover and mutation\n            next_generation = []\n            for _ in range(pop_size):\n                parent1_index = selected_indices[random.randint(0, pop_size - 1)]\n                parent2_index = selected_indices[random.randint(0, pop_size - 1)]\n                # Adaptive crossover rate\n                crossover_rate = max(\n                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))\n                )\n                if random.uniform(0, 1) < crossover_rate:\n                    crossover_point = random.randint(1, len(hyp_GA) - 1)\n                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]\n                else:\n                    child = population[parent1_index]\n                # Adaptive mutation rate\n                mutation_rate = max(\n                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))\n                )\n                for j in range(len(hyp_GA)):\n                    if random.uniform(0, 1) < mutation_rate:\n                        child[j] += random.uniform(-0.1, 0.1)\n                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])\n                next_generation.append(child)\n            # Replace the old population with the new generation\n            population = next_generation\n        # Print the best solution found\n        best_index = fitness_scores.index(max(fitness_scores))\n        best_individual = population[best_index]\n        print(\\\"Best solution found:\\\", best_individual)\n        # Plot results\n        plot_evolve(evolve_csv)\n        LOGGER.info(\n            f'Hyperparameter evolution finished {opt.evolve} generations", "CodeBleu_score": 26.915267490749308}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}, {"code": "def marginal_log_mean_coeff(self, t)\n\tqdm\n\n", "CodeBleu_score": 1.182071031236628}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.760204510857477}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.760204510857477}, {"code": "def inverse_lambda(self, lamb)\n\tsolve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \\\"\\\"\\\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\\\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\\\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \\\"\\\"\\\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \\\"\\\"\\\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \\\"\\\"\\\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n\n\n", "CodeBleu_score": 28.760204510857477}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tos((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n", "CodeBleu_score": 27.41742715759684}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tv | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \\\"Classifier-free diffusion guidance.\\\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 32.758214409738216}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}, {"code": "def data_prediction_fn(self, x, t)\n\t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \\\"noise\\\" or \\\"x_start\\\" or \\\"v\\\" or \\\"score\\\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \\\"uncond\\\" or \\\"classifier\\\" or \\\"classifier-free\\\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \\\"classifier\\\" or \\\"classifier-free\\\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \\\"classifier-free\\\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \\\"\\\"\\\"\n\n    def get_model_input_time(t_continuous):\n        \\\"\\\"\\\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \\\"\\\"\\\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \\\"noise\\\":\n            return output\n        elif model_type == \\\"x_start\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \\\"v\\\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \\\"score\\\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \\\"\\\"\\\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \\\"\\\"\\\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \\\"\\\"\\\"\n        The noise predicition model function that is used for DPM-Solver.\n        \\\"\\\"\\\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \\\"uncond\\\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \\\"classifier\\\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \\\"classifier-free\\\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\\\"noise\\\", \\\"x_start\\\", \\\"v\\\"]\n    assert guidance_type in [\\\"uncond\\\", \\\"classifier\\\", \\\"classifier-free\\\"]\n    return model_fn\n\n", "CodeBleu_score": 34.59521230890662}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tn                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n", "CodeBleu_score": 23.011468303461502}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\talpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = alpha.log()\n    log_alpha.backward(torch.ones_like(taus))\n    return log_alpha.grad * alpha\n\n", "CodeBleu_score": 12.909822620738714}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus = taus.clone().detach().requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = alpha.log()\n    alpha.backward(torch.ones_like(taus))\n    return log_alpha.detach().cpu() * alpha.detach().cpu()\n\n", "CodeBleu_score": 18.57329394778342}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\talpha = t2alpha_fn(beta_0, beta_1, taus.clone().detach().cpu())\n    log_alpha = alpha.log()\n    alpha.backward(torch.ones_like(taus))\n    grad_log_alpha = taus.grad.clone().detach().cpu()\n    return alpha * grad_log_alpha\n\n", "CodeBleu_score": 16.502760203607465}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n", "CodeBleu_score": 9.164677981974474}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\td(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n", "CodeBleu_score": 14.300713602292802}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.51898569544345}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.51898569544345}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\ti': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n", "CodeBleu_score": 35.51898569544345}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\tif x.dim() > target_dims:\n        raise ValueError(f\"The number of dimensions of the tensor {x.dim()} is greater than the target dimensions {target_dims}.\")\n    elif x.dim() == target_dims:\n        return x\n    else:\n        diff = target_dims - x.dim()\n        expand_shape = [1] * diff + [-1]\n        return x.expand(*expand_shape).clone().detach() if x.device.type == \"mps\" else x.expand(*expand_shape)\n\n", "CodeBleu_score": 35.73120674185646}, {"code": "def append_dims(x, target_dims)\n\tcurrent_dims = len(x.shape)\n    num_dims_to_add = target_dims - current_dims\n    if num_dims_to_add < 0:\n        raise ValueError(f\"Target number of dimensions ({target_dims}) must be greater than or equal to the current number of dimensions ({current_dims}).\")\n    for _ in range(num_dims_to_add):\n        x = x.unsqueeze(0)\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n    return x\n\n", "CodeBleu_score": 36.493108089277065}, {"code": "def append_dims(x, target_dims)\n\tcur_dims = x.ndim\n    num_dims_to_add = target_dims - cur_dims\n    if num_dims_to_add < 0:\n        raise ValueError(f'Target dimensions must be greater than or equal to current dimensions. Current dimensions: {cur_dims}, target dimensions: {target_dims}')\n    if num_dims_to_add == 0:\n        return x\n    if x.device.type == 'mps':\n        return x.detach().clone().expand(x.shape + (1,) * num_dims_to_add)\n    return x.expand(x.shape + (1,) * num_dims_to_add)\n\n", "CodeBleu_score": 30.84639984167421}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\tif digest is not None:\n        digest = digest.lower().replace('sha256:', '')\n    if not Path(path).exists():\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        with urllib.request.urlopen(url) as source, open(path, 'wb') as output:\n            with contextlib.closing(hashlib.sha256()) as sha256:\n                while True:\n                    buffer = source.read(8192)\n                    if not buffer:\n                        break\n                    output.write(buffer)\n                    sha256.update(buffer)\n            if digest is not None and sha256.hexdigest() != digest:\n                raise ValueError(f'downloaded file {path} has digest {sha256.hexdigest()} but expected {digest}')\n    return path\n\n", "CodeBleu_score": 44.881099557297865}, {"code": "def download_file(path, url, digest=None)\n\tpath = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            if not check_sha256(path, digest):\n                raise RuntimeError(f'File {path} exists but its SHA-256 digest does not match the expected value of {digest}')\n        return path\n    with urllib.request.urlopen(url) as source, path.open('wb') as target:\n        shutil.copyfileobj(source, target)\n    if digest is not None:\n        if not check_sha256(path, digest):\n            raise RuntimeError(f'Downloaded file {path} has an SHA-256 digest of {sha256(path)}, which does not match the expected value of {digest}')\n    return path\n\n", "CodeBleu_score": 41.722871261284276}, {"code": "def download_file(path, url, digest=None)\n\tif not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if path.exists() and not digest:\n        return path\n    if not path.exists() or (digest and not sha256(path) == digest):\n        with urllib.request.urlopen(url) as response, path.open('wb') as out_file:\n            shutil.copyfileobj(response, out_file)\n    if digest and not sha256(path) == digest:\n        raise ValueError(f'File {path} did not have expected digest {digest}')\n    return path\n\n", "CodeBleu_score": 41.469353573524}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\tif isinstance(model, nn.DataParallel):\n        model = model.module\n    previous_mode = model.training\n    model.train(mode)\n    yield\n    model.train(previous_mode)\n\n", "CodeBleu_score": 23.020478988593574}, {"code": "def train_mode(model, mode=True)\n\tmodes = {}\n    for name, module in model.named_modules():\n        modes[name] = module.training\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for name, module in model.named_modules():\n            module.train(modes[name])\n\n", "CodeBleu_score": 35.21565529189796}, {"code": "def train_mode(model, mode=True)\n\t# Save the current training mode of each module\n    current_modes = [module.training for module in model.modules()]\n    # Set the model to the specified training mode\n    for module in model.modules():\n        module.train(mode)\n    try:\n        # Yield control to the caller\n        yield\n    finally:\n        # Restore the original training mode of each module\n        for module, mode in zip(model.modules(), current_modes):\n            module.train(mode)\n\n", "CodeBleu_score": 46.451009849537506}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\tfor averaged_model_param, model_param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_model_param.data.mul_(1 - decay).add_(model_param.data, alpha=decay)\n    for averaged_model_buffer, model_buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_model_buffer.data.copy_(model_buffer.data)\n\n", "CodeBleu_score": 29.631854848506674}, {"code": "def ema_update(model, averaged_model, decay)\n\tfor averaged_model_param, model_param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_model_param.copy_(averaged_model_param * decay + (1 - decay) * model_param)\n    for averaged_model_buffer, model_buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_model_buffer.copy_(model_buffer)\n\n", "CodeBleu_score": 29.85486989025013}, {"code": "def ema_update(model, averaged_model, decay)\n\taveraged_model.load_state_dict(model.state_dict())\n    averaged_model.load_state_dict(\n        {\n            name: decay * averaged_model.state_dict()[name] + (1 - decay) * model.state_dict()[name]\n            for name in model.state_dict()\n        }\n    )\n    averaged_model.load_state_dict(model.state_dict())\n\n", "CodeBleu_score": 24.205561465386445}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tor more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \\\"\\\"\\\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \\\"\\\"\\\"Returns the state of the class as a :class:`dict`.\\\"\\\"\\\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \\\"\\\"\\\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \\\"\\\"\\\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \\\"\\\"\\\"Gets the current EMA decay rate.\\\"\\\"\\\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \\\"\\\"\\\"Updates the step count.\\\"\\\"\\\"\n        self.last_epoch += 1\n\n", "CodeBleu_score": 22.70727181325717}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(nn.Identity())\n        else:\n            return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n", "CodeBleu_score": 40.10683462033068}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(nn.Identity())\n        else:\n            return nn.Identity()\n\n", "CodeBleu_score": 40.3547536004637}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        activation = nn.ELU()\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    else:\n        raise ValueError(\"Invalid activation type\")\n\n    if antialias:\n        activation = Activation1d(activation)\n\n    return activation\n\n", "CodeBleu_score": 57.61811399136029}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t):\n        \\\"\\\"\\\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \\\"\\\"\\\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n", "CodeBleu_score": 25.08983103026739}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\tif dim != -1:\n        idx = idx.transpose(dim, -1)\n    embeddings = self.codebook(idx)\n    if dim != -1:\n        embeddings = embeddings.transpose(dim, -1)\n    return embeddings\n\n", "CodeBleu_score": 55.03797011700233}, {"code": "def idx2vq(self, idx, dim=-1)\n\t# Check if the given indices are in the range of the codebook\n    if (idx.min() < 0) or (idx.max() >= self.codebook.weight.size(0)):\n        raise ValueError(\"The given indices are out of range. The indices must be in the range [0, {}].\".format(self.codebook.weight.size(0)))\n\n    # Retrieve the embeddings corresponding to the given indices\n    embeddings = self.codebook(idx)\n\n    # Move the last dimension to the given dimension\n    if dim != -1:\n        embeddings = embeddings.transpose(dim, -1)\n\n    return embeddings\n\n", "CodeBleu_score": 49.80725278454254}, {"code": "def idx2vq(self, idx, dim=-1)\n\tidx = idx.unsqueeze(-1).expand_as(self.codebook.weight)\n    emb = torch.gather(self.codebook.weight, dim, idx)\n    if dim != -1:\n        emb = emb.transpose(-1, dim)\n    return emb\n\n", "CodeBleu_score": 34.155781316871476}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}, {"code": "def encode(self, x, quantize=False)\n\tself.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n", "CodeBleu_score": 28.42584778478356}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            logpy.info(f\"EMA: Switching to EMA weights for {context}\")\n        self.model_ema.store()\n        self.model_ema.apply()\n        yield\n        if context is not None:\n            logpy.info(f\"EMA: Restoring original weights for {context}\")\n        self.model_ema.restore()\n    else:\n        yield\n\n", "CodeBleu_score": 49.501965932650634}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        with self.model_ema.apply_and_restore_parameters(self, context=context):\n            yield\n    else:\n        yield\n\n", "CodeBleu_score": 13.718102702467366}, {"code": "def ema_scope(self, context=None)\n\tif self.use_ema:\n        if context is not None:\n            logpy.info(f\"EMA weights are active for {context}.\")\n        self.model_ema.store()\n        self.model_ema.apply()\n        yield\n        if context is not None:\n            logpy.info(f\"EMA weights are restored for {context}.\")\n        self.model_ema.restore()\n    else:\n        yield\n\n", "CodeBleu_score": 48.41713868586518}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\ttorch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \\\"\\\"\\\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \\\"\\\"\\\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n", "CodeBleu_score": 25.17346561284971}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.60384611524725}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.60384611524725}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tlayer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \\\"\\\"\\\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n", "CodeBleu_score": 17.60384611524725}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\ttimesteps = timesteps.long().view(-1)\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(half_dim, dtype=torch.float32).view(1, -1) * emb.view(-1, 1)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(1, -1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.cat([emb, torch.zeros(1, 1)], dim=1)\n    pos_emb = emb.repeat(timesteps.shape[0], 1)\n\n    return pos_emb\n\n", "CodeBleu_score": 66.45708498967986}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\ttimesteps = timesteps.long().view(-1)\n    assert timesteps.dim() == 1\n    assert embedding_dim % 2 == 0\n    half_dim = embedding_dim // 2\n    emb = timesteps.float() / (10000.0)\n    emb = torch.log(1.0 + emb) / math.log(10000.0)\n    emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=1).view(-1, half_dim)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 52.38241385520994}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\ttimesteps = timesteps.view(-1)\n    timesteps = timesteps.float() / (10000.0)\n    timesteps = timesteps + 1\n    return torch.cat([torch.sin(timesteps), torch.cos(timesteps)], dim=-1)\n\n", "CodeBleu_score": 10.458958472718999}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}, {"code": "def pytorch_attention(q, k, v)\n\t,i:i+l].to(torch.float32), scale_factor=2.0, mode=\\\"nearest\\\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 6.229067007202447}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}, {"code": "def apply_control(h, control, name)\n\tnn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \\\"\\\"\\\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \\\"\\\"\\\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n", "CodeBleu_score": 30.538817062117936}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t# get quantized latent vectors\n    z_q = torch.matmul(indices, self.embedding.weight).view(shape)\n    # reshape back to match original input shape\n    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 16.309207892370086}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# get quantized latent vectors\n    z_q = torch.matmul(F.one_hot(indices, self.n_e).type(torch.float32), self.embedding.weight)\n    # reshape back to match original input shape\n    z_q = z_q.view(shape)\n    return z_q\n\n", "CodeBleu_score": 15.027081854253435}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# get quantized latent vectors\n    z_q = torch.matmul(indices, self.embedding.weight).view(shape)\n    # reshape back to match original input shape\n    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 16.309207892370086}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 22.98572238296496}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 22.98572238296496}, {"code": "def make_noise(self)\n\t))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 22.98572238296496}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}, {"code": "def mean_latent(self, num_latent)\n\t       # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n", "CodeBleu_score": 17.515598594088804}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\tif shift > 0:\n        shift = np.random.randint(-shift, shift, 3)\n        img = img + shift\n        img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 59.10448980823173}, {"code": "def color_jitter(img, shift)\n\tif shift > 0:\n        shift_factor = np.random.uniform(-shift, shift, 3)\n        img = img + shift_factor\n        img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 60.623764061890206}, {"code": "def color_jitter(img, shift)\n\tif shift == 0:\n        return img\n    img = img.astype(np.float32)\n    img[:, :, 0] = np.clip(img[:, :, 0] + np.random.uniform(-shift, shift), 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1] + np.random.uniform(-shift, shift), 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2] + np.random.uniform(-shift, shift), 0, 1)\n    return img\n\n", "CodeBleu_score": 40.75687826684262}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}, {"code": "def feed_data(self, data)\n\td_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n\n\n", "CodeBleu_score": 27.56564725325307}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}, {"code": "def construct_img_pyramid(self)\n\tpretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n\n\n", "CodeBleu_score": 21.747546766147032}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}, {"code": "def save(self, epoch, current_iter)\n\tators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"val\\\"][\\\"suffix\\\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\\\"name\\\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}", "CodeBleu_score": 27.12927640965836}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}, {"code": "def clean_folder(folder)\n\tException:\n    print('please install cog and realesrgan package')\n\n", "CodeBleu_score": 1.7869101912517769}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.exps):\n        sample_size = len(self.exps)\n\n    indices = random.sample(range(len(self.exps)), sample_size)\n    batch = [self.exps[i] for i in indices]\n    self.exps = [self.exps[i] for i in range(len(self.exps)) if i not in indices]\n\n    return batch\n\n", "CodeBleu_score": 51.335137101909226}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in batch]\n    return batch\n\n", "CodeBleu_score": 65.35181653502008}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    sample_size = int(sample_size)\n    if sample_size <= 0:\n        return []\n    sample = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in sample]\n    return sample\n\n", "CodeBleu_score": 56.96044946375605}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t# flatten the configs\n    all_configs_flattened = []\n    for config in self.all_configs:\n        config_flattened = flatten_config(config, self.tuning_space)\n        all_configs_flattened.append(config_flattened)\n\n    # convert to numpy array\n    all_configs_flattened = np.array(all_configs_flattened)\n\n    # predict estimated performance\n    estimated_perf = self.cost_model.predict(all_configs_flattened)\n\n    # sort the indices based on the metric\n    sorted_indices = np.argsort(estimated_perf)\n\n    # return the top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 26.428597872123305}, {"code": "def find_estimated_top_configs(self)\n\tall_configs = self.all_configs\n    num_all_configs = self.num_all_configs\n\n    logger.info(f\"Find estimated top configs\")\n    config_dims = self.dims\n    flattened_configs = []\n    for config in all_configs:\n        flattened_configs.append(flatten_config(config, config_dims))\n    flattened_configs = np.array(flattened_configs)\n\n    logger.info(f\"Predict cost for {num_all_configs} configs\")\n    predicted_costs = self.cost_model.predict(flattened_configs)\n\n    logger.info(f\"Sort cost for {num_all_configs} configs\")\n    sorted_indices = np.argsort(predicted_costs)\n    sorted_indices = sorted_indices[: self.num_top_configs]\n\n    logger.info(f\"Estimated top configs: {sorted_indices}\")\n    return sorted_indices\n\n", "CodeBleu_score": 28.5727139048861}, {"code": "def find_estimated_top_configs(self)\n\t# flatten all configs\n    flattened_configs = [\n        flatten_config(config, self.dims) for config in self.all_configs\n    ]\n    # convert to numpy array\n    flattened_configs = np.array(flattened_configs)\n\n    # collect numerical values\n    numerical_configs = []\n    for config in flattened_configs:\n        numerical_config = []\n        for key, value in config.items():\n            if isinstance(value, numbers.Number):\n                numerical_config.append(value)\n        numerical_configs.append(numerical_config)\n\n    numerical_configs = np.array(numerical_configs)\n\n    # predict cost\n    predicted_costs = self.cost_model.predict(numerical_configs)\n\n    # sort based on metric\n    if self.metric == AUTOTUNING_METRIC_PATH:\n        sorted_indices = np.argsort(predicted_costs)\n    elif self.metric == AUTOTUNING_METRIC_LATENCY:\n        sorted_indices = np.argsort(predicted_costs)\n    else:\n        raise ValueError(\n            f\"Metric {self.metric} is not supported for model-based tuning.\"\n        )\n\n    return sorted_indices\n\n", "CodeBleu_score": 30.432947682496668}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.Mapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 79.40125770558255}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.Mapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 79.40125770558255}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}, {"code": "def run_after_tuning(self)\n\t       if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \\\"auto\\\":  # GRADIENT_ACCUMULATION_STEPS: \\\"auto\\\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\\\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\\\")\n        assert gas > 0, \\\"Gradient accumulation steps must be positive.\\\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \\\"\\\"\\\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \\\"\\\"\\\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\\\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\\\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\\\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\\\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\\\"result_dir\\\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \\\"cmd.txt\\\"), \\\"r\\\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \\\"ds_config.json\\\"), \\\"r\\\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \\\"ds_config_optimal.json\\\")\n            json.dump(ds_config, open(ds_config_path, \\\"w\\\"))\n\n            cmd_path = os.path.join(self.results_dir, \\\"cmd_optimal.txt\\\")\n            with open(cmd_path, \\\"w\\\") as fd:\n                fd.write(\\\" \\\".join(cmd))\n                fd.write(\\\"", "CodeBleu_score": 30.52429580324778}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\tmaster_port = self.get_free_port(reservations)\n\n    # set result directory and master port\n    exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n    exp[\\\"master_port\\\"] = self.get_free_port(reservations)\n\n    # update user arguments\n    for k, v in self.arg_mappings.items():\n        k = k.strip()\n        v = v.strip()\n        if k in exp[\\\"ds_config\\\"]:\n            exp[\\\"ds_config\\\"][k] = v\n\n    # start experiment\n    stderr_file = os.path.join(exp[\\\"result_dir\\\"], \\\"stderr.log\\\")\n    with open(stderr_file, \\\"w\\\") as fd:\n        logger.info(f\\\"Starting experiment {exp['name']}\\\")\n        fd.write(json.dumps(exp))\n    exp_id = exp[\\\"exp_id\\\"]\n    self.running_experiments[exp_id] = (exp, time.time())\n    thread = threading.Thread(target=self.run_exp, args=(exp,))\n    thread.start()\n\n\n\n", "CodeBleu_score": 34.203783417952174}, {"code": "def run_job(self, exp: dict, reservations)\n\t# set master port and result directory\n    master_port = self.get_free_port()\n    exp[self.arg_mappings[\"--master_port\"]] = master_port\n    exp[self.arg_mappings[\"--result_dir\"]] = exp[\"result_dir\"]\n\n    # update user arguments\n    for k, v in self.arg_mappings.items():\n        if k not in exp:\n            exp[k] = v\n\n    # create a new thread to run the experiment\n    t = threading.Thread(target=self.run_exp, args=(exp, reservations,))\n    t.start()\n\n    # store running experiment details\n    exp_id = exp[\"exp_id\"]\n    self.running_experiments[exp_id] = (exp, t)\n\n", "CodeBleu_score": 24.802258854155852}, {"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp[\"exp_id\"]\n    exp_name = exp[\"name\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_script = exp[\"script\"]\n    exp_args = exp[\"args\"]\n    exp_ds_config = exp[\"ds_config\"]\n    exp_env_vars = exp[\"env_vars\"]\n\n    if self.args.verbose:\n        logger.info(f\"Running experiment {exp_name}\")\n\n    # configure experiment\n    exp_args = self.configure_experiment(exp_args, exp_ds_config)\n\n    # update user arguments based on argument mappings\n    for arg_key, arg_val in self.arg_mappings.items():\n        exp_args = self.update_arg(exp_args, arg_key, arg_val)\n\n    # start experiment\n    exp_script = os.path.join(self.exps_dir, exp_script)\n    exp_args = [exp_script] + exp_args\n    exp_env_vars[\"DEEPSPEED_RESULT_DIR\"] = exp_result_dir\n\n    # start experiment in a new thread\n    thread = threading.Thread(\n        target=self.start_experiment,\n        args=(exp_id, exp_args, exp_env_vars, reservations),\n    )\n    thread.start()\n\n    # store running experiment details\n    self.running_experiments[exp_id] = (exp, thread)\n\n", "CodeBleu_score": 31.894267283592615}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t[]\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \\\"r\\\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\\\"exp_id\\\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n                        metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n                        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \\\"stderr.log\\\")\n                    model_info_file = os.path.join(result_dir, \\\"model_info.json\\\")\n                    metric_file = os.path.join(result_dir, \\\"metrics.json\\\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\\\"exp_id\\\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\\\"exp_id\\\"]\n        exp[\\\"master_port\\\"] = self.args.master_port + exp_id\n        exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \\\"auto\\\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\\\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\\\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\\\"result_dir\\\"]\n                stderr_file = os.path.join(exp_dir, \\\"stderr.log\\\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\\\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\\\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \\\"\\\"\n        for node in self.nodes:\n            status += f\\\"{node.host} ({len(node.idle_slots)} idle gpus), \\\"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\\\"exp_id\\\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\\\"exp_id\\\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\\\"exp_id\\\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \\\"\\\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \\\",\\\".join(map(str, reservation.slots))\n                    desc += f\\\"{reservation.node.host}:{slots}@\\\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\\\"exp_id\\\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n\n\n", "CodeBleu_score": 18.66758095041363}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n\n\n", "CodeBleu_score": 26.47565290100487}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\tsystem = platform.system()\n    architecture = platform.machine()\n    version = platform.version()\n    processor = platform.processor()\n    python_version = platform.python_version()\n    return {\n        \"system\": system,\n        \"architecture\": architecture,\n        \"version\": version,\n        \"processor\": processor,\n        \"python_version\": python_version,\n    }\n\n", "CodeBleu_score": 45.212289347561274}, {"code": "def system_info()\n\tinfo = dict()\n    info[\"os\"] = platform.system()\n    info[\"arch\"] = platform.architecture()[0]\n    info[\"version\"] = platform.version()\n    info[\"processor\"] = platform.processor()\n    info[\"python\"] = sys.version\n    return info\n\n", "CodeBleu_score": 30.68221470846484}, {"code": "def system_info()\n\tinfo = {}\n    info['os'] = platform.system()\n    info['architecture'] = platform.architecture()[0]\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 42.89348899145141}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\timport torch\n    import GPUtil\n\n    device_names = GPUtil.getGPUs()\n    cuda = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return {\n        \"cuda\": cuda,\n        \"cuda_version\": cuda_version,\n        \"device_names\": [x.name for x in device_names],\n    }\n\n", "CodeBleu_score": 43.85830667877235}, {"code": "def cuda_info()\n\timport torch\n\n    return {\n        \"CUDA\": torch.cuda.is_available(),\n        \"CUDA_version\": torch.version.cuda,\n        \"GPU_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n    }\n\n", "CodeBleu_score": 69.05889396955168}, {"code": "def cuda_info()\n\timport torch\n\n    return {\n        \"GPU device names\": torch.cuda.get_device_name(),\n        \"CUDA availability\": torch.cuda.is_available(),\n        \"CUDA version\": torch.version.cuda,\n    }\n\n", "CodeBleu_score": 41.83039226279246}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Get class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config.encoder_class_name_key\n    class_id_to_class_name = encoder_manager.config.encoder_class_id_to_class_name\n\n    # Setup accuracy counters\n    class_accuracy = dict()\n    total_accuracy = 0\n    total_items = 0\n\n    # Loop over all items in the dataset\n    for item in tqdm(dataset_items):\n        # Get class name\n        class_name = item[class_name_key]\n\n        # Skip item if class name is not available\n        if class_name is None:\n            continue\n\n        # Get class ID\n        class_id = None\n        if class_id_to_class_name is not None:\n            class_id = class_id_to_class_name[class_name]\n\n        # Get audio file\n        audio_file = item['audio_file']\n\n        # Compute embedding\n        embedding = encoder_manager.compute_embedding(audio_file, class_id)\n\n        # Predict class label\n        predicted_class_name = None\n        if class_id_to_class_name is not None:\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n\n        # Update accuracy counters\n        if predicted_class_name is not None:\n            # Update class accuracy\n            if class_name not in class_accuracy:\n                class_accuracy[class_name] = 0\n            if class_name == predicted_class_name:\n                class_accuracy[class_name] += 1\n\n            # Update total accuracy\n            total_accuracy += 1\n\n        # Update total items\n        total_items += 1\n\n    # Print accuracy for each class\n    for class_name in class_accuracy:\n        print('Accuracy for class {}: {:.2f}%'.format(class_name, 100 * class_accuracy[class_name] / total_items))\n\n    #", "CodeBleu_score": 31.860707044590875}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Get the class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_name = encoder_manager.config.class_id_to_name\n    # Compute the encoder accuracy for each item in the dataset\n    class_accuracies = {}\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\", unit=\"items\"):\n        # Retrieve the audio file path\n        audio_filepath = item.audio_filepath\n        # Compute the embedding\n        embeddings = encoder_manager.embed_utterance(audio_filepath)\n        # Compute the class label\n        if class_name_key is not None:\n            # Retrieve the class name\n            class_name = item[class_name_key]\n            # Predict the class ID\n            class_id = encoder_manager.predict_class_id(embeddings)\n            # Record the accuracy for this class\n            if class_id not in class_accuracies:\n                class_accuracies[class_id] = {\"class_name\": class_name, \"count\": 0, \"correct\": 0}\n            class_accuracies[class_id][\"count\"] += 1\n            if class_name == class_id_to_name[class_id]:\n                class_accuracies[class_id][\"correct\"] += 1\n    # Print the accuracy for each class and the average accuracy across all classes\n    print(\"Class name\\tCount\\tCorrect\\tAccuracy\")\n    for class_id, class_accuracy in class_accuracies.items():\n        class_name = class_accuracy[\"class_name\"]\n        count = class_accuracy[\"count\"]\n        correct = class_accuracy[\"correct\"]\n        accuracy = correct / count\n        print(f\"{class_name}\\t{count}\\t{correct}\\t{accuracy:.3f}\")\n    print(f\"Average\\t{sum(class_accuracy['count'] for class_accuracy in class_accuracies.values", "CodeBleu_score": 30.188552467727593}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Load the class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n    # Create a dictionary to store the accuracy for each class\n    class_accuracy = {}\n    # Loop over all items in the dataset\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # Extract the audio file path\n        audio_file = item[class_name_key]\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # Predict the class label if criteria and mappings are available\n        if class_id_to_class_name is not None:\n            # Convert the embedding to a torch tensor\n            embedding = torch.from_numpy(embedding).unsqueeze(0)\n            # Predict the class label\n            class_id = encoder_manager.predict_class_label(embedding)\n            # Convert the class ID to class name\n            class_name = class_id_to_class_name[class_id]\n        else:\n            class_name = None\n        # Store the accuracy for the class\n        if class_name not in class_accuracy:\n            class_accuracy[class_name] = 0\n        # Increment the accuracy for the class\n        class_accuracy[class_name] += 1\n    # Calculate the accuracy for each class\n    for class_name, accuracy in class_accuracy.items():\n        # Calculate the accuracy for the class\n        class_accuracy[class_name] = accuracy / len(dataset_items)\n    # Calculate the average accuracy across all classes\n    average_accuracy = sum(class_accuracy.values()) / len(class_accuracy)\n    # Print the accuracy for each class and the average accuracy\n    print(\"Class Accuracy:\")\n    for class_name, accuracy in class_accuracy.items():\n        print(\"{}: {:.2f}%\".format(class", "CodeBleu_score": 35.50218839628702}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\toutput_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_path = output_path.replace(args.input_ext, args.output_ext)\n    output_dir = os.path.dirname(output_path)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    else:\n        try:\n            audio, sr = torchaudio.load(audio_path)\n            audio = audio.squeeze(0)\n            audio = remove_silence(audio, args.vad_model, args.vad_device, args.vad_threshold, args.vad_window_length, args.vad_moving_average_width)\n            if audio.shape[0] == 0:\n                return output_path, False\n            else:\n                torchaudio.save(output_path, audio, sr)\n                return output_path, True\n        except Exception as e:\n            print(e)\n            return output_path, False\n\n", "CodeBleu_score": 45.700037635776994}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# Adjust output path\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n    # If the file already exists and force is not specified, return the path and a False flag\n    if os.path.exists(audio_path) and not args.force:\n        return audio_path, False\n    # Otherwise, create the necessary directory structure, remove silence from the audio, and save the processed audio\n    os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n    audio = torch.load(audio_path.replace(args.input_dir, args.input_dir_vad))\n    audio, detected = remove_silence(audio, vad_model, vad_params)\n    if detected:\n        torch.save(audio, audio_path)\n    # Finally, return the output path and a flag indicating if speech was detected\n    return audio_path, detected\n\n", "CodeBleu_score": 41.837201740665655}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# Adjust path\n    audio_path = str(audio_path)\n    input_dir = os.path.dirname(args.input_dir)\n    output_dir = os.path.dirname(args.output_dir)\n    if input_dir != output_dir:\n        audio_path = audio_path.replace(input_dir, output_dir)\n    # Check if file exists\n    if os.path.exists(audio_path) and not args.force:\n        return audio_path, False\n    # Create directory structure\n    pathlib.Path(os.path.dirname(audio_path)).mkdir(parents=True, exist_ok=True)\n    # Remove silence\n    if args.remove_silence:\n        vad_model, vad_utils = get_vad_model_and_utils(args.vad_model)\n        audio, sr = vad_utils.load(audio_path, sr=None)\n        audio = remove_silence(audio, vad_model, vad_utils, args.vad_threshold)\n        vad_utils.save(audio, sr, audio_path, args.vad_save_path)\n    return audio_path, True\n\n", "CodeBleu_score": 44.182182490306865}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    else:\n        copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.{}\".format(file_ext)), recursive=True)\n    func_args = [(f, output_sr) for f in files]\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, func_args), total=len(files)))\n    print(\"Done!\")\n\n", "CodeBleu_score": 38.54029437790668}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if not os.path.exists(os.path.join(output_dir, \"resampled\")):\n        os.makedirs(os.path.join(output_dir, \"resampled\"))\n    if not os.path.exists(os.path.join(output_dir, \"resampled\")):\n        os.makedirs(os.path.join(output_dir, \"resampled\"))\n    copytree(input_dir, os.path.join(output_dir, \"resampled\"))\n    input_files = glob.glob(os.path.join(output_dir, \"resampled\", \"**\", \"*.\" + file_ext), recursive=True)\n    func_args = [(filename, output_sr) for filename in input_files]\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, func_args), total=len(func_args)))\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 37.298419753420056}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        copytree(input_dir, output_dir)\n    filenames = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    if len(filenames) == 0:\n        print(\"No files found with extension \" + file_ext + \".\")\n        return\n    func_args = [(filename, output_sr) for filename in filenames]\n    print(\"Resampling \" + str(len(filenames)) + \" files to \" + str(output_sr) + \" Hz...\")\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, func_args), total=len(filenames)))\n    print(\"Done!\")\n\n", "CodeBleu_score": 42.300500726878475}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t# initialize evaluation loss\n    eval_loss = 0.0\n\n    # initialize number of batches\n    num_batches = 0\n\n    # switch to evaluation mode\n    model.eval()\n\n    # deactivate autograd for evaluation\n    with torch.no_grad():\n\n        # iterate over batches\n        for batch in data_loader:\n\n            # compute input and label shape\n            input_shape = (\n                batch.input.size(0),\n                batch.input.size(1),\n                batch.input.size(2) * batch.input.size(3),\n            )\n            label_shape = (\n                batch.label.size(0),\n                batch.label.size(1),\n                batch.label.size(2) * batch.label.size(3),\n            )\n\n            # reshape input and label tensors\n            batch.input = batch.input.view(*input_shape)\n            batch.label = batch.label.view(*label_shape)\n\n            # move input and label tensors to the GPU if available\n            if use_cuda:\n                batch.input = batch.input.cuda()\n                batch.label = batch.label.cuda()\n\n            # compute model output\n            output = model(batch.input)\n\n            # compute loss\n            loss = criterion(output, batch.label)\n\n            # add loss to evaluation loss\n            eval_loss += loss.item()\n\n            # increment number of batches\n            num_batches += 1\n\n            # log statistics\n            if global_step % c.log_interval == 0:\n                print(\n                    \"Eval Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                        epoch,\n                        batch_idx * len(batch.input),\n                        len(data_loader.dataset),\n                        100.0 * batch_idx / len(data_loader),\n                        loss.item(),\n                    )\n                )\n\n    # compute average evaluation loss\n    eval_loss = eval_loss / num_batches\n\n    # log statistics", "CodeBleu_score": 29.668733033640603}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t# switch to eval mode\n    model.eval()\n\n    # initialize evaluation loss\n    eval_loss = 0\n\n    # initialize UMAP plot\n    embeddings = []\n    labels = []\n\n    # iterate over data\n    for i, (inputs, labels) in enumerate(data_loader):\n\n        # reshape inputs to group samples of each class\n        inputs = inputs.view(inputs.size(0), -1, inputs.size(-1))\n\n        # move inputs and labels to device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # compute loss\n        loss = criterion(model(inputs), labels)\n\n        # update evaluation loss\n        eval_loss += loss.item()\n\n        # append embeddings and labels\n        embeddings.append(model.module.embedding.detach().cpu())\n        labels.append(labels.detach().cpu())\n\n        # log evaluation statistics\n        if global_step % c.log_interval == 0:\n            print(\n                f\"Evaluation: [{i+1}/{len(data_loader)}] \"\n                f\"Loss: {loss.item():.6f}\"\n            )\n\n    # average evaluation loss\n    eval_loss /= len(data_loader)\n\n    # concatenate embeddings and labels\n    embeddings = torch.cat(embeddings, dim=0)\n    labels = torch.cat(labels, dim=0)\n\n    # log evaluation statistics\n    print(f\"Evaluation: Average Loss: {eval_loss:.6f}\")\n\n    # generate and log UMAP plot\n    plot_embeddings(embeddings, labels, global_step)\n\n    # return average evaluation loss\n    return eval_loss\n\n", "CodeBleu_score": 29.9492677399102}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    running_loss = 0.0\n    running_loss_per_class = {k: 0.0 for k in range(c.num_classes)}\n    running_loss_per_class_per_utterance = {k: {k2: 0.0 for k2 in range(c.num_utter_per_class)} for k in range(c.num_classes)}\n    running_loss_per_class_per_utterance_count = {k: {k2: 0.0 for k2 in range(c.num_utter_per_class)} for k in range(c.num_classes)}\n    for i, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        for j in range(c.num_classes):\n            running_loss_per_class[j] += loss[labels == j].sum().item()\n            for k in range(c.num_utter_per_class):\n                running_loss_per_class_per_utterance[j][k] += loss[labels == j][k * c.num_classes : (k + 1) * c.num_classes].sum().item()\n                running_loss_per_class_per_utterance_count[j][k] += (labels == j)[k * c.num_classes : (k + 1) * c.num_classes].sum().item()\n    running_loss /= len(data_loader)\n    for j in range(c.num_classes):\n        running_loss_per_class[j] /= running_loss_per_class_per_utterance_count[j][0]\n        for k in range(c.num_utter_per_class):\n            running_loss_per_class_per_utterance[j][k] /= running_loss_per_class_per_utterance_count[j][k]\n    print(\n        \" > E", "CodeBleu_score": 30.500717807431627}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = f\"{model_name.upper()}_CONFIG\"\n    config_class = None\n    if model_name == \"xtts\":\n        try:\n            config_class = getattr(import_module(f\"TTS.config.xtts_configs.{model_name}\"), config_class_name)\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"The model {model_name} is not supported. Please check the documentation for supported models.\"\n            )\n    else:\n        try:\n            config_class = getattr(import_module(f\"TTS.config.tts_configs.{model_name}\"), config_class_name)\n        except ModuleNotFoundError:\n            try:\n                config_class = getattr(import_module(f\"TTS.config.tts_configs.{model_name}\"), config_class_name)\n            except ModuleNotFoundError:\n                try:\n                    config_class = getattr(import_module(f\"TTS.config.tts_configs.{model_name}\"), config_class_name)\n                except ModuleNotFoundError:\n                    raise ModuleNotFoundError(\n                        f\"The model {model_name} is not supported. Please check the documentation for supported models.\"\n                    )\n    return config_class\n\n", "CodeBleu_score": 31.91710210251596}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = f\"{model_name.capitalize()}Config\"\n    if model_name == \"xtts\":\n        try:\n            config_class = getattr(\n                importlib.import_module(f\"TTS.config.{model_name}_config\"),\n                config_class_name,\n            )\n        except ModuleNotFoundError:\n            config_class = getattr(\n                importlib.import_module(f\"TTS.config.{model_name}_config\"),\n                \"XttsConfig\",\n            )\n    else:\n        try:\n            config_class = getattr(\n                importlib.import_module(f\"TTS.config.{model_name}_config\"),\n                config_class_name,\n            )\n        except ModuleNotFoundError:\n            config_class = getattr(\n                importlib.import_module(f\"TTS.config.{model_name}_config\"),\n                \"TtsConfig\",\n            )\n    return config_class\n\n", "CodeBleu_score": 30.935199859945435}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = f\"{model_name.capitalize()}Config\"\n    try:\n        if model_name == \"xtts\":\n            config_class = getattr(\n                importlib.import_module(\"TTS.config.xtts_config\"), config_class_name\n            )\n        else:\n            config_class = getattr(\n                importlib.import_module(\"TTS.config.tts_config\"), config_class_name\n            )\n    except ModuleNotFoundError:\n        for path in CONFIG_PATHS:\n            try:\n                config_class = getattr(\n                    importlib.import_module(f\"{path}.{model_name.capitalize()}Config\"),\n                    config_class_name,\n                )\n                break\n            except ModuleNotFoundError:\n                continue\n        else:\n            raise ModuleNotFoundError(\n                f\"Could not find the configuration class for {model_name}.\"\n            )\n    return config_class\n\n", "CodeBleu_score": 34.53262882964878}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = None\n    if config_path.endswith(\".yaml\"):\n        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown file type for {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class(config_dict)\n    return config\n\n", "CodeBleu_score": 40.00986483254663}, {"code": "def load_config(config_path: str) -> Coqpit\n\t# determine the file extension\n    file_extension = os.path.splitext(config_path)[1]\n    if file_extension == \".yaml\":\n        config_dict = yaml.load(open(config_path, \"r\"), Loader=yaml.FullLoader)\n    elif file_extension == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\"[!] Unknown file extension: {file_extension}\")\n\n    # get the model name\n    model_name = _process_model_name(config_dict)\n\n    # register the appropriate config class\n    config_class = register_config(model_name)\n\n    # create an instance of the config class\n    config = config_class()\n\n    # populate the config with the loaded data\n    config.populate(config_dict)\n\n    return config\n\n", "CodeBleu_score": 42.276541289411895}, {"code": "def load_config(config_path: str) -> Coqpit\n\t# read the file\n    if config_path.endswith(\".yaml\"):\n        config_dict = yaml.load(open(config_path, \"r\"), Loader=yaml.FullLoader)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\"[!] Unknown config file type: {config_path}\")\n\n    # get the model name\n    model_name = _process_model_name(config_dict)\n\n    # register the config class\n    config_class = register_config(model_name)\n\n    # create the config object\n    config = config_class()\n\n    # populate the config object\n    config.from_dict(config_dict)\n\n    # return the config object\n    return config\n\n", "CodeBleu_score": 41.886556946582445}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t# return the set of files that are valid\n    return set(\n        [\n            os.path.join(dp, f)\n            for dp, dn, fn in os.walk(basePath)\n            for f in fn\n            if (\n                (validExts is None or os.path.splitext(f)[1].lower() in validExts)\n                and (contains is None or contains in f)\n            )\n        ]\n    )\n\n", "CodeBleu_score": 30.5233432709281}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\tfor root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            ext = os.path.splitext(file)[1]\n            if validExts is not None and ext.lower() not in validExts:\n                continue\n            yield os.path.join(root, file)\n\n", "CodeBleu_score": 38.79450891440598}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\tif validExts is None:\n        validExts = tuple()\n    if not isinstance(validExts, tuple):\n        validExts = tuple(validExts)\n    for root, dirs, files in os.walk(basePath):\n        for f in files:\n            if contains is not None and contains not in f:\n                continue\n            ext = os.path.splitext(f)[1].lower()\n            if ext in validExts:\n                yield os.path.join(root, f)\n\n", "CodeBleu_score": 43.454237927968705}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t# clear the GPU cache\n    clear_gpu_cache()\n    # check if the required paths are provided\n    if not xtts_checkpoint:\n        print(\"Please provide a valid checkpoint path\")\n        return\n    if not xtts_config:\n        print(\"Please provide a valid config path\")\n        return\n    if not xtts_vocab:\n        print(\"Please provide a valid vocab path\")\n        return\n    # load the configuration from the provided path\n    config = XttsConfig.from_json_file(xtts_config)\n    # initialize the XTTS model\n    model = Xtts(config)\n    # load the model checkpoint and vocabulary\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    model.load_state_dict(torch.load(xtts", "CodeBleu_score": 36.96598883806544}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t# clear the GPU cache\n    clear_gpu_cache()\n    # check if the required paths are provided\n    if not xtts_checkpoint:\n        raise ValueError(\"Please provide a checkpoint path for the XTTS model.\")\n    if not xtts_config:\n        raise ValueError(\"Please provide a config path for the XTTS model.\")\n    if not xtts_vocab:\n        raise ValueError(\"Please provide a vocab path for the XTTS model.\")\n    # load the configuration from the provided path\n    config = XttsConfig(xtts_config)\n    # initialize the XTTS model\n    model = Xtts(config)\n    # load the model checkpoint and vocabulary\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.load_state_dict(tor", "CodeBleu_score": 34.39393051984424}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if not xtts_checkpoint:\n        return \"Please provide a valid checkpoint path.\"\n    if not xtts_config:\n        return \"Please provide a valid config path.\"\n    if not xtts_vocab:\n        return \"Please provide a valid vocab path.\"\n    print(\"Loading configuration...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    print(\"Initializing model...\")\n    model = Xtts(config)\n    print(\"Loading checkpoint...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"model\"])\n    print(\"Loading vocab...\")\n    model.load_state_dict(torch.load(xtts_vocab, map_location=\"cpu\")[\"model\"])\n    if torch.cuda.is_available():\n        model = model.cuda()\n    print(\"Model loaded.\")\n    return \"Model loaded.\"\n\n", "CodeBleu_score": 48.27598121370805}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# Compute number of frames based on hop length\n    num_frames = (num_frames - self.audio_config[\\\"hop_length\\\"] + 1) // self.audio_config[\\\"hop_length\\\"]\n\n    # Ensure num frames is not greater than input length\n    num_frames = min(num_frames, x.shape[1])\n\n    # Compute offset positions for evaluation\n    offset_positions = np.arange(0, num_frames, num_frames / num_eval)\n    offset_positions = np.round(offset_positions).astype(int)\n\n    # Extract frames at these offsets\n    x_frames = []\n    for offset in offset_positions:\n        x_frames.append(x[:, offset * self.audio_config[\\\"hop_length\\\"] : (offset + 1) * self.audio_config[\\\"hop_length\\\"]])\n\n    # Concatenate frames and perform inference\n    x_frames = torch.cat(x_frames, dim=1)\n    embeddings = self.inference(x_frames, l2_norm=l2_norm)\n\n    # Compute mean of embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=1)\n\n    return embeddings\n\n", "CodeBleu_score": 46.06946750260911}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# Adjust number of frames based on hop length\n    if self.hop_length != self.frame_shift:\n        num_frames = num_frames - (num_frames - 1) * (self.hop_length - self.frame_shift)\n\n    # Ensure number of frames does not exceed input length\n    num_frames = min(num_frames, int(x.shape[1] / self.hop_length))\n\n    # Calculate offset positions for evaluation\n    offset_positions = np.linspace(0, x.shape[1] - num_frames * self.hop_length, num_eval)\n    offset_positions = (\n        np.clip(offset_positions, 0, x.shape[1] - num_frames * self.hop_length)\n        .astype(int)\n        .tolist()\n    )\n\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    embeddings = []\n    for offset in offset_positions:\n        frames = x[:, offset : offset + num_frames * self.hop_length].unsqueeze(0)\n        embeddings.append(self.inference(frames, l2_norm=l2_norm))\n    embeddings = torch.cat(embeddings, dim=0)\n\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = F.normalize(embeddings, p=2, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 42.26547054527467}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# Adjust the number of frames based on the hop length\n    num_frames = num_frames // self.audio_config[\"hop_length\"] + 1\n    # Ensure that the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[0])\n    # Calculate offset positions for evaluation\n    offsets = np.arange(0, x.shape[0] - num_frames + 1, num_frames)\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference\n    embeddings = []\n    for offset in offsets:\n        x_embed = x[offset : offset + num_frames]\n        embeddings.append(self.inference(x_embed, l2_norm=l2_norm))\n    embeddings = np.concatenate(embeddings, axis=0)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(axis=0, keepdims=True)\n    if l2_norm:\n        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    return embeddings\n\n", "CodeBleu_score": 42.931339305971036}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 78.20358793718887}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.37285942574928}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.92381336384244}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\tif num_classes_in_batch > 10:\n        print(\"Too many classes in this batch, plotting only the first 10\")\n        num_classes_in_batch = 10\n    num_utterances_per_class = np.zeros(num_classes_in_batch)\n    for i in range(len(embeddings)):\n        num_utterances_per_class[int(embeddings[i][-1])] += 1\n    embeddings = embeddings[:, 0:2]\n    reducer = umap.UMAP(n_neighbors=15, min_dist=0.3, metric=\"correlation\")\n    embeddings = reducer.fit_transform(embeddings)\n    plt.figure(figsize=(10, 10))\n    plt.title(\"UMAP projection\", fontsize=20)\n    plt.scatter(\n        embeddings[:, 0],\n        embeddings[:, 1],\n        s=5,\n        c=colormap[embeddings[:, -1].astype(int)],\n    )\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.savefig(\"umap.png\")\n    plt.close()\n    return plt\n\n", "CodeBleu_score": 42.44091060245417}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\tnum_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = num_classes_in_batch // num_classes\n    embeddings_per_class = []\n    for i in range(num_classes):\n        embeddings_per_class.append(embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class])\n    embeddings_per_class = np.concatenate(embeddings_per_class, axis=0)\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.0, metric=\"cosine\")\n    embeddings_reduced = reducer.fit_transform(embeddings_per_class)\n    fig = plt.figure()\n    plt.scatter(embeddings_reduced[:, 0], embeddings_reduced[:, 1], c=np.arange(num_classes))\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.savefig(\"umap\")\n    plt.close()\n    return fig\n\n", "CodeBleu_score": 27.44338781866839}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate the number of utterances per class\n    num_utts_per_class = np.zeros(num_classes_in_batch)\n    for i in range(num_classes_in_batch):\n        num_utts_per_class[i] = np.count_nonzero(embeddings[:, 0] == i)\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric=\"cosine\")\n    embeddings_2d = reducer.fit_transform(embeddings)\n\n    # A scatter plot is created with colors representing different classes\n    plt.figure(figsize=(10, 10))\n    for i in range(num_classes_in_batch):\n        plt.scatter(\n            embeddings_2d[np.where(embeddings[:, 0] == i), 0],\n            embeddings_2d[np.where(embeddings[:, 0] == i), 1],\n            s=num_utts_per_class[i] * 2,\n            c=colormap[i],\n        )\n    plt.title(\"UMAP projection\", fontsize=20)\n    plt.axis(\"equal\")\n    plt.savefig(\"umap\")\n    plt.close()\n\n    return plt.gcf()\n\n", "CodeBleu_score": 43.159505137744745}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tN, M, D = dvecs.shape\n    cosine_sim = []\n    for spkr in range(N):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, 0)\n        for utt in range(M):\n            sim = F.cosine_similarity(dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1)\n            sim = torch.clamp(sim, min=1e-6)\n            cosine_sim.append(sim)\n    cosine_sim = torch.cat(cosine_sim, dim=0)\n    return cosine_sim\n\n", "CodeBleu_score": 31.930997476967704}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t# pylint: disable=E1102\n    # pylint: disable=E1101\n    cos_sims = []\n    for spkr in range(len(dvecs)):\n        for utt in range(len(dvecs[spkr])):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sims.append(\n                F.cosine_similarity(dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1)\n            )\n    return torch.cat(cos_sims).unsqueeze(0)\n\n", "CodeBleu_score": 28.684203301912348}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcosine_sim = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cosine_sim.append(F.cosine_similarity(dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1).clamp(min=0.001))\n    return torch.cat(cosine_sim)\n\n", "CodeBleu_score": 28.529537740679405}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        loss_row = []\n        for utt_idx, utterance in enumerate(speaker):\n            loss_row.append(\n                -torch.log(\n                    cos_sim_matrix[spkr_idx, utt_idx, spkr_idx]\n                    / torch.sum(cos_sim_matrix[spkr_idx, utt_idx, :])\n                )\n            )\n        loss_row = torch.stack(loss_row)\n        losses.append(loss_row)\n    return torch.stack(losses)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrast.", "CodeBleu_score": 34.447332642806074}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            losses.append(\n                F.log_softmax(cos_sim_matrix[spkr_idx, utt_idx], dim=0)\n                * cos_sim_matrix[spkr_idx, utt_idx]\n            )\n    return torch.cat(losses, dim=0)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 24.95186920922815}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tembed_loss = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            # get the cosine similarity scores for the current embedding\n            cos_sim_scores = cos_sim_matrix[spkr_idx, utt_idx, :]\n            # get the softmax of the cosine similarity scores\n            softmax = F.softmax(cos_sim_scores, dim=0)\n            # get the negative log of the softmax\n            neg_log = -torch.log(softmax)\n            # get the loss for the current embedding\n            embed_loss.append(neg_log[spkr_idx])\n    return torch.stack(embed_loss)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrast.", "CodeBleu_score": 27.537352933058905}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.logsigmoid(cos_sim_matrix[j, i, j]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 42.60483087324748}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(\n                -torch.log(\n                    torch.sigmoid(\n                        torch.clamp(\n                            cos_sim_matrix[j, i, :] - cos_sim_matrix[j, i, j],\n                            1e-6,\n                        )\n                    )\n                )\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 46.50993971651392}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tN, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.logsigmoid(cos_sim_matrix[j, i, j]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 42.60483087324748}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t# pylint: disable=W0212\n    # pylint: disable=R0914\n    # pylint: disable=R0912\n    # pylint: disable=R0915\n    # pylint: disable=R0913\n    # pylint: disable=R0911\n    # pylint: disable=R0916\n    # pylint: disable=R0915\n    # pylint: disable=R0914\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable=R0916\n    # pylint: disable=R0915\n    # pylint: disable=R0914\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable=R0916\n    # pylint: disable=R0915\n    # pylint: disable=R0914\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable=R0916\n    # pylint: disable=R0915\n    # pylint: disable=R0914\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable=R0916\n    # pylint: disable=R0915\n    # pylint: disable=R0914\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t# pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t# pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable=W0212\n    # pylint: disable", "CodeBleu_score": 3.6585365853658542}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in [*voice_dirs, *extra_voice_dirs]:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n            if not voices.get(root):\n                for file in files:\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        if root not in voices:\n                            voices[root] = []\n                        voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 27.79600993972445}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t# get the directory that this file is in\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    # get the path to the voices directory\n    voices_dir = os.path.join(dir_path, \"voices\")\n    # get the list of voice directories\n    voice_dirs = [voices_dir]\n    voice_dirs.extend(extra_voice_dirs)\n    # get the list of voice files\n    voice_files = {}\n    for voice_dir in voice_dirs:\n        # get the list of files in the voice directory\n        files = glob(os.path.join(voice_dir, \"**\", \"*\"), recursive=True)\n        # get the list of voice files\n        voice_files[voice_dir] = []\n        # iterate through each file\n        for file in files:\n            # check if the file is a .npz file\n            if file.endswith(\".npz\"):\n                # append the file to the list of voice files\n                voice_files[voice_dir].append(file)\n            # check if the file is a .wav or .mp3 file\n            elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                # append the file to the list of voice files\n                voice_files[voice_dir].append(file)\n    # return the list of voice files\n    return voice_files\n\n", "CodeBleu_score": 25.61328955546045}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in [VOICE_DIR] + extra_voice_dirs:\n        for subdir, dirs, files in os.walk(voice_dir):\n            if len(files) > 0:\n                voices[subdir] = []\n                for file in files:\n                    if file.endswith(\".npz\"):\n                        voices[subdir].append(os.path.join(subdir, file))\n                    elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[subdir].append(os.path.join(subdir, file))\n            else:\n                for file in files:\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[subdir] = [os.path.join(subdir, file)]\n    return voices\n\n", "CodeBleu_score": 29.77459558296653}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    dirs = [\"./TTS/tts/layers/bark/hubert/pretrained_voices\"]\n    dirs.extend(extra_voice_dirs)\n    voices = get_voices(dirs)\n\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found in {dirs}\")\n\n    voice_paths = voices[voice]\n\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n\n    voice_path = voice_paths[0]\n\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        voice_path = convert_audio(voice_path)\n        semantic, coarse, fine = model.generate_voice(voice_path)\n\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 49.3545506201477}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n    dirs = [\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\"),\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices_extra\"),\n    ]\n    dirs.extend(extra_voice_dirs)\n    voices = get_voices(dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found in any of the specified directories.\")\n    if len(voices[voice]) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}, please specify the exact path.\")\n    voice_path = voices[voice][0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice_path} does not exist.\")\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    else:\n        voice_path = convert_audio(voice_path)\n        return load_npz(voice_path)\n\n", "CodeBleu_score": 45.804017431223734}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tdirs = [model.voice_dir] + extra_voice_dirs\n    voices = get_voices(extra_voice_dirs=extra_voice_dirs)\n    voice_paths = voices.get(voice, [])\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Found multiple voices for {voice} in {dirs}. Please specify the voice by its path.\"\n        )\n    if len(voice_paths) == 0:\n        raise ValueError(f\"Could not find voice {voice} in {dirs}.\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Could not find voice {voice_path}.\")\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        semantic, coarse, fine = generate_voice(\n            model, voice_path, extra_voice_dirs=extra_voice_dirs\n        )\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 41.098509658534056}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \\\"\\\"\\\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \\\"\\\"\\\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n", "CodeBleu_score": 25.42919006581103}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t# create a new list to store the processed contexts\n    processed_contexts = []\n\n    # iterate over the batch elements\n    for i, (context_i, len_i) in enumerate(zip(context, lens)):\n        # extract the context up to the specified length\n        context_i = context_i[:, :len_i]\n\n        # apply the convolutional layers\n        for conv_layer in self.convolutions:\n            context_i = conv_layer(context_i)\n\n        # append the processed context to the list\n        processed_contexts.append(context_i)\n\n    # create a new tensor with the processed contexts\n    processed_contexts = torch.stack(processed_contexts)\n\n    # pad the processed contexts to create a consistent batch size\n    processed_contexts = nn.utils.rnn.pad_sequence(processed_contexts, batch_first=True)\n\n    # return the processed contexts\n    return processed_contexts\n\n", "CodeBleu_score": 31.716343027670174}, {"code": "def run_padded_sequence(self, context, lens)\n\t# TODO: add batch size check\n    # TODO: add length check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check\n    # TODO: add context check\n    # TODO: add lens check", "CodeBleu_score": 1.459942084942085}, {"code": "def run_padded_sequence(self, context, lens)\n\tbatch_size = context.size(0)\n    max_len = context.size(2)\n    output = []\n    for i in range(batch_size):\n        len_ = lens[i]\n        len_ = min(len_, max_len)\n        context_ = context[:, :, :len_]\n        context_ = self.conv_layers(context_)\n        output.append(context_)\n    output = torch.stack(output, 0)\n    return output\n\n", "CodeBleu_score": 26.3094184162112}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    fn_results = fn(packed_context)[0]\n    unpacked_fn_results, _ = nn.utils.rnn.pad_packed_sequence(fn_results, batch_first=True)\n    fn_results = unpacked_fn_results[sorted_indices.argsort()]\n    return fn_results\n\n", "CodeBleu_score": 18.95571954717436}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(\n        sorted_context, sorted_lens.tolist(), batch_first=True\n    )\n    packed_output = fn(packed_context)\n    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    _, unsorted_indices = torch.sort(sorted_indices)\n    output = output[unsorted_indices]\n    return output\n\n", "CodeBleu_score": 18.352017456229532}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    packed_outputs = fn(packed_context)\n    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n    _, unsorted_indices = torch.sort(sorted_indices)\n    outputs = outputs[unsorted_indices]\n    return outputs\n\n", "CodeBleu_score": 16.915081303298763}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 26.13505344630211}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 26.13505344630211}, {"code": "def remove_weight_norm(self)\n\ttorch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n", "CodeBleu_score": 26.13505344630211}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'seresnet')\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'seresnet')\n    if mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inceptionv3_advanced')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 47.74752192094823}, {"code": "def map_mx_to_torch_model(mx_name)\n\ttorch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'se_resnet_')\n    torch_name = torch_name.replace('senet_', 'senet_')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n\n", "CodeBleu_score": 49.62466283960772}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = 'senet_' + mx_name[6:]\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 34.64675185512371}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\tnum_samples = self.num_samples // self.dist_num_replicas\n    if self.is_training or self.repeats > 1:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 29.581271139358822}, {"code": "def _num_samples_per_worker(self)\n\tif self.num_samples is None:\n        raise ValueError(\n            \"Dataset length is unknown, please pass `num_samples` explicitely. \"\n            \"The number of steps needs to be known in advance for the learning rate scheduler.\"\n        )\n    if self.repeats > 0:\n        num_samples = self.num_samples * self.repeats\n    else:\n        num_samples = self.num_samples\n    if self.is_training or self.dist_num_replicas > 1:\n        num_samples = math.ceil(num_samples / self.dist_num_replicas)\n    if self.batch_size is not None and self.batch_size > 1:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 44.28985592422083}, {"code": "def _num_samples_per_worker(self)\n\tif self.num_samples is not None:\n        return self.num_samples\n    if self.repeats > 1:\n        # repeats is a user-specified parameter, so we round up\n        num_samples = math.ceil(self.ds.num_samples * self.repeats / self.dist_num_replicas)\n    else:\n        num_samples = self.ds.num_samples\n    if self.batch_size is not None:\n        # batch_size is a user-specified parameter, so we round up\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 47.37953965409969}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    if not os.path.isdir(folder):\n        raise NotADirectoryError(f'{folder} is not a directory')\n    images = []\n    dirs = [d.name for d in os.scandir(folder) if d.is_dir()]\n    if leaf_name_only:\n        dirs = [d.split(os.sep)[-1] for d in dirs]\n    for target in dirs:\n        d = os.path.join(folder, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, types):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx.get(target, None))\n                    images.append(item)\n    if sort:\n        images.sort(key=lambda x: natural_key(x[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 36.92381603577953}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t# A set of valid image extensions\n    if types is None:\n        types = get_img_extensions()\n    types = {t.lower() for t in types}\n\n    # A set of class labels\n    classes = set()\n\n    # A list of (image path, class index) pairs\n    image_paths_and_targets = []\n\n    # Walk through the folder tree and search for image files\n    for root, dirs, files in os.walk(folder, followlinks=True):\n        # Skip any folders that are not sorted into classes\n        if leaf_name_only:\n            dirs[:] = [d for d in dirs if os.path.isfile(os.path.join(root, d))]\n\n        for f in files:\n            f_path = os.path.join(root, f)\n            f_ext = os.path.splitext(f)[1].lower()\n            if f_ext in types:\n                classes.add(os.path.basename(root))\n                image_paths_and_targets.append((f_path, classes.index(os.path.basename(root))))\n\n    # Build a class-to-index mapping\n    if class_to_idx is None:\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(sorted(classes, key=natural_key))}\n    elif sorted(class_to_idx.keys()) != sorted(classes, key=natural_key):\n        raise ValueError(\n            f\"The class-to-index mapping does not match the folder structure.", "CodeBleu_score": 42.25506021823456}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif not os.path.isdir(folder):\n        raise FileNotFoundError(f'{folder} is not a directory')\n    if class_to_idx is None:\n        class_to_idx = {}\n    if types is None:\n        types = get_img_extensions()\n    class_to_idx = {k: v for k, v in class_to_idx.items()}\n    class_to_idx = {k: v for k, v in sorted(class_to_idx.items(), key=natural_key)}\n    classes = set(class_to_idx.keys())\n    images_and_targets = []\n    for target in sorted(os.listdir(folder)):\n        d = os.path.join(folder, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, types):\n                    path = os.path.join(root, fname)\n                    item = path\n                    if leaf_name_only:\n                        item = fname\n                    target = target\n                    if not leaf_name_only:\n                        target = os.path.relpath(root, folder)\n                    if target not in classes:\n                        if target == '':\n                            target = '.'\n                        classes.add(target)\n                        class_to_idx[target] = len(class_to_idx)\n                    images_and_targets.append((item, class_to_idx[target]))\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 37.72691837257489}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tif wds is None:\n        return None\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return None\n    return worker_info.seed % 2**32\n\n", "CodeBleu_score": 67.11835550493839}, {"code": "def pytorch_worker_seed()\n\ttry:\n        worker_info = get_worker_info()\n        worker_seed = worker_info.seed\n    except ValueError:\n        worker_seed = None\n\n    if worker_seed is None:\n        worker_seed = wds_rank()\n\n    return worker_seed\n\n", "CodeBleu_score": 42.762740170599415}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2**32\n    return wds_rank() % 2**32\n\n", "CodeBleu_score": 51.87852094933666}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 59.10557318594345}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lut)\n    return img", "CodeBleu_score": 59.10557318594345}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    return img", "CodeBleu_score": 85.74403031881391}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n", "CodeBleu_score": 7.914260028340872}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tn        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,", "CodeBleu_score": 2.662687882582535}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning(\n            \"The length of transforms in x is %d, but n is %d. \"\n            \"The mean of x is calculated and repeated to match n.\",\n            len(x),\n            n,\n        )\n        x = (np.mean(x, axis=0),) * n\n    assert len(x) == n, \"The length of x must be n.\"\n    return x\n\n", "CodeBleu_score": 47.46471434169835}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning(\"The length of transforms is not equal to the number of channels. \"\n                        \"The mean of transforms will be used.\")\n        x = (np.mean(x, axis=0),) * n\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 46.36076659696986}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning(f\"The length of x is {len(x)}, but it should be {n}.\")\n        x = (np.mean(x, axis=0),) * n\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 43.69189161105208}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t9d5bf\n", "CodeBleu_score": 4.502688172043011}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 64.8572898609103}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\twith tf.name_scope('decode_and_center_crop'):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        image_height = shape[0]\n        image_width = shape[1]\n        padded_center_crop_size = tf.cast(\n            ((image_size / (image_size + CROP_PADDING)) *\n             tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n            tf.int32)\n        offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n        offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n        crop_window = tf.stack([offset_height, offset_width,\n                                padded_center_crop_size, padded_center_crop_size])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n        image = tf.image.resize([image], [image_size, image_size],\n                                resize_method)[0]\n        return image\n\n", "CodeBleu_score": 94.76662038832868}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\twith tf.name_scope('decode_and_center_crop', [image_bytes, image_size]):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        image_height = shape[0]\n        image_width = shape[1]\n\n        padded_center_crop_size = tf.cast(\n            ((image_size / (image_size + CROP_PADDING)) *\n             tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n            tf.int32)\n\n        offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n        offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n        crop_window = tf.stack([offset_height, offset_width,\n                                padded_center_crop_size, padded_center_crop_size])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n        image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n        return image\n\n", "CodeBleu_score": 61.92900513825579}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 39.141567712765784}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.54727842231387}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.54727842231387}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tas tf", "CodeBleu_score": 4.308069381598793}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif self.metric is None:\n        raise ValueError(\"self.metric is\", self.metric)\n    metric_cls = getattr(metric_path, self.metric)\n    self.metric = metric_cls(self)\n    if self.predictor is None:\n        raise ValueError(\"self.predictor is\", self.predictor)\n    predictor_cls = getattr(predictor_path, self.predictor)\n    self.predictor = predictor_cls(self)\n    self.eval_dataloader = eval_dataloader\n\n    def __call__(self):\n        try:\n            print(self.predictor.pred_dir)\n            for pred_file in glob.glob(\n                self.predictor.pred_dir + \"/*_merged.npy\"\n            ):\n                outputs = np.load(pred_file)\n                results = self.metric.compute_metrics(outputs)\n                self.metric.print_computed_metrics(results)\n\n            outputs = np.load(os.path.join(self.predictor.pred_dir, \"merged.npy\"))\n            results = self.metric.compute_metrics(outputs)\n            return {\"results\": results, \"metric\": self.metric}\n        except FileNotFoundError:\n            print(\"", "CodeBleu_score": 32.11301185570564}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif self.metric is None:\n        raise ValueError(\"self.metric is\", self.metric)\n    metric_cls = getattr(metric_path, self.metric)\n    self.metric = metric_cls(self)\n    if self.predictor is None:\n        raise ValueError(\"self.predictor is\", self.predictor)\n    predictor_cls = getattr(predictor_path, self.predictor)\n    self.predictor = predictor_cls(self)\n    self.eval_dataloader = eval_dataloader\n    try:\n        print(self.predictor.pred_dir)\n        for pred_file in glob.glob(self.predictor.pred_dir + \"/*_merged.npy\"):\n            outputs = np.load(pred_file)\n            results = self.metric.compute_metrics(outputs)\n            self.metric.print_computed_metrics(results)\n\n        outputs = np.load(os.path.join(self.predictor.pred_dir, \"merged.npy\"))\n        results = self.metric.compute_metrics(outputs)\n        return {\"results\": results, \"metric\": self.metric}\n    except FileNotFoundError:\n        print(\"", "CodeBleu_score": 32.59886972151862}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif self.metric is None:\n        raise ValueError(\"self.metric is\", self.metric)\n    metric_cls = getattr(metric_path, self.metric)\n    self.metric = metric_cls(self)\n    if self.predictor is None:\n        raise ValueError(\"self.predictor is\", self.predictor)\n    predictor_cls = getattr(predictor_path, self.predictor)\n    self.predictor = predictor_cls(self)\n    self.eval_dataloader = eval_dataloader\n\n    self.predictor.predict(model, self.eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \"_merged.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n\n    outputs = np.load(os.path.join(self.predictor.pred_dir, \"merged.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 34.28682272132069}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\\\"rank1\\\"]\n    rank5 = metrics[\\\"rank5\\\"]\n    miou = metrics[\\\"miou\\\"]\n    print(\n        \\\"Rank@1: {:.4f} - Rank@5: {:.4f} - mIoU: {:.4f}\\\".format(\n            rank1, rank5, miou\n        )\n    )\n\n", "CodeBleu_score": 32.83635435332402}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[self.metric_names[0]]\n    rank5 = metrics[self.metric_names[1]]\n    miou = metrics[self.metric_names[2]]\n    print(\n        \"rank1: {:.4f} - rank5: {:.4f} - miou: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )\n\n\n", "CodeBleu_score": 41.15961304176171}, {"code": " def print_computed_metrics(self, metrics)\n\trank1 = metrics[\\\"rank1\\\"]\n    rank5 = metrics[\\\"rank5\\\"]\n    miou = metrics[\\\"miou\\\"]\n    print(\n    \\\"Rank1: {:.4f} - Rank5: {:.4f} - mIoU: {:.4f}\\\".format(\n        rank1, rank5, miou\n    )", "CodeBleu_score": 28.52388591151073}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\trank1 = 0\n    rank5 = 0\n    ious = []\n    for i, segment in enumerate(segments):\n        # Calculate IoUs between segment and all ground truth segments.\n        # If there is no ground truth segment, the IoU is 0.\n        iou_values = []\n        for gt in data[i]:\n            iou_values.append(self._iou(segment, gt))\n        # Calculate the rank of the segment.\n        # If there is no ground truth segment, the rank is 0.\n        rank_values = []\n        for gt in data[i]:\n            rank_values.append(self._rank(segment, gt))\n        # Calculate the mean IoU and rank across all ground truth segments.\n        iou = np.mean(iou_values)\n        rank = np.mean(rank_values)\n        # Calculate the percentage of segments with rank 1 and rank 5.\n        rank1 += rank == 1\n        rank5 += rank <= 5\n        ious.append(iou)\n    # Calculate the mean IoU and rank across all segments.\n    rank1 = rank1/len(data)*100\n    rank5 = rank5/len(data)*100\n    miou = np.mean(ious)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 35.06445849279263}, {"code": "def _eval_predictions(self, segments, data)\n\trank1 = 0\n    rank5 = 0\n    ious = []\n    for i in range(len(segments)):\n        segment = segments[i]\n        groundtruth = data[i]\n        # calculate IoUs\n        ious_per_segment = []\n        for j in range(len(segment)):\n            pred = segment[j]\n            iou = self._iou(pred, groundtruth)\n            ious_per_segment.append(iou)\n        # calculate rank\n        rank = self._rank(ious_per_segment, groundtruth)\n        # update rank1 and rank5\n        if rank == 1:\n            rank1 += 1\n        if rank <= 5:\n            rank5 += 1\n        # update mean IoU\n        ious.append(np.mean(ious_per_segment))\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n    miou = np.mean(ious)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 36.303205183341156}, {"code": "def _eval_predictions(self, segments, data)\n\trank1, rank5, iou = 0, 0, 0\n    for segment, gt in zip(segments, data):\n        gt = [tuple(g) for g in gt]\n        ious = [self._iou(pred, gt) for pred in segment]\n        rank1 += self._rank(ious, 1)\n        rank5 += self._rank(ious, 5)\n        iou += sum(ious)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    iou /= len(segments)\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 23.527376253708162}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# set model to eval mode\n    model.eval()\n    model.to(self.device)\n\n    # initialize empty lists for scores\n    all_results = []\n\n    # iterate over the dataloader\n    for step, inputs in enumerate(tqdm(eval_dataloader)):\n        # transfer data to appropriate context\n        inputs = self._transfer_to_gpu(inputs, self.device)\n\n        # make predictions\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # update the results\n        all_results.extend(self._process_outputs(outputs))\n\n    # finalize and return the results\n    return self._finalize_results(all_results, output_file)\n\n", "CodeBleu_score": 34.1275005667807}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# initialize empty lists for scores\n    self.scores = []\n    # set model to evaluation mode\n    model.eval()\n    # transfer model to GPU 0\n    model.cuda()\n    # iterate over the dataloader\n    for data in tqdm(eval_dataloader):\n        # transfer data to the appropriate context\n        data = tuple(t.cuda(non_blocking=True) for t in data)\n        # make predictions without computing gradients\n        with torch.no_grad():\n            # get the predictions\n            output = model(*data)\n        # update the outputs\n        self.update_output(output)\n    # finalize and return the results\n    return self.finalize()\n\n", "CodeBleu_score": 44.54121087245522}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# Setup\n    model.eval()\n    model.to(self.device)\n    outputs = []\n    # Loop\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n            batch = self._transfer_batch_to_device(batch, self.device)\n            outputs.append(self._make_prediction(model, batch))\n    # Finalize\n    outputs = self._process_outputs(outputs)\n    if output_file is not None:\n        self._save_results(output_file, outputs)\n    return outputs\n\n", "CodeBleu_score": 32.74004651957739}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}, {"code": "def _aggregate_scores(self, scores)\n\tnimport random", "CodeBleu_score": 2.195355177078013}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tcom/DmZhukov/CrossTask`\n        from dp import dp\n        vid, task = sample['video_id'][0], sample['task'][0]\n        sample = self.to_ctx(sample)\n        # compute the average logits over sliding windows.\n        output = model(**sample)\n        batch_logits = output[\\\"logits\\\"].cpu()\n\n        video_len = sample[\\\"video_len\\\"][0]\n\n        # the following version is slow.\n        logits = torch.zeros((video_len, batch_logits.size(1)))\n        logits_counts = torch.zeros((video_len, 1), dtype=torch.long)\n        # use the same loop as aligner to recover.\n        batch_logit_idx = 0\n        for window_start in range(0, video_len, self.sliding_window):\n            video_end = min(video_len - window_start, self.sliding_window_size)\n            logits[window_start: window_start + video_end] += batch_logits[\n                batch_logit_idx: batch_logit_idx + video_end]\n            batch_logit_idx += video_end\n            logits_counts[window_start: window_start + video_end] += torch.ones((video_end, 1), dtype=torch.long)\n\n            if (video_len - window_start) <= self.sliding_window_size:\n                break\n\n        logits /= logits_counts\n        assert logits.size() == (video_len, batch_logits.size(1)), \\\"{}, {}\\\".format(logits.size(), video_len)\n\n        O = self.lsm(logits)\n        y = np.zeros(O.size(), dtype=np.float32)\n        dp(y, -O.detach().cpu().numpy())\n        if task not in Y_pred:\n            Y_pred[task] = {}\n        Y_pred[task][vid] = y\n        annot_path = os.path.join(\n            self.annotation_path, task+'_'+vid+'.csv')\n        if os.path.exists(annot_path):\n            if task not in Y_true:\n                Y_true[task] = {}\n            Y_true[task][vid] = self._read_assignment(\n                *y.shape, annot_path)\n\n    def finalize(self, Y_pred, Y_true, output_file=None):\n        if output_file is not None:\n            with open(\n                    os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                    \\\"wb\\\") as fw:\n                pickle.dump(\n                    {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                    protocol=pickle.HIGHEST_PROTOCOL)\n        return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 32.49948780157978}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.2318132436907}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tsample_size = 0\n    loss = 0\n    for log in logging_outputs:\n        loss += log[\"loss\"]\n        sample_size += log[\"sample_size\"]\n    metrics.log_scalar(\"loss\", loss / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 32.33784724289956}, {"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.2318132436907}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tFace Inc. team.", "CodeBleu_score": 3.160098991052877}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t(the \\\"License\\\");", "CodeBleu_score": 5.369863013698663}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t# remove 'img_id' from meta_keys if present\n    if 'img_id' in cfg.get('meta_keys', []):\n        cfg.meta_keys.remove('img_id')\n\n    # find the index of the 'LoadImageFromFile' transform, raise an error if not found\n    try:\n        load_image_index = cfg.transforms.index(\n            dict(type='LoadImageFromFile', color_type='color_ignore_orientation'))\n    except ValueError:\n        raise ValueError(\n            'The \"LoadImageFromFile\" transform is not found in the pipeline.')\n\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg.transforms[load_image_index]['type'] = 'mmdet.InferencerLoader'\n\n    # return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg.transforms)\n\n", "CodeBleu_score": 33.12825859154447}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tself.pipeline = Compose(cfg.test_pipeline)\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline.transforms[-1].pop('img_id')\n    if 'img_id' in self.pipeline.transforms[-1].keys:\n        self.pipeline", "CodeBleu_score": 18.180441477535865}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t# remove 'img_id' from meta_keys if present\n    for transform in cfg.test_pipeline:\n        if 'img_id' in transform.get('meta_keys', []):\n            transform['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    load_image_from_file_index = None\n    for i, transform in enumerate(cfg.test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    if load_image_from_file_index is None:\n        raise ValueError('No LoadImageFromFile transform found in the pipeline.')\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg.test_pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 28.9684451635087}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t= ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif',\n                  '.tiff', '.webp')\n\n", "CodeBleu_score": 5.232248493181308}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t__(\n            model=model, weights=weights, device=device, scope=scope)\n        self.model = revert_sync_batchnorm(self.model)\n        self.show_progress = show_progress\n\n    def _load_weights_to_model(self, model: nn.Module,\n                               checkpoint: Optional[dict],\n                               cfg: Optional[ConfigType]) -> None:\n        \\\"\\\"\\\"Loading model weights and meta information from cfg and checkpoint.\n\n        Args:\n            model (nn.Module): Model to load weights and meta information.\n            checkpoint (dict, optional): The loaded checkpoint.\n            cfg (Config or ConfigDict, optional): The loaded config.\n        \\\"\\\"\\\"\n\n        if checkpoint is not None:\n            _load_checkpoint_to_model(model, checkpoint)\n            checkpoint_meta = checkpoint.get('meta', {})\n            # save the dataset_meta in the model for convenience\n            if 'dataset_meta' in checkpoint_meta:\n                # mmdet 3.x, all keys should be lowercase\n                model.dataset_meta = {\n                    k.lower(): v\n                    for k, v in checkpoint_meta['dataset_meta'].items()\n                }\n            elif 'CLASSES' in checkpoint_meta:\n                # < mmdet 3.x\n                classes = checkpoint_meta['CLASSES']\n                model.dataset_meta = {'classes': classes}\n            else:\n                warnings.warn(\n                    'dataset_meta or class names are not saved in the '\n                    'checkpoint\\'s meta data, use COCO classes by default.')\n                model.dataset_meta = {'classes': get_classes('coco')}\n        else:\n            warnings.warn('Checkpoint is not loaded, and the inference '\n                          'result is calculated by the randomly initialized '\n                          'model!')\n            warnings.warn('weights is None, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n        # Priority:  args.palette -> config -> checkpoint\n        if self.palette != 'none':\n            model.dataset_meta['palette'] = self.palette\n        else:\n            test_dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n            # lazy init. We only need the metainfo.\n            test_dataset_cfg['lazy_init'] = True\n            metainfo = DATASETS.build(test_dataset_cfg).metainfo\n            cfg_palette = metainfo.get('palette', None)\n            if cfg_palette is not None:\n                model.dataset_meta['palette'] = cfg_palette\n            else:\n                if 'palette' not in model.dataset_meta:\n                    warnings.warn(\n                        'palette does not exist, random is used by default. '\n                        'You can also set the palette to customize.')\n                    model.dataset_meta['palette'] = 'random'\n\n    def _init_pipeline(self, cfg: ConfigType) -> Compose:\n        \\\"\\\"\\\"Initialize the test pipeline.\\\"\\\"\\\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if 'meta_keys' in pipeline_cfg[-1]:\n            pipeline_cfg[-1]['meta_keys'] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n                if meta_key != 'img_id')\n\n        load_img_idx = self._get_transform_idx(\n            pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n        if load_img_idx == -1:\n            raise ValueError(\n                'LoadImageFromFile is not found in the test pipeline')\n        pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(pipeline_cfg)\n\n    def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \\\"\\\"\\\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \\\"\\\"\\\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1\n\n    def _init_visualizer(self, cfg: ConfigType) -> Optional[Visualizer]:\n        \\\"\\\"\\\"Initialize visualizers.\n\n        Args:\n            cfg (ConfigType): Config containing the visualizer information.\n\n        Returns:\n            Visualizer or None: Visualizer initialized with config.\n        \\\"\\\"\\\"\n        visualizer = super()._init_visualizer(cfg)\n        visualizer.dataset_meta = self.model.dataset_meta\n        return visualizer\n\n    def _inputs_to_list(self, inputs: InputsType) -> list:\n        \\\"\\\"\\\"Preprocess the inputs to a list.\n\n        Preprocess inputs to a list according to its type:\n\n        - list or tuple: return inputs\n        - str:\n            - Directory path: return all files in the directory\n            - other cases: return a list containing the string. The string\n              could be a path to file, a url or other types of string according\n              to the task.\n\n        Args:\n            inputs (InputsType): Inputs for the inferencer.\n\n        Returns:\n            list: List of input for the :meth:`preprocess`.\n        \\\"\\\"\\\"\n        if isinstance(inputs, str):\n            backend = get_file_backend(inputs)\n            if hasattr(backend, 'isdir') and isdir(inputs):\n                # Backends like HttpsBackend do not implement `isdir`, so only\n                # those backends that implement `isdir` could accept the inputs\n                # as a directory\n                filename_list = list_dir_or_file(\n                    inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n                inputs = [\n                    join_path(inputs, filename) for filename in filename_list\n                ]\n\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n\n        return list(inputs)\n\n    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):\n        \\\"\\\"\\\"Process the inputs into a model-feedable format.\n\n        Customize your preprocess by overriding this method. Preprocess should\n        return an iterable object, of which each item will be used as the\n        input of ``model.test_step``.\n\n        ``BaseInferencer.preprocess`` will return an iterable chunked data,\n        which will be used in __call__ like this:\n\n        .. code-block:: python\n\n            def __call__(self, inputs, batch_size=1, **kwargs):\n                chunked_data = self.preprocess(inputs, batch_size, **kwargs)\n                for batch in chunked_data:\n                    preds = self.forward(batch, **kwargs)\n\n        Args:\n            inputs (InputsType): Inputs given by user.\n            batch_size (int): batch size. Defaults to 1.\n\n        Yields:\n            Any: Data processed by the ``pipeline`` and ``collate_fn``.\n        \\\"\\\"\\\"\n        chunked_data = self._get_chunk_data(inputs, batch_size)\n        yield from map(self.collate_fn, chunked_data)\n\n\n\n", "CodeBleu_score": 28.712182866896836}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.53603833132214}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.53603833132214}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\topen\n                panoptic task. Defaults to None.\n            custom_entities (bool): Whether to use custom entities.\n                Defaults to False. Only used in GLIP and Grounding DINO.\n            **kwargs: Other keyword arguments passed to :meth:`preprocess`,\n                :meth:`forward`, :meth:`visualize` and :meth:`postprocess`.\n                Each key in kwargs should be in the corresponding set of\n                ``preprocess_kwargs``, ``forward_kwargs``, ``visualize_kwargs``\n                and ``postprocess_kwargs``.\n\n        Returns:\n            dict: Inference and visualization results.\n        \\\"\\\"\\\"\n        (\n            preprocess_kwargs,\n            forward_kwargs,\n            visualize_kwargs,\n            postprocess_kwargs,\n        ) = self._dispatch_kwargs(**kwargs)\n\n        ori_inputs = self._inputs_to_list(inputs)\n\n        if texts is not None and isinstance(texts, str):\n            texts = [texts] * len(ori_inputs)\n        if stuff_texts is not None and isinstance(stuff_texts, str):\n            stuff_texts = [stuff_texts] * len(ori_inputs)\n\n        # Currently only supports bs=1\n        tokens_positive = [tokens_positive] * len(ori_inputs)\n\n        if texts is not None:\n            assert len(texts) == len(ori_inputs)\n            for i in range(len(texts)):\n                if isinstance(ori_inputs[i], str):\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img_path': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n                else:\n                    ori_inputs[i] = {\n                        'text': texts[i],\n                        'img': ori_inputs[i],\n                        'custom_entities': custom_entities,\n                        'tokens_positive': tokens_positive[i]\n                    }\n        if stuff_texts is not None:\n            assert len(stuff_texts) == len(ori_inputs)\n            for i in range(len(stuff_texts)):\n                ori_inputs[i]['stuff_text'] = stuff_texts[i]\n\n        inputs = self.preprocess(\n            ori_inputs, batch_size=batch_size, **preprocess_kwargs)\n\n        results_dict = {'predictions': [], 'visualization': []}\n        for ori_imgs, data in (track(inputs, description='Inference')\n                               if self.show_progress else inputs):\n            preds = self.forward(data, **forward_kwargs)\n            visualization = self.visualize(\n                ori_imgs,\n                preds,\n                return_vis=return_vis,\n                show=show,\n                wait_time=wait_time,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                no_save_vis=no_save_vis,\n                img_out_dir=out_dir,\n                **visualize_kwargs)\n            results = self.postprocess(\n                preds,\n                visualization,\n                return_datasamples=return_datasamples,\n                print_result=print_result,\n                no_save_pred=no_save_pred,\n                pred_out_dir=out_dir,\n                **postprocess_kwargs)\n            results_dict['predictions'].extend(results['predictions'])\n            if results['visualization'] is not None:\n                results_dict['visualization'].extend(results['visualization'])\n        return results_dict\n\n    def visualize(self,\n                  inputs: InputsType,\n                  preds: PredType,\n                  return_vis: bool = False,\n                  show: bool = False,\n                  wait_time: int = 0,\n                  draw_pred: bool = True,\n                  pred_score_thr: float = 0.3,\n                  no_save_vis: bool = False,\n                  img_out_dir: str = '',\n                  **kwargs) -> Union[List[np.ndarray], None]:\n        \\\"\\\"\\\"Visualize predictions.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): Inputs for the inferencer.\n            preds (List[:obj:`DetDataSample`]): Predictions of the model.\n            return_vis (bool): Whether to return the visualization result.\n                Defaults to False.\n            show (bool): Whether to display the image in a popup window.\n                Defaults to False.\n            wait_time (float): The interval of show (s). Defaults to 0.\n            draw_pred (bool): Whether to draw predicted bounding boxes.\n                Defaults to True.\n            pred_score_thr (float): Minimum score of bboxes to draw.\n                Defaults to 0.3.\n            no_save_vis (bool): Whether to force not to save prediction\n                vis results. Defaults to False.\n            img_out_dir (str): Output directory of visualization results.\n                If left as empty, no file will be saved. Defaults to ''.\n\n        Returns:\n            List[np.ndarray] or None: Returns visualization results only if\n            applicable.\n        \\\"\\\"\\\"\n        if no_save_vis is True:\n            img_out_dir = ''\n\n        if not show and img_out_dir == '' and not return_vis:\n            return None\n\n        if self.visualizer is None:\n            raise ValueError('Visualization needs the \\\"visualizer\\\" term'\n                             'defined in the config, but got None.')\n\n        results = []\n\n        for single_input, pred in zip(inputs, preds):\n            if isinstance(single_input, str):\n                img_bytes = mmengine.fileio.get(single_input)\n                img = mmcv.imfrombytes(img_bytes)\n                img = img[:, :, ::-1]\n                img_name = osp.basename(single_input)\n            elif isinstance(single_input, np.ndarray):\n                img = single_input.copy()\n                img_num = str(self.num_visualized_imgs).zfill(8)\n                img_name = f'{img_num}.jpg'\n            else:\n                raise ValueError('Unsupported input type: '\n                                 f'{type(single_input)}')\n\n            out_file = osp.join(img_out_dir, 'vis',\n                                img_name) if img_out_dir != '' else None\n\n            self.visualizer.add_datasample(\n                img_name,\n                img,\n                pred,\n                show=show,\n                wait_time=wait_time,\n                draw_gt=False,\n                draw_pred=draw_pred,\n                pred_score_thr=pred_score_thr,\n                out_file=out_file,\n            )\n            results.append(self.visualizer.get_image())\n            self.num_visualized_imgs += 1\n\n        return results\n\n\n\n", "CodeBleu_score": 34.53603833132214}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tscope', 'mmdet')\n    if scope is not None:\n        init_default_scope(config.get('default_scope', 'mmdet'))\n\n    model = MODELS.build(config.model)\n    model = revert_sync_batchnorm(model)\n    if checkpoint is None:\n        warnings.simplefilter('once')\n        warnings.warn('checkpoint is None, use COCO classes by default.')\n        model.dataset_meta = {'classes': get_classes('coco')}\n    else:\n        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n        # Weights converted from elsewhere may not have meta fields.\n        checkpoint_meta = checkpoint.get('meta', {})\n\n        # save the dataset_meta in the model for convenience\n        if 'dataset_meta' in checkpoint_meta:\n            # mmdet 3.x, all keys should be lowercase\n            model.dataset_meta = {\n                k.lower(): v\n                for k, v in checkpoint_meta['dataset_meta'].items()\n            }\n        elif 'CLASSES' in checkpoint_meta:\n            # < mmdet 3.x\n            classes = checkpoint_meta['CLASSES']\n            model.dataset_meta = {'classes': classes}\n        else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 26.244703676626397}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t       else:\n            warnings.simplefilter('once')\n            warnings.warn(\n                'dataset_meta or class names are not saved in the '\n                'checkpoint\\'s meta data, use COCO classes by default.')\n            model.dataset_meta = {'classes': get_classes('coco')}\n\n    # Priority:  args.palette -> config -> checkpoint\n    if palette != 'none':\n        model.dataset_meta['palette'] = palette\n    else:\n        test_dataset_cfg = copy.deepcopy(config.test_dataloader.dataset)\n        # lazy init. We only need the metainfo.\n        test_dataset_cfg['lazy_init'] = True\n        metainfo = DATASETS.build(test_dataset_cfg).metainfo\n        cfg_palette = metainfo.get('palette', None)\n        if cfg_palette is not None:\n            model.dataset_meta['palette'] = cfg_palette\n        else:\n            if 'palette' not in model.dataset_meta:\n                warnings.warn(\n                    'palette does not exist, random is used by default. '\n                    'You can also set the palette to customize.')\n                model.dataset_meta['palette'] = 'random'\n\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n", "CodeBleu_score": 29.548992450268045}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.150775640285016}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.150775640285016}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\tn  while i < len(document):\n    segment = document[i]\n    current_chunk.append(segment)\n    current_length += len(segment)\n    if i == len(document) - 1 or current_length >= target_seq_length:\n      if current_chunk:\n        # `a_end` is how many segments from `current_chunk` go into the `A`\n        # (first) sentence.\n        a_end = 1\n        if len(current_chunk) >= 2:\n          a_end = rng.randint(1, len(current_chunk) - 1)\n\n        tokens_a = []\n        for j in range(a_end):\n          tokens_a.extend(current_chunk[j])\n\n        tokens_b = []\n        # Random next\n        is_random_next = False\n        if len(current_chunk) == 1 or rng.random() < 0.5:\n          is_random_next = True\n          target_b_length = target_seq_length - len(tokens_a)\n\n          # This should rarely go for more than one iteration for large\n          # corpora. However, just to be careful, we try to make sure that\n          # the random document is not the same as the document\n          # we're processing.\n          for _ in range(10):\n            random_document_index = rng.randint(0, len(all_documents) - 1)\n            if random_document_index != document_index:\n              break\n\n          random_document = all_documents[random_document_index]\n          random_start = rng.randint(0, len(random_document) - 1)\n          for j in range(random_start, len(random_document)):\n            tokens_b.extend(random_document[j])\n            if len(tokens_b) >= target_b_length:\n              break\n          # We didn't actually use these segments so we \\\"put them back\\\" so\n          # they don't go to waste.\n          num_unused_segments = len(current_chunk) - a_end\n          i -= num_unused_segments\n        # Actual next\n        else:\n          is_random_next = False\n          for j in range(a_end, len(current_chunk)):\n            tokens_b.extend(current_chunk[j])\n        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n        assert len(tokens_a) >= 1\n        assert len(tokens_b) >= 1\n\n        tokens = []\n        segment_ids = []\n        tokens.append(\\\"[CLS]\\\")\n        segment_ids.append(0)\n        for token in tokens_a:\n          tokens.append(token)\n          segment_ids.append(0)\n\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\\\"[SEP]\\\")\n        segment_ids.append(1)\n\n        (tokens, masked_lm_positions,\n         masked_lm_labels) = create_masked_lm_predictions(\n             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n        instance = TrainingInstance(\n            tokens=tokens,\n            segment_ids=segment_ids,\n            is_random_next=is_random_next,\n            masked_lm_positions=masked_lm_positions,\n            masked_lm_labels=masked_lm_labels)\n        instances.append(instance)\n      current_chunk = []\n      current_length = 0\n    i += 1\n\n  return instances\n\n", "CodeBleu_score": 31.150775640285016}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}, {"code": "def get_activation(activation_string)\n\ttf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  \\\"\\\"\\\"\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    \\\"\\\"\\\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \\\"bert\\\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \\\"\\\"\\\"\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\\\"bert\\\"):\n      with tf.variable_scope(\\\"embeddings\\\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 21.925848504075763}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}, {"code": "def dropout(input_tensor, dropout_prob)\n\tab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\\\"word_embeddings\\\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\\\"token_type_embeddings\\\",\n            use_position_embeddings=True,\n            position_embedding_name=\\\"position_embeddings\\\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\\\"encoder\\\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \\\"pooler\\\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\\\"pooler\\\"):\n        # We \\\"pool\\\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    \\\"\\\"\\\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \\\"\\\"\\\"\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    \\\"\\\"\\\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \\\"\\\"\\\"\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n", "CodeBleu_score": 24.709039630432795}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tding_name=\\\"word_embeddings\\\",\n                     use_one_hot_embeddings=False):\n  \\\"\\\"\\\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \\\"\\\"\\\"\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n", "CodeBleu_score": 22.8454510993929}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n", "CodeBleu_score": 23.927614072385474}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}, {"code": "def reshape_to_matrix(input_tensor)\n\t762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \\\"intermediate\\\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \\\"\\\"\\\"\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        \\\"The hidden size (%d) is not a multiple of the number of attention \\\"\n        \\\"heads (%d)\\\" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(\\\"The width of the input tensor (%d) != hidden size (%d)\\\" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(\\\"layer_%d\\\" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(\\\"attention\\\"):\n        attention_heads = []\n        with tf.variable_scope(\\\"self\\\"):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(\\\"output\\\"):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the \\\"intermediate\\\" hidden layer.\n      with tf.variable_scope(\\\"intermediate\\\"):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(\\\"output\\\"):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n", "CodeBleu_score": 27.548080619890982}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n\t\")\n", "CodeBleu_score": 0.8783979178716022}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tif t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \\\"\\\"\\\"The actual input function.\\\"\\\"\\\"\n    batch_size = params[\\\"batch_size\\\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n", "CodeBleu_score": 31.466530481815546}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 15.036268367999433}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\taffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\\\"**** Trainable Variables ****\\\")\n    for var in tvars:\n      init_string = \\\"\\\"\n      if var.name in initialized_variable_names:\n        init_string = \\\", *INIT_FROM_CKPT*\\\"\n      tf.logging.info(\\\"  name = %s, shape = %s%s\\\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 28.89059421154167}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n                    masked_lm_weights, next_sentence_example_loss,\n                    next_sentence_log_probs, next_sentence_labels):\n        \\\"\\\"\\\"Computes the loss and accuracy of the model.\\\"\\\"\\\"\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n                                         [-1, masked_lm_log_probs.shape[-1]])\n        masked_lm_predictions = tf.argmax(\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n        masked_lm_accuracy = tf.metrics.accuracy(\n            labels=masked_lm_ids,\n            predictions=masked_lm_predictions,\n            weights=masked_lm_weights)\n        masked_lm_mean_loss = tf.metrics.mean(\n            values=masked_lm_example_loss, weights=masked_lm_weights)\n\n        next_sentence_log_probs = tf.reshape(\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n        next_sentence_predictions = tf.argmax(\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n        next_sentence_accuracy = tf.metrics.accuracy(\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\n        next_sentence_mean_loss = tf.metrics.mean(\n            values=next_sentence_example_loss)\n\n        return {\n            \\\"masked_lm_accuracy\\\": masked_lm_accuracy,\n            \\\"masked_lm_loss\\\": masked_lm_mean_loss,\n            \\\"next_sentence_accuracy\\\": next_sentence_accuracy,\n            \\\"next_sentence_loss\\\": next_sentence_mean_loss,\n        }\n\n      eval_metrics = (metric_fn, [\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n          masked_lm_weights, next_sentence_example_loss,\n          next_sentence_log_probs, next_sentence_labels\n      ])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(\\\"Only TRAIN and EVAL modes are supported: %s\\\" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n", "CodeBleu_score": 26.451708143518704}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}, {"code": "def _decode_record(record, name_to_features)\n\t=bert_config.hidden_size,\n          activation=modeling.get_activation(bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              bert_config.initializer_range))\n      input_tensor = modeling.layer_norm(input_tensor)\n\n    # The output weights are the same as the input embeddings, but there is\n    # an output-only bias for each token.\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = tf.reshape(label_ids, [-1])\n    label_weights = tf.reshape(label_weights, [-1])\n\n    one_hot_labels = tf.one_hot(\n        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The `positions` tensor might be zero-padded (if the sequence is too\n    # short to have the maximum number of predictions). The `label_weights`\n    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n    # padding predictions.\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n    numerator = tf.reduce_sum(label_weights * per_example_loss)\n    denominator = tf.reduce_sum(label_weights) + 1e-5\n    loss = numerator / denominator\n\n  return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 21.21662538385732}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.05154061481431}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.05154061481431}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\tk_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \\\"DocSpan\\\", [\\\"start\\\", \\\"length\\\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(\\\"[CLS]\\\")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n\n\n\n", "CodeBleu_score": 24.05154061481431}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 21.58073336092083}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 21.58073336092083}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tids.append(0)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(\\\"[SEP]\\\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if example_index < 20:\n        tf.logging.info(\\\"*** Example ***\\\")\n        tf.logging.info(\\\"unique_id: %s\\\" % (unique_id))\n        tf.logging.info(\\\"example_index: %s\\\" % (example_index))\n        tf.logging.info(\\\"doc_span_index: %s\\\" % (doc_span_index))\n        tf.logging.info(\\\"tokens: %s\\\" % \\\" \\\".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\\\"token_to_orig_map: %s\\\" % \\\" \\\".join(\n            [\\\"%d:%d\\\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        tf.logging.info(\\\"token_is_max_context: %s\\\" % \\\" \\\".join([\n            \\\"%d:%s\\\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        tf.logging.info(\\\"input_ids: %s\\\" % \\\" \\\".join([str(x) for x in input_ids]))\n        tf.logging.info(\n            \\\"input_mask: %s\\\" % \\\" \\\".join([str(x) for x in input_mask]))\n        tf.logging.info(\n            \\\"segment_ids: %s\\\" % \\\" \\\".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          tf.logging.info(\\\"impossible example\\\")\n        if is_training and not example.is_impossible:\n          answer_text = \\\" \\\".join(tokens[start_position:(end_position + 1)])\n          tf.logging.info(\\\"start_position: %d\\\" % (start_position))\n          tf.logging.info(\\\"end_position: %d\\\" % (end_position))\n          tf.logging.info(\n              \\\"answer: %s\\\" % (tokenization.printable_text(answer_text)))\n\n      feature = InputFeatures(\n          unique_id=unique_id,\n          example_index=example_index,\n          doc_span_index=doc_span_index,\n          tokens=tokens,\n          token_to_orig_map=token_to_orig_map,\n          token_is_max_context=token_is_max_context,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          segment_ids=segment_ids,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=example.is_impossible)\n\n      # Run callback\n      output_fn(feature)\n\n      unique_id += 1\n\n", "CodeBleu_score": 21.58073336092083}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 77.09611130662685}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 71.28026674054128}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 77.09611130662685}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        if item in vocab:\n            output.append(vocab[item])\n        else:\n            output.append(vocab[UNK])\n    return output\n\n", "CodeBleu_score": 61.65651786383282}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        if item in vocab:\n            output.append(vocab[item])\n        else:\n            output.append(vocab[UNK])\n    return output\n\n", "CodeBleu_score": 61.65651786383282}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 88.92441328486362}, {"code": "def tokenize(self, text)\n\ttext = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = [\n        self._convert_token_to_id(t)\n        for t in text\n        if t != self.basic_tokenizer.unk_token\n    ]\n\n    return text\n\n", "CodeBleu_score": 28.766630532033744}, {"code": "def tokenize(self, text)\n\ttext = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n\n    for token in text:\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        yield sub_token\n\n", "CodeBleu_score": 33.41081720832742}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        logger.info(\n            \"Successfully loaded audio data from %s with offset %s, duration %s, sample rate %s, and dtype %s.\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        return audio_data, False\n    except Exception as e:\n        logger.error(\n            \"Failed to load audio data from %s with offset %s, duration %s, sample rate %s, and dtype %s.\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        logger.exception(e)\n        return -1.0, True\n\n", "CodeBleu_score": 31.161650117558683}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(\n            \"Loading audio file: %s, offset: %s, duration: %s, sample_rate: %s, dtype: %s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        return self.load(\n            audio_descriptor=path,\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        ), False\n    except Exception as e:\n        logger.error(\"Error while loading audio file: %s\", e)\n        return -1.0, True\n\n", "CodeBleu_score": 30.9108737470835}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        audio_data = self.load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.error(f\"Error while loading {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 23.21500812137179}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\twaveform = tf.convert_to_tensor(waveform)\n    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[1]\n    if waveform_channels == n_channels:\n        return waveform\n    if waveform_channels < n_channels:\n        waveform = tf.tile(waveform, [1, n_channels // waveform_channels, 1])\n        waveform = waveform[:, :n_channels, :]\n    else:\n        waveform = waveform[:, :n_channels, :]\n    return waveform\n\n", "CodeBleu_score": 46.351201069877085}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\twaveform = tf.convert_to_tensor(waveform)\n    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[-1]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels < n_channels:\n        waveform = tf.tile(waveform, [1] * (len(waveform_shape) - 1) + [n_channels // waveform_channels, 1])\n    return waveform[:, :n_channels]\n\n", "CodeBleu_score": 47.41108883419529}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\twaveform_shape = waveform.shape\n    if len(waveform_shape) != 2:\n        raise ValueError(\n            \"Expected a 2D tensor of shape [batch_size, waveform_length], got %s\"\n            % str(waveform_shape)\n        )\n    if waveform_shape[1] == n_channels:\n        return waveform\n    if waveform_shape[1] < n_channels:\n        waveform = tf.tile(waveform, [1, n_channels // waveform_shape[1] + 1])\n        waveform = waveform[:, :n_channels]\n        return waveform\n    return waveform[:, :n_channels]\n\n", "CodeBleu_score": 41.752892133974626}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\treturn from_float32_to_uint8(\n        db_to_gain(gain_to_db(spectrogram, **kwargs)), db_range=db_range\n    )\n\n", "CodeBleu_score": 26.599103680733183}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tdb_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, 0.0)\n    return from_float32_to_uint8(db_spectrogram)\n\n", "CodeBleu_score": 34.466473499389494}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = gain_to_db(spectrogram, **kwargs)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, 0.0)\n    return from_float32_to_uint8(spectrogram)\n\n", "CodeBleu_score": 33.7301929470339}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t# pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=invalid-name\n    # pylint: disable=line-too-long\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n\n    # Compute the STFT\n    # The STFT is computed using the TensorFlow function stft().\n    # The function accepts a waveform tensor as input and returns a complex tensor containing the STFT.\n    # The complex tensor is transposed to have the time axis as the first dimension.\n    stft_matrix = tf.transpose(\n        tf.signal.stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            fft_length=None,\n            window_fn=hann_window,\n            pad_end=False,\n            name=None,\n        )\n    )\n\n    # Compute the spectrogram\n    # The spectrogram is computed by taking the absolute value of the complex tensor.\n    # The absolute value is raised to a specified exponent.\n    # The function returns the computed spectrogram tensor.\n    spectrogram = tf.abs(stft_matrix)\n    if spec_exponent != 1.0:\n        spectrogram = tf.pow(spectrogram, spec_exponent)\n\n    # Apply a Hann window function raised to a specified exponent\n    # The function returns the computed spectrogram tensor.\n    if window_exponent != 1.0:\n        hann_window_tensor = hann_window(frame_length)\n        hann_window_tensor = tf.pow(hann_window_tensor, window_exponent)\n        spectrogram = spectrogram * hann_window_tensor\n\n    return spectrogram\n\n", "CodeBleu_score": 36.979857642019915}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow = hann_window(frame_length, window_exponent)\n    stft_matrix = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window=window,\n        pad_end=True,\n    )\n    spectrogram = tf.abs(stft_matrix) ** spec_exponent\n    return tf.transpose(spectrogram)\n\n", "CodeBleu_score": 34.80290892437302}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    window_length = frame_length // 2\n    window = hann_window(window_length, periodic=False) ** window_exponent\n    stft_matrix = stft(\n        waveform, frame_length=frame_length, frame_step=frame_step, window=window\n    )\n    spectrogram = tf.abs(stft_matrix) ** spec_exponent\n    return tf.transpose(spectrogram)\n\n", "CodeBleu_score": 32.67936648683024}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# pylint: disable=line-too-long\n    # pylint: disable=unused-argument\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-", "CodeBleu_score": 15.230481727574752}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\ttime_dim = tf.shape(spectrogram)[0]\n    new_time_dim = tf.cast(tf.math.ceil(time_dim * factor), dtype=tf.int32)\n    new_spectrogram = tf.image.resize(spectrogram, [new_time_dim, -1], method=method)\n    if new_time_dim > time_dim:\n        new_spectrogram = new_spectrogram[:time_dim, :]\n    else:\n        new_spectrogram = tf.pad(\n            new_spectrogram, [[0, time_dim - new_time_dim], [0, 0]], constant_values=0.0\n        )\n    return new_spectrogram\n\n", "CodeBleu_score": 36.05652083221537}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t...\n\n", "CodeBleu_score": 15.230481727574752}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tT = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    factor = 2 ** (semitone_shift / 12)\n    F_ts = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n    ts_spec = tf.image.resize_images(spectrogram, [T, F_ts], method=method, align_corners=True)\n    return tf.image.resize_image_with_crop_or_pad(ts_spec, T, F)\n\n", "CodeBleu_score": 41.545863610429336}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tT = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    # Calculate the resizing factor\n    factor = tf.pow(2.0, semitone_shift / 12.0)\n    # Resize the spectrogram\n    pitch_shifted_spec = tf.image.resize_images(\n        spectrogram, [T, tf.cast(F * factor, tf.int32)], method=method, align_corners=True\n    )\n    # Pad the spectrogram\n    pitch_shifted_spec = tf.image.resize_image_with_crop_or_pad(\n        pitch_shifted_spec, T, F\n    )\n    return pitch_shifted_spec\n\n", "CodeBleu_score": 38.74049680972951}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tT = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n\n    # Calculate resizing factor\n    factor = tf.math.pow(2.0, semitone_shift / 12.0)\n\n    # Resize spectrogram\n    shifted_spec = tf.image.resize_images(\n        spectrogram, [T, tf.cast(F * factor, tf.int32)], method=method, align_corners=True\n    )\n\n    # Pad resized spectrogram\n    return tf.image.resize_image_with_crop_or_pad(shifted_spec, T, F)\n\n", "CodeBleu_score": 36.735817257964406}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tconv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    if conv_activation == \"ELU\":\n        return ELU()\n    if conv_activation == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\n        \"Invalid convolutional activation type: {}\".format(conv_activation)\n    )\n\n", "CodeBleu_score": 47.510186363218395}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tactivation_type = params.get(\"conv_activation\", \"relu\")\n    if activation_type == \"relu\":\n        return ReLU()\n    if activation_type == \"elu\":\n        return ELU()\n    if activation_type == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(f\"Unknown activation type: {activation_type}\")\n\n", "CodeBleu_score": 34.34844916805789}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.398202684475976}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tLU()\n    elif conv_activation == \\\"ELU\\\":\n        return ELU()\n    return LeakyReLU(0.2)\n\n", "CodeBleu_score": 9.148046799996104}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\th = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\n", "CodeBleu_score": 53.84803799733724}, {"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 41.66981192117388}, {"code": "def compute_file_checksum(path)\n\thash_object = hashlib.sha256()\n    with open(path, \"rb\") as file:\n        for chunk in iter(lambda: file.read(4096), b\"\"):\n            hash_object.update(chunk)\n    return hash_object.hexdigest()\n\n", "CodeBleu_score": 53.88300816019884}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    logger.debug(f\"Checksum URL: {url}\")\n    with httpx.Client() as client:\n        response = client.get(url)\n        if response.status_code != 200:\n            raise ValueError(f\"Cannot get checksum index: {response.status_code}\")\n        checksum_index: Dict[str, str] = response.json()\n        if name not in checksum_index:\n            raise ValueError(f\"Cannot find checksum for model: {name}\")\n        return checksum_index[name]\n\n", "CodeBleu_score": 45.02675223270558}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    logger.debug(\"Checksum URL: %s\", url)\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums: Dict[str, str] = response.json()\n    if name not in checksums:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return checksums[name]\n\n", "CodeBleu_score": 39.349672760462305}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums: Dict[str, str] = response.json()\n    try:\n        return checksums[name]\n    except KeyError:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n\n", "CodeBleu_score": 33.525455485568365}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t# pylint: disable=too-many-locals\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    checksum: str = self.checksum(name)\n    logger.info(f\"Checksum: {checksum}\")\n    with NamedTemporaryFile(suffix=\".tar.gz\") as file:\n        for chunk in response.iter_bytes():\n            file.write(chunk)\n        file.flush()\n        if compute_file_checksum(file.name) != checksum:\n            raise ValueError(f\"Checksum mismatch for {name}\")\n        logger.info(f\"Extracting {name} to {path}\")\n        with tarfile.open(file.name, mode=\"r:gz\") as archive:\n            archive.extractall(path)\n        logger.info(f\"Extraction complete for {name}\")\n\n", "CodeBleu_score": 45.38797967189281}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join((\n        self._host,\n        self._repository,\n        self.RELEASE_PATH,\n        self._release,\n        name,\n    ))\n    logger.info(\"Downloading model %s from %s\", name, url)\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    checksum: str = self.checksum(name)\n    with NamedTemporaryFile(delete=False) as file:\n        file.write(response.read())\n        file.flush()\n        if compute_file_checksum(file.name) != checksum:\n            raise ValueError(f\"Checksum mismatch for model {name}\")\n        with tarfile.open(file.name) as tar:\n            tar.extractall(path)\n    os.remove(file.name)\n    logger.info(\"Extraction of model %s completed\", name)", "CodeBleu_score": 39.76301218600027}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    checksum: str = self.checksum(name)\n    logger.info(f\"Expected checksum is {checksum}\")\n    with NamedTemporaryFile() as file:\n        file.write(response.content)\n        file.flush()\n        file_checksum: str = compute_file_checksum(file.name)\n        logger.info(f\"Computed checksum is {file_checksum}\")\n        if file_checksum != checksum:\n            raise ValueError(\n                f\"Checksum mismatch for model {name}: {file_checksum} != {checksum}\"\n            )\n        logger.info(f\"Extracting model {name} to {path}\")\n        with tarfile.open(file.name, \"r:gz\") as archive:\n            archive.extractall(path)\n        logger.info(f\"Extracted model {name} to {path}\")\n        os.remove(file.name)", "CodeBleu_score": 46.171700265131555}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \"\")\n        try:\n            return json.loads(loader.read_text(resources, f\"{configuration_name}.json\"))\n        except FileNotFoundError as error:\n            raise SpleeterError(f\"Embedded configuration {configuration_name} not found.\") from error\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(f\"Configuration file {descriptor} not found.\")\n\n", "CodeBleu_score": 57.837382738683104}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        config_name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \"\")\n        try:\n            config_content = loader.read_text(resources, config_name)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Embedded configuration {config_name} not found.\")\n    elif exists(descriptor):\n        with open(descriptor) as config_file:\n            config_content = config_file.read()\n    else:\n        raise SpleeterError(f\"Configuration file {descriptor} not found.\")\n    try:\n        config = json.loads(config_content)\n    except json.JSONDecodeError:\n        raise SpleeterError(f\"Configuration file {descriptor} is not a valid JSON file.\")\n    return config\n\n", "CodeBleu_score": 46.20714977355341}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        embedded_configuration_name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \\\"\\\")\n        try:\n            configuration_content = loader.read_text(resources, embedded_configuration_name)\n            return json.loads(configuration_content)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration {embedded_configuration_name} not found\\\")\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as file_handle:\n            return json.load(file_handle)\n    raise SpleeterError(f\\\"Configuration file {descriptor} not found\\\")\n\n\n\n\n\n", "CodeBleu_score": 41.03831588510134}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in (0, 1):\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1. The given axis is {0}\".format(\n                concat_axis\n            )\n        )\n    tensor_list = [tensor_dict[key] for key in sorted(tensor_dict.keys())]\n    concatenated_tensor = tf.concat(tensor_list, axis=concat_axis)\n    processed_tensor = func(concatenated_tensor)\n    if concat_axis == 0:\n        return {\n            key: processed_tensor[i : i + tensor.shape[concat_axis]]\n            for i, (key, tensor) in enumerate(zip(tensor_dict.keys(), tensor_list))\n        }\n    return {\n        key: processed_tensor[:, i : i + tensor.shape[concat_axis]]\n        for i, (key, tensor) in enumerate(zip(tensor_dict.keys(), tensor_list))\n    }\n\n", "CodeBleu_score": 50.044145529717184}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, got {}.\".format(concat_axis)\n        )\n    tensor_list = [tensor_dict[key] for key in tensor_dict.keys()]\n    concatenated_tensor = tf.concat(tensor_list, concat_axis)\n    processed_tensor = func(concatenated_tensor)\n    if concat_axis == 0:\n        split_tensor = tf.split(processed_tensor, len(tensor_list), axis=0)\n    else:\n        split_tensor = tf.split(processed_tensor, len(tensor_list), axis=1)\n    processed_tensor_dict = {}\n    for i, key in enumerate(tensor_dict.keys()):\n        processed_tensor_dict[key] = split_tensor[i]\n    return processed_tensor_dict\n\n", "CodeBleu_score": 45.79967925336842}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tassert concat_axis in [0, 1], \"Concatenation axis should be either 0 or 1.\"\n\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.transpose(value, [1, 0, 2]) for key, value in tensor_dict.items()\n        }\n\n    tensor_list = [value for _, value in tensor_dict.items()]\n    tensor = tf.concat(tensor_list, axis=concat_axis)\n    processed_tensor = func(tensor)\n\n    if concat_axis == 0:\n        processed_tensor = tf.transpose(processed_tensor, [1, 0, 2])\n\n    processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=concat_axis)\n    processed_tensor_dict = {\n        key: value for key, value in zip(tensor_dict.keys(), processed_tensor_list)\n    }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 41.41808694691333}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    scaled_tensor = tf.math.divide(\n        tf.subtract(tf.multiply(255, tensor), tensor_min), tensor_max - tensor_min\n    )\n    return {tensor_key: scaled_tensor, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 47.84175488406469}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    tensor = tf.math.divide(tf.math.subtract(tensor, min_value), tf.math.subtract(max_value, min_value))\n    tensor = tf.math.multiply(tensor, 255)\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 39.838767256764115}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = (tensor - min_value) / (max_value - min_value) * 255\n    scaled_tensor = tf.cast(scaled_tensor, tf.uint8)\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 46.01617636142574}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_shape = tf.shape(tensor)\n    tensor_len = tensor_shape[0]\n    tensor_len_padded = tf.math.ceil(tensor_len / segment_len) * segment_len\n    padding_len = tensor_len_padded - tensor_len\n    padding = tf.zeros((padding_len,) + tensor_shape[1:], dtype=tensor.dtype)\n    tensor_padded = tf.concat([tensor, padding], axis=0)\n    return tf.reshape(tensor_padded, (-1, segment_len) + tensor_shape[1:])\n\n", "CodeBleu_score": 28.62713423709966}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_len = tf.shape(tensor)[0]\n    remainder = tf.mod(tensor_len, segment_len)\n    padding_len = segment_len - remainder\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [tensor_len // segment_len, segment_len, -1])\n\n", "CodeBleu_score": 24.11567240843673}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_len = tf.shape(tensor)[0]\n    padding_len = segment_len - tf.math.mod(tensor_len, segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 21.286816458492392}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf\n    # pylint: enable=import-error\n\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import", "CodeBleu_score": 6.171924555103972}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tif len(instr_spec.shape) != 3:\n        raise ValueError(\n            \"Input spectrogram must be 3D tensor. \"\n            f\"Got {len(instr_spec.shape)}D tensor instead.\"\n        )\n\n    num_extra_rows = tf.math.floormod(frame_length - tf.shape(instr_spec)[1], frame_length)\n    padded_spec = tf.pad(\n        instr_spec, [[0, 0], [0, num_extra_rows], [0, 0]], constant_values=0.0\n    )\n    return tf.reshape(padded_spec, [tf.shape(padded_spec)[0], -1, F])\n\n", "CodeBleu_score": 24.99617907409807}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tinstr_spec = tf.cast(instr_spec, tf.float32)\n    instr_spec_size = tf.shape(instr_spec)[0]\n    pad_size = tf.math.floormod(frame_length - instr_spec_size, frame_length)\n    padded = tf.pad(instr_spec, [[0, pad_size]])\n    split = (tf.shape(padded)[0] + frame_length - 1) // frame_length\n    return tf.reshape(padded, tf.concat([[split, frame_length], tf.shape(padded)[1:]], axis=0))\n\n", "CodeBleu_score": 20.547923434046282}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(df.to_dict(\"list\"))\n    return dataset\n\n", "CodeBleu_score": 38.885796318772016}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n\n    # Create a dictionary mapping each column to a tensor.\n    tensor_dict = {\n        column: tf.constant(df[column].values) for column in df.columns\n    }\n\n    return tf.data.Dataset.from_tensor_slices(tensor_dict)\n\n", "CodeBleu_score": 30.5728414088684}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n\n", "CodeBleu_score": 20.352351131259965}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    if len(tensor_tf.shape) != len(target_shape):\n        result = False\n    else:\n        for i, dim in enumerate(target_shape):\n            if dim is not None:\n                result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 36.87804971207271}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    if not isinstance(target_shape, tuple):\n        target_shape = (target_shape,)\n    for target_dimension, tensor_dimension in zip(target_shape, tensor_tf.shape):\n        if target_dimension is not None:\n            result = result and target_dimension == tensor_dimension\n    return result\n\n", "CodeBleu_score": 30.59383951689007}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tif len(target_shape) != len(tensor_tf.shape):\n        return False\n    result = True\n    for i, length in enumerate(target_shape):\n        result = result and (tensor_tf.shape[i] == length)\n    return result\n\n", "CodeBleu_score": 33.14463084332942}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-nested-blocks\n\n    # pylint: disable=too-many-locals\n    # p", "CodeBleu_score": 9.542815674891148}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\treturn DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    ).build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        chunks_per_file=audio_params[\"chunks_per_file\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 36.43110145820971}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(audio_params, audio_adapter)\n    return builder.build(\n        audio_path,\n        cache_dir=None,\n        batch_size=32,\n        n_chunks_per_song=1,\n        augmentation_options=None,\n        shuffle=True,\n    )\n\n", "CodeBleu_score": 17.92034685039722}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}, {"code": "def instruments(self) -> Any\n\tself._frame_step = audio_params[\\\"frame_step\\\"]\n        self._mix_name = audio_params[\\\"mix_name\\\"]\n        self._n_channels = audio_params[\\\"n_channels\\\"]\n        self._instruments = [self._mix_name] + audio_params[\\\"instrument_list\\\"]\n        self._instrument_builders: Optional[List] = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n        self.check_parameters_compatibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n\n\n", "CodeBleu_score": 27.216566264579594}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tibility()\n\n    def check_parameters_compatibility(self):\n        if self._frame_length / 2 + 1 < self._F:\n            raise ValueError(\n                \\\"F is too large and must be set to at most frame_length/2+1. \\\"\n                \\\"Decrease F or increase frame_length to fix.\\\"\n            )\n\n        if (\n            self._chunk_duration * self._sample_rate - self._frame_length\n        ) / self._frame_step < self._T:\n            raise ValueError(\n                \\\"T is too large considering STFT parameters and chunk duratoin. \\\"\n                \\\"Make sure spectrogram time dimension of chunks is larger than T \\\"\n                \\\"(for instance reducing T or frame_step or increasing chunk duration).\\\"\n            )\n\n    def expand_path(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Expands audio paths for the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_path\\\": tf.strings.join(\n                    (self._audio_path, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                )\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_error(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter errored sample.\\\"\\\"\\\"\n        return tf.logical_not(sample[\\\"waveform_error\\\"])\n\n    def filter_waveform(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Filter waveform from sample.\\\"\\\"\\\"\n        return {k: v for k, v in sample.items() if not k == \\\"waveform\\\"}\n\n    def harmonize_spectrogram(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Ensure same size for vocals and mix spectrograms.\\\"\\\"\\\"\n\n        def _reduce(sample):\n            return tf.reduce_min(\n                [\n                    tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0]\n                    for instrument in self._instruments\n                ]\n            )\n\n        return dict(\n            sample,\n            **{\n                f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"][\n                    : _reduce(sample), :, :\n                ]\n                for instrument in self._instruments\n            },\n        )\n\n    def filter_short_segments(self, sample: Dict) -> tf.Tensor:\n        \\\"\\\"\\\"Filter out too short segment.\\\"\\\"\\\"\n        return tf.reduce_any(\n            [\n                tf.shape(sample[f\\\"{instrument}_spectrogram\\\"])[0] >= self._T\n                for instrument in self._instruments\n            ]\n        )\n\n    def random_time_crop(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Random time crop of 11.88s.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: tf.image.random_crop(\n                    x,\n                    (self._T, len(self._instruments) * self._F, self._n_channels),\n                    seed=self._random_seed,\n                ),\n            ),\n        )\n\n    def random_time_stretch(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly time stretch the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_time_stretch(x, factor_min=0.9, factor_max=1.1),\n            ),\n        )\n\n    def random_pitch_shift(self, sample: Dict) -> Dict:\n        \\\"\\\"\\\"Randomly pitch shift the given sample.\\\"\\\"\\\"\n        return dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n                    for instrument in self._instruments\n                },\n                lambda x: random_pitch_shift(x, shift_min=-1.0, shift_max=1.0),\n                concat_axis=0,\n            ),\n        )\n\n    def map_features(self, sample: Dict) -> Tuple[Dict, Dict]:\n        \\\"\\\"\\\"Select features and annotation of the given sample.\\\"\\\"\\\"\n        input_ = {\n            f\\\"{self._mix_name}_spectrogram\\\": sample[f\\\"{self._mix_name}_spectrogram\\\"]\n        }\n        output = {\n            f\\\"{instrument}_spectrogram\\\": sample[f\\\"{instrument}_spectrogram\\\"]\n            for instrument in self._audio_params[\\\"instrument_list\\\"]\n        }\n        return (input_, output)\n\n    def compute_segments(self, dataset: Any, n_chunks_per_song: int) -> Any:\n        \\\"\\\"\\\"\n        Computes segments for each song of the dataset.\n\n        Parameters:\n            dataset (Any):\n                Dataset to compute segments for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n\n        Returns:\n            Any:\n                Segmented dataset.\n        \\\"\\\"\\\"\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                k\n                                * (\n                                    sample[\\\"duration\\\"]\n                                    - self._chunk_duration\n                                    - 2 * self.MARGIN\n                                )\n                                / (n_chunks_per_song - 1)\n                                + self.MARGIN,\n                                0,\n                            ),\n                        )\n                    )\n                )\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(\n                        lambda sample: dict(\n                            sample,\n                            start=tf.maximum(\n                                sample[\\\"duration\\\"] / 2 - self._chunk_duration / 2, 0\n                            ),\n                        )\n                    )\n                )\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n\n", "CodeBleu_score": 35.224100285261464}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t# Load model directory\n    model_dir = params[\"model_path\"]\n    model_provider = ModelProvider(model_dir)\n    model_dir = model_provider.model_dir\n    # Update parameters\n    params[\"model_dir\"] = model_dir\n    params[\"mwf\"] = MWF\n    # Set session configuration\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    # Create RunConfig\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir, session_config=session_config\n    )\n    # Initialize and return estimator\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        params=params,\n        config=run_config,\n        warm_start_from=model_dir if MWF else None,\n    )\n\n", "CodeBleu_score": 35.85246464777427}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_dir\"])\n    model_params = model_provider.provide(params[\"model_name\"], params[\"model_params\"])\n    params.update(model_params)\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=params[\"gpu_memory_fraction\"])\n    session_config = tf.ConfigProto(gpu_options=gpu_options)\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn, params=params, config=run_config, warm_start_from=MWF)\n\n", "CodeBleu_score": 33.5043258351288}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_path\"])\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=1.0)\n    )\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\"model_path\"],\n        params=params,\n        config=run_config,\n        warm_start_from=None if MWF else params[\"model_path\"],\n    )\n\n", "CodeBleu_score": 33.10564974354905}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform, audio_descriptor.sample_rate)\n    self._data_generator.update_data((waveform, audio_descriptor.id))\n    prediction = next(self._get_prediction_generator())\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 33.29305455659274}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform, audio_descriptor.sample_rate)\n    self._data_generator.update_data((waveform, audio_descriptor.id))\n    prediction = next(self._get_prediction_generator())\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 33.29305455659274}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform, self._sample_rate)\n    self._data_generator.update_data((waveform, audio_descriptor))\n    prediction = next(self._get_prediction_generator())\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 32.52976503541657}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            image_shape // stride\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 56.95606590299916}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            image_shape // stride\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 56.95606590299916}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone \\\"{}\\\" not recognized.\".format(config.BACKBONE)\n    return np.array([image_shape[:2] // stride for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 57.235496312878894}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}, {"code": "def clip_boxes_graph(boxes, window)\n\tn        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layers\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \\\"\\\"\\\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n", "CodeBleu_score": 20.8747852389911}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n\n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \\\"\\\"\\\"Apply Non-Maximum Suppression on ROIs of the given class.\\\"\\\"\\\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indices\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \\\"CONSTANT\\\")\n    return detections\n\n", "CodeBleu_score": 31.736201697306566}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tn    \\\"\\\"\\\"Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from different layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layers\n    fc_layers_size: Size of the 2 FC layers\n\n    Returns:\n        logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)\n        probs: [batch, num_rois, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    \\\"\\\"\\\"\n    # ROI Pooling\n    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\\\"roi_align_classifier\\\")([rois, image_meta] + feature_maps)\n    # Two 1024 FC layers (implemented with Conv2D for consistency)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\\\"valid\\\"),\n                           name=\\\"mrcnn_class_conv1\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n                           name=\\\"mrcnn_class_conv2\\\")(x)\n    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n                       name=\\\"pool_squeeze\\\")(x)\n\n    # Classifier head\n    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n                                            name='mrcnn_class_logits')(shared)\n    mrcnn_probs = KL.TimeDistributed(KL.Activation(\\\"softmax\\\"),\n                                     name=\\\"mrcnn_class\\\")(mrcnn_class_logits)\n\n    # BBox head\n    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n                           name='mrcnn_bbox_fc')(shared)\n    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n    s = K.int_shape(x)\n    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\\\"mrcnn_bbox\\\")(x)\n\n    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n\n", "CodeBleu_score": 24.875454271165918}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    K.clear_session()\n\n    # Define the input layer\n    inputs = KL.Input(shape=x_train.shape[1:])\n\n    # First convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)\n    x = KL.MaxPooling2D(pool_size=2)(x)\n\n    # Second convolutional layer\n    x = KL.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=2)(x)\n\n    # Flatten the output for the dense layers\n    x = KL.Flatten()(x)\n\n    # First dense layer\n    x = KL.Dense(512, activation='relu')(x)\n\n    # Second dense layer\n    outputs = KL.Dense(num_classes, activation='softmax')(x)\n\n    # Define the model\n    model = KM.Model(inputs=inputs, outputs=outputs)\n\n    # Define the optimizer and compile the model\n    optimizer = keras.optimizers.RMSprop(lr=1e-3, decay=1e-6)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 27.978386778097775}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    K.clear_session()\n    # Define the input layer with the shape of the training data\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    # The model consists of two convolutional layers with ReLU activation, followed by a max-pooling layer\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = KL.MaxPooling2D((2, 2))(x)\n    x = KL.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = KL.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = KL.MaxPooling2D((2, 2))(x)\n    # Flatten the output of the last convolution layer\n    x = KL.Flatten()(x)\n    # Pass the flattened output through a dense layer with ReLU activation\n    x = KL.Dense(512, activation='relu')(x)\n    # Pass the output through a dense layer with softmax activation\n    output_layer = KL.Dense(num_classes, activation='softmax')(x)\n    # Define the model as a Keras Model\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n    # Return the model\n    return model\n\n", "CodeBleu_score": 33.632130824140596}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the graph\n    K.clear_session()\n\n    # Input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # First convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu')(input_layer)\n    x = KL.MaxPooling2D(pool_size=2)(x)\n\n    # Second convolutional layer\n    x = KL.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=2)(x)\n\n    # Flatten\n    x = KL.Flatten()(x)\n\n    # Dense layer with ReLU activation\n    x = KL.Dense(units=512, activation='relu')(x)\n\n    # Dense layer with softmax activation\n    output_layer = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # Define the model\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n\n    # Compile the model\n    model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 29.44115394097977}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\tboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 66.57859288118833}, {"code": "def extract_bboxes(mask)\n\tboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 66.57859288118833}, {"code": "def extract_bboxes(mask)\n\tboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 66.57859288118833}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Compute intersection areas\n    inter_area = np.minimum(boxes[:, 2], box[2]) - np.maximum(boxes[:, 0], box[0]) + 1\n    inter_area = np.clip(inter_area, 0, None)\n    inter_area = inter_area * np.minimum(boxes[:, 3], box[3]) - np.maximum(boxes[:, 1], box[1]) + 1\n    inter_area = np.clip(inter_area, 0, None)\n\n    # Compute union areas\n    union_area = box_area + boxes_area - inter_area\n\n    # Compute IoU\n    iou = inter_area / union_area\n\n    return iou\n\n", "CodeBleu_score": 40.50489253231937}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Compute the intersection areas\n    intersection_areas = box_area * np.logical_and(boxes[:, 0] < box[2], box[0] < boxes[:, 2]) * np.logical_and(boxes[:, 1] < box[3], box[1] < boxes[:, 3])\n    # Compute the union areas\n    union_areas = box_area + boxes_area - intersection_areas\n    # Compute the IoU values\n    iou = intersection_areas / union_areas\n    return iou\n\n", "CodeBleu_score": 29.897170403456318}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Compute intersection areas\n    inter_areas = np.minimum(box[2:], boxes[:, 2:]) - np.maximum(box[:2], boxes[:, :2])\n    inter_areas = np.clip(inter_areas, 0, np.inf)\n    inter_areas = inter_areas[:, 0] * inter_areas[:, 1]\n    # Compute union areas\n    union_areas = box_area + boxes_area - inter_areas\n    # Compute IoU\n    iou = inter_areas / union_areas\n    return iou\n\n\n\n", "CodeBleu_score": 25.56370338321015}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t# Calculate the area of each box in both sets\n    boxes1_area = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    boxes2_area = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n    # Initialize the matrix of overlaps\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2_area = boxes2_area[i]\n        # Calculate the intersection of box1 and box2\n        x1 = np.maximum(boxes1[:, 0], boxes2[i, 0])\n        y1 = np.maximum(boxes1[:, 1], boxes2[i, 1])\n        x2 = np.minimum(boxes1[:, 2], boxes2[i, 2])\n        y2 = np.minimum(boxes1[:, 3], boxes2[i, 3])\n        # Calculate the intersection area\n        intersection = np.maximum(x2 - x1 + 1, 0) * np.maximum(y2 - y1 + 1, 0)\n        # Calculate the union area\n        union = box1_area + box2_area - intersection\n        # Store the IoU value\n        overlaps[:, i] = intersection / union\n    return overlaps\n\n", "CodeBleu_score": 37.52816765511555}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# Compute area of boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    area2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 55.22725520258685}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# Compute area of boxes.", "CodeBleu_score": 0.6291343111878056}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}, {"code": "def load_image(self, image_id):\n\twidth = gt_box[:, 3] - gt_box[:, 1]\n    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.log(gt_height / height)\n    dw = tf.log(gt_width / width)\n\n    result = tf.stack([dy, dx, dh, dw], axis=1)\n    return result\n\n", "CodeBleu_score": 24.69586896090794}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t# Number of rows in the grid\n    N = len(images)\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, N + 1)]\n    else:\n        assert len(titles) == N,\n            'Number of titles must match number of images'\n    # Create a figure with sub-plots\n    fig, axes = plt.subplots(ncols=cols, nrows=N // cols, figsize=(cols * 5, N // cols * 5))\n    # Adjust vertical spacing if we have a lot of images\n    if N > cols:\n        axes = axes.ravel()\n    for i in range(N):\n        # Create a subplot\n        ax = axes[i]\n        # Remove ticks from the plot\n        ax.set_xticks([])\n        ax.set_yticks([])\n        # Set title if provided\n        if titles is not None:\n            ax.set_title(titles[i], fontsize=20)\n        # Loop over images and place them in the grid\n        img = images[i]\n        # Rescale the image if too big\n        if max(img.shape) > 800:\n            img = skimage.transform.resize(img, (800, 800))\n        # Normalize image\n        if norm is not None:\n            img = norm(img)\n        # Show the image\n        ax.imshow(img, cmap=cmap, interpolation=interpolation)\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 28.743033921006116}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t# Create a figure with a grid of subplots.\n    fig, axes = plt.subplots(ncols=cols, nrows=math.ceil(len(images) / float(cols)),\n                             figsize=(12.8, 4.8 * math.ceil(len(images) / float(cols))))\n    # Adjust vertical spacing between subplots.\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    # Iterate over the images and titles in order and display them.\n    for i, (image, title) in enumerate(zip(images, titles)):\n        # Create an axes on the grid with the same dimensions as the image.\n        ax = axes[i // cols, i % cols]\n        # Display the image.\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        # Set the title.\n        ax.set_title(title, fontsize=9)\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    # Ensure the plot is shown correctly with multiple plots per page.\n    plt.show()\n\n", "CodeBleu_score": 39.422960754868384}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t# Number of rows in the grid\n    n_rows = len(images) // cols + 1\n    # Make sure the axes are ordered correctly\n    plt.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0.05, hspace=0.05)\n    # Create a figure to hold the images\n    fig = plt.figure(figsize=(cols * 2, n_rows * 2))\n    # Iterate over the images\n    for i, img in enumerate(images):\n        # Create an axes on the figure for each image\n        sp = plt.subplot(n_rows, cols, i + 1)\n        # If a title was provided, display it\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=20)\n        # Display the image\n        plt.imshow(img, cmap=cmap, norm=norm, interpolation=interpolation)\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 41.06660038401475}]}
