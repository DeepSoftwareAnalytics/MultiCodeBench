{"instance_id": "ba8707e206f9", "generation_result": [{"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\twait_time = max(2, self.loop_wait - 2) * 1000\n    return wait_time, self._orig_kazoo_connect(*args)\n\n", "CodeBleu_score": 49.98718091134488}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\treturn (self._loop_wait - 2) * 1000, self._orig_kazoo_connect(*args)\n\n", "CodeBleu_score": 45.77494841847421}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\twait_time = max(2, self._loop_wait - 2) * 1000\n    return wait_time, self._orig_kazoo_connect(*args)\n\n", "CodeBleu_score": 49.39289508493929}]}
{"instance_id": "88e24a7c26a4", "generation_result": [{"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\tSError):\n    pass\n\n", "CodeBleu_score": 8.409228908440152}]}
{"instance_id": "2af7d7b3c887", "generation_result": [{"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\tasync_object.set_exception(SessionExpiredError())\n            return False\n        return super(PatroniKazooClient, self)._call(request, async_object)\n\n", "CodeBleu_score": 5.472925588799063}]}
{"instance_id": "43e1bc044d8e", "generation_result": [{"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\tsame time, set_ttl method will reestablish connection and return\n        # `!True`, otherwise we will close existing connection and let kazoo\n        # open the new one.\n        if not self.set_ttl(config['ttl']) and loop_wait_changed:\n            self._client._connection._socket.close()\n\n    def set_ttl(self, ttl: int) -> Optional[bool]:\n        \\\"\\\"\\\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\\\"\\\"\\\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True\n\n    @property\n    def ttl(self) -> int:\n        return int(self._client._session_timeout / 1000.0)\n\n    def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout\n\n    def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    @staticmethod\n    def member(name: str, value: str, znode: ZnodeStat) -> Member:\n        return Member.from_node(znode.version, name, znode.ephemeralOwner, value)\n\n    def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []\n\n    def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members\n\n    def _postgresql_cluster_loader(self, path: str) -> Cluster:\n        \\\"\\\"\\\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n\n        :param path: the path in DCS where to load :class:`Cluster` from.\n\n        :returns: :class:`Cluster` instance.\n        \\\"\\\"\\\"\n        nodes = set(self.get_children(path))\n\n        # get initialize flag\n        initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n\n        # get global dynamic configuration\n        config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n        config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n\n        # get timeline history\n        history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n        history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n\n        # get synchronization state\n        sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n        sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n\n        # get list of members\n        members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n\n        # get leader\n        leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n        if leader:\n            member = Member(-1, leader[0], None, {})\n            member = ([m for m in members if m.name == leader[0]] or [member])[0]\n            leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n\n        # get last known leader lsn and slots\n        status = self.get_status(path, leader)\n\n        # failover key\n        failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n\n\n", "CodeBleu_score": 35.93112282353281}]}
{"instance_id": "93d126ab8a22", "generation_result": [{"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\tget_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n        failover = failover and Failover.from_node(failover[1].version, failover[0])\n\n        # get failsafe topology\n        failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n        try:\n            failsafe = json.loads(failsafe[0]) if failsafe else None\n        except Exception:\n            failsafe = None\n\n        return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)\n\n    def _mpp_cluster_loader(self, path: str) -> Dict[int, Cluster]:\n        \\\"\\\"\\\"Load and build all PostgreSQL clusters from a single MPP cluster.\n\n        :param path: the path in DCS where to load Cluster(s) from.\n\n        :returns: all MPP groups as :class:`dict`, with group IDs as keys and :class:`Cluster` objects as values.\n        \\\"\\\"\\\"\n        ret: Dict[int, Cluster] = {}\n        for node in self.get_children(path):\n            if self._mpp.group_re.match(node):\n                ret[int(node)] = self._postgresql_cluster_loader(path + node + '/')\n        return ret\n\n    def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        try:\n            return self._client.retry(loader, path)\n        except Exception:\n            logger.exception('get_cluster')\n            raise ZooKeeperError('ZooKeeper in not responding properly')\n\n    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:\n        try:\n            if retry:\n                self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)\n            else:\n                self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)\n            return True\n        except Exception:\n            logger.exception('Failed to create %s', path)\n        return False\n\n    def attempt_to_acquire_leader(self) -> bool:\n        try:\n            self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                               makepath=True, ephemeral=True)\n            return True\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            if not isinstance(e, NodeExistsError):\n                logger.error('Failed to create %s: %r', self.leader_path, e)\n        logger.info('Could not take out TTL lock')\n        return False\n\n    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,\n                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:\n        value_bytes = value.encode('utf-8')\n        try:\n            if retry:\n                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)\n            else:\n                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)\n            return ret.version\n        except NoNodeError:\n            if do_not_create_empty and not value_bytes:\n                return True\n            elif version is None:\n                if self._create(key, value_bytes, retry):\n                    return 0\n            else:\n                return False\n        except Exception:\n            logger.exception('Failed to update %s', key)\n        return False\n\n    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.failover_path, value, version) is not False\n\n    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:\n        return self._set_or_create(self.config_path, value, version, retry=True) is not False\n\n    def initialize(self, create_new: bool = True, sysid: str = \\\"\\\") -> bool:\n        sysid_bytes = sysid.encode('utf-8')\n        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \\\n            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)\n\n    def touch_member(self, data: Dict[str, Any]) -> bool:\n        cluster = self.cluster\n        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)\n        member_data = self.__last_member_data or member and member.data\n        if member and member_data:\n            # We want delete the member ZNode if our session doesn't match with session id on our member key\n            if self._client.client_id is not None and member.session != self._client.client_id[0]:\n                logger.warning('Recreating the member ZNode due to ownership mismatch')\n                try:\n                    self._client.delete_async(self.member_path).get(timeout=1)\n                except NoNodeError:\n                    pass\n                except Exception:\n                    return False\n                member = None\n\n        encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')\n        if member and member_data:\n            if deep_compare(data, member_data):\n                return True\n        else:\n            try:\n                self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)\n                self.__last_member_data = data\n                return True\n            except Exception as e:\n                if not isinstance(e, NodeExistsError):\n                    logger.exception('touch_member')\n                    return False\n        try:\n            self._client.set_async(self.member_path, encoded_data).get(timeout=1)\n            self.__last_member_data = data\n            return True\n        except Exception:\n            logger.exception('touch_member')\n\n        return False\n\n    def take_leader(self) -> bool:\n        return self.attempt_to_acquire_leader()\n\n    def _write_leader_optime(self, last_lsn: str) -> bool:\n        return self._set_or_create(self.leader_optime_path, last_lsn) is not False\n\n    def _write_status(self, value: str) -> bool:\n        return self._set_or_create(self.status_path, value) is not False\n\n    def _write_failsafe(self, value: str) -> bool:\n        return self._set_or_create(self.failsafe_path, value) is not False\n\n\n\n", "CodeBleu_score": 50.729973732590715}]}
{"instance_id": "f68719982812", "generation_result": [{"code": "def poll(self) -> bool\n\tif self._next_poll and time.time() < self._next_poll:\n        return False\n    self._next_poll = time.time() + self._poll_interval\n    try:\n        url = uri(self._exhibitors[0], self._exhibitor_port, self._uri_path)\n        logger.debug('polling exhibitor at %s', url)\n        response = requests_get(url)\n        response.raise_for_status()\n        data = response.json()\n        if 'servers' not in data or 'port' not in data:\n            logger.warning('unexpected exhibitor data: %s', data)\n            return False\n        new_zookeeper_hosts = ','.join('%s:%s' % (server, data['port']) for server in data['servers'])\n        if self._zookeeper_hosts != new_zookeeper_hosts:\n            self._zookeeper_hosts = new_zookeeper_hosts\n            logger.info('exhibitor data changed: %s', self._zookeeper_hosts)\n            return True\n        return False\n    except Exception as e:\n        logger.warning('error polling exhibitor: %s', e)\n        return False\n\n\n\n", "CodeBleu_score": 44.15968581575258}, {"code": "def poll(self) -> bool\n\tif self._next_poll is not None and self._next_poll > time.time():\n        return False\n\n    self._next_poll = time.time() + self.TIMEOUT\n\n    try:\n        response = requests_get(uri('http', self._exhibitors[0], self._exhibitor_port, self._uri_path), timeout=self.TIMEOUT)\n        response.raise_for_status()\n        data = response.json()\n    except Exception:\n        logger.exception('error polling exhibitor')\n        return False\n\n    if 'servers' not in data or 'port' not in data:\n        logger.warning('invalid exhibitor response: %s', data)\n        return False\n\n    exhibitor_hosts = ','.join('%s:%s' % (host, data['port']) for host in data['servers'])\n    if self._zookeeper_hosts != exhibitor_hosts:\n        self._zookeeper_hosts = exhibitor_hosts\n        logger.info('exhibitor hosts changed: %s', self._zookeeper_hosts)\n\n    return True\n\n", "CodeBleu_score": 45.675963300040756}, {"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() >= self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        try:\n            response = requests_get(uri(self._exhibitors, self._exhibitor_port, self._uri_path), timeout=self.TIMEOUT)\n            if response.status_code == 200:\n                data = json.loads(response.text)\n                if 'servers' in data and 'port' in data:\n                    servers = data['servers']\n                    port = data['port']\n                    if self._zookeeper_hosts != f'{servers}:{port}':\n                        logger.info('exhibitor returned new zookeeper connection string: %s:%s', servers, port)\n                        self._zookeeper_hosts = f'{servers}:{port}'\n                        return True\n        except Exception:\n            logger.exception('error polling exhibitor')\n    return False\n\n", "CodeBleu_score": 43.6981146806981}]}
{"instance_id": "022ed3721913", "generation_result": [{"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        self.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 77.84703236539076}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        self._hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 77.84525036500787}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\tif self._ensemble_provider:\n        self._ensemble_provider.poll()\n        self._client.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 77.76152927104384}]}
{"instance_id": "c7d2e1db0cda", "generation_result": [{"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n    this.nodeCache.getListenable().addListener(nodeCacheListener);\n}", "CodeBleu_score": 63.76642256241134}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 80.2370284474588}]}
{"instance_id": "c779f567d3e2", "generation_result": [{"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}, {"code": "public void destroy(){\n\tif (zookeeperListener == null) {\n        return;\n    }\n\n    String group = getGroup();\n    String dataId = getDataId();\n\n    logUnsubscribeStarted();\n\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}", "CodeBleu_score": 99.49756467665183}]}
{"instance_id": "7f0ce4db487b", "generation_result": [{"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload();\n    }\n\n    return new HashMap<String, String>();\n}", "CodeBleu_score": 66.10011149697608}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload();\n    } else {\n        return null;\n    }\n}", "CodeBleu_score": 65.73137341170514}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\n        return zookeeperServer.getPayload();\n    }\n\n    return null;\n}", "CodeBleu_score": 65.31737738018012}]}
{"instance_id": "7365d194b7ee", "generation_result": [{"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.getListenable().addListener(new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData);\n            if (config == null) {\n                return;\n            }\n\n            zookeeperSubscribeCallback.callback(config);\n        }\n    });\n    nodeCache.start();\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, path);\n    return zookeeperListener;\n}", "CodeBleu_score": 61.60502970961102}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    boolean hasPath = hasPath(path);\n    if (!hasPath) {\n        return null;\n    }\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            String config = convertConfig(childData.getPath());\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n\n    nodeCache.getListenable().addListener(nodeCacheListener);\n    nodeCache.start();\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 69.732790896445}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.getListenable().addListener(new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            ChildData childData = nodeCache.getCurrentData();\n            if (childData == null) {\n                return;\n            }\n\n            byte[] bytes = childData.getData();\n            if (bytes == null) {\n                return;\n            }\n\n            String config = new String(bytes);\n            zookeeperSubscribeCallback.callback(config);\n        }\n    });\n    nodeCache.start();\n\n    ZookeeperListener zookeeperListener = new ZookeeperListener();\n    zookeeperListener.setNodeCache(nodeCache);\n    zookeeperListener.setZookeeperSubscribeCallback(zookeeperSubscribeCallback);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 61.70595684762266}]}
{"instance_id": "6f492670162b", "generation_result": [{"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.initWithNiwsConfig(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 86.97031261131865}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.init(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 79.1570797161004}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.initWithNiwsConfig(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n\n    return serverList;\n}", "CodeBleu_score": 86.97031261131865}]}
{"instance_id": "17c01a5f8ef8", "generation_result": [{"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.error(\"Failed to get RegisterListenerExecutor from application context\", e);\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 91.09638065942949}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.info(\"No RegisterListenerExecutor found\");\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 91.09638065942949}, {"code": "public void register(ZookeeperRegistration registration){\n\t// LOG.info(\"Registering service...\");\n\n    try {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.info(\"No RegisterListenerExecutor found in application context\");\n    }\n\n    serviceRegistry.register(registration);\n}", "CodeBleu_score": 91.57592266484517}]}
{"instance_id": "99392dfa2335", "generation_result": [{"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.deregister(registration);\n}", "CodeBleu_score": 91.10458663952991}]}
{"instance_id": "02a835946e90", "generation_result": [{"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}, {"code": "public void close(){\n\ttry {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n\n    serviceRegistry.close();\n}", "CodeBleu_score": 99.3595927181432}]}
{"instance_id": "64cf817f5945", "generation_result": [{"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId();\n\n    logSubscribeStart(group, dataId, globalConfig);\n\n    try {\n        ZookeeperSubscribeCallback callback = new ZookeeperSubscribeCallback(group, dataId, globalConfig, configType, configAdapter);\n        ZookeeperListener listener = zookeeperOperation.subscribeConfig(group, dataId, callback);\n\n        logSubscribeSuccess(group, dataId, globalConfig);\n\n        return listener;\n    } catch (Exception e) {\n        logSubscribeFailure(group, dataId, globalConfig, e);\n    }\n\n    return null;\n}", "CodeBleu_score": 49.103983604174616}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId();\n\n    String configType = globalConfig ? \"Global\" : \"Partial\";\n    String subscribeStartLog = \"Subscribe \" + configType + \" config start, group=\" + group + \", dataId=\" + dataId;\n    LOG.info(subscribeStartLog);\n\n    ZookeeperListener zookeeperListener = null;\n    try {\n        zookeeperListener = zookeeperOperation.subscribeConfig(group, dataId, new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                String configType = globalConfig ? \"Global\" : \"Partial\";\n                String configUpdateLog = \"Receive \" + configType + \" config update, group=\" + group + \", dataId=\" + dataId + \", config=\" + config;\n                LOG.info(configUpdateLog);\n\n                ConfigType configTypeEnum = globalConfig ? ConfigType.GLOBAL : ConfigType.PARTIAL;\n                callbackConfig(configTypeEnum, config);\n            }\n        });\n    } catch (Exception e) {\n        String subscribeFailedLog = \"Subscribe \" + configType + \" config failed, group=\" + group + \", dataId=\" + dataId;\n        LOG.error(subscribeFailedLog, e);\n    }\n\n    String subscribeEndLog = \"Subscribe \" + configType + \" config end, group=\" + group + \", dataId=\" + dataId;\n    LOG.info(subscribeEndLog);\n\n    return zookeeperListener;\n}", "CodeBleu_score": 46.67479344735729}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = getGroup();\n    String dataId = getDataId(globalConfig);\n\n    logSubscribeStart(group, dataId);\n\n    ZookeeperSubscribeCallback zookeeperSubscribeCallback = new ZookeeperSubscribeCallback(group, dataId, getConfigType(), getConfigCache());\n    try {\n        zookeeperOperation.subscribeConfig(group, dataId, zookeeperSubscribeCallback);\n    } catch (Exception e) {\n        logSubscribeFailed(group, dataId, e);\n        return null;\n    }\n\n    logSubscribeSucceed(group, dataId);\n\n    return zookeeperSubscribeCallback;\n}", "CodeBleu_score": 50.02621815005027}]}
{"instance_id": "be408acc71f6", "generation_result": [{"code": "public CuratorFramework curatorFramework(){\n\tString zkAddress = environment.getProperty(ZookeeperConstant.ZOOKEEPER_ADDRESS);\n    if (StringUtils.isEmpty(zkAddress)) {\n        throw new DiscoveryException(\"Zookeeper address is null\");\n    }\n\n    int retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class, ZookeeperConstant.DEFAULT_RETRY_COUNT);\n    int sleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, Integer.class, ZookeeperConstant.DEFAULT_SLEEP_TIME);\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zkAddress)\n            .retryPolicy(new ExponentialBackoffRetry(sleepTime, retryCount))\n            .build();\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 71.44808105579577}, {"code": "public CuratorFramework curatorFramework(){\n\tString connectionString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION_STRING);\n    if (StringUtils.isEmpty(connectionString)) {\n        throw new DiscoveryException(\"Zookeeper connection string is empty\");\n    }\n\n    int retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class);\n    int sleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, Integer.class);\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder().connectString(connectionString).retryPolicy(new ExponentialBackoffRetry(sleepTime, retryCount)).build();\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 62.42540344402836}, {"code": "public CuratorFramework curatorFramework(){\n\tString zookeeperAddress = environment.getProperty(ZookeeperConstant.ZOOKEEPER_ADDRESS);\n    if (StringUtils.isEmpty(zookeeperAddress)) {\n        throw new DiscoveryException(\"Zookeeper address is null or empty\");\n    }\n\n    int retryCount = Integer.parseInt(environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, \"3\"));\n    int sleepTime = Integer.parseInt(environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, \"1000\"));\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.newClient(zookeeperAddress, new ExponentialBackoffRetry(sleepTime, retryCount));\n    curatorFramework.start();\n\n    return curatorFramework;\n}", "CodeBleu_score": 52.13961440663638}]}
{"instance_id": "3628d7b6f0a5", "generation_result": [{"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t>Copyright: Copyright (c) 2017-2050</p>\n * <p>Company: Nepxion</p>\n * @author Haojun Ren\n * @author Ning Zhang\n * @version 1.0\n */\n", "CodeBleu_score": 3.555294114895601}]}
{"instance_id": "68a13b54ad7c", "generation_result": [{"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(),\n            zooKeeperClientProperties.getMaxRetries(),\n            zooKeeperClientProperties.getMaxSleepMs());\n    curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getConnectString())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 61.53981168416654}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries());\n    curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getConnectString())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 63.57225196360879}, {"code": "public  CuratorFramework createCuratorFramework(){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries());\n    curatorFramework = CuratorFrameworkFactory.builder()\n            .connectString(zooKeeperClientProperties.getConnectString())\n            .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n            .namespace(zooKeeperClientProperties.getNamespace())\n            .retryPolicy(retryPolicy)\n            .build();\n    curatorFramework.start();\n    return curatorFramework;\n}", "CodeBleu_score": 63.57225196360879}]}
{"instance_id": "018a3d7b5f59", "generation_result": [{"code": "public void lock(String path) throws DistributedLockException{\n\tThread currentThread = Thread.currentThread();\n    InterProcessLock lock = locksMap.get(currentThread);\n    if (lock == null) {\n        try {\n            lock = new InterProcessMutex(curatorFrameworkClient.getCuratorClient(), path);\n            lock.acquire();\n            locksMap.put(currentThread, (InterProcessMutex) lock);\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(e);\n        }\n    }\n}", "CodeBleu_score": 45.18642235657193}, {"code": "public void lock(String path) throws DistributedLockException{\n\tInterProcessMutex lock = locksMap.get(Thread.currentThread());\n    if (lock == null) {\n        try {\n            lock = new InterProcessMutex(curatorFrameworkClient.getClient(), path);\n            lock.acquire();\n            locksMap.put(Thread.currentThread(), lock);\n        } catch (Exception e) {\n            throw new DistributedLockException(\"Failed to acquire lock for path: \" + path, e);\n        }\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(\"Failed to re-acquire lock for path: \" + path, e);\n        }\n    }\n}", "CodeBleu_score": 50.302053439443114}, {"code": "public void lock(String path) throws DistributedLockException{\n\tThread currentThread = Thread.currentThread();\n    InterProcessLock lock = locksMap.get(currentThread);\n    if (lock == null) {\n        try {\n            lock = new InterProcessMutex(curatorFrameworkClient.getClient(), path);\n            lock.acquire();\n            locksMap.put(currentThread, (InterProcessMutex) lock);\n        } catch (Exception e) {\n            throw new DistributedLockException(\"Failed to acquire lock for path: \" + path, e);\n        }\n    } else {\n        try {\n            lock.acquire();\n        } catch (Exception e) {\n            throw new DistributedLockException(\"Failed to acquire lock for path: \" + path, e);\n        }\n    }\n}", "CodeBleu_score": 48.33800426455208}]}
{"instance_id": "2068af09c973", "generation_result": [{"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    DtpProperties.Zookeeper zookeeper = dtpProperties.getZookeeper();\n    switch (zookeeper.getConfigType()) {\n        case PROPERTIES:\n            return genPropertiesTypeMap(curatorFramework, nodePath);\n        case JSON:\n            String configKey = zookeeper.getConfigKey();\n            String configValue = curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey));\n            return ConfigHandler.parseConfig(configValue, JSON.getFileType());\n        default:\n            return Collections.emptyMap();\n    }\n}", "CodeBleu_score": 46.923601103637914}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    Map<Object, Object> propertiesMap;\n    if (dtpProperties.getConfigFileType() == PROPERTIES) {\n        propertiesMap = genPropertiesTypeMap(curatorFramework, nodePath);\n    } else {\n        String configKey = dtpProperties.getConfigKey();\n        String configValue = getConfigValue(curatorFramework, ZKPaths.makePath(nodePath, configKey));\n        propertiesMap = ConfigHandler.parse(configValue, dtpProperties.getConfigFileType());\n    }\n    return propertiesMap;\n}", "CodeBleu_score": 53.3017423258188}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    if (PROPERTIES.equals(dtpProperties.getConfigFileType())) {\n        return genPropertiesTypeMap(curatorFramework, nodePath);\n    } else {\n        String configKey = dtpProperties.getConfigKey();\n        String configValue = new String(curatorFramework.getData().forPath(ZKPaths.makePath(nodePath, configKey)), StandardCharsets.UTF_8);\n        ConfigHandler configHandler = ConfigHandler.getInstance(JSON.getType());\n        return configHandler.parse(configValue);\n    }\n}", "CodeBleu_score": 51.707879624894524}]}
{"instance_id": "3d3c9020d299", "generation_result": [{"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = BinderHelper.bind(environment, DtpProperties.PREFIX, DtpProperties.class);\n    Map<String, Object> map = BinderHelper.bind(environment, DtpProperties.PREFIX, Map.class);\n    if (map.containsKey(DtpProperties.PREFIX)) {\n        map.remove(DtpProperties.PREFIX);\n    }\n    if (dtpProperties.getZookeeper().isEnable()) {\n        if (CuratorUtil.getClient(dtpProperties) == null) {\n            log.warn(\"zookeeper client is null\");\n            return;\n        }\n        if (!CuratorUtil.checkExists(dtpProperties)) {\n            log.warn(\"zookeeper config path does not exist\");\n            return;\n        }\n        if (CuratorUtil.getProperties(dtpProperties) == null) {\n            log.warn(\"zookeeper config is null\");\n            return;\n        }\n        MutablePropertySources propertySources = environment.getPropertySources();\n        if (propertySources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n            propertySources.remove(ZK_PROPERTY_SOURCE_NAME);\n        }\n        propertySources.addLast(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, CuratorUtil.getProperties(dtpProperties)));\n    }\n}", "CodeBleu_score": 40.702446854149855}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = BinderHelper.bindDtpProperties(environment);\n    Map<String, Object> properties = BinderHelper.bindZkProperties(dtpProperties);\n    if (properties.size() > 0) {\n        MutablePropertySources propertySources = environment.getPropertySources();\n        if (!propertySources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n            propertySources.addLast(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties));\n        }\n    }\n}", "CodeBleu_score": 49.82322305840725}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties properties = BinderHelper.bind(environment, DtpProperties.class);\n    if (properties == null) {\n        return;\n    }\n    Map<String, Object> zkProperties = CuratorUtil.getZkProperties(properties);\n    if (zkProperties == null) {\n        return;\n    }\n    MutablePropertySources propertySources = environment.getPropertySources();\n    if (!propertySources.contains(ZK_PROPERTY_SOURCE_NAME)) {\n        propertySources.addFirst(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, zkProperties));\n    }\n}", "CodeBleu_score": 45.49824949301035}]}
{"instance_id": "0a934e7885be", "generation_result": [{"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterAdapter<IServiceRouteManager, ZookeeperServiceRouteManager>(container =>\n    {\n        var serializer = container.Resolve<ISerializer<byte[]>>();\n        var routeFactory = container.Resolve<IServiceRouteFactory>();\n        var logger = container.Resolve<ILogger<ZookeeperServiceRouteManager>>();\n        var provider = container.Resolve<IZookeeperClientProvider>();\n        return new ZookeeperServiceRouteManager(serializer, routeFactory, logger, provider, configInfo);\n    }).InstancePerLifetimeScope();\n    return this;\n}", "CodeBleu_score": 43.40263763278859}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterAdapter<IServiceRouteManager, ZookeeperServiceRouteManager>((sp, p) => {\n        var serializer = sp.GetService<ISerializer<byte[]>>() ?? new DefaultBinarySerializer();\n        var loggerFactory = sp.GetService<ILoggerFactory>();\n        var logger = loggerFactory.CreateLogger<ZookeeperServiceRouteManager>();\n        var routeFactory = sp.GetService<IServiceRouteFactory>();\n        var clientProvider = sp.GetService<IZookeeperClientProvider>();\n        var routeManager = new ZookeeperServiceRouteManager(serializer, logger, routeFactory, clientProvider, configInfo);\n        return routeManager;\n    }).InstancePerLifetimeScope();\n    return this;\n}", "CodeBleu_score": 34.37420533478313}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tbuilder.RegisterAdapter<ISerializer<string>, NonVoidReturningServiceEntryDescriptorSerializer>((sp, p) => new NonVoidReturningServiceEntryDescriptorSerializer(\n        sp.Resolve<IEnumerable<ITypeParser>>(),\n        sp.Resolve<IEnumerable<IConverterProvider>>(),\n        sp.Resolve<ILogger<NonVoidReturningServiceEntryDescriptorSerializer>>()\n    )).As(typeof(ISerializer<string>)).InstancePerLifetimeScope();\n    builder.RegisterAdapter<ISerializer<byte[]>, ServiceEntryDescriptorSerializer>((sp, p) => new ServiceEntryDescriptorSerializer(\n        sp.Resolve<IEnumerable<ITypeParser>>(),\n        sp.Resolve<IEnumerable<IConverterProvider>>(),\n        sp.Resolve<ILogger<ServiceEntryDescriptorSerializer>>()\n    )).As(typeof(ISerializer<byte[]>)).InstancePerLifetimeScope();\n    builder.RegisterAdapter<ISerializer<byte[]>, ServiceRouteFactory>((sp, p) => new ServiceRouteFactory(\n        sp.Resolve<ISerializer<byte[]>>(),\n        sp.Resolve<ISerializer<string>>(),\n        sp.Resolve<ILogger<ServiceRouteFactory>>()\n    )).As(typeof(ISerializer<byte[]>)).InstancePerLifetimeScope();\n    builder.RegisterAdapter<ISerializer<byte[]>, ZookeeperServiceRouteManager>((sp, p) => new ZookeeperServiceRouteManager(\n        sp.Resolve<ISerializer<byte[]>>(),\n        sp.Resolve<IZookeeperClientProvider>(),\n        sp.Resolve<ILogger<ZookeeperServiceRouteManager>>(),\n        configInfo\n    )).As(typeof(IServiceRouteManager)).InstancePerLifetimeScope();\n    return this;\n}", "CodeBleu_score": 49.459659756038185}]}
{"instance_id": "c30a17587331", "generation_result": [{"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider =>\n            {\n                var result = new ZooKeeperServiceCommandManager(\n                    GetConfigInfo(configInfo),\n                    provider.GetRequiredService<ISerializer<byte[]>>(),\n                    provider.GetRequiredService<ISerializer<string>>(),\n                    provider.GetRequiredService<IServiceCommandFactory>(),\n                    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n                    provider.GetRequiredService<IZookeeperClientProvider>());\n                return result;\n            });\n            return this;\n        }", "CodeBleu_score": 74.66706106835468}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider => {\n   var result = new ZooKeeperServiceCommandManager(\n    GetConfigInfo(configInfo),\n    provider.GetRequiredService<ISerializer<byte[]>>(),\n    provider.GetRequiredService<ISerializer<string>>(),\n    provider.GetRequiredService<IServiceCommandFactory>(),\n    provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n    provider.GetRequiredService<IZookeeperClientProvider>());\n   return result;\n  });\n  return this;\n }", "CodeBleu_score": 73.02405856032499}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tUseCommandManager(builder, provider => {\n        var result = new ZooKeeperServiceCommandManager(\n            GetConfigInfo(configInfo),\n            provider.GetRequiredService<ISerializer<byte[]>>(),\n            provider.GetRequiredService<ISerializer<string>>(),\n            provider.GetRequiredService<IServiceCommandFactory>(),\n            provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n            provider.GetRequiredService<IZookeeperClientProvider>());\n        return result;\n    });\n    return this;\n}", "CodeBleu_score": 75.97335245208711}]}
{"instance_id": "213cc4aa9b19", "generation_result": [{"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(nameof(path));\n    }\n\n    if (provider == null && Path.IsPathRooted(path)) {\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n    }\n\n    var source = new ZookeeperConfigurationSource {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 65.72110322535033}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(\"File path must be a non-empty string.\", nameof(path));\n    }\n\n    path = PathHelper.GetFullPath(builder.GetBasePath(), path);\n\n    if (provider == null && Path.IsPathRooted(path)) {\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n    }\n\n    var source = new ZookeeperConfigurationSource() {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 68.0594084135983}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\tif (builder == null) {\n        throw new ArgumentNullException(nameof(builder));\n    }\n    if (string.IsNullOrEmpty(path)) {\n        throw new ArgumentException(Resources.Error_InvalidFilePath, nameof(path));\n    }\n\n    if (provider == null && Path.IsPathRooted(path)) {\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n    }\n\n    var source = new ZookeeperConfigurationSource() {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    return builder;\n}", "CodeBleu_score": 61.98598989652765}]}
{"instance_id": "15bbffa82a13", "generation_result": [{"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u79fb\u9664\u5730\u5740\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.RoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeer.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\");\n    }\n}", "CodeBleu_score": 31.033050671468054}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u79fb\u9664\u5730\u5740\u3002\");\n    var routes = await GetRoutesAsync();\n    routes = routes.ToArray();\n    foreach (var route in routes)\n    {\n        route.Address = route.Address.Except(Address).ToArray();\n    }\n    await SetRoutesAsync(routes);\n    if (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u5730\u5740\u79fb\u9664\u6210\u529f\u3002\");\n}", "CodeBleu_score": 62.91904622416471}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tif (Address == null || !Address.Any()) {\n        return;\n    }\n    var routes = await GetRoutesAsync();\n    foreach (var route in routes) {\n        foreach (var address in Address) {\n            route.Address.Remove(address);\n        }\n    }\n    await SetRoutesAsync(routes);\n}", "CodeBleu_score": 53.01198585029322}]}
{"instance_id": "125d23de2a60", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\tSurging.Core.Zookeeper.WatcherProvider;", "CodeBleu_score": 8.059658417404334}]}
{"instance_id": "b020a235d57f", "generation_result": [{"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 15.77835252311384}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 15.77835252311384}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tInfo.RoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 15.77835252311384}]}
{"instance_id": "169ed4cc2ad4", "generation_result": [{"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 41.675487418790134}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 41.675487418790134}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.Address = route.Address.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<ServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.ServiceDescriptor.Id));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.ServiceDescriptor.Id == route.ServiceDescriptor.Id).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.Address.Concat(\n                          route.Address.Except(serviceRoute.Address)).ToList();\n\n                        foreach (var address in route.Address)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.Address = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n        private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n        {\n            var path = _configInfo.RoutePath;\n            if (!path.EndsWith(\\\"/\\\"))\n                path += \\\"/\\\";\n            routes = routes.ToArray();\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                if (_routes != null)\n                {\n                    var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n                    var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n                    foreach (var deletedRouteId in deletedRouteIds)\n                    {\n                        var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                        if (addresses.Contains(hostAddr))\n                        {\n                            var nodePath = $\\\"{path}{deletedRouteId}\\\";\n                            await zooKeeper.Item2.deleteAsync(nodePath);\n                        }\n                    }\n                }\n            }\n        }\n\n        private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper,  string path)\n        {\n            zooKeeper.Item1.WaitOne();\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n                return;\n\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation($\\\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n            var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n            var nodePath = \\\"/\\\";\n\n            foreach (var children in childrens)\n            {\n                nodePath += children;\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                {\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                nodePath += \\\"/\\\";\n            }\n        }\n\n        private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\\\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\\\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }\n\n        private async Task<ServiceRoute> GetRoute(string path)\n        {\n            ServiceRoute result = null;\n            var zooKeeper = await GetZooKeeper();\n            var watcher = new NodeMonitorWatcher(GetZooKeeper, path,\n                 async (oldData, newData) => await NodeChange(oldData, newData));\n            if (await zooKeeper.Item2.existsAsync(path) != null)\n            {\n                var data = (await zooKeeper.Item2.getDataAsync(path, watcher)).Data;\n                watcher.SetCurrentData(data);\n                result = await GetRoute(data);\n            }\n            return result;\n        }\n\n        private async Task<ServiceRoute[]> GetRoutes(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.RoutePath;\n            if (!rootPath.EndsWith(\\\"/\\\"))\n                rootPath += \\\"/\\\";\n\n            childrens = childrens.ToArray();\n            var routes = new List<ServiceRoute>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\\\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8def\u7531\u4fe1\u606f\u3002\\\");\n\n                var nodePath = $\\\"{rootPath}{children}\\\";\n                var route = await GetRoute(nodePath);\n                if (route != null)\n                    routes.Add(route);\n            }\n\n            return routes.ToArray();\n        }\n\n        private async Task EnterRoutes()\n        {\n            if (_routes != null)\n                return;\n            var zooKeeper = await GetZooKeeper();\n            zooKeeper.Item1.WaitOne();\n            var watcher = new ChildrenMonitorWatcher(GetZooKeeper, _configInfo.RoutePath,\n             async (oldChildrens, newChildrens) => await ChildrenChange(oldChildrens, newChildrens));\n            if (await zooKeeper.Item2.existsAsync(_configInfo.RoutePath, watcher) != null)\n            {\n                var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.RoutePath, watcher);\n                var childrens = result.Children.ToArray();\n                watcher.SetCurrentData(childrens);\n                _routes = await GetRoutes(childrens);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Warning))\n                    _logger.LogWarning($\\\"\u65e0\u6cd5\u83b7\u53d6\u8def\u7531\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.RoutePath}\uff0c\u4e0d\u5b58\u5728\u3002\\\");\n                _routes = new ServiceRoute[0];\n            }\n        }\n\n        private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }\n\n\n\n", "CodeBleu_score": 41.675487418790134}]}
{"instance_id": "af01261d3199", "generation_result": [{"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");", "CodeBleu_score": 39.52706247767582}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            } else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");\n\n                var", "CodeBleu_score": 39.49441864988212}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        await CreateSubdirectory(zooKeeper, _configInfo.SubscriberPath);\n\n        var path = _configInfo.SubscriberPath;\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n\n        subscribers = subscribers.ToArray();\n\n        if (_subscribers != null) {\n            var oldSubscriberIds = _subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newSubscriberIds = subscribers.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedSubscriberIds = oldSubscriberIds.Except(newSubscriberIds).ToArray();\n            foreach (var deletedSubscriberId in deletedSubscriberIds) {\n                var nodePath = $\"{path}{deletedSubscriberId}\";\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n        }\n\n        foreach (var serviceSubscriber in subscribers) {\n            var nodePath = $\"{path}{serviceSubscriber.ServiceDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(serviceSubscriber);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            } else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");\n\n                var", "CodeBleu_score": 39.49441864988212}]}
{"instance_id": "fda02cec3296", "generation_result": [{"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }", "CodeBleu_score": 30.310649567347404}]}
{"instance_id": "6c3018ff0c0d", "generation_result": [{"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}, {"code": "private async Task EnterSubscribers(){\n\tn                            }", "CodeBleu_score": 1.080881360365572}]}
{"instance_id": "60ff973ba60a", "generation_result": [{"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\tawait zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }", "CodeBleu_score": 10.539941641068564}]}
{"instance_id": "4597b75f2b3d", "generation_result": [{"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (manualResetEvent, zooKeeper) = await _zooKeeperCall().ConfigureAwait(false);\n    if (watchedEvent.Type == EventType.NodeDataChanged) {\n        var data = await zooKeeper.GetDataAsync(Path, true).ConfigureAwait(false);\n        _action(_currentData, data.Data);\n        _currentData = data.Data;\n    }\n    manualResetEvent.Set();\n}", "CodeBleu_score": 49.68868012683258}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (resetEvent, zooKeeper) = await _zooKeeperCall();\n    try {\n        if (watchedEvent.Type == EventType.NodeDataChanged) {\n            var data = await zooKeeper.GetDataAsync(Path);\n            _action(_currentData, data.Data);\n            _currentData = data.Data;\n        }\n\n        await zooKeeper.SetWatcherAsync(Path, this);\n    }\n    catch (Exception e) {\n        Logger.LogError(e, \"Zookeeper node monitor watcher error.\");\n    }\n    finally {\n        resetEvent.Set();\n    }\n}", "CodeBleu_score": 56.99159167268153}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tvar (manualResetEvent, zooKeeper) = await _zooKeeperCall().ConfigureAwait(false);\n    if (watchedEvent.Type == Event.EventType.NodeDataChanged) {\n        var data = await zooKeeper.getDataAsync(watchedEvent.Path, this).ConfigureAwait(false);\n        _action(_currentData, data.Data);\n        _currentData = data.Data;\n    }\n\n    manualResetEvent.Set();\n}", "CodeBleu_score": 51.15715778031258}]}
{"instance_id": "cb4f0e22a0d5", "generation_result": [{"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.RegisterType<DefaultZookeeperClientProvider>().As<IZookeeperClientProvider>().SingleInstance()\n        .WithParameter(new TypedParameter(typeof(ConfigInfo), configInfo))\n        .WithParameter(new TypedParameter(typeof(ILogger<DefaultZookeeperClientProvider>),\n            builder.Services.GetOrAddLogService(typeof(DefaultZookeeperClientProvider))));\n    return builder;\n}", "CodeBleu_score": 50.0484633722835}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.RegisterType<DefaultZookeeperClientProvider>().As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}", "CodeBleu_score": 27.100168259518796}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tbuilder.Services.AddSingleton(provider => new DefaultZookeeperClientProvider(\n        GetConfigInfo(configInfo),\n        provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>(),\n        provider.GetRequiredService<IHealthCheckService>(),\n        provider.GetRequiredService<IZookeeperAddressSelector>()));\n    return builder;\n}", "CodeBleu_score": 65.09760489028771}]}
{"instance_id": "80709a4b2b69", "generation_result": [{"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u8bbe\u7f6emqtt\u8def\u7531\u914d\u7f6e\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n\n        foreach (var route in routes) {\n            var nodePath = $\"{path}/{route.ServiceDescriptor.Id}\";\n            var data = _serializer.Serialize(route);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u3002\");\n                await zooKeeper.", "CodeBleu_score": 65.99479424290347}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u8bbe\u7f6emqtt\u8def\u7531\u914d\u7f6e\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n\n        if (routes != null && routes.Any()) {\n            foreach (var route in routes) {\n                var nodePath = $\"{path}/{route.ServiceDescriptor.Id}\";\n                if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                    if (_logger.IsEnabled(LogLevel.Debug))\n                        _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                    await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                }\n                var data = _serializer.Serialize(route);\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{nodePath}\u3002\");\n                await", "CodeBleu_score": 63.49767941561992}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\u914d\u7f6e\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.MqttRoutePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n\n        foreach (var route in routes) {\n            var nodePath = $\"{path}/{route.ServiceDescriptor.Id}\";\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n\n            var data = _serializer.Serialize(route);\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a", "CodeBleu_score": 63.82041226141215}]}
{"instance_id": "69ff4d3427cd", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\teperClientProvider;\n\n        public ZooKeeperMqttServiceRouteManager(ConfigInfo configInfo, ISerializer<byte[]> serializer,\n            ISerializer<string> stringSerializer, IMqttServiceFactory mqttServiceFactory,\n            ILogger<ZooKeeperMqttServiceRouteManager> logger, IZookeeperClientProvider zookeeperClientProvider) : base(stringSerializer)\n        {\n            _configInfo = configInfo;\n            _serializer = serializer;\n            _mqttServiceFactory = mqttServiceFactory;\n            _logger = logger;\n            _zookeeperClientProvider = zookeeperClientProvider;\n            EnterRoutes().Wait();\n        }\n\n\n        /// <summary>\n        /// \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684mqtt\u670d\u52a1\u8def\u7531\u4fe1\u606f\u3002\n        /// </summary>\n        /// <returns>\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</returns>\n        public override async Task<IEnumerable<MqttServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }\n\n        /// <summary>\n        /// \u6e05\u7a7a\u6240\u6709\u7684mqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        public override async Task ClearAsync()\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6e05\u7a7a\u6240\u6709mqtt\u8def\u7531\u914d\u7f6e\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                var path = _configInfo.MqttRoutePath;\n                var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n                var index = 0;\n                while (childrens.Count() > 1)\n                {\n                    var nodePath = \\\"/\\\" + string.Join(\\\"/\\\", childrens);\n\n                    if (await zooKeeper.Item2.existsAsync(nodePath) != null)\n                    {\n                        var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                        if (result?.Children != null)\n                        {\n                            foreach (var child in result.Children)\n                            {\n                                var childPath = $\\\"{nodePath}/{child}\\\";\n                                if (_logger.IsEnabled(LogLevel.Debug))\n                                    _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\\\");\n                                await zooKeeper.Item2.deleteAsync(childPath);\n                            }\n                        }\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\\\");\n                        await zooKeeper.Item2.deleteAsync(nodePath);\n                    }\n                    index++;\n                    childrens = childrens.Take(childrens.Length - index).ToArray();\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"\u8def\u7531\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\\\");\n            }\n        }\n\n        /// <summary>\n        /// \u8bbe\u7f6emqtt\u670d\u52a1\u8def\u7531\u3002\n        /// </summary>\n        /// <param name=\\\"routes\\\">\u670d\u52a1\u8def\u7531\u96c6\u5408\u3002</param>\n        /// <returns>\u4e00\u4e2a\u4efb\u52a1\u3002</returns>\n        protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n        {\n            if (_logger.IsEnabled(LogLevel.Information))\n                _logger.LogInformation(\\\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\\\");\n            var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n            foreach (var zooKeeper in zooKeepers)\n            {\n                await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n\n                var path = _configInfo.MqttRoutePath;\n\n                routes = routes.ToArray();\n\n                foreach (var serviceRoute in routes)\n                {\n                    var nodePath = $\\\"{path}{serviceRoute.MqttDescriptor.Topic}\\\";\n                    var nodeData = _serializer.Serialize(serviceRoute);\n                    if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\\\");\n\n                        await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n                    }\n                    else\n                    {\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\\\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\\\");\n\n                        var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                        if (!DataEquals(nodeData, onlineData))\n                            await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Information))\n                    _logger.LogInformation(\\\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\\\");\n            }\n        }\n\n        public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n        {\n            var routes = await GetRoutesAsync();\n            foreach (var route in routes)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(Address);\n            }\n            await base.SetRoutesAsync(routes);\n        }\n\n        public override async Task RemoveByTopicAsync(string topic, IEnumerable<AddressModel> endpoint)\n        {\n            var routes = await GetRoutesAsync();\n            var route = routes.Where(p => p.MqttDescriptor.Topic == topic).SingleOrDefault();\n            if (route != null)\n            {\n                route.MqttEndpoint = route.MqttEndpoint.Except(endpoint);\n                await base.SetRoutesAsync(new MqttServiceRoute[] { route });\n            }\n        }\n\n        public override async Task SetRoutesAsync(IEnumerable<MqttServiceRoute> routes)\n        {\n            var hostAddr = NetUtils.GetHostAddress();\n            var serviceRoutes = await GetRoutes(routes.Select(p => p.MqttDescriptor.Topic));\n            if (serviceRoutes.Count() > 0)\n            {\n                foreach (var route in routes)\n                {\n                    var serviceRoute = serviceRoutes.Where(p => p.MqttDescriptor.Topic == route.MqttDescriptor.Topic).FirstOrDefault();\n                    if (serviceRoute != null)\n                    {\n                        var addresses = serviceRoute.MqttEndpoint.Concat(\n                          route.MqttEndpoint.Except(serviceRoute.MqttEndpoint)).ToList();\n\n                        foreach (var address in route.MqttEndpoint)\n                        {\n                            addresses.Remove(addresses.Where(p => p.ToString() == address.ToString()).FirstOrDefault());\n                            addresses.Add(address);\n                        }\n                        route.MqttEndpoint = addresses;\n                    }\n                }\n            }\n            await RemoveExceptRoutesAsync(routes, hostAddr);\n            await base.SetRoutesAsync(routes);\n        }\n\n\n\n", "CodeBleu_score": 49.99266835240297}]}
{"instance_id": "8856055e4872", "generation_result": [{"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.CachePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n            if (await zooKeeper.Item2.existsAsync(nodePath) != null) {\n                var result = await zooKeeper.Item2.getChildrenAsync(nodePath);\n                if (result?.Children != null) {\n                    foreach (var child in result.Children) {\n                        var childPath = $\"{nodePath}/{child}\";\n                        if (_logger.IsEnabled(LogLevel.Debug))\n                            _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\n                        await zooKeeper.Item2.deleteAsync(childPath);\n                    }\n                }\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.deleteAsync(nodePath);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u6e05\u7a7a\u5b8c\u6210\u3002\");\n    }\n}", "CodeBleu_score": 51.74985623388327}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u6240\u6709\u7f13\u5b58\u914d\u7f6e\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.CachePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        if (_logger.IsEnabled(LogLevel.Debug))\n            _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{path}\u3002\");\n        var result = await zooKeeper.Item2.getChildrenAsync(path);\n        if (result?.Children != null) {\n            foreach (var child in result.Children) {\n                var childPath = $\"{path}/{child}\";\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{childPath}\u3002\");\n                var cacheDescriptor = cacheDescriptors.FirstOrDefault(p => p.Id == child);\n                if (cacheDescriptor != null) {\n                    var data = _stringSerializer.Serialize(cacheDescriptor);\n                    if (await zooKeeper.Item2.existsAsync(childPath) != null) {\n                        var cacheData", "CodeBleu_score": 61.67902530033038}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tif (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u6240\u6709\u7f13\u5b58\u914d\u7f6e\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers) {\n        var path = _configInfo.CachePath;\n        var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n        var index = 0;\n        while (childrens.Count() > 1) {\n            var nodePath = \"/\" + string.Join(\"/\", childrens);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            index++;\n            childrens = childrens.Take(childrens.Length - index).ToArray();\n        }\n        foreach (var cacheDescriptor in cacheDescriptors) {\n            var cachePath = $\"{path}/{cacheDescriptor.Id}\";\n            if (await zooKeeper.Item2.existsAsync(cachePath) == null) {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{cachePath}\u3002\");\n                await zooKeeper.Item2.createAsync(cachePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            var data = _stringSerializer.Serialize(cacheDescriptor);\n            var result = await zooKeeper.Item2.getDataAsync(cachePath);\n            if (result == null || !data.Equals(result.Data.ToArray())) {\n                if (_logger.IsEnabled(LogLevel.Debug))", "CodeBleu_score": 63.3234842325776}]}
{"instance_id": "57d92c17de83", "generation_result": [{"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t.Implementation;", "CodeBleu_score": 6.729241383528476}]}
{"instance_id": "b276a07da453", "generation_result": [{"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    var address = new List<AddressModel>();\n    foreach (var addressModel in _config.Addresses) {\n        _healthCheckService.Monitor(addressModel);\n        var task = _healthCheckService.IsHealth(addressModel);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        address.Add(addressModel);\n    }\n    if (address.Count == 0) {\n        if (_logger.IsEnabled(Level.Warning))\n            _logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n        return default(ValueTuple<ManualResetEvent, ZooKeeper>);\n    }\n    foreach (var ipAddress in address) {\n        var vt = _zookeeperAddressSelector.SelectAsync(new AddressSelectContext\n        {\n            Descriptor = new ServiceDescriptor { Id = nameof(DefaultZookeeperClientProvider) },\n            Address = new List<AddressModel> { ipAddress }\n        });\n        var addr = vt.IsCompletedSuccessfully ? vt.Result : await vt;\n        if (addr != null) {\n            var ipAddress = addr as IpAddressModel;\n            result.Add(CreateZooKeeper(ipAddress));\n        }\n    }\n    return result;\n}", "CodeBleu_score": 65.78082762254091}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar result = new List<(ManualResetEvent, ZooKeeper)>();\n    var address = new List<AddressModel>();\n    foreach (var addressModel in _config.Addresses) {\n        _healthCheckService.Monitor(addressModel);\n        var task = _healthCheckService.IsHealth(addressModel);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        address.Add(addressModel);\n    }\n    if (address.Count == 0) {\n        if (_logger.IsEnabled(Level.Warning))\n            _logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n        return default((ManualResetEvent, ZooKeeper)[]);\n    }\n    var vt = _zookeeperAddressSelector.SelectAsync(new AddressSelectContext {\n        Descriptor = new ServiceDescriptor { Id = nameof(DefaultZookeeperClientProvider) },\n        Address = address\n    });\n    var addrs = vt.IsCompletedSuccessfully ? vt.Result : await vt;\n    if (addrs != null) {\n        foreach (var addr in addrs) {\n            var ipAddress = addr as IpAddressModel;\n            result.Add(CreateZooKeeper(ipAddress));\n        }\n    }\n    return result;\n}", "CodeBleu_score": 66.41802626568818}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tvar zookeeperClients = new List<(ManualResetEvent, ZooKeeper)>();\n    var address = new List<AddressModel>();\n    foreach (var addressModel in _config.Addresses) {\n        _healthCheckService.Monitor(addressModel);\n        var task = _healthCheckService.IsHealth(addressModel);\n        if (!(task.IsCompletedSuccessfully ? task.Result : await task)) {\n            continue;\n        }\n        address.Add(addressModel);\n    }\n    if (address.Count == 0) {\n        if (_logger.IsEnabled(LogLevel.Warning))\n            _logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n        return default((ManualResetEvent, ZooKeeper)[]);\n    }\n    foreach (var ipAddress in address) {\n        var vt = _zookeeperAddressSelector.SelectAsync(new AddressSelectContext {\n            Descriptor = new ServiceDescriptor { Id = nameof(DefaultZookeeperClientProvider) },\n            Address = address\n        });\n        var addr = vt.IsCompletedSuccessfully ? vt.Result : await vt;\n        if (addr != null) {\n            var ipAddress = addr as IpAddressModel;\n            zookeeperClients.Add(CreateZooKeeper(ipAddress));\n        }\n    }\n    return zookeeperClients;\n}", "CodeBleu_score": 59.658074401973124}]}
{"instance_id": "dbb6463e7ee8", "generation_result": [{"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\treturn (helper != null) ? helper.getZooKeeper() : null;\n}", "CodeBleu_score": 4.378318678224832}]}
{"instance_id": "0d9f311aa482", "generation_result": [{"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = null;\n    try {\n        zooKeeper = getZooKeeper();\n        if (zooKeeper != null) {\n            zooKeeper.register(new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                    // do nothing\n                }\n            });\n        }\n    } finally {\n        if (zooKeeper != null) {\n            if (waitForShutdownTimeoutMs > 0) {\n                zooKeeper.close(waitForShutdownTimeoutMs);\n            } else {\n                zooKeeper.close();\n            }\n        }\n    }\n}", "CodeBleu_score": 43.66462808382616}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tZooKeeper zooKeeper = null;\n    try {\n        zooKeeper = getZooKeeper();\n    } catch (Exception e) {\n        // ignore\n    }\n\n    if (zooKeeper != null) {\n        try {\n            zooKeeper.register(new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                    // do nothing\n                }\n            });\n        } catch (Exception e) {\n            // ignore\n        }\n\n        if (waitForShutdownTimeoutMs > 0) {\n            try {\n                Thread.sleep(waitForShutdownTimeoutMs);\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n\n        try {\n            zooKeeper.close();\n        } catch (Exception e) {\n            // ignore\n        }\n    }\n}", "CodeBleu_score": 51.84511326463607}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tif (helper != null) {\n        ZooKeeper zooKeeper = helper.getZooKeeper();\n        if (zooKeeper != null) {\n            zooKeeper.register(new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                }\n            });\n            zooKeeper.close(waitForShutdownTimeoutMs);\n        }\n    }\n}", "CodeBleu_score": 33.67310195510838}]}
{"instance_id": "d57ef4592303", "generation_result": [{"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\terDriver;", "CodeBleu_score": 2.190573750570888}]}
{"instance_id": "d722f6d0beb6", "generation_result": [{"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tZookeeperClient(\n            String connectString,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                new FixedEnsembleProvider(connectString),\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     */\n    public CuratorZookeeperClient(\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy) {\n        this(\n                new DefaultZookeeperFactory(),\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                watcher,\n                retryPolicy,\n                false);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        this(\n                zookeeperFactory,\n                ensembleProvider,\n                sessionTimeoutMs,\n                connectionTimeoutMs,\n                0,\n                watcher,\n                retryPolicy,\n                canBeReadOnly);\n    }\n\n    /**\n     * @param zookeeperFactory factory for creating {@link ZooKeeper} instances\n     * @param ensembleProvider the ensemble provider\n     * @param sessionTimeoutMs session timeout\n     * @param connectionTimeoutMs connection timeout\n     * @param waitForShutdownTimeoutMs default timeout fo close operation\n     * @param watcher default watcher or null\n     * @param retryPolicy the retry policy to use\n     * @param canBeReadOnly if true, allow ZooKeeper client to enter\n     *                      read only mode in case of a network partition. See\n     *                      {@link ZooKeeper#ZooKeeper(String, int, Watcher, long, byte[], boolean)}\n     *                      for details\n     * @since 4.0.2\n     */\n    public CuratorZookeeperClient(\n            ZookeeperFactory zookeeperFactory,\n            EnsembleProvider ensembleProvider,\n            int sessionTimeoutMs,\n            int connectionTimeoutMs,\n            int waitForShutdownTimeoutMs,\n            Watcher watcher,\n            RetryPolicy retryPolicy,\n            boolean canBeReadOnly) {\n        if (sessionTimeoutMs < connectionTimeoutMs) {\n            log.warn(String.format(\n                    \\\"session timeout [%d] is less than connection timeout [%d]\\\",\n                    sessionTimeoutMs, connectionTimeoutMs));\n        }\n\n        retryPolicy = Preconditions.checkNotNull(retryPolicy, \\\"retryPolicy cannot be null\\\");\n        ensembleProvider = Preconditions.checkNotNull(ensembleProvider, \\\"ensembleProvider cannot be null\\\");\n\n        this.connectionTimeoutMs = connectionTimeoutMs;\n        this.waitForShutdownTimeoutMs = waitForShutdownTimeoutMs;\n        state = new ConnectionState(\n                zookeeperFactory, ensembleProvider, sessionTimeoutMs, watcher, tracer, canBeReadOnly);\n        setRetryPolicy(retryPolicy);\n    }\n\n    /**\n     * Return the managed ZK instance.\n     *\n     * @return client the client\n     * @throws Exception if the connection timeout has elapsed or an exception occurs in a background process\n     */\n    public ZooKeeper getZooKeeper() throws Exception {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        return state.getZooKeeper();\n    }\n\n    /**\n     * Return a new retry loop. All operations should be performed in a retry loop\n     *\n     * @return new retry loop\n     */\n    public RetryLoop newRetryLoop() {\n        return new RetryLoopImpl(retryPolicy.get(), tracer);\n    }\n\n    /**\n     * Return a new \\\"session fail\\\" retry loop. See {@link SessionFailRetryLoop} for details\n     * on when to use it.\n     *\n     * @param mode failure mode\n     * @return new retry loop\n     */\n    public SessionFailRetryLoop newSessionFailRetryLoop(SessionFailRetryLoop.Mode mode) {\n        return new SessionFailRetryLoop(this, mode);\n    }\n\n    /**\n     * Returns true if the client is current connected\n     *\n     * @return true/false\n     */\n    public boolean isConnected() {\n        return state.isConnected();\n    }\n\n    /**\n     * This method blocks until the connection to ZK succeeds. Use with caution. The block\n     * will timeout after the connection timeout (as passed to the constructor) has elapsed\n     *\n     * @return true if the connection succeeded, false if not\n     * @throws InterruptedException interrupted while waiting\n     */\n    public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n        Preconditions.checkState(started.get(), \\\"Client is not started\\\");\n\n        log.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n        OperationTrace trace = startAdvancedTracer(\\\"blockUntilConnectedOrTimedOut\\\");\n\n        internalBlockUntilConnectedOrTimedOut();\n\n        trace.commit();\n\n        boolean localIsConnected = state.isConnected();\n        log.debug(\\\"blockUntilConnectedOrTimedOut() end. isConnected: \\\" + localIsConnected);\n\n        return localIsConnected;\n    }\n\n    /**\n     * Must be called after construction\n     *\n     * @throws IOException errors\n     */\n    public void start() throws Exception {\n        log.debug(\\\"Starting\\\");\n\n        if (!started.compareAndSet(false, true)) {\n            throw new IllegalStateException(\\\"Already started\\\");\n        }\n\n        state.start();\n    }\n\n    /**\n     * Close the client.\n     *\n     * Same as {@link #close(int) } using the timeout set at construction time.\n     *\n     * @see #close(int)\n     */\n    @Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }\n\n    /**\n     * Close this client object as the {@link #close() } method.\n     * This method will wait for internal resources to be released.\n     *\n     * @param waitForShutdownTimeoutMs timeout (in milliseconds) to wait for resources to be released.\n     *                  Use zero or a negative value to skip the wait.\n     */\n\n\n", "CodeBleu_score": 28.312005690881612}]}
{"instance_id": "8492e37ce46e", "generation_result": [{"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t       setRetryPolicy(retryPolicy);\n    }", "CodeBleu_score": 2.1862344791245825}]}
{"instance_id": "4a01f5365731", "generation_result": [{"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && createMode.isEphemeral()) {\n        if (sessionId != client.getZooKeeper().getSessionId()) {\n            log.warn(\"Session ID changed during protected mode operation.  Deleting old node: {}\", foundNode);\n            client.delete().forPath(foundNode);\n            sessionId = client.getZooKeeper().getSessionId();\n            foundNode = null;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 45.45350756076046}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long newSessionId = client.getZooKeeper().getSessionId();\n        if (newSessionId != sessionId) {\n            log.debug(\"Session ID changed from {} to {}.  Deleting old protected node {}\", sessionId, newSessionId, foundNode);\n            client.delete().forPath(foundNode);\n            sessionId = newSessionId;\n            return null;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 49.19726326787534}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId != 0) && createMode.isEphemeral()) {\n        long newSessionId = client.getZooKeeper().getSessionId();\n        if (newSessionId != sessionId) {\n            log.debug(\"Session ID changed from {} to {}\", sessionId, newSessionId);\n            client.delete().forPath(foundNode);\n            sessionId = newSessionId;\n            return null;\n        }\n    }\n    return foundNode;\n}", "CodeBleu_score": 43.55149926477915}]}
{"instance_id": "126d0922a699", "generation_result": [{"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tOperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = null;\n    try {\n        RetryLoop.callWithRetry(client.getZookeeperClient(), new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n                resultStat = client.getZooKeeper().setACL(path, aclList, version);\n                return null;\n            }\n        });\n    } catch (Throwable e) {\n        backgrounding.checkError(e, null);\n    }\n    trace.setReturnCode(KeeperException.Code.OK.intValue())\n            .setPath(path)\n            .setStat(resultStat)\n            .commit();\n    return resultStat;\n}", "CodeBleu_score": 57.22522482251513}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tOperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = null;\n    try {\n        RetryLoop.callWithRetry(\n                client.getZookeeperClient(),\n                new Callable<Stat>() {\n                    @Override\n                    public Stat call() throws Exception {\n                        return client.getZooKeeper().setACL(path, aclList, version);\n                    }\n                });\n        resultStat = new Stat();\n    } catch (Throwable e) {\n        resultStat = null;\n        backgrounding.checkError(e, null);\n    } finally {\n        trace.setReturnCode(Code.get(Code.OK.intValue())).setPath(path).setStat(resultStat).commit();\n    }\n    return resultStat;\n}", "CodeBleu_score": 65.1516245194108}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tOperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = null;\n    try {\n        RetryLoop.callWithRetry(\n            client.getZookeeperClient(),\n            new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n                    client.getZooKeeper().setACL(path, aclList, version);\n                    return null;\n                }\n            }\n        );\n        resultStat = new Stat();\n        resultStat.setAversion(client.getZooKeeper().exists(path, false).getAversion());\n    } catch (Exception e) {\n        resultStat = null;\n    } finally {\n        trace.setReturnCode(KeeperException.Code.OK.intValue())\n                .setPath(path)\n                .setStat(resultStat)\n                .commit();\n    }\n    return resultStat;\n}", "CodeBleu_score": 51.67109268247201}]}
{"instance_id": "e07b079ff61f", "generation_result": [{"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkNotNull(payload, \"payload cannot be null\");\n\n    this.thisId = thisId;\n    this.membershipPath = membershipPath;\n\n    // create a node for this member\n    pen = new PersistentNode(client, ZKPaths.makePath(membershipPath, thisId), payload, CreateMode.EPHEMERAL);\n    pen.start();\n\n    // create a cache for the membership path\n    CuratorCache cache = CuratorCache.builder(client, membershipPath).build();\n    cache.start();\n    this.cache = CuratorCacheBridge.newCuratorCacheBridge(cache);\n}", "CodeBleu_score": 53.47455159470549}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkNotNull(payload, \"payload cannot be null\");\n    Preconditions.checkArgument(thisId.length() > 0, \"thisId cannot be empty\");\n\n    this.membershipPath = membershipPath;\n    this.thisId = thisId;\n\n    // Create a CuratorCache for the membershipPath\n    cache = CuratorCacheBridge.bridge(new CuratorCache(client, membershipPath, false));\n    cache.start();\n\n    // Create a PersistentNode for the member using ephemeral mode\n    pen = new PersistentNode(client, ZKPaths.makePath(membershipPath, thisId), CreateMode.EPHEMERAL, payload);\n    pen.start();\n}", "CodeBleu_score": 53.72256203044018}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tPreconditions.checkNotNull(client, \"client cannot be null\");\n    Preconditions.checkNotNull(membershipPath, \"membershipPath cannot be null\");\n    Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    Preconditions.checkNotNull(payload, \"payload cannot be null\");\n\n    this.thisId = thisId;\n    this.membershipPath = membershipPath;\n    cache = CuratorCacheBridge.build(client, membershipPath, true);\n    cache.start();\n\n    try {\n        pen = new PersistentNode(client, ZKPaths.makePath(membershipPath, thisId), CreateMode.EPHEMERAL, payload);\n        pen.start();\n    } catch (Exception e) {\n        throw Throwables.propagate(e);\n    }\n}", "CodeBleu_score": 50.30488022869884}]}
{"instance_id": "d11c76b2b912", "generation_result": [{"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path asynchronously\n    // if successful, set up a watch on the node\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            System.out.println(\"Created node name is: \" + name);\n\n            // set up a watch on the node\n            async.getChildren().watched().forPath(path).whenComplete((children, watchException) -> {\n                if (watchException != null) {\n                    // there was a problem\n                    watchException.printStackTrace();\n                } else {\n                    System.out.println(\"Children of node: \" + children);\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 76.47465335679358}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path asynchronously\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            // the node was created successfully\n            System.out.println(\"Created node name is: \" + name);\n\n            // set up a watch on the node\n            async.getChildren().watched().withWatcher((curatorFramework, watchedEvent) -> {\n                System.out.println(\\\"Watched event: \\\" + watchedEvent);\n            }).forPath(path).whenComplete((children, watchException) -> {\n                if (watchException != null) {\n                    // there was a problem with the watch\n                    watchException.printStackTrace();\n                } else {\n                    System.out.println(\\\"Children of node \\\" + path + \\\": \\\" + children);\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 72.893961613702}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n    // create a node at the given path asynchronously\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem\n            exception.printStackTrace();\n        } else {\n            // the node was created successfully\n            System.out.println(\"Created node name is: \" + name);\n\n            // set up a watch on the node\n            async.getChildren().watched().withWatcher((curatorFramework, event) -> {\n                System.out.println(\\\"Node \\\" + event.getPath() + \\\" changed\\\");\n            }).forPath(name).whenComplete((children, watchException) -> {\n                if (watchException != null) {\n                    // there was a problem setting up the watch\n                    watchException.printStackTrace();\n                }\n            });\n        }\n    });\n}", "CodeBleu_score": 74.1616150132436}]}
{"instance_id": "4571de52ad5d", "generation_result": [{"code": "private List<OpResult> doOperation() throws Exception{\n\tList<Op> operations = transaction.getOperations();\n    List<OpResult> results = client.getZooKeeper().multi(operations);\n    if (results.size() > 0) {\n        OpResult firstResult = results.get(0);\n        if (firstResult.getType() == ZooDefs.OpCode.error) {\n            OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n            KeeperException.Code code = KeeperException.Code.get(errorResult.getErr());\n            throw KeeperException.create(code, \"\");\n        }\n    }\n\n    return results;\n}", "CodeBleu_score": 64.38597922736842}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZooKeeper().multi(transaction.getOps());\n    if (resultList.size() > 0) {\n        OpResult firstResult = resultList.get(0);\n        if (firstResult.getType() == ZooDefs.OpCode.error) {\n            OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n            KeeperException.Code code = KeeperException.Code.get(errorResult.getErr());\n            throw KeeperException.create(code);\n        }\n    }\n\n    return resultList;\n}", "CodeBleu_score": 65.12975058912149}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<OpResult> resultList = client.getZooKeeper().multi(transaction.getOps());\n    if (resultList.size() > 0) {\n        OpResult firstResult = resultList.get(0);\n        if (firstResult.getType() == OpResult.ERROR) {\n            OpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n            KeeperException.Code code = KeeperException.Code.get(errorResult.getErr());\n            throw KeeperException.create(code);\n        }\n    }\n    return resultList;\n}", "CodeBleu_score": 59.98952688495546}]}
{"instance_id": "99263700ce6d", "generation_result": [{"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\ttry {\n        Collection<String> serviceNames = serviceDiscovery.queryForNames();\n        for (String serviceName : serviceNames) {\n            System.out.println(serviceName + \":\");\n            Collection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n            for (ServiceInstance<InstanceDetails> instance : instances) {\n                outputInstance(instance);\n            }\n        }\n    } catch (KeeperException.NoNodeException e) {\n        System.err.println(\"No services registered.\");\n    } finally {\n        CloseableUtils.closeQuietly(serviceDiscovery);\n    }\n}", "CodeBleu_score": 70.37934686727208}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\ttry {\n        Collection<String> serviceNames = serviceDiscovery.queryForNames();\n        for (String serviceName : serviceNames) {\n            System.out.println(serviceName + \":\");\n            for (ServiceInstance<InstanceDetails> instance : serviceDiscovery.queryForInstances(serviceName)) {\n                outputInstance(instance);\n            }\n        }\n    } catch (KeeperException.NoNodeException e) {\n        System.err.println(\"No services currently registered.\");\n    } finally {\n        CloseableUtils.closeQuietly(serviceDiscovery);\n    }\n}", "CodeBleu_score": 62.527663244624044}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\ttry {\n        for (String name : serviceDiscovery.queryForNames()) {\n            System.out.println(name);\n            for (ServiceInstance<InstanceDetails> instance : serviceDiscovery.queryForInstances(name)) {\n                outputInstance(instance);\n            }\n        }\n    } catch (KeeperException.NoNodeException e) {\n        System.err.println(\"No instances registered\");\n    }\n    CloseableUtils.closeQuietly(serviceDiscovery);\n}", "CodeBleu_score": 47.857485886134256}]}
{"instance_id": "891a390fa04d", "generation_result": [{"code": "private List<String> filterAndSortChildren(List<String> children){\n\tchildren.remove(READY_NODE);\n    Collections.sort(children);\n    return children;\n}", "CodeBleu_score": 15.17044440611669}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\tList<String> filtered = Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String input) {\n            return !input.equals(READY_NODE);\n        }\n    }));\n    Collections.sort(filtered);\n    return filtered;\n}", "CodeBleu_score": 65.08136137593698}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\treturn Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String input) {\n            return !input.equals(READY_NODE);\n        }\n    })).stream().sorted().collect(Collectors.toList());\n}", "CodeBleu_score": 42.768502951720144}]}
{"instance_id": "f25af95a13b6", "generation_result": [{"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\thas a quota\\\");\n                }", "CodeBleu_score": 4.143640237530894}]}
{"instance_id": "269eaac5cec6", "generation_result": [{"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\tcreate(path, data.getBytes(), acl, flags);\n            outputStream.write((\\\"Created \\\" + newPath).getBytes());\n        }", "CodeBleu_score": 5.694995595802182}]}
{"instance_id": "c25db637a176", "generation_result": [{"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    assertPathListNotNull(pathList);\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).deleteAsync(path);\n        }\n    }\n}", "CodeBleu_score": 79.25187036694943}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    if (pathList == null) {\n        throw new IllegalArgumentException(\"path list is null\");\n    }\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            delete(serverId, path);\n        }\n    } else {\n        zookeeperMap.get(serverId).delete(pathList);\n    }\n}", "CodeBleu_score": 72.99044824803813}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\tassertZookeeperExists(serverId);\n    if (pathList == null || pathList.isEmpty()) {\n        throw new IllegalArgumentException(\"pathList is null or empty\");\n    }\n    if (pathList.size() < 20) {\n        pathList.forEach(path -> {\n            try {\n                zookeeperMap.get(serverId).delete(path);\n            } catch (Exception e) {\n                log.error(\"delete path error \" + serverId + \" -> \" + path, e);\n            }\n        });\n    } else {\n        pathList.parallelStream().forEach(path -> {\n            try {\n                zookeeperMap.get(serverId).delete(path);\n            } catch (Exception e) {\n                log.error(\"delete path error \" + serverId + \" -> \" + path, e);\n            }\n        });\n    }\n}", "CodeBleu_score": 52.77656841141158}]}
{"instance_id": "3ecb1ce78f90", "generation_result": [{"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to \" + url + \"...\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .build();\n    var connection = factory.create(params);\n    writer.write(\"Connected to \" + url + \".\");\n    var main = new ZooKeeperMain(connection);\n    return new Terminal(id, url, connection, main);\n}", "CodeBleu_score": 57.96831028100357}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"connecting to \" + url);\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .aclList(null)\n            .maxRetries(10)\n            .connectionTimeout(1000)\n            .retryIntervalTime(1000)\n            .sessionTimeout(10000)\n            .build();\n    var connection = factory.createAsync(params, null);\n    writer.write(\"successfully connected to \" + url);\n    var client = connection.getZookeeperClient();\n    var zk = new ZooKeeperMain(client);\n    return new Terminal(id, url, connection, zk);\n}", "CodeBleu_score": 63.07637571850358}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\twriter.write(\"Connecting to Zookeeper instance \" + url + \"...\n\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .maxRetries(10)\n            .connectionTimeout(3000)\n            .retryIntervalTime(1000)\n            .sessionTimeout(10000)\n            .build();\n    var connection = factory.create(params);\n    writer.write(\"Connected to Zookeeper instance \" + url + \".\n\");\n    var zk = new ZooKeeperMain(connection);\n    return new Terminal(id, url, connection, zk);\n}", "CodeBleu_score": 64.49384654780359}]}
{"instance_id": "a3606e777eb5", "generation_result": [{"code": "public void blockUntilConnected(){\n\tint retries = 6;\n    while (retries > 0) {\n        try {\n            sshClient.connect(getSshHost(), getSshPort());\n            if (getSshPassword() != null && !getSshPassword().isBlank()) {\n                log.info(\"use password auth to create ssh-tunnel\");\n                sshClient.authPassword(getSshUsername(), getSshPassword());\n            } else if (getSshKeyFilePath() != null && !getSshKeyFilePath().isBlank()) {\n                log.info(\"use key file {} auth to create ssh-tunnel\", getSshKeyFilePath());\n                sshClient.authPublickey(getSshUsername(), getSshKeyFilePath());\n            } else {\n                log.info(\"use default key auth to create ssh-tunnel\");\n                sshClient.authPublickey(getSshUsername());\n            }\n            proxySocket = new ServerSocket();\n            proxySocket.setReuseAddress(true);\n            proxySocket.bind(new InetSocketAddress(localhost, localPort));\n            new Thread(() -> {\n                try {\n                    Parameters param = new Parameters(localhost, localPort, remoteHost, remotePort);\n                    sshClient.newLocalPortForwarder(param, proxySocket).listen();\n                } catch (IOException e) {\n                    throw new IllegalStateException(e.getMessage(), e);\n                }\n            }).start();\n            return;\n        } catch (IOException e) {\n            if (e.getClass().getSimpleName().contains(\"Timeout\")) {\n                throw new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n            }\n            if (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n                throw new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n            }\n            log.error(\"create ssh-tunnel failed\", e);\n            this.close();\n            throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n        }\n        retries--;\n        try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e)", "CodeBleu_score": 31.693482851372533}, {"code": "public void blockUntilConnected(){\n\tint retries = 6;\n    while (retries > 0) {\n        try {\n            if (sshClient.isConnected()) {\n                return;\n            }\n        } catch (IllegalStateException e) {\n            log.info(\"SSH connection is closed, try to reconnect.\");\n        }\n        try {\n            sshClient.connect(getSshHost(), getSshPort());\n            if (getSshPassword() != null && !getSshPassword().isBlank()) {\n                log.info(\"use password auth to create ssh-tunnel\");\n                sshClient.authPassword(getSshUsername(), getSshPassword());\n            } else if (getSshKeyFilePath() != null && !getSshKeyFilePath().isBlank()) {\n                log.info(\"use key file {} auth to create ssh-tunnel\", getSshKeyFilePath());\n                sshClient.authPublickey(getSshUsername(), getSshKeyFilePath());\n            } else {\n                log.info(\"use default key auth to create ssh-tunnel\");\n                sshClient.authPublickey(getSshUsername());\n            }\n            return;\n        } catch (IOException e) {\n            if (e.getClass().getSimpleName().contains(\"Timeout\")) {\n                throw new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n            }\n            if (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n                throw new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n            }\n            log.error(\"create ssh-tunnel failed\", e);\n            this.close();\n            throw new IllegalStateException(\"create ssh-tunnel failed\", e);\n        }\n        retries--;\n        try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e) {\n            log.error(\"thread sleep error\", e);\n            Thread.currentThread().interrupt();\n        }\n    }\n    throw new IllegalStateException(\"create ssh-tunnel failed\");\n}", "CodeBleu_score": 36.54204580880135}, {"code": "public void blockUntilConnected(){\n\tint retries = 6;\n    while (true) {\n        try {\n            createAsync();\n            return;\n        } catch (Exception e) {\n            log.error(\"SSH tunnel connection failed, retrying...\", e);\n            retries--;\n            if (retries == 0) {\n                close();\n                throw new IllegalStateException(\"SSH tunnel connection failed\", e);\n            }\n            try {\n                Thread.sleep(1000);\n            } catch (InterruptedException e1) {\n                close();\n                throw new IllegalStateException(\"SSH tunnel connection failed\", e1);\n            }\n        }\n    }\n}", "CodeBleu_score": 40.20276287854136}]}
{"instance_id": "a4e5b886b72b", "generation_result": [{"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 96.41257553887742}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 96.41257553887742}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\ttry {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed, serverId: {}, paths: {}\", serverId, pathList, e);\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 79.51505884003389}]}
{"instance_id": "4fdbc9ec2e75", "generation_result": [{"code": "public void disconnect(String id){\n\tCompletableFuture.runAsync(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        configurationDomainService.decrementConnectTimes(id);\n        treeItemCache.removeServer(id);\n    });\n}", "CodeBleu_score": 51.27717989089544}, {"code": "public void disconnect(String id){\n\tCompletableFuture.runAsync(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        configurationDomainService.decrementConnectTimes(id);\n    });\n}", "CodeBleu_score": 51.55336977189967}, {"code": "public void disconnect(String id){\n\tCompletableFuture.runAsync(() -> {\n        var serverConfig = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfig);\n        configurationDomainService.decrementConnectTimes(id);\n    });\n}", "CodeBleu_score": 51.55336977189967}]}
{"instance_id": "6bf0d7ac9390", "generation_result": [{"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\tnew TextFlow(highlights.toArray(new Text[0]));\n                    return new ZkNodeSearchResult(path, textFlow, item);\n                }", "CodeBleu_score": 7.622251745289576}]}
{"instance_id": "635dea120e55", "generation_result": [{"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}, {"code": "public void executeCommand(String id, String command){\n\t,\n                           String data,\n                           Consumer<Throwable> errorCallback) {\n        return Try.of(() -> zookeeperDomainService.set(serverId, nodePath, data))\n                .onFailure(errorCallback::accept)\n                .get();\n    }\n\n    public boolean hasServerConfiguration(String id) {\n        if (id == null) {\n            return false;\n        }\n        return configurationDomainService.containServerConfig(id);\n    }\n\n    public void saveServerConfiguration(ServerConfigurationVO serverConfigurationVO) {\n        SSHTunnelConfiguration tunnelConfig = null;\n        if (serverConfigurationVO.isSshEnabled()) {\n            tunnelConfig = SSHTunnelConfiguration.builder()\n                    .remoteHost(serverConfigurationVO.getRemoteServer())\n                    .remotePort(serverConfigurationVO.getRemoteServerPort())\n                    .sshHost(serverConfigurationVO.getSshServer())\n                    .sshPort(serverConfigurationVO.getSshServerPort())\n                    .localhost(serverConfigurationVO.getZkHost())\n                    .localPort(serverConfigurationVO.getZkPort())\n                    .sshUsername(serverConfigurationVO.getSshUsername())\n                    .sshPassword(serverConfigurationVO.getSshPassword())\n                    .sshKeyFilePath(serverConfigurationVO.getSshKeyFilePath())\n                    .build();\n        }\n\n        ConnectionConfiguration advanceConfig = new ConnectionConfiguration();\n        if (serverConfigurationVO.isEnableConnectionAdvanceConfiguration()) {\n            ConnectionConfigurationVO inputAdvanceConfig =\n                    serverConfigurationVO.getConnectionConfiguration();\n            advanceConfig.setConnectionTimeout(inputAdvanceConfig.getConnectionTimeout());\n            advanceConfig.setSessionTimeout(inputAdvanceConfig.getSessionTimeout());\n            advanceConfig.setMaxRetries(inputAdvanceConfig.getMaxRetries());\n            advanceConfig.setRetryIntervalTime(inputAdvanceConfig.getRetryIntervalTime());\n        }\n\n        boolean idIsBlank = serverConfigurationVO.getId().isBlank();\n        var serverConfiguration = ServerConfiguration.builder()\n                .id(idIsBlank ? UUID.randomUUID().toString() : serverConfigurationVO.getId())\n                .alias(serverConfigurationVO.getZkAlias())\n                .host(serverConfigurationVO.getZkHost())\n                .port(serverConfigurationVO.getZkPort())\n                .aclList(new ArrayList<>(List.of(Objects.toString(serverConfigurationVO.getAcl(), \\\"\\\").split(\\\"", "CodeBleu_score": 24.21452494099863}]}
{"instance_id": "8627d161f5fc", "generation_result": [{"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}, {"code": "public Properties loadZookeeperSystemProperties(){\n\tUiConfiguration()\n                .getMainSplitPaneDividerPosition();\n    }", "CodeBleu_score": 2.593828201691505}]}
{"instance_id": "dce20e143e04", "generation_result": [{"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t(new Configuration.FontConfiguration(newSize));\n    }", "CodeBleu_score": 2.23021903912926}]}
{"instance_id": "d89f8e6bfb31", "generation_result": [{"code": "private String response(Socket client){\n\ttry {\n        BufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n        StringBuilder builder = new StringBuilder();\n        String line;\n        while ((line = reader.readLine()) != null) {\n            builder.append(line).append(\"\n\");\n        }\n        client.close();\n        return builder.toString();\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 63.86676423335693}, {"code": "private String response(Socket client){\n\ttry {\n        InputStream inputStream = client.getInputStream();\n        BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));\n        StringBuilder builder = new StringBuilder();\n        String line;\n        while ((line = reader.readLine()) != null) {\n            builder.append(line);\n            builder.append(\"\n\");\n        }\n        client.close();\n        return builder.toString();\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 61.55172895205197}, {"code": "private String response(Socket client){\n\tStringBuilder response = new StringBuilder();\n    try {\n        InputStreamReader inputStreamReader = new InputStreamReader(client.getInputStream());\n        BufferedReader bufferedReader = new BufferedReader(inputStreamReader);\n        String line;\n        while ((line = bufferedReader.readLine()) != null) {\n            response.append(line);\n            response.append(\"\n\");\n        }\n        client.close();\n        return response.toString();\n    } catch (IOException e) {\n        throw new IllegalStateException(e);\n    }\n}", "CodeBleu_score": 51.84417781205709}]}
{"instance_id": "b0a277c5e3c0", "generation_result": [{"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n    CuratorFramework client = CuratorFrameworkFactory.builder()\n            .connectString(params.getConnectString())\n            .retryPolicy(retryPolicy)\n            .sessionTimeoutMs(params.getSessionTimeout())\n            .connectionTimeoutMs(params.getConnectionTimeout())\n            .build();\n    client.start();\n    try {\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n    } catch (InterruptedException e) {\n        client.close();\n        throw new RuntimeException(\"Failed to connect to Zookeeper\", e);\n    }\n    CuratorZookeeperConnection connection = new CuratorZookeeperConnection(params, client);\n    connection.addListener(new ConnectionStateListener() {\n        @Override\n        public void stateChanged(CuratorFramework client, ConnectionState state) {\n            if (state == ConnectionState.SUSPENDED || state == ConnectionState.LOST) {\n                connection.close();\n            }\n        }\n    });\n    return connection;\n}", "CodeBleu_score": 40.5715523024011}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tCuratorFramework client = createClient(params);\n    try {\n        client.start();\n        client.blockUntilConnected(5, TimeUnit.SECONDS);\n        return new CuratorZookeeperConnection(client, params.getServerListener());\n    } catch (Exception e) {\n        client.close();\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 43.781173803126116}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\tlog.info(\"create zookeeper connection with params: {}\", params);\n    CuratorFramework client = createClient(params);\n    try {\n        client.start();\n        if (!client.blockUntilConnected(5, TimeUnit.SECONDS)) {\n            throw new RuntimeException(\"zookeeper connection failed\");\n        }\n    } catch (InterruptedException e) {\n        client.close();\n        throw new RuntimeException(e);\n    }\n    return new CuratorZookeeperConnection(client, params);\n}", "CodeBleu_score": 68.97775137726066}]}
{"instance_id": "b144bcd67462", "generation_result": [{"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tif (isSync.getAndSet(true)) {\n        log.info(\"Synchronization is already in progress.\");\n        return;\n    }\n    log.info(\"Synchronization started.\");\n    try {\n        for (ZookeeperNodeListener listener : listeners) {\n            treeCache.getListenable().addListener(new CuratorTreeCacheListener(listener));\n        }\n        treeCache.start();\n    } catch (Exception e) {\n        log.error(\"An error occurred while synchronizing Zookeeper nodes.\", e);\n        treeCache.close();\n    }\n}", "CodeBleu_score": 39.51993229406169}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tif (isSync.get()) {\n        log.info(\"[sync] synchronization is already in progress, skip.\");\n        return;\n    }\n    log.info(\"[sync] synchronization start.\");\n    try {\n        treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n        treeCache.start();\n    } catch (Exception e) {\n        log.error(\"[sync] synchronization error.\", e);\n        close();\n    }\n}", "CodeBleu_score": 34.732530214939395}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tif (isSync.compareAndSet(false, true)) {\n        log.info(\"start syncing zookeeper nodes...\");\n        treeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n        try {\n            treeCache.start();\n        } catch (Exception e) {\n            log.error(\"failed to start tree cache\", e);\n            treeCache.close();\n        }\n    } else {\n        log.info(\"syncing zookeeper nodes already in progress...\");\n    }\n}", "CodeBleu_score": 38.573628214700975}]}
{"instance_id": "6452f272f2d7", "generation_result": [{"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n    pathChildrenCache.getListenable().addListener(listener);\n    pathChildrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n}", "CodeBleu_score": 82.20049899617024}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache cache = new PathChildrenCache(client, path, true);\n    cache.getListenable().addListener(listener);\n    cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n}", "CodeBleu_score": 69.9834257660432}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tPathChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n    pathChildrenCache.getListenable().addListener(listener);\n}", "CodeBleu_score": 64.52877946507382}]}
{"instance_id": "17f786e147c3", "generation_result": [{"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start\");\n    for (String path : pathList) {\n        try {\n            this.curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    logger.info(\"Unregister service end\");\n    this.curatorClient.close();\n}", "CodeBleu_score": 64.16250063554283}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start\");\n    for (String path : pathList) {\n        try {\n            curatorClient.deletePath(path);\n            logger.info(\"Unregister service path: {}\", path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n    logger.info(\"Unregister service end\");\n}", "CodeBleu_score": 52.57238988145163}, {"code": "public void unregisterService(){\n\tlogger.info(\"Unregister service start\");\n    for (String path : pathList) {\n        try {\n            curatorClient.deletePath(path);\n        } catch (Exception e) {\n            logger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n        }\n    }\n    curatorClient.close();\n    logger.info(\"Unregister service end\");\n}", "CodeBleu_score": 55.272497608963434}]}
{"instance_id": "32fec6cfa353", "generation_result": [{"code": "public void addListener(IZkStateListener listener){\n\tif (started) {\n        client.subscribeStateChanges(listener);\n    } else {\n        logger.warn(\"Zkclient has not been started!\");\n    }\n}", "CodeBleu_score": 19.41007175959436}, {"code": "public void addListener(IZkStateListener listener){\n\tif (state == KeeperState.SyncConnected) {\n        client.subscribeStateChanges(listener);\n    } else {\n        completableFuture.whenComplete(this::makeClientReady);\n        client.subscribeStateChanges(listener);\n    }\n}", "CodeBleu_score": 25.88703483768842}, {"code": "public void addListener(IZkStateListener listener){\n\tif (client == null) {\n        completableFuture.whenComplete(this::makeClientReady);\n    }\n    client.subscribeStateChanges(listener);\n}", "CodeBleu_score": 24.324449890280565}]}
{"instance_id": "619d57562a76", "generation_result": [{"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n            .connectString(serverAddr)\n            .retryPolicy(retryPolicy)\n            .connectionTimeoutMs(5000);\n    if (authority != null) {\n        builder = builder.authorization(\"digest\", authority.getBytes());\n    }\n    client = builder.build();\n    client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n        @Override\n        public void stateChanged(CuratorFramework client, ConnectionState newState) {\n            if (newState == ConnectionState.LOST) {\n                stateChanged(StateListener.CONNECTION_SUSPENDED);\n            } else if (newState == ConnectionState.CONNECTED) {\n                stateChanged(StateListener.CONNECTION_RECONNECTED);\n            } else if (newState == ConnectionState.RECONNECTED) {\n                stateChanged(StateListener.CONNECTION_RECONNECTED);\n            }\n        }\n    });\n    try {\n        client.start();\n    } catch (Exception e) {\n        throw new IllegalStateException(\"CuratorZookeeperClient init failed\", e);\n    }\n}", "CodeBleu_score": 69.09259959124152}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tthis(serverAddr, authority, 10000);\n}", "CodeBleu_score": 3.4102573673108427}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tthis(serverAddr, authority, DEFAULT_SESSION_TIMEOUT);\n}", "CodeBleu_score": 3.4102573870448625}]}
{"instance_id": "9e7fb18946e4", "generation_result": [{"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}", "CodeBleu_score": 92.91861730823078}]}
{"instance_id": "2c3fae642654", "generation_result": [{"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\tn    string_vector_t chdnodes;\n    memset(&chdnodes, 0, sizeof(string_vector_t));\n\n    int ret = zk_get_chdnodes_with_status(zh, path, chdnodes, status);\n    switch (ret)\n    {\n    case QCONF_OK:\n        chdnodeval_to_tblval(tblkey, chdnodes, tblval, status);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_chdval(chdnodes, status, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&chdnodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 37.721910561111294}]}
{"instance_id": "0b5ab34e1dd9", "generation_result": [{"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\tswitch (ret)\n    {\n    case QCONF_OK:\n        batchnodeval_to_tblval(tblkey, nodes, tblval);\n        ret = hash_tbl_set(_shm_tbl, tblkey, tblval);\n        if (QCONF_OK == ret)\n        {\n#ifdef QCONF_CURL_ENABLE\n            if (_fb_enable) feedback_generate_batchval(nodes, fb_val);\n#endif\n            add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_ADD_OR_MODIFY, fb_val);\n        }\n        deallocate_String_vector(&nodes);\n        ret = (QCONF_ERR_SAME_VALUE == ret) ? QCONF_OK : ret;\n        return ret;\n    case QCONF_NODE_NOT_EXIST:\n        ret = hash_tbl_remove(_shm_tbl, tblkey);\n        add_change_trigger_node(tblkey, tblval, QCONF_TRIGGER_TYPE_REMOVE);\n        return ret;\n    default:\n        return ret;\n    }\n}", "CodeBleu_score": 43.97371775449482}]}
{"instance_id": "19fc5847a7c5", "generation_result": [{"code": "def make_digest_acl_credential(username, password)\n\tm = hashlib.sha1()\n    m.update(username)\n    m.update(b\":\")\n    m.update(password)\n    return username + \":\" + b64encode(m.digest()).decode(\"ascii\")\n\n", "CodeBleu_score": 33.15183804278487}, {"code": "def make_digest_acl_credential(username, password)\n\thashed = hashlib.sha1(username + \\\":\\\" + password).digest()\n    return \\\":\\\" + username + \\\":\\\" + b64encode(hashed).strip()\n\n", "CodeBleu_score": 14.434626513845322}, {"code": "def make_digest_acl_credential(username, password)\n\tm = hashlib.sha1()\n    m.update(username.encode())\n    m.update(b\":\")\n    m.update(password.encode())\n    return username + \":\" + b64encode(m.digest()).decode()\n\n", "CodeBleu_score": 32.406625025013774}]}
{"instance_id": "4a062391deb0", "generation_result": [{"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}, {"code": "def holds_lock(self)\n\t(self.client.get, self._entries_path)\n        return stat.children_count\n\n", "CodeBleu_score": 1.7722394475727772}]}
{"instance_id": "aaac7523d2b5", "generation_result": [{"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}, {"code": "def consume(self)\n\t.client.KazooClient` instance.\n        :param path: The queue path to use in ZooKeeper.\n        \\\"\\\"\\\"\n        super(Queue, self).__init__(client, path)\n        self._children = []\n\n    def __len__(self):\n        \\\"\\\"\\\"Return queue size.\\\"\\\"\\\"\n        return super(Queue, self).__len__()\n\n    def get(self):\n        \\\"\\\"\\\"\n        Get item data and remove an item from the queue.\n\n        :returns: Item data or None.\n        :rtype: bytes\n        \\\"\\\"\\\"\n        self._ensure_paths()\n        return self.client.retry(self._inner_get)\n\n    def _inner_get(self):\n        if not self._children:\n            self._children = self.client.retry(\n                self.client.get_children, self.path\n            )\n            self._children = sorted(self._children)\n        if not self._children:\n            return None\n        name = self._children[0]\n        try:\n            data, stat = self.client.get(self.path + \\\"/\\\" + name)\n            self.client.delete(self.path + \\\"/\\\" + name)\n        except NoNodeError:  # pragma: nocover\n            # the first node has vanished in the meantime, try to\n            # get another one\n            self._children = []\n            raise ForceRetryError()\n\n        self._children.pop(0)\n        return data\n\n    def put(self, value, priority=100):\n        \\\"\\\"\\\"Put an item into the queue.\n\n        :param value: Byte string to put into the queue.\n        :param priority:\n            An optional priority as an integer with at most 3 digits.\n            Lower values signify higher priority.\n        \\\"\\\"\\\"\n        self._check_put_arguments(value, priority)\n        self._ensure_paths()\n        path = \\\"{path}/{prefix}{priority:03d}-\\\".format(\n            path=self.path, prefix=self.prefix, priority=priority\n        )\n        self.client.create(path, value, sequence=True)\n\n", "CodeBleu_score": 30.52071515100414}]}
{"instance_id": "bed0c998ac4c", "generation_result": [{"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}, {"code": "def _read_socket(self, read_timeout)\n\tbuffer, offset = self._read_header(timeout)\n            if header.xid != xid:\n                raise RuntimeError(\n                    \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                    xid,\n                    header.xid,\n                )\n            if header.zxid > 0:\n                zxid = header.zxid\n            if header.err:\n                callback_exception = EXCEPTIONS[header.err]()\n                self.logger.debug(\n                    \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n                )\n                raise callback_exception\n            return zxid\n\n        msg = self._read(4, timeout)\n        length = int_struct.unpack(msg)[0]\n        msg = self._read(length, timeout)\n\n        if hasattr(request, \\\"deserialize\\\"):\n            try:\n                obj, _ = request.deserialize(msg, 0)\n            except Exception:\n                self.logger.exception(\n                    \\\"Exception raised during deserialization \\\"\n                    \\\"of request: %s\\\",\n                    request,\n                )\n\n                # raise ConnectionDropped so connect loop will retry\n                raise ConnectionDropped(\\\"invalid server response\\\")\n            self.logger.log(BLATHER, \\\"Read response %s\\\", obj)\n            return obj, zxid\n\n        return zxid\n\n    def _submit(self, request, timeout, xid=None):\n        \\\"\\\"\\\"Submit a request object with a timeout value and optional\n        xid\\\"\\\"\\\"\n        b = bytearray()\n        if xid:\n            b.extend(int_struct.pack(xid))\n        if request.type:\n            b.extend(int_struct.pack(request.type))\n        b += request.serialize()\n        self.logger.log(\n            (BLATHER if isinstance(request, Ping) else logging.DEBUG),\n            \\\"Sending request(xid=%s): %s\\\",\n            xid,\n            request,\n        )\n        self._write(int_struct.pack(len(b)) + b, timeout)\n\n    def _write(self, msg, timeout):\n        \\\"\\\"\\\"Write a raw msg to the socket\\\"\\\"\\\"\n        sent = 0\n        msg_length = len(msg)\n        with self._socket_error_handling():\n            while sent < msg_length:\n                s = self.handler.select([], [self._socket], [], timeout)[1]\n                if not s:  # pragma: nocover\n                    # If the write list is empty, we got a timeout. We don't\n                    # have to check rlist and xlist as we don't set any\n                    raise self.handler.timeout_exception(\n                        \\\"socket time-out\\\" \\\" during write\\\"\n                    )\n                msg_slice = buffer(msg, sent)\n                try:\n                    bytes_sent = self._socket.send(msg_slice)\n                except ssl.SSLError as e:\n                    if e.errno in (\n                        ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n\n\n", "CodeBleu_score": 35.279406372707946}]}
{"instance_id": "b10e2c2c410b", "generation_result": [{"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}, {"code": "def zk_loop(self)\n\tANT_WRITE,\n                    ):\n                        continue\n                    else:\n                        raise\n                if not bytes_sent:\n                    raise ConnectionDropped(\\\"socket connection broken\\\")\n                sent += bytes_sent\n\n    def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\\\"Received EVENT: %s\\\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\\\"Received unknown event %r\\\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\\\"watch\\\", watch, (ev,)))\n\n    def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \\\"xids do not match, expected %r \\\" \\\"received %r\\\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \\\"Exception raised during deserialization \\\"\n                        \\\"of request: %s\\\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \\\"Received response(xid=%s): %r\\\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \\\"watcher\\\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \\\"Read close response\\\")\n            return CLOSE_RESPONSE\n\n    def _read_socket(self, read_timeout):\n        \\\"\\\"\\\"Called when there's something to read on the socket\\\"\\\"\\\"\n        client = self.client\n\n        header, buffer, offset = self._read_header(read_timeout)\n        if header.xid == PING_XID:\n            self.logger.log(BLATHER, \\\"Received Ping\\\")\n            self.ping_outstanding.clear()\n        elif header.xid == AUTH_XID:\n            self.logger.log(BLATHER, \\\"Received AUTH\\\")\n\n            request, async_object, xid = client._pending.popleft()\n            if header.err:\n                async_object.set_exception(AuthFailedError())\n                client._session_callback(KeeperState.AUTH_FAILED)\n            else:\n                async_object.set(True)\n        elif header.xid == WATCH_XID:\n            self._read_watch_event(buffer, offset)\n        else:\n            self.logger.log(BLATHER, \\\"Reading for header %r\\\", header)\n\n            return self._read_response(header, buffer, offset)\n\n    def _send_request(self, read_timeout, connect_timeout):\n        \\\"\\\"\\\"Called when we have something to send out on the socket\\\"\\\"\\\"\n        client = self.client\n        try:\n            request, async_object = client._queue[0]\n        except IndexError:\n            # Not actually something on the queue, this can occur if\n            # something happens to cancel the request such that we\n            # don't clear the socket below after sending\n            try:\n                # Clear possible inconsistence (no request in the queue\n                # but have data in the read socket), which causes cpu to spin.\n                self._read_sock.recv(1)\n            except OSError:\n                pass\n            return\n\n        # Special case for testing, if this is a _SessionExpire object\n        # then throw a SessionExpiration error as if we were dropped\n        if request is _SESSION_EXPIRED:\n            raise SessionExpiredError(\\\"Session expired: Testing\\\")\n        if request is _CONNECTION_DROP:\n            raise ConnectionDropped(\\\"Connection dropped: Testing\\\")\n\n        # Special case for auth packets\n        if request.type == Auth.type:\n            xid = AUTH_XID\n        else:\n            self._xid = (self._xid % 2147483647) + 1\n            xid = self._xid\n\n        self._submit(request, connect_timeout, xid)\n        client._queue.popleft()\n        self._read_sock.recv(1)\n        client._pending.append((request, async_object, xid))\n\n    def _send_ping(self, connect_timeout):\n        self.ping_outstanding.set()\n        self._submit(PingInstance, connect_timeout, PING_XID)\n\n        # Determine if we need to check for a r/w server\n        if self._ro_mode:\n            result = advance_iterator(self._ro_mode)\n            if result:\n                self._rw_server = result\n                raise RWServerAvailable()\n\n\n\n", "CodeBleu_score": 25.377129803989646}]}
{"instance_id": "42b3d26a3540", "generation_result": [{"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}, {"code": "def _partitioner(self, identifier, members, partitions)\n\tn    def allocating(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ALLOCATING`\n        state\\\"\\\"\\\"\n        return self.state == PartitionState.ALLOCATING\n\n    @property\n    def acquired(self):\n        \\\"\\\"\\\"Corresponds to the :attr:`PartitionState.ACQUIRED` state\\\"\\\"\\\"\n        return self.state == PartitionState.ACQUIRED\n\n    def wait_for_acquire(self, timeout=30):\n        \\\"\\\"\\\"Wait for the set to be partitioned and acquired\n\n        :param timeout: How long to wait before returning.\n        :type timeout: int\n\n        \\\"\\\"\\\"\n        self._acquire_event.wait(timeout)\n\n    def release_set(self):\n        \\\"\\\"\\\"Call to release the set\n\n        This method begins the step of allocating once the set has\n        been released.\n\n        \\\"\\\"\\\"\n        self._release_locks()\n        if self._locks:  # pragma: nocover\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n        else:\n            with self._state_change:\n                if self.failed:\n                    return\n                self._set_state(PartitionState.ALLOCATING)\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def finish(self):\n        \\\"\\\"\\\"Call to release the set and leave the party\\\"\\\"\\\"\n        self._release_locks()\n        self._fail_out()\n\n    def _fail_out(self):\n        with self._state_change:\n            self._set_state(PartitionState.FAILURE)\n        if self._party.participating:\n            try:\n                self._party.leave()\n            except KazooException:  # pragma: nocover\n                pass\n\n    def _allocate_transition(self, result):\n        \\\"\\\"\\\"Called when in allocating mode, and the children settled\\\"\\\"\\\"\n\n        # Did we get an exception waiting for children to settle?\n        if result.exception:  # pragma: nocover\n            self._fail_out()\n            return\n\n        children, async_result = result.get()\n        children_changed = self._client.handler.event_object()\n\n        def updated(result):\n            with self._state_change:\n                children_changed.set()\n                if self.acquired:\n                    self._set_state(PartitionState.RELEASE)\n\n        with self._state_change:\n            # We can lose connection during processing the event\n            if not self.allocating:\n                return\n\n            # Remember the state ID to check later for race conditions\n            state_id = self.state_id\n\n            # updated() will be called when children change\n            async_result.rawlink(updated)\n\n        # Check whether the state has changed during the lock acquisition\n        # and abort the process if so.\n        def abort_if_needed():\n            if self.state_id == state_id:\n                if children_changed.is_set():\n                    # The party has changed. Repartitioning...\n                    self._abort_lock_acquisition()\n                    return True\n                else:\n                    return False\n            else:\n                if self.allocating or self.acquired:\n                    # The connection was lost and user initiated a new\n                    # allocation process. Abort it to eliminate race\n                    # conditions with locks.\n                    with self._state_change:\n                        self._set_state(PartitionState.RELEASE)\n\n                return True\n\n        # Split up the set\n        partition_set = self._partition_func(\n            self._identifier, list(self._party), self._set\n        )\n\n        # Proceed to acquire locks for the working set as needed\n        for member in partition_set:\n            lock = self._client.Lock(self._lock_path + \\\"/\\\" + str(member))\n\n            while True:\n                try:\n                    # We mustn't lock without timeout because in that case we\n                    # can get a deadlock if the party state will change during\n                    # lock acquisition.\n                    lock.acquire(timeout=self._max_reaction_time)\n                except LockTimeout:\n                    if abort_if_needed():\n                        return\n                except KazooException:\n                    return self.finish()\n                else:\n                    break\n\n            self._locks.append(lock)\n\n            if abort_if_needed():\n                return\n\n        # All locks acquired. Time for state transition.\n        with self._state_change:\n            if self.state_id == state_id and not children_changed.is_set():\n                self._partition_set = partition_set\n                self._set_state(PartitionState.ACQUIRED)\n                self._acquire_event.set()\n                return\n\n        if not abort_if_needed():\n            # This mustn't happen. Means a logical error.\n            self._fail_out()\n\n    def _release_locks(self):\n        \\\"\\\"\\\"Attempt to completely remove all the locks\\\"\\\"\\\"\n        self._acquire_event.clear()\n        for lock in self._locks[:]:\n            try:\n                lock.release()\n            except KazooException:  # pragma: nocover\n                # We proceed to remove as many as possible, and leave\n                # the ones we couldn't remove\n                pass\n            else:\n                self._locks.remove(lock)\n\n    def _abort_lock_acquisition(self):\n        \\\"\\\"\\\"Called during lock acquisition if a party change occurs\\\"\\\"\\\"\n\n        self._release_locks()\n\n        if self._locks:\n            # This shouldn't happen, it means we couldn't release our\n            # locks, abort\n            self._fail_out()\n            return\n\n        self._child_watching(self._allocate_transition, client_handler=True)\n\n    def _child_watching(self, func=None, client_handler=False):\n        \\\"\\\"\\\"Called when children are being watched to stabilize\n\n        This actually returns immediately, child watcher spins up a\n        new thread/greenlet and waits for it to stabilize before\n        any callbacks might run.\n\n        :param client_handler: If True, deliver the result using the\n                               client's event handler.\n        \\\"\\\"\\\"\n        watcher = PatientChildrenWatch(\n            self._client, self._party_path, self._time_boundary\n        )\n        asy = watcher.start()\n        if func is not None:\n            # We spin up the function in a separate thread/greenlet\n            # to ensure that the rawlink's it might use won't be\n            # blocked\n            if client_handler:\n                func = partial(self._client.handler.spawn, func)\n            asy.rawlink(func)\n        return asy\n\n    def _establish_sessionwatch(self, state):\n        \\\"\\\"\\\"Register ourself to listen for session events, we shut down\n        if we become lost\\\"\\\"\\\"\n        with self._state_change:\n            if self.failed:\n                pass\n            elif state == KazooState.LOST:\n                self._client.handler.spawn(self._fail_out)\n            elif not self.release:\n                self._set_state(PartitionState.RELEASE)\n\n        return state == KazooState.LOST\n\n\n\n", "CodeBleu_score": 33.95874197059282}]}
{"instance_id": "738974d4e9c0", "generation_result": [{"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n\n", "CodeBleu_score": 62.52363801861767}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n\n", "CodeBleu_score": 62.52363801861767}, {"code": "def leave(self)\n\ttry:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = False\n\n", "CodeBleu_score": 62.52363801861767}]}
{"instance_id": "a321fef7d100", "generation_result": [{"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (!status) {\n            hdfsOverFtpServer.startServer();\n        }\n        map.put(\"is_running\", true);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 58.44383117477261}, {"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (!status) {\n            hdfsOverFtpServer.startServer();\n            status = hdfsOverFtpServer.statusServer();\n            map.put(\"is_running\", status);\n            return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n        } else {\n            map.put(\"is_running\", status);\n            return new Result(true, map, \"FTP \u670d\u52a1\u5df2\u542f\u52a8\");\n        }\n    } catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 48.80717773371032}, {"code": "public Result start(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if (status) {\n            map.put(\"is_running\", true);\n            return new Result(true, map, \"FTP \u670d\u52a1\u5df2\u542f\u52a8\");\n        }\n        hdfsOverFtpServer.startServer();\n        map.put(\"is_running\", true);\n        return new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 49.16753242168682}]}
{"instance_id": "24b088689114", "generation_result": [{"code": "public Result stop(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u72b6\u6001\u83b7\u53d6\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u72b6\u6001\u83b7\u53d6\u6210\u529f\");\n    }\n}", "CodeBleu_score": 57.216340727824864}, {"code": "public Result stop(){\n\tMap map = new HashMap<>();\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        map.put(\"is_running\", status);\n        return new Result(true, map, \"FTP \u670d\u52a1\u72b6\u6001\u83b7\u53d6\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        map.put(\"is_running\", false);\n        return new Result(true, map, \"FTP \u670d\u52a1\u72b6\u6001\u83b7\u53d6\u6210\u529f\");\n    }\n}", "CodeBleu_score": 57.175093068637686}, {"code": "public Result stop(){\n\ttry {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 97.12561037093238}]}
{"instance_id": "af0884ffda66", "generation_result": [{"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null || authorities.size() == 0) {\n\t\treturn null;\n\t}\n\n\tfor (Authority authority : authorities) {\n\t\tif (authority.canAuthorize(request)) {\n\t\t\tif (authority.authorize(request)) {\n\t\t\t\treturn request;\n\t\t\t} else {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn null;\n}", "CodeBleu_score": 33.86650565378715}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tif (authorities == null) {\n\t\treturn null;\n\t}\n\n\tfor (Authority auth : authorities) {\n\t\trequest = auth.authorize(request);\n\t\tif (request == null) {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\treturn request;\n}", "CodeBleu_score": 32.82795915580914}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tAuthorizationRequest updatedRequest = request;\n    for (Authority authority : authorities) {\n        updatedRequest = authority.authorize(updatedRequest);\n        if (updatedRequest == null) {\n            return null;\n        }\n    }\n    return updatedRequest;\n}", "CodeBleu_score": 30.88155986998857}]}
{"instance_id": "920d28a0fe76", "generation_result": [{"code": "public boolean mkdir(){\n\ttry {\n        FileSystem dfs = HdfsOverFtpSystem.getDfs();\n        return dfs.mkdirs(fullPath);\n    } catch (IOException e) {\n        e.printStackTrace();\n        return false;\n    }\n}", "CodeBleu_score": 78.14894894002083}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\treturn dfs.mkdirs(fullPath);\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 78.10397586165209}, {"code": "public boolean mkdir(){\n\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\treturn dfs.mkdirs(fullPath);\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 78.10397586165209}]}
{"instance_id": "a2b04232d387", "generation_result": [{"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}, {"code": "public boolean delete(){\n\tweed.ftp.users.HdfsUser;", "CodeBleu_score": 2.392333606644359}]}
{"instance_id": "3407087c9af6", "generation_result": [{"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}, {"code": "public boolean move(FtpFile ftpFile){\n\tThis class implements all actions to HDFS\n */", "CodeBleu_score": 12.506976807455313}]}
{"instance_id": "91227000a139", "generation_result": [{"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.6726163643569896}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.6726163643569896}, {"code": "public List<FtpFile> listFiles(){\n\t(path);\n\t\tthis.fullPath = new Path(homePath + path);\n\t\tthis.user = (HdfsUser) user;\n\t}", "CodeBleu_score": 3.6726163643569896}]}
{"instance_id": "aec4623134b5", "generation_result": [{"code": "public void configure(){\n\tif (isConfigured) {\n\t\tthrow new IllegalStateException(\"Must be called before configure()\");\n\t}\n\n\tisConfigured = true;\n\n\ttry {\n\t\tuserDataProp = new BaseProperties();\n\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t} catch (IOException e) {\n\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\"Failed to load users file: \" + userDataFile.getAbsolutePath(), e);\n\t}\n\n\t// convert any deprecated properties\n\tconvertDeprecatedProperties();\n}", "CodeBleu_score": 47.51125684152757}, {"code": "public void configure(){\n\tif (isConfigured) {\n\t\tthrow new IllegalStateException(\"Must be called before configure()\");\n\t}\n\n\tisConfigured = true;\n\n\ttry {\n\t\tif (userDataFile.exists()) {\n\t\t\tuserDataProp = new BaseProperties();\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t} else {\n\t\t\tuserDataProp = new BaseProperties();\n\t\t}\n\t} catch (IOException ex) {\n\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\"Failed to load user data file \" + userDataFile, ex);\n\t}\n\n\tconvertDeprecatedProperties();\n}", "CodeBleu_score": 44.9840368069853}, {"code": "public void configure(){\n\tisConfigured = true;\n\n\tuserDataProp = new BaseProperties();\n\ttry {\n\t\tif (userDataFile.exists()) {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t}\n\t} catch (IOException e) {\n\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\"Failed to load users file: \" + userDataFile.getAbsolutePath(),\n\t\t\t\te);\n\t}\n\n\t// convert deprecated properties\n\tconvertDeprecatedProperties();\n}", "CodeBleu_score": 45.850763490709554}]}
{"instance_id": "b5a2587f14d0", "generation_result": [{"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}, {"code": "private void saveUserData() throws FtpException{\n\textends AbstractUserManager {\n\n\tprivate final Logger LOG = LoggerFactory\n\t\t\t.getLogger(HdfsUserManager.class);\n\n\tprivate final static String DEPRECATED_PREFIX = \\\"FtpServer.user.\\\";\n\n\tprivate final static String PREFIX = \\\"ftpserver.user.\\\";\n\n\tprivate static BaseProperties userDataProp;\n\n\tprivate File userDataFile = new File(\\\"users.conf\\\");\n\n\tprivate boolean isConfigured = false;\n\n\tprivate PasswordEncryptor passwordEncryptor = new Md5PasswordEncryptor();\n\n\n\t/**\n\t * Retrieve the file used to load and store users\n\t *\n\t * @return The file\n\t */\n\tpublic File getFile() {\n\t\treturn userDataFile;\n\t}\n\n\t/**\n\t * Set the file used to store and read users. Must be set before\n\t * {@link #configure()} is called.\n\t *\n\t * @param propFile A file containing users\n\t */\n\tpublic void setFile(File propFile) {\n\t\tif (isConfigured) {\n\t\t\tthrow new IllegalStateException(\\\"Must be called before configure()\\\");\n\t\t}\n\n\t\tthis.userDataFile = propFile;\n\t}\n\n\n\t/**\n\t * Retrieve the password encryptor used for this user manager\n\t *\n\t * @return The password encryptor. Default to {@link Md5PasswordEncryptor}\n\t *         if no other has been provided\n\t */\n\tpublic PasswordEncryptor getPasswordEncryptor() {\n\t\treturn passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Set the password encryptor to use for this user manager\n\t *\n\t * @param passwordEncryptor The password encryptor\n\t */\n\tpublic void setPasswordEncryptor(PasswordEncryptor passwordEncryptor) {\n\t\tthis.passwordEncryptor = passwordEncryptor;\n\t}\n\n\n\t/**\n\t * Lazy init the user manager\n\t */\n\tprivate void lazyInit() {\n\t\tif (!isConfigured) {\n\t\t\tconfigure();\n\t\t}\n\t}\n\n\t/**\n\t * Configure user manager.\n\t */\n\tpublic void configure() {\n\t\tisConfigured = true;\n\t\ttry {\n\t\t\tuserDataProp = new BaseProperties();\n\n\t\t\tif (userDataFile != null && userDataFile.exists()) {\n\t\t\t\tFileInputStream fis = null;\n\t\t\t\ttry {\n\t\t\t\t\tfis = new FileInputStream(userDataFile);\n\t\t\t\t\tuserDataProp.load(fis);\n\t\t\t\t} finally {\n\t\t\t\t\tIoUtils.close(fis);\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\\\"Error loading user data file : \\\"\n\t\t\t\t\t\t\t+ userDataFile.getAbsolutePath(), e);\n\t\t}\n\n\t\tconvertDeprecatedPropertyNames();\n\t}\n\n\tprivate void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\\\"Failed to save updated user data\\\", e);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic synchronized void save(User usr, boolean renamePush) throws FtpException {\n\t\tlazyInit();\n\t\tuserDataProp.setProperty(PREFIX + usr.getName() + \\\".rename.push\\\", renamePush);\n\t\tsave(usr);\n\t}\n\n\t/**\n\t * Save user data. Store the properties.\n\t */\n\tpublic synchronized void save(User usr) throws FtpException {\n\t\tlazyInit();\n\n\t\t// null value check\n\t\tif (usr.getName() == null) {\n\t\t\tthrow new NullPointerException(\\\"User name is null.\\\");\n\t\t}\n\t\tString thisPrefix = PREFIX + usr.getName() + '.';\n\n\t\t// set other properties\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_PASSWORD, getPassword(usr));\n\n\t\tString home = usr.getHomeDirectory();\n\t\tif (home == null) {\n\t\t\thome = \\\"/\\\";\n\t\t}\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_HOME, home);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_ENABLE, usr.getEnabled());\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_WRITE_PERM, usr\n\t\t\t\t.authorize(new WriteRequest()) != null);\n\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_IDLE_TIME, usr\n\t\t\t\t.getMaxIdleTime());\n\n\t\tTransferRateRequest transferRateRequest = new TransferRateRequest();\n\t\ttransferRateRequest = (TransferRateRequest) usr\n\t\t\t\t.authorize(transferRateRequest);\n\n\t\tif (transferRateRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_UPLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxUploadRate());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_DOWNLOAD_RATE,\n\t\t\t\t\ttransferRateRequest.getMaxDownloadRate());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_UPLOAD_RATE);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_DOWNLOAD_RATE);\n\t\t}\n\n\t\t// request that always will succeed\n\t\tConcurrentLoginRequest concurrentLoginRequest = new ConcurrentLoginRequest(\n\t\t\t\t0, 0);\n\t\tconcurrentLoginRequest = (ConcurrentLoginRequest) usr\n\t\t\t\t.authorize(concurrentLoginRequest);\n\n\t\tif (concurrentLoginRequest != null) {\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_NUMBER,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLogins());\n\t\t\tuserDataProp.setProperty(thisPrefix + ATTR_MAX_LOGIN_PER_IP,\n\t\t\t\t\tconcurrentLoginRequest.getMaxConcurrentLoginsPerIP());\n\t\t} else {\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_NUMBER);\n\t\t\tuserDataProp.remove(thisPrefix + ATTR_MAX_LOGIN_PER_IP);\n\t\t}\n\n\t\tsaveUserData();\n\t}\n\n\t/**\n\t * @throws FtpException\n\t */\n\n\n", "CodeBleu_score": 14.996514871673686}]}
{"instance_id": "5cf2989cf513", "generation_result": [{"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 84.58771483281748}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    } catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 82.33147782493528}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}", "CodeBleu_score": 84.58771483281748}]}
{"instance_id": "ce383a9a6497", "generation_result": [{"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.debug(\"Opening file at path: {} with buffer size: {}\", path, bufferSize);\n    Path qualifiedPath = qualify(path);\n    return new FSDataInputStream(new BufferedInputStream(seaweedFileSystemStore.open(qualifiedPath), bufferSize));\n}", "CodeBleu_score": 20.259822260432344}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.info(\"open path={} bufferSize={}\", path, bufferSize);\n    Path qualifiedPath = qualify(path);\n    return new FSDataInputStream(new BufferedInputStream(seaweedFileSystemStore.open(qualifiedPath), bufferSize));\n}", "CodeBleu_score": 19.34004781078102}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tLOG.debug(\"opening file: {}, bufferSize: {}\", path, bufferSize);\n    Path qualifiedPath = qualify(path);\n    return new FSDataInputStream(new BufferedInputStream(seaweedFileSystemStore.open(qualifiedPath), bufferSize));\n}", "CodeBleu_score": 19.710912730262002}]}
{"instance_id": "f0fc12072c02", "generation_result": [{"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tnimport org.apache.hadoop.fs.permission.AclEntry;", "CodeBleu_score": 3.9965975893477443}]}
{"instance_id": "5104c071af46", "generation_result": [{"code": "public boolean hasCapability(String capability){\n\tif (capability == null) {\n        return false;\n    }\n    String lowerCaseCapability = capability.toLowerCase(Locale.ROOT);\n    return (HSYNC.equals(lowerCaseCapability) || HFLUSH.equals(lowerCaseCapability)) && supportFlush;\n}", "CodeBleu_score": 38.50214320008761}, {"code": "public boolean hasCapability(String capability){\n\tif (capability.toLowerCase(Locale.ROOT).equals(HSYNC)) {\n        return supportFlush;\n    }\n    if (capability.toLowerCase(Locale.ROOT).equals(HFLUSH)) {\n        return supportFlush;\n    }\n    return false;\n}", "CodeBleu_score": 53.41352581761718}, {"code": "public boolean hasCapability(String capability){\n\tif (capability != null) {\n        String lowerCaseCapability = capability.toLowerCase(Locale.ROOT);\n        if (HSYNC.equals(lowerCaseCapability) || HFLUSH.equals(lowerCaseCapability)) {\n            return supportFlush;\n        }\n    }\n    return false;\n}", "CodeBleu_score": 41.00972225167774}]}
{"instance_id": "6989c860c79b", "generation_result": [{"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (closed) {\n        return;\n    }\n    if (offset <= lastFlushOffset) {\n        return;\n    }\n    if (lastError != null) {\n        throw lastError;\n    }\n    if (offset > position) {\n        throw new IOException(\"Flush offset \" + offset + \" is larger than current position \" + position);\n    }\n    if (offset < lastFlushOffset) {\n        throw new IOException(\"Flush offset \" + offset + \" is smaller than last flush offset \" + lastFlushOffset);\n    }\n    if (offset > lastTotalAppendOffset) {\n        throw new IOException(\"Flush offset \" + offset + \" is larger than last total append offset \" + lastTotalAppendOffset);\n    }\n    // write metadata\n    if (shouldSaveMetadata) {\n        try {\n            filerClient.updateEntry(path, entry);\n        } catch (IOException e) {\n            lastError = e;\n            throw e;\n        }\n    }\n    lastFlushOffset = offset;\n}", "CodeBleu_score": 32.64152153438111}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (lastError != null) {\n        throw lastError;\n    }\n\n    if (offset <= lastFlushOffset) {\n        return;\n    }\n\n    long bytesToFlush = position - offset;\n    if (bytesToFlush <= 0) {\n        return;\n    }\n\n    if (bytesToFlush > Integer.MAX_VALUE) {\n        throw new IllegalArgumentException(\"Cannot flush more than \" + Integer.MAX_VALUE + \" bytes\");\n    }\n\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Flush {} bytes to service from offset {}\", bytesToFlush, offset);\n    }\n\n    if (supportFlush) {\n        final long startTime = System.nanoTime();\n        try {\n            if (shouldSaveMetadata) {\n                entry.setAttributes(FilerProto.FuseAttributes.newBuilder(entry.getAttributes())\n                        .setMtime(System.currentTimeMillis() / 1000L)\n                        .build());\n            }\n            final long fileSize = position;\n            final String fileName = getFileName(path);\n            final String parentDirectory = getParentDirectory(path);\n            final String fullPath = path;\n            final String fileId = filerClient.createFile(parentDirectory, fileName, entry.build());\n            final long offsetToFlush = offset;\n            final long lengthToFlush = bytesToFlush;\n            final String fileUrl = filerClient.getUrl() + \"/\" + fileId;\n            final String replication = this.replication;\n            final String collection = this.collection;\n\n            // flush data\n            final Callable<Void> callable = new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n                    try {\n                        if (LOG.isDebugEnabled()) {\n                            LOG.debug(\"Flush {} bytes to file {}\", lengthToFlush, fileUrl);\n                        }\n                        filerClient.write(fileUrl, offsetToFlush, lengthToFlush, new ByteBuffer[]{buffer}, replication, collection);\n                    } catch (IOException e) {\n                        LOG.error(\"Fl", "CodeBleu_score": 33.277263331362576}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tif (closed) {\n        return;\n    }\n    if (offset <= lastFlushOffset) {\n        return;\n    }\n    if (lastError != null) {\n        throw lastError;\n    }\n    try {\n        final FilerProto.Entry entry = this.entry.build();\n        if (shouldSaveMetadata) {\n            filerClient.updateEntry(entry);\n        }\n        lastFlushOffset = offset;\n    } catch (IOException e) {\n        lastError = e;\n        throw e;\n    }\n}", "CodeBleu_score": 37.55705365912113}]}
{"instance_id": "56e8fb9de5da", "generation_result": [{"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 13.410861361010628}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 13.410861361010628}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\ttrue;\n    private final FilerClient filerClient;\n    private final String path;\n    private final int bufferSize;\n    private final int maxConcurrentRequestCount;\n    private final ThreadPoolExecutor threadExecutor;\n    private final ExecutorCompletionService<Void> completionService;\n    private final ConcurrentLinkedDeque<WriteOperation> writeOperations;\n    private final boolean shouldSaveMetadata = false;\n    private FilerProto.Entry.Builder entry;\n    private long position;\n    private boolean closed;\n    private volatile IOException lastError;\n    private long lastFlushOffset;\n    private long lastTotalAppendOffset = 0;\n    private ByteBuffer buffer;\n    private long outputIndex;\n    private String replication = \\\"\\\";\n    private String collection = \\\"\\\";\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath) {\n        this(filerClient, fullpath, \\\"\\\");\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String fullpath, final String replication) {\n        this(filerClient, fullpath, null, 0, 8 * 1024 * 1024, replication);\n    }\n\n    public SeaweedOutputStream(FilerClient filerClient, final String path, FilerProto.Entry.Builder entry,\n                               final long position, final int bufferSize, final String replication) {\n        this.filerClient = filerClient;\n        this.replication = replication;\n        this.path = path;\n        this.position = position;\n        this.closed = false;\n        this.lastError = null;\n        this.lastFlushOffset = 0;\n        this.bufferSize = bufferSize;\n        this.buffer = ByteBufferPool.request(bufferSize);\n        this.writeOperations = new ConcurrentLinkedDeque<>();\n\n        this.maxConcurrentRequestCount = Runtime.getRuntime().availableProcessors();\n\n        this.threadExecutor\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }\n\n    public void setReplication(String replication) {\n        this.replication = replication;\n    }\n    public void setCollection(String collection) {\n        this.collection = collection;\n    }\n\n    public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\\\"://\\\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\\\"/\\\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\\\"/\\\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        if (lastSlashIndex == 0) {\n            return \\\"/\\\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }\n\n    public static String getFileName(String path) {\n        if (path.indexOf(\\\"/\\\") < 0) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\\\"/\\\");\n        return path.substring(lastSlashIndex + 1);\n    }\n\n    private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n        try {\n            SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n        } catch (Exception ex) {\n            throw new IOException(ex);\n        }\n        this.lastFlushOffset = offset;\n    }\n\n    @Override\n    public void write(final int byteVal) throws IOException {\n        write(new byte[]{(byte) (byteVal & 0xFF)});\n    }\n\n    @Override\n    public synchronized void write(final byte[] data, final int off, final int length)\n            throws IOException {\n        maybeThrowLastError();\n\n        if (data == null) {\n            return;\n        }\n\n        if (off < 0 || length < 0 || length > data.length - off) {\n            throw new IndexOutOfBoundsException();\n        }\n\n        // System.out.println(path + \\\" write [\\\" + (outputIndex + off) + \\\",\\\" + ((outputIndex + off) + length) + \\\")\\\");\n\n        int currentOffset = off;\n        int writableBytes = bufferSize - buffer.position();\n        int numberOfBytesToWrite = length;\n\n        while (numberOfBytesToWrite > 0) {\n\n            if (numberOfBytesToWrite < writableBytes) {\n                buffer.put(data, currentOffset, numberOfBytesToWrite);\n                break;\n            }\n\n            // System.out.println(path + \\\"     [\\\" + (outputIndex + currentOffset) + \\\",\\\" + ((outputIndex + currentOffset) + writableBytes) + \\\") \\\" + buffer.capacity());\n            buffer.put(data, currentOffset, writableBytes);\n            currentOffset += writableBytes;\n            writeCurrentBufferToService();\n            numberOfBytesToWrite = numberOfBytesToWrite - writableBytes;\n            writableBytes = bufferSize - buffer.position();\n        }\n\n    }\n\n    /**\n     * Flushes this output stream and forces any buffered output bytes to be\n     * written out. If any data remains in the payload it is committed to the\n     * service. Data is queued for writing and forced out to the service\n     * before the call returns.\n     */\n    @Override\n    public void flush() throws IOException {\n        if (supportFlush) {\n            flushInternalAsync();\n        }\n    }\n\n    /**\n     * Force all data in the output stream to be written to Azure storage.\n     * Wait to return until this is complete. Close the access to the stream and\n     * shutdown the upload thread pool.\n     * If the blob was created, its lease will be released.\n     * Any error encountered caught in threads and stored will be rethrown here\n     * after cleanup.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        if (closed) {\n            return;\n        }\n\n        LOG.debug(\\\"close path: {}\\\", path);\n        try {\n            flushInternal();\n            threadExecutor.shutdown();\n        } finally {\n            lastError = new IOException(\\\"Stream is closed!\\\");\n            ByteBufferPool.release(buffer);\n            buffer = null;\n            closed = true;\n            writeOperations.clear();\n            if (!threadExecutor.isShutdown()) {\n                threadExecutor.shutdownNow();\n            }\n        }\n\n    }\n\n    private synchronized void writeCurrentBufferToService() throws IOException {\n        if (buffer.position() == 0) {\n            return;\n        }\n\n        position += submitWriteBufferToService(buffer, position);\n\n        buffer = ByteBufferPool.request(bufferSize);\n\n    }\n\n\n\n", "CodeBleu_score": 13.410861361010628}]}
{"instance_id": "bf27d955c38b", "generation_result": [{"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}, {"code": "private void waitForTaskToComplete() throws IOException{\n\t);\n    }", "CodeBleu_score": 2.1064656244047315}]}
{"instance_id": "5c7a311273ae", "generation_result": [{"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 23.091131677226063}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 23.091131677226063}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\n                = new ThreadPoolExecutor(maxConcurrentRequestCount,\n                maxConcurrentRequestCount,\n                120L,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>());\n        this.completionService = new ExecutorCompletionService<>(this.threadExecutor);\n\n        this.entry = entry;\n        if (this.entry == null) {\n            long now = System.currentTimeMillis() / 1000L;\n\n            this.entry = FilerProto.Entry.newBuilder()\n                    .setName(getFileName(path))\n                    .setIsDirectory(false)\n                    .setAttributes(FilerProto.FuseAttributes.newBuilder()\n                            .setFileMode(0755)\n                            .setCrtime(now)\n                            .setMtime(now)\n                            .clearGroupName()\n                    );\n        }\n\n    }", "CodeBleu_score": 23.091131677226063}]}
{"instance_id": "d8f07e66f690", "generation_result": [{"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"User cannot be null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"User home directory cannot be null\");\n\t}\n\tthis.user = user;\n\thomePath = user.getHomeDirectory().getAbsolutePath();\n}", "CodeBleu_score": 66.41415440630085}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"User must not be null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"User home directory must not be null\");\n\t}\n\tthis.user = user;\n\tthis.homePath = user.getHomeDirectory().getAbsolutePath();\n}", "CodeBleu_score": 67.41687128192588}, {"code": "protected HdfsFileSystemView(User user){\n\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"User object cannot be null\");\n\t}\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"User home directory cannot be null\");\n\t}\n\tthis.user = user;\n\tthis.homePath = user.getHomeDirectory();\n}", "CodeBleu_score": 67.91641618898221}]}
{"instance_id": "27807db0eb0f", "generation_result": [{"code": "public boolean changeWorkingDirectory(String dir){\n\tif (dir.equals(\"..\")) {\n\t\treturn false;\n\t}\n\tif (dir.equals(\".\")) {\n\t\treturn true;\n\t}\n\tPath path = new Path(dir);\n\tif (!path.isAbsolute()) {\n\t\tpath = new Path(currPath, dir);\n\t}\n\tif (path.isAbsolute()) {\n\t\tif (path.toUri().getPath().equals(File.separator)) {\n\t\t\tcurrPath = File.separator;\n\t\t} else {\n\t\t\tcurrPath = path.toUri().getPath();\n\t\t}\n\t} else {\n\t\tcurrPath = path.toUri().getPath();\n\t}\n\treturn true;\n}", "CodeBleu_score": 35.27062325672937}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tif (dir.equals(File.separator)) {\n\t\tcurrPath = File.separator;\n\t\treturn true;\n\t}\n\tif (dir.startsWith(File.separator)) {\n\t\tcurrPath = dir;\n\t} else {\n\t\tcurrPath = currPath + dir;\n\t}\n\tif (currPath.endsWith(File.separator)) {\n\t\tcurrPath = currPath.substring(0, currPath.length() - 1);\n\t}\n\tif (currPath.startsWith(File.separator)) {\n\t\tcurrPath = currPath.substring(1);\n\t}\n\tif (currPath.equals(File.separator)) {\n\t\treturn true;\n\t}\n\tString[] dirs = currPath.split(File.separator);\n\tStringBuilder sb = new StringBuilder();\n\tfor (int i = 0; i < dirs.length; i++) {\n\t\tif (dirs[i].equals(\"..\")) {\n\t\t\tif (sb.length() == 0) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tsb.delete(sb.lastIndexOf(File.separator), sb.length());\n\t\t} else if (!dirs[i].equals(\".\")) {\n\t\t\tsb.append(File.separator).append(dirs[i]);\n\t\t}\n\t}\n\tif (sb.length() == 0) {\n\t\tsb.append(File.separator);\n\t}\n\tcurrPath = sb.toString();\n\treturn true;\n}", "CodeBleu_score": 34.79378184356586}, {"code": "public boolean changeWorkingDirectory(String dir){\n\tboolean ret = false;\n\tif (dir.equals(\".\")) {\n\t\treturn true;\n\t}\n\tif (dir.equals(\"..\")) {\n\t\treturn false;\n\t}\n\tif (dir.startsWith(File.separator)) {\n\t\tcurrPath = dir;\n\t} else {\n\t\tcurrPath = currPath + File.separator + dir;\n\t}\n\tif (currPath.equals(File.separator)) {\n\t\tret = true;\n\t} else {\n\t\tPath path = new Path(currPath);\n\t\ttry {\n\t\t\tif (fs.exists(path)) {\n\t\t\t\tif (fs.isDirectory(path)) {\n\t\t\t\t\tret = true;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\treturn ret;\n}", "CodeBleu_score": 34.52301258152249}]}
{"instance_id": "be7aba23d184", "generation_result": [{"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    try {\n        this.filerClient = new FilerClient(host, port, grpcPort, conf.get(ACCESS_MODE_KEY, \"direct\"));\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 34.9300095517294}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    this.filerClient = new FilerClient(host, port, grpcPort, conf.get(ACCESS_MODE, \"direct\"));\n}", "CodeBleu_score": 28.921315408635383}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\n    this.filerClient = new FilerClient(host, port, grpcPort, conf.get(ACCESS_MODE, \"direct\"));\n}", "CodeBleu_score": 28.921315408635383}]}
{"instance_id": "7943a1723c23", "generation_result": [{"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\\\")) {\n            filerClient.setAccessVolumeServerByFilerProxy();\n        }\n\n    }", "CodeBleu_score": 14.599485549777999}]}
{"instance_id": "36634cee9ed3", "generation_result": [{"code": "public void configure(JobConf conf){\n\tsetConf(conf);\n  try {\n    hostName = InetAddress.getLocalHost().getHostName();\n  } catch (Exception e) {\n    hostName = \"localhost\";\n  }\n  threadsPerMap = conf.getInt(\"threads.per.map\", 1);\n  filesPerThread = conf.getInt(\"files.per.thread\", 1);\n  pool = Executors.newFixedThreadPool(threadsPerMap, new ThreadFactory() {\n    public Thread newThread(Runnable r) {\n      Thread t = Executors.defaultThreadFactory().newThread(r);\n      t.setDaemon(true);\n      return t;\n    }\n  });\n}", "CodeBleu_score": 70.44757717923292}, {"code": "public void configure(JobConf conf){\n\tsetConf(conf);\n  try {\n    hostName = InetAddress.getLocalHost().getHostName();\n  } catch (Exception e) {\n    hostName = \"localhost\";\n  }\n  threadsPerMap = conf.getInt(\"mapred.tasktracker.map.tasks.maximum\", 2);\n  filesPerThread = conf.getInt(\"juicefs.bench.files.per.thread\", 100);\n  pool = Executors.newFixedThreadPool(threadsPerMap);\n  for (int i = 0; i < threadsPerMap; i++) {\n    pool.submit(new Worker());\n  }\n}", "CodeBleu_score": 58.19037449177262}, {"code": "public void configure(JobConf conf){\n\tsetConf(conf);\n  try {\n    hostName = InetAddress.getLocalHost().getHostName();\n  } catch (Exception e) {\n    hostName = \"localhost\";\n  }\n  threadsPerMap = conf.getInt(\"threadsPerMap\", 1);\n  filesPerThread = conf.getInt(\"filesPerThread\", 1);\n  pool = Executors.newFixedThreadPool(threadsPerMap);\n  for (int i = 0; i < threadsPerMap; i++) {\n    pool.submit(new Runnable() {\n      @Override\n      public void run() {\n        try {\n          openStream();\n        } catch (IOException e) {\n          e.printStackTrace();\n        }\n      }\n    });\n  }\n}", "CodeBleu_score": 55.68393151680435}]}
{"instance_id": "53cd9065153d", "generation_result": [{"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n  final String scheme = uri.getScheme();\n  final String user = UserGroupInformation.getCurrentUser().getShortUserName();\n  final String trashEmptierName = \"Trash emptier for \" + scheme + \":\" + host + \" for \" + user;\n  final String trashEmptierThreadName = \"Trash emptier thread for \" + scheme + \":\" + host + \" for \" + user;\n  final String trashEmptierSuperUser = \"hdfs.\" + scheme + \".\" + host + \".trash.emptier.\" + user;\n  final String trashEmptierSuperGroup = \"supergroup\";\n  final String trashEmptierSuperUserKeytab = getConf(conf, \"trash.emptier.superuser.keytab\", null);\n  final String trashEmptierSuperUserPrincipal = getConf(conf, \"trash.emptier.superuser.principal\", null);\n  final String trashCheckpointDir = getConf(conf, \"trash.checkpoint.dir\", null);\n  final String trashCheckpointPrefix = getConf(conf, \"trash.checkpoint.prefix\", null);\n  final String trashCheckpointInterval = getConf(conf, \"trash.checkpoint.interval\", \"10m\");\n  final String trashCheckpointAge = getConf(conf, \"trash.checkpoint.age\", \"7d\");\n  final String trashEmptierInterval = getConf(conf, \"trash.emptier.interval\", \"10m\");\n  final String trashEmptierAge = getConf(conf, \"trash.emptier.age\", \"7d\");\n  final String trashEmptierDeleteImmediately = getConf(conf, \"trash.emptier.delete.immediately\", \"false\");\n  final String trashEmptierInodeDeleteImmediately = getConf(conf, \"trash.emptier.inode.delete.immediately\", \"false\");\n  final String trashEmptierInodeDeleteBatchSize = getConf(conf, \"trash.emptier.inode.delete.batch.size\", \"1000\");", "CodeBleu_score": 26.853335062037843}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tString host = uri.getHost();\n    String key = \"fs.trash.interval\";\n    int interval = conf.getInt(key, 60) * 1000;\n    if (interval <= 0) {\n        return;\n    }\n    final String trashRoot = getConf(conf, \"fs.trash.checkpoint.dir\", \"/.Trash\");\n    final String user = getConf(conf, \"fs.trash.checkpoint.user\", \"root\");\n    final String group = getConf(conf, \"fs.trash.checkpoint.group\", \"root\");\n    final String supergroup = getConf(conf, \"fs.trash.checkpoint.supergroup\", \"supergroup\");\n    final String superuser = getConf(conf, \"fs.trash.checkpoint.superuser\", \"hdfs\");\n    final String supergroupKeytab = getConf(conf, \"fs.trash.checkpoint.supergroup.keytab\", \"/etc/security/keytabs/hdfs.headless.keytab\");\n    final String superuserKeytab = getConf(conf, \"fs.trash.checkpoint.superuser.keytab\", \"/etc/security/keytabs/hdfs.headless.keytab\");\n    final String superuserPrincipal = getConf(conf, \"fs.trash.checkpoint.superuser.principal\", \"hdfs-trash@EXAMPLE.COM\");\n    final String supergroupPrincipal = getConf(conf, \"fs.trash.checkpoint.supergroup.principal\", \"hdfs-trash@EXAMPLE.COM\");\n    final String trashCheckpointUri = getConf(conf, \"fs.trash.checkpoint.uri\", \"jfs://\" + host + \"/.Trash\");\n    final String trashCheckpointRoot = getConf(conf, \"fs.trash.checkpoint.root\", \"/.Trash\");\n    final String trashCheckpointUser = getConf(conf, \"fs.trash.checkpoint.user\", \"root\");\n    final String trashCheckpointGroup = getConf(conf, \"fs.trash.checkpoint.group\", \"root\");\n    final String trashCheckpointSupergroup =", "CodeBleu_score": 28.663170144389504}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tfinal String host = uri.getHost();\n  final String superUser = \"superuser\";\n  final String trashConf = \"fs.trash.interval\";\n  final String trashInterval = \"10\";\n  final String trashCheckpoint = \"fs.trash.checkpoint.interval\";\n  final String trashCheckpointInterval = \"300\";\n  final String trashPath = \"fs.trash.checkpoint.dir\";\n  final String trashPathValue = \"/.TrashCheckpoints\";\n  final String trashPathCheckpoint = \"fs.trash.checkpoint.path\";\n  final String trashPathCheckpointValue = \"/.TrashCheckpoints\";\n  final String trashPathCheckpointTime = \"fs.trash.checkpoint.path.time\";\n  final String trashPathCheckpointTimeValue = \"10\";\n  final String trashPathCheckpointTimeUnit = \"fs.trash.checkpoint.path.time.unit\";\n  final String trashPathCheckpointTimeUnitValue = \"MINUTES\";\n  final String trashPathCheckpointTimeUnitValue2 = \"SECONDS\";\n  final String trashPathCheckpointTimeUnitValue3 = \"HOURS\";\n  final String trashPathCheckpointTimeUnitValue4 = \"DAYS\";\n  final String trashPathCheckpointTimeUnitValue5 = \"WEEKS\";\n  final String trashPathCheckpointTimeUnitValue6 = \"MONTHS\";\n  final String trashPathCheckpointTimeUnitValue7 = \"YEARS\";\n  final String trashPathCheckpointTimeUnitValue8 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue9 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue10 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue11 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue12 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue13 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue14 = \"FOREVER\";\n  final String trashPathCheckpointTimeUnitValue15 = \"FORE", "CodeBleu_score": 18.468141643383333}]}
{"instance_id": "ede797f6a8f2", "generation_result": [{"code": "public static void setContext(String context) throws Exception{\n\tCallerContext callerContext = CallerContext.getCurrent();\n    if (callerContext != null && callerContext.isValid()) {\n        String currentContext = callerContext.getContext();\n        if (currentContext.isEmpty()) {\n            callerContext.setContext(context);\n        } else if (!currentContext.contains(context)) {\n            callerContext.setContext(currentContext + \" \" + context);\n        }\n    } else {\n        callerContext = new CallerContext.Builder(context).build();\n    }\n    CallerContext.setCurrent(callerContext);\n}", "CodeBleu_score": 42.20590723480425}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext currentContext = CallerContext.getCurrent();\n  if (currentContext == null || currentContext.isInvalid()) {\n    CallerContext newContext = CallerContext.create(context, null);\n    newContext.setCurrent();\n  } else if (!currentContext.isSigned() && !currentContext.getContext().contains(context)) {\n    currentContext.append(context);\n    currentContext.setCurrent();\n  }\n}", "CodeBleu_score": 40.23079235481987}, {"code": "public static void setContext(String context) throws Exception{\n\tCallerContext ctx = CallerContext.getCurrent();\n    if (ctx == null || !ctx.isValid()) {\n        ctx = new CallerContext.Builder(context).build();\n    } else if (!ctx.isSigned()) {\n        if (!ctx.getContext().contains(context)) {\n            ctx.append(context);\n        }\n    }\n    CallerContext.setCurrent(ctx);\n}", "CodeBleu_score": 37.91635738342688}]}
{"instance_id": "56968a526f6a", "generation_result": [{"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t, just return and fail lazily on connection\n  if (!libhdfs_->status().ok()) {\n    return Status::Ok();\n  }\n  struct hdfsBuilder* builder = libhdfs_->hdfsNewBuilder();\n  if (builder == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\n        \\\"Failed to connect to hdfs, could not create connection builder\\\"));\n  }\n  libhdfs_->hdfsBuilderSetForceNewInstance(builder);\n  if (name_node_uri.empty())\n    name_node_uri = \\\"default\\\";\n  libhdfs_->hdfsBuilderSetNameNode(builder, name_node_uri.c_str());\n  if (!username.empty())\n    libhdfs_->hdfsBuilderSetUserName(builder, username.c_str());\n  if (!kerb_ticket_cache_path.empty()) {\n    libhdfs_->hdfsBuilderSetKerbTicketCachePath(\n        builder, kerb_ticket_cache_path.c_str());\n  }\n  // TODO: Set config strings\n  hdfs_ = libhdfs_->hdfsBuilderConnect(builder);\n  if (hdfs_ == nullptr) {\n    // TODO: errno for better options\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\\\"Failed to connect to HDFS namenode: \\\") + name_node_uri));\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 40.835479941902136}]}
{"instance_id": "f33adb3e8cb6", "generation_result": [{"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}, {"code": "Status HDFS::sync(const URI& uri){\n\t_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}", "CodeBleu_score": 33.33771729381209}]}
{"instance_id": "893c46ab9675", "generation_result": [{"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t(\n        std::string(\\\"Cannot read from from '\\\") + uri.to_string() +\n        \\\"'; offset > typemax(tOffset)\\\"));\n  }", "CodeBleu_score": 3.570883686488324}]}
{"instance_id": "3da0b4d8ae16", "generation_result": [{"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tif (StringUtils.isBlank(hbaseCfgString)) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE, \"Hbase config is empty.\");\n    }\n    try {\n        Configuration hbaseConfig = Configuration.from(hbaseCfgString);\n        String quorum = hbaseConfig.getString(HConstants.ZOOKEEPER_QUORUM);\n        if (StringUtils.isBlank(quorum)) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.REQUIRED_VALUE,\n                    \"The Hbase config item \" + HConstants.ZOOKEEPER_QUORUM + \" is empty.\");\n        }\n        String[] quorumArr = quorum.split(\",\");\n        for (int i = 0; i < quorumArr.length; i++) {\n            String host = quorumArr[i];\n            if (!host.contains(\":\")) {\n                quorumArr[i] = host + \":2181\";\n            }\n        }\n        quorum = StringUtils.join(quorumArr, \",\");\n        String znodeParent = hbaseConfig.getString(HConstants.ZOOKEEPER_ZNODE_PARENT,\n                HBaseConstant.DEFAULT_ZNODE_PARENT);\n        return new Pair<>(quorum, znodeParent);\n    }\n    catch (Exception e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The Hbase config item is not a valid json string.\", e);\n    }\n}", "CodeBleu_score": 38.55997012312382}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tConfiguration hbaseConfig = Configuration.from(hbaseCfgString);\n    String zkQuorum = hbaseConfig.getString(HConstants.ZOOKEEPER_QUORUM);\n    String zkPort = hbaseConfig.getString(HConstants.ZOOKEEPER_CLIENT_PORT);\n    String znodeParent = hbaseConfig.getString(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (StringUtils.isBlank(zkPort)) {\n        zkPort = \"2181\";\n    }\n    if (StringUtils.isBlank(znodeParent)) {\n        znodeParent = HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT;\n    }\n    return new Pair<>(zkQuorum + \":\" + zkPort, znodeParent);\n}", "CodeBleu_score": 42.09331354531901}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\tConfiguration hbaseConfig = Configuration.from(hbaseCfgString);\n    String quorum = hbaseConfig.get(HConstants.ZOOKEEPER_QUORUM);\n    String znodeParent = hbaseConfig.get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (quorum == null) {\n        throw AddaxException.asAddaxException(Hbase11xSQLWriterErrorCode.REQUIRED_VALUE,\n                \"The configuration item '\" + HConstants.ZOOKEEPER_QUORUM + \"' is required for HBase connection.\");\n    }\n    if (!quorum.contains(\":\")) {\n        quorum += \":2181\";\n    }\n    if (znodeParent == null) {\n        znodeParent = HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT;\n    }\n    return new Pair<>(quorum, znodeParent);\n}", "CodeBleu_score": 40.785253203189285}]}
{"instance_id": "860cae2bb7f4", "generation_result": [{"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\tString tableName = cfg.getTable();\n    String namespace = cfg.getNamespace();\n    String zkQuorum = cfg.getZkQuorum();\n    String znodeParent = cfg.getZnodeParent();\n    String hbaseConfig = cfg.getHbaseConfig();\n    String nullMode = cfg.getNullMode();\n    String encoding = cfg.getEncoding();\n    String batchSize = cfg.getBatchSize();\n    String writeBufferSize = cfg.getWriteBufferSize();\n    String autoFlush = cfg.getAutoFlush();\n    String rowkeyColumn = cfg.getRowkeyColumn();\n    String versionColumn = cfg.getVersionColumn();\n    String deleteRowkeyColumn = cfg.getDeleteRowkeyColumn();\n    String deleteRowkeyColumnType = cfg.getDeleteRowkeyColumnType();\n    String deleteMode = cfg.getDeleteMode();\n    String deleteColumn = cfg.getDeleteColumn();\n    String deleteColumnType = cfg.getDeleteColumnType();\n    String deleteVersionColumn = cfg.getDeleteVersionColumn();\n    String deleteVersionColumnType = cfg.getDeleteVersionColumnType();\n    String writeBufferPeriodicFlushTimeoutMillis = cfg.getWriteBufferPeriodicFlushTimeoutMillis();\n    String writeBufferFlushMaxActions = cfg.getWriteBufferFlushMaxActions();\n    String writeBufferFlushMaxSize = cfg.getWriteBufferFlushMaxSize();\n    String writeBufferFlushIntervalMs = cfg.getWriteBufferFlushIntervalMs();\n    String maxRetryTimes = cfg.getMaxRetryTimes();\n    String retryInterval = cfg.getRetryInterval();\n    String retryIntervalUpperLimit = cfg.getRetryIntervalUpperLimit();\n    String retryMaxInterval = cfg.getRetryMaxInterval();\n    String retryMaxElapsedTime = cfg.getRetryMaxElapsedTime();\n    String retryReset = cfg.getRetryReset();\n    String retrySleepIncrementMillis = cfg.getRetrySleepIncrementMillis();\n    String retrySleepMaxMillis = cfg.getRetryS", "CodeBleu_score": 22.355421242270467}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\t// 1. \u68c0\u67e5\u914d\u7f6e\u7684\u5fc5\u586b\u9879\u662f\u5426\u90fd\u914d\u7f6e\u4e86\n    validateMandatoryConfig(cfg);\n\n    // 2. \u68c0\u67e5\u914d\u7f6e\u7684\u5217\u662f\u5426\u90fd\u5b58\u5728\n    validateColumns(cfg);\n\n    // 3. \u68c0\u67e5\u914d\u7f6e\u7684\u5217\u7684\u7c7b\u578b\u662f\u5426\u652f\u6301\n    validateColumnType(cfg);\n\n    // 4. \u68c0\u67e5\u914d\u7f6e\u7684\u5217\u7684\u7c7b\u578b\u662f\u5426\u652f\u6301\n    validateColumnType(cfg);\n\n    // 5. \u68c0\u67e5\u914d\u7f6e\u7684\u5217\u7684\u7c7b\u578b\u662f\u5426\u652f\u6301\n    validateColumnType(cfg);\n}", "CodeBleu_score": 4.090311769710137}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\t// \u68c0\u67e5\u914d\u7f6e\u9879\u662f\u5426\u4e3a\u7a7a\n    checkConfig(cfg);\n\n    // \u68c0\u67e5kerberos\u914d\u7f6e\n    checkKerberosConfig(cfg);\n\n    // \u83b7\u53d6hbase\u914d\u7f6e\n    Map<String, String> hbaseConfig = getHbaseConfig(cfg.getHbaseConfig());\n\n    // \u83b7\u53d6hbase\u7684zk quorum\u548cznode\n    Pair<String, String> zkQuorumAndZnode = getHbaseConfig(cfg.getHbaseConfig());\n\n    // \u83b7\u53d6hbase\u7684thin connect\u914d\u7f6e\n    Map<String, String> thinConnectConfig = getThinConnectConfig(cfg.getHbaseConfig());\n\n    // \u83b7\u53d6hbase\u7684namespace\u548ctable\n    Pair<String, String> namespaceAndTable = getNamespaceAndTable(cfg.getTable());\n\n    // \u83b7\u53d6hbase\u7684zk quorum\u548cznode\n    String zkQuorum = zkQuorumAndZnode.getFirst();\n    String znode = zkQuorumAndZnode.getSecond();\n\n    // \u83b7\u53d6hbase\u7684namespace\u548ctable\n    String namespace = namespaceAndTable.getFirst();\n    String table = namespaceAndTable.getSecond();\n\n    // \u83b7\u53d6hbase\u7684client\n    Admin admin = getHbaseAdmin(hbaseConfig, zkQuorum, znode);\n\n    // \u68c0\u67e5hbase\u7684table\u662f\u5426\u5b58\u5728\n    checkTableExists(admin, namespace, table);\n\n    // \u83b7\u53d6hbase\u7684table schema\n    Map<String, String> columnFamilyMap = getTableSchema(admin, namespace, table);\n\n    // \u68c0\u67e5hbase\u7684column\u662f\u5426\u5b58\u5728\n    checkColumnExists(columnFamilyMap, cfg.getColumn());\n\n    // \u5173\u95edhbase\u7684client\n    closeHbaseAdmin(admin);\n}", "CodeBleu_score": 32.141674713844175}]}
{"instance_id": "61721c327cf6", "generation_result": [{"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tnimport org.apache.phoenix.schema.types.PDataType;", "CodeBleu_score": 4.524379522802614}]}
{"instance_id": "9ceb7f0bb717", "generation_result": [{"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n        PTable schema;\n        try {\n            schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n        }\n        catch (SQLException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to get the metadata of table \\\" + cfg.getTableName(), e);\n        }\n        List<String> columnNames = cfg.getColumns();\n        try {\n\n            for (String colName : columnNames) {\n                schema.getColumnForColumnName(colName);\n            }\n        }\n        catch (ColumnNotFoundException e) {\n            // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column '\\\" + e.getColumnName() + \\\"' your configured does not exists in the target table \\\"  + cfg.getTableName(), e);\n        }\n        catch (SQLException e) {\n            // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                    \\\"The column validation of target table \\\" + cfg.getTableName() + \\\"has got failure\\\", e);\n        }\n    }", "CodeBleu_score": 19.72892726336096}]}
{"instance_id": "247fc8882f6f", "generation_result": [{"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\tn     * \u83b7\u53d6JDBC\u8fde\u63a5\uff0c\u8f7b\u91cf\u7ea7\u8fde\u63a5\uff0c\u4f7f\u7528\u5b8c\u540e\u5fc5\u987b\u663e\u5f0fclose\n     *\n     * @param cfg configuration\n     * @return database connection class {@link Connection}\n     */\n    public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\\\"Connecting to HBase cluster [{}] ...\\\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\\\"hadoop.security.authentication\\\", \\\"Kerberos\\\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\\\"org.apache.phoenix.jdbc.PhoenixDriver\\\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Unable to connect to hbase cluster, please check the configuration and cluster status \\\", e);\n        }\n        LOG.debug(\\\"Connected to HBase cluster successfully.\\\");\n        return conn;\n    }\n\n    private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                LOG.error(message);\n                throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n            }\n        }\n    }\n\n    /**\n     * \u521b\u5efa thin client jdbc\u8fde\u63a5\n     *\n     * @param cfg hbase configuration string\n     * @return Connection\n     * @throws SQLException sql connection exception\n     */\n    public static Connection getThinClientJdbcConnection(HbaseSQLWriterConfig cfg)\n            throws SQLException\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.info(\\\"Connecting to HBase cluster [{}] use thin client ...\\\", connStr);\n        Connection conn = DriverManager.getConnection(connStr, cfg.getUsername(), cfg.getPassword());\n        String userNamespaceQuery = \\\"use \\\" + cfg.getNamespace();\n        try (Statement statement = conn.createStatement()) {\n            statement.executeUpdate(userNamespaceQuery);\n            return conn;\n        }\n        catch (Exception e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \\\"Can not connection to the namespace.\\\", e);\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn hbase sql\u7684jdbc\u8fde\u63a5\n     * @param fullTableName \u76ee\u6807\u8868\u7684\u5b8c\u6574\u8868\u540d\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException sql exception\n     */\n    public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }\n\n    /**\n     * \u83b7\u53d6\u4e00\u5f20\u8868\u7684\u5143\u6570\u636e\u4fe1\u606f\n     *\n     * @param conn phoenix connection\n     * @param namespace hbase table's namespace\n     * @param fullTableName hbase full-quality table name\n     * @param isThinClient \u662f\u5426\u4f7f\u7528thin client\n     * @return \u8868\u7684\u5143\u6570\u636e {@link PTable}\n     * @throws SQLException exception\n     */\n    public static PTable getTableSchema(Connection conn, String namespace, String fullTableName, boolean isThinClient)\n            throws\n            SQLException\n    {\n        LOG.info(\\\"Start to get table schema of namespace={}, fullTableName={}\\\", namespace, fullTableName);\n        if (!isThinClient) {\n            return getTableSchema(conn, fullTableName);\n        }\n        else {\n            if (pTable == null) {\n                try (ResultSet result = conn.getMetaData().getColumns(null, namespace, fullTableName, null)) {\n                    ThinClientPTable retTable = new ThinClientPTable();\n                    retTable.setColTypeMap(parseColType(result));\n                    pTable = retTable;\n                }\n            }\n            return pTable;\n        }\n    }\n\n    /**\n     * \u89e3\u6790\u5b57\u6bb5\n     *\n     * @param rs Resultset\n     * @return Map pair\n     * @throws SQLException exception\n     */\n    public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException\n    {\n        Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n        ResultSetMetaData md = rs.getMetaData();\n        int columnCount = md.getColumnCount();\n\n        while (rs.next()) {\n            String colName = null;\n            PDataType colType = null;\n            for (int i = 1; i <= columnCount; i++) {\n                if (\\\"TYPE_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n                }\n                else if (\\\"COLUMN_NAME\\\".equals(md.getColumnLabel(i))) {\n                    colName = (String) rs.getObject(i);\n                }\n            }\n            if (colType == null || colName == null) {\n                throw new SQLException(\\\"ColType or colName is null, colType : \\\" + colType + \\\" , colName : \\\" + colName);\n            }\n            cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n        }\n        return cols;\n    }\n\n    /**\n     * \u6e05\u7a7a\u8868\n     *\n     * @param conn database connection {@link Connection}\n     * @param tableName the table's name\n     */\n\n\n", "CodeBleu_score": 20.644761298269984}]}
{"instance_id": "ac87e1b9a54e", "generation_result": [{"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}, {"code": "private void addSourceFileByType(String filePath){\n\tDS.toNanos(1);\n\n    private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;\n    private final org.apache.hadoop.conf.Configuration hadoopConf;\n    private final boolean haveKerberos;\n    private final HashSet<String> sourceHDFSAllFilesList = new HashSet<>();\n    private String specifiedFileType = null;\n    private String kerberosKeytabFilePath;\n    private String kerberosPrincipal;\n\n    public DFSUtil(Configuration taskConfig)\n    {\n        hadoopConf = new org.apache.hadoop.conf.Configuration();\n        //io.file.buffer.size \u6027\u80fd\u53c2\u6570\n        //http://blog.csdn.net/yangjl38/article/details/7583374\n        Configuration hadoopSiteParams = taskConfig.getConfiguration(Key.HADOOP_CONFIG);\n        JSONObject hadoopSiteParamsAsJsonObject = JSON.parseObject(taskConfig.getString(Key.HADOOP_CONFIG));\n        if (null != hadoopSiteParams) {\n            Set<String> paramKeys = hadoopSiteParams.getKeys();\n            for (String each : paramKeys) {\n                hadoopConf.set(each, hadoopSiteParamsAsJsonObject.getString(each));\n            }\n        }\n        hadoopConf.set(HdfsConstant.HDFS_DEFAULT_KEY, taskConfig.getString(Key.DEFAULT_FS));\n\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);\n        if (haveKerberos) {\n            this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);\n            this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }\n        this.kerberosAuthentication(this.kerberosPrincipal, this.kerberosKeytabFilePath);\n\n        LOG.debug(\\\"hadoopConfig details:{}\\\", JSON.toJSONString(this.hadoopConf));\n    }\n\n    private void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n    {\n        if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n            UserGroupInformation.setConfiguration(hadoopConf);\n            try {\n                UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n            }\n            catch (Exception e) {\n                String message = String.format(\\\"kerberos\u8ba4\u8bc1\u5931\u8d25,\u8bf7\u786e\u5b9akerberosKeytabFilePath[%s]\u548ckerberosPrincipal[%s]\u586b\u5199\u6b63\u786e\\\",\n                        kerberosKeytabFilePath, kerberosPrincipal);\n                throw AddaxException.asAddaxException(HdfsReaderErrorCode.KERBEROS_LOGIN_ERROR, message, e);\n            }\n        }\n    }\n\n    /**\n     * \u83b7\u53d6\u6307\u5b9a\u8def\u5f84\u5217\u8868\u4e0b\u7b26\u5408\u6761\u4ef6\u7684\u6240\u6709\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n     *\n     * @param srcPaths \u8def\u5f84\u5217\u8868\n     * @param specifiedFileType \u6307\u5b9a\u6587\u4ef6\u7c7b\u578b\n     * @return set of string\n     */\n    public Set<String> getAllFiles(List<String> srcPaths, String specifiedFileType)\n    {\n\n        this.specifiedFileType = specifiedFileType;\n\n        if (!srcPaths.isEmpty()) {\n            for (String eachPath : srcPaths) {\n                LOG.info(\\\"get HDFS all files in path = [{}]\\\", eachPath);\n                getHDFSAllFiles(eachPath);\n            }\n        }\n        return sourceHDFSAllFilesList;\n    }\n\n    private void addSourceFileIfNotEmpty(FileStatus f)\n    {\n        if (f.isFile()) {\n            String filePath = f.getPath().toString();\n            if (f.getLen() > 0) {\n                addSourceFileByType(filePath);\n            }\n            else {\n                LOG.warn(\\\"It will ignore file [{}] because it is empty.\\\", filePath);\n            }\n        }\n    }\n\n    public void getHDFSAllFiles(String hdfsPath)\n    {\n\n        try {\n            FileSystem hdfs = FileSystem.get(hadoopConf);\n            //\u5224\u65adhdfsPath\u662f\u5426\u5305\u542b\u6b63\u5219\u7b26\u53f7\n            if (hdfsPath.contains(\\\"*\\\") || hdfsPath.contains(\\\"?\\\")) {\n                Path path = new Path(hdfsPath);\n                FileStatus[] stats = hdfs.globStatus(path);\n                for (FileStatus f : stats) {\n                    if (f.isFile()) {\n                        addSourceFileIfNotEmpty(f);\n                    }\n                    else if (f.isDirectory()) {\n                        getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n                    }\n                }\n            }\n            else {\n                getHDFSAllFilesNORegex(hdfsPath, hdfs);\n            }\n        }\n        catch (IOException e) {\n            LOG.error(\\\"IO exception occurred while reading file(s) under [{}].\\\", hdfsPath);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, e);\n        }\n    }\n\n    private void getHDFSAllFilesNORegex(String path, FileSystem hdfs)\n            throws IOException\n    {\n\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\n        Path listFiles = new Path(path);\n\n        // If the network disconnected, this method will retry 45 times\n        // each time the retry interval for 20 seconds\n        // \u83b7\u53d6\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u7684\u6240\u6709\u4e8c\u7ea7\u5b50\u6587\u4ef6\u76ee\u5f55\n        FileStatus[] stats = hdfs.listStatus(listFiles);\n\n        for (FileStatus f : stats) {\n            // \u5224\u65ad\u662f\u4e0d\u662f\u76ee\u5f55\uff0c\u5982\u679c\u662f\u76ee\u5f55\uff0c\u9012\u5f52\u8c03\u7528\n            if (f.isDirectory()) {\n                LOG.info(\\\"The [{}] is directory, reading all files in the directory.\\\", f.getPath());\n                getHDFSAllFilesNORegex(f.getPath().toString(), hdfs);\n            }\n            else if (f.isFile()) {\n                addSourceFileIfNotEmpty(f);\n            }\n            else {\n                String message = String.format(\\\"The [%s] neither directory nor file,ignore it.\\\", f.getPath());\n                LOG.info(message);\n            }\n        }\n    }\n\n    // \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u7c7b\u578b\u7684\u8def\u5f84\u52a0\u5165sourceHDFSAllFilesList\n\n\n", "CodeBleu_score": 23.9891902657904}]}
{"instance_id": "5e6a66d06162", "generation_result": [{"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tkerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);\n            this.hadoopConf.set(HdfsConstant.HADOOP_SECURITY_AUTHENTICATION_KEY, \\\"kerberos\\\");\n            // fix Failed to specify server's Kerberos principal name\n            if (Objects.equals(hadoopConf.get(\\\"dfs.namenode.kerberos.principal\\\", \\\"\\\"), \\\"\\\")) {\n                // get REALM\n                String serverPrincipal = \\\"nn/_HOST@\\\" + Iterables.get(Splitter.on('@').split(this.kerberosPrincipal), 1);\n                hadoopConf.set(\\\"dfs.namenode.kerberos.principal\\\", serverPrincipal);\n            }\n        }", "CodeBleu_score": 13.836466192485181}]}
{"instance_id": "c450f959616c", "generation_result": [{"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t.build()) {\n            Group group = reader.read();\n\n            if (null == column || column.isEmpty()) {\n                MessageType schema = ParquetFileReader.open(HadoopInputFile.fromPath(parquetFilePath, hadoopConf)).getFooter().getFileMetaData().getSchema();\n                List<org.apache.parquet.schema.Type> fields = schema.getFields();\n                column = new ArrayList<>(fields.size());\n\n                String sType;\n                // \u7528\u6237\u6ca1\u6709\u586b\u5199\u5177\u4f53\u7684\u5b57\u6bb5\u4fe1\u606f\uff0c\u9700\u8981\u4eceparquet\u6587\u4ef6\u6784\u5efa\n                for (int i = 0; i < schema.getFields().size(); i++) {\n                    ColumnEntry columnEntry = new ColumnEntry();\n                    columnEntry.setIndex(i);\n                    columnEntry.setType(getJavaType(fields.get(i)));\n                    column.add(columnEntry);\n                }\n            }\n            while (group != null) {\n                transportParquetRecord(column, group, recordSender, taskPluginCollector, nullFormat);\n                group = reader.read();\n            }\n        }\n        catch (IOException e) {\n            String message = String.format(\\\"IO exception occurred while reading the parquet-file [%s]\\\", sourceParquetFilePath);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n        }\n    }", "CodeBleu_score": 34.0452260363788}]}
{"instance_id": "1bf487d2cc64", "generation_result": [{"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}, {"code": "public static void closeConnection(Connection hConnection){\n\t.HBaseConstant;", "CodeBleu_score": 3.4687352033450107}]}
{"instance_id": "680728cbc3ec", "generation_result": [{"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}, {"code": "public static void closeAdmin(Admin admin){\n\tnimport org.apache.commons.lang3.Validate;", "CodeBleu_score": 3.485893345016727}]}
{"instance_id": "ce50bd53abef", "generation_result": [{"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tbyte[] rowkey = CellUtil.cloneRow(cell);\n        String rowkeyStr = Bytes.toString(rowkey);\n        record.addColumn(new LongColumn(rowkeyStr.hashCode()));\n        record.addColumn(new LongColumn(rowkeyStr.hashCode()));\n\n        byte[] family = CellUtil.cloneFamily(cell);\n        String familyStr = Bytes.toString(family);\n        record.addColumn(new StringColumn(familyStr));\n\n        byte[] qualifier = CellUtil.cloneQualifier(cell);\n        String qualifierStr = Bytes.toString(qualifier);\n        record.addColumn(new StringColumn(qualifierStr));\n\n        long timestamp = cell.getTimestamp();\n        record.addColumn(new LongColumn(timestamp));\n\n        byte[] value = CellUtil.cloneValue(cell);\n        String valueStr = Bytes.toString(value);\n        record.addColumn(new StringColumn(valueStr));\n    }", "CodeBleu_score": 32.414617188914384}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\tString rowKey = Bytes.toString(CellUtil.cloneRow(cell));\n        long timestamp = cell.getTimestamp();\n        String family = Bytes.toString(CellUtil.cloneFamily(cell));\n        String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));\n        String value = Bytes.toString(CellUtil.cloneValue(cell));\n        String columnType = this.familyQualifierMap.get(family).get(qualifier);\n        String columnFormat = this.familyQualifierMap.get(family).get(qualifier + HBaseConstant.COLUMN_FORMAT);\n        String columnValue = this.familyQualifierMap.get(family).get(qualifier + HBaseConstant.COLUMN_VALUE);\n        if (StringUtils.isBlank(columnType)) {\n            columnType = HBaseConstant.DEFAULT_TYPE;\n        }\n        if (StringUtils.isBlank(columnFormat)) {\n            columnFormat = HBaseConstant.DEFAULT_FORMAT;\n        }\n        if (StringUtils.isBlank(columnValue)) {\n            columnValue = HBaseConstant.DEFAULT_VALUE;\n        }\n        String rowKeyColumn = this.familyQualifierMap.get(HBaseConstant.ROW_KEY_FLAG).get(HBaseConstant.ROW_KEY_FLAG);\n        String rowKeyType = this.familyQualifierMap.get(HBaseConstant.ROW_KEY_FLAG).get(HBaseConstant.ROW_KEY_FLAG + HBaseConstant.COLUMN_TYPE);\n        String rowKeyFormat = this.familyQualifierMap.get(HBaseConstant.ROW_KEY_FLAG).get(HBaseConstant.ROW_KEY_FLAG + HBaseConstant.COLUMN_FORMAT);\n        String rowKeyValue = this.familyQualifierMap.get(HBaseConstant.ROW_KEY_FLAG).get(HBaseConstant.ROW_KEY_FLAG + HBaseConstant.COLUMN_VALUE);\n        if (StringUtils.isBlank(rowKeyType)) {\n            rowKeyType = HBaseConstant.DEFAULT_TYPE;\n        }\n        if (StringUtils.isBlank(rowKeyFormat)) {\n            row", "CodeBleu_score": 38.53262173752452}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\t// 1. rowkey\n        String rowkey = Bytes.toString(CellUtil.cloneRow(cell));\n        record.addColumn(this.rowkeyColumnIndex, new LongColumn(rowkey));\n\n        // 2. timestamp\n        long timestamp = cell.getTimestamp();\n        record.addColumn(this.timestampColumnIndex, new LongColumn(timestamp));\n\n        // 3. family:qualifier\n        String family = Bytes.toString(CellUtil.cloneFamily(cell));\n        String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));\n        String familyQualifier = family + HBaseConstant.DELIMITER + qualifier;\n        record.addColumn(this.familyQualifierColumnIndex, new LongColumn(familyQualifier));\n\n        // 4. value\n        String value = Bytes.toString(CellUtil.cloneValue(cell));\n        String type = this.familyQualifierMap.get(family).get(qualifier);\n        record.addColumn(this.valueColumnIndex, Hbase20xHelper.convertCellValue(value, type));\n    }", "CodeBleu_score": 31.484654481580566}]}
{"instance_id": "29e8f3a99f0c", "generation_result": [{"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t.apache.hadoop.hbase.util.Pair;", "CodeBleu_score": 7.685148797388076}]}
{"instance_id": "f5ad51545b10", "generation_result": [{"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t0, \\\"hbaseConfig\u4e0d\u80fd\u4e3a\u7a7aMap\u7ed3\u6784!\\\");\n            for (Map.Entry<String, String> entry : hbaseConfigMap.entrySet()) {\n                hConfiguration.set(entry.getKey(), entry.getValue());\n            }\n        }", "CodeBleu_score": 14.821635030216196}]}
{"instance_id": "8618eef8b0e9", "generation_result": [{"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 43.83898579367364}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 43.83898579367364}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\n    this.headers = new DefaultTbQueueMsgHeaders(record.headers());\n    this.data = record.value();\n}", "CodeBleu_score": 43.83898579367364}]}
{"instance_id": "46484df184dd", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tn * distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */", "CodeBleu_score": 7.199712493122161}]}
{"instance_id": "4ac56ef9f89a", "generation_result": [{"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tn\n    private final TbQueueAdmin coreAdmin;\n    private final TbQueueAdmin ruleEngineAdmin;\n    private final TbQueueAdmin jsExecutorRequestAdmin;\n    private final TbQueueAdmin jsExecutorResponseAdmin;\n    private final TbQueueAdmin transportApiRequestAdmin;\n    private final TbQueueAdmin transportApiResponseAdmin;\n    private final TbQueueAdmin notificationAdmin;\n    private final TbQueueAdmin fwUpdatesAdmin;\n    private final TbQueueAdmin vcAdmin;\n    private final TbQueueAdmin housekeeperAdmin;\n    private final TbQueueAdmin housekeeperReprocessingAdmin;\n\n    private final AtomicLong consumerCount = new AtomicLong();\n\n    public KafkaTbCoreQueueFactory(TopicService topicService,\n                                   TbKafkaSettings kafkaSettings,\n                                   TbServiceInfoProvider serviceInfoProvider,\n                                   TbQueueCoreSettings coreSettings,\n                                   TbQueueRuleEngineSettings ruleEngineSettings,\n                                   TbQueueTransportApiSettings transportApiSettings,\n                                   TbQueueRemoteJsInvokeSettings jsInvokeSettings,\n                                   TbQueueVersionControlSettings vcSettings,\n                                   TbKafkaConsumerStatsService consumerStatsService,\n                                   TbQueueTransportNotificationSettings transportNotificationSettings,\n                                   TbKafkaTopicConfigs kafkaTopicConfigs) {\n        this.topicService = topicService;\n        this.kafkaSettings = kafkaSettings;\n        this.serviceInfoProvider = serviceInfoProvider;\n        this.coreSettings = coreSettings;\n        this.ruleEngineSettings = ruleEngineSettings;\n        this.transportApiSettings = transportApiSettings;\n        this.jsInvokeSettings = jsInvokeSettings;\n        this.vcSettings = vcSettings;\n        this.consumerStatsService = consumerStatsService;\n        this.transportNotificationSettings = transportNotificationSettings;\n\n        this.coreAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getCoreConfigs());\n        this.ruleEngineAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getRuleEngineConfigs());\n        this.jsExecutorRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorRequestConfigs());\n        this.jsExecutorResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getJsExecutorResponseConfigs());\n        this.transportApiRequestAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiRequestConfigs());\n        this.transportApiResponseAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getTransportApiResponseConfigs());\n        this.notificationAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getNotificationsConfigs());\n        this.fwUpdatesAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getFwUpdatesConfigs());\n        this.vcAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getVcConfigs());\n        this.housekeeperAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperConfigs());\n        this.housekeeperReprocessingAdmin = new TbKafkaAdmin(kafkaSettings, kafkaTopicConfigs.getHousekeeperReprocessingConfigs());\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-transport-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> createRuleEngineNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-rule-engine-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreMsg>> createTbCoreMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(coreAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueProducer<TbProtoQueueMsg<ToCoreNotificationMsg>> createTbCoreNotificationsMsgProducer() {\n        TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n        requestBuilder.settings(kafkaSettings);\n        requestBuilder.clientId(\\\"tb-core-to-core-notifications-\\\" + serviceInfoProvider.getServiceId());\n        requestBuilder.defaultTopic(topicService.buildTopicName(coreSettings.getTopic()));\n        requestBuilder.admin(notificationAdmin);\n        return requestBuilder.build();\n    }\n\n    @Override\n    public TbQueueConsumer<TbProtoQueueMsg<ToCoreMsg>> createToCoreMsgConsumer() {\n        TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n        consumerBuilder.settings(kafkaSettings);\n        consumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n        consumerBuilder.clientId(\\\"tb-core-consumer-\\\" + serviceInfoProvider.getServiceId() + \\\"-\\\" + consumerCount.incrementAndGet());\n        consumerBuilder.groupId(topicService.buildTopicName(\\\"tb-core-node\\\"));\n        consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreMsg.parseFrom(msg.getData()), msg.getHeaders()));\n        consumerBuilder.admin(coreAdmin);\n        consumerBuilder.statsService(consumerStatsService);\n        return consumerBuilder.build();\n    }\n\n    @Override\n\n\n", "CodeBleu_score": 41.19403527934631}]}
{"instance_id": "e13a82b26338", "generation_result": [{"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"[{}] Subscribing to no topics.\", clientId);\n        consumer.unsubscribe();\n    } else {\n        log.info(\"[{}] Subscribing to topics: {}\", clientId, topicNames);\n        admin.createTopics(topicNames);\n        consumer.subscribe(topicNames);\n    }\n}", "CodeBleu_score": 55.89344006951418}, {"code": "protected void doSubscribe(List<String> topicNames){\n\tif (topicNames.isEmpty()) {\n        log.info(\"[{}] Consumer is not subscribed to any topics.\", topic);\n        consumer.unsubscribe();\n    } else {\n        log.info(\"[{}] Consumer is subscribed to topics: {}\", topic, topicNames);\n        if (admin != null) {\n            try {\n                admin.createTopics(topicNames);\n            } catch (Exception e) {\n                log.warn(\"[{}] Failed to create topics: {}\", topic, topicNames, e);\n            }\n        }\n        consumer.subscribe(topicNames);\n    }\n}", "CodeBleu_score": 49.49116566841534}, {"code": "protected void doSubscribe(List<String> topicNames){\n\ttry {\n        if (topicNames.isEmpty()) {\n            log.info(\"[{}] No topics provided, unsubscribing consumer.\", topic);\n            consumer.unsubscribe();\n        } else {\n            log.info(\"[{}] Subscribing to topics: {}\", topic, topicNames);\n            List<String> topicsToSubscribe = new ArrayList<>(topicNames);\n            if (topic.isBlank()) {\n                topicsToSubscribe.add(topic);\n            }\n            consumer.subscribe(topicsToSubscribe);\n        }\n    } catch (Exception e) {\n        log.error(\"[{}] Failed to subscribe consumer to topics: {}\", topic, topicNames, e);\n    }\n}", "CodeBleu_score": 47.62217905774182}]}
{"instance_id": "8a20d0b48703", "generation_result": [{"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.debug(\"Polling topic: {}, duration: {} ms\", topic, durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    log.trace(\"Poll result: {}\", records);\n    if (records.isEmpty()) {\n        return Collections.emptyList();\n    } else {\n        List<ConsumerRecord<String, byte[]>> result = new ArrayList<>(records.count());\n        records.iterator().forEachRemaining(result::add);\n        return result;\n    }\n}", "CodeBleu_score": 71.77347882645228}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"[{}] Polling for [{}] ms\", topic, durationInMillis);\n    List<ConsumerRecord<String, byte[]>> records = new ArrayList<>();\n    try {\n        ConsumerRecords<String, byte[]> consumerRecords = consumer.poll(Duration.ofMillis(durationInMillis));\n        log.trace(\"[{}] Polled [{}] records\", topic, consumerRecords.count());\n        if (!consumerRecords.isEmpty()) {\n            consumerRecords.forEach(records::add);\n        }\n    } catch (Exception e) {\n        log.error(\"[{}] Failed to poll records\", topic, e);\n    }\n    stopWatch.stop();\n    log.trace(\"[{}] Polled [{}] records in [{}] ms\", topic, records.size(), stopWatch.getTotalTimeMillis());\n    return records;\n}", "CodeBleu_score": 61.73948981428916}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\tStopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"Polling topic [{}] for up to {} ms.\", topic, durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    log.trace(\"Polled topic [{}] for {} ms. Got {} records.\", topic, stopWatch.getTotalTimeMillis(), records.count());\n    if (records.isEmpty()) {\n        return Collections.emptyList();\n    } else {\n        return new ArrayList<>(records);\n    }\n}", "CodeBleu_score": 65.78495233172536}]}
{"instance_id": "328e4c39a8e1", "generation_result": [{"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tkuOTY1IDAgMCAwIDQyLjYtMjQuNzc2bDI1LjYzMyAxOC4xNDNjLTEuNDIzIDQuODQtMi4yMiA5Ljk0Ni0yLjIyIDE1LjI0IDAgMjkuODc5IDI0LjMwOSA1NC4xODQgNTQuMTg0IDU0LjE4NCAyOS44NzUgMCA1NC4xODQtMjQuMzA1IDU0LjE4NC01NC4xODQgMC0yOS44NzktMjQuMzA5LTU0LjE4NC01NC4xODQtNTQuMTg0em0wLTEyNi42OTVjMTQuNDg3IDAgMjYuMjcgMTEuNzg4IDI2LjI3IDI2LjI3MXMtMTEuNzgzIDI2LjI3LTI2LjI3IDI2LjI3LTI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.5391018207466622}]}
{"instance_id": "35f835e8c387", "generation_result": [{"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tI2LjI3LTExLjc4Ny0yNi4yNy0yNi4yN2MwLTE0LjQ4MyAxMS43ODMtMjYuMjcxIDI2LjI3LTI2LjI3MXptLTE1OC4xLTQ5LjMzN2MwLTE0LjQ4MyAxMS43ODQtMjYuMjcgMjYuMjcxLTI2LjI3czI2LjI3IDExLjc4NyAyNi4yNyAyNi4yN2MwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 5.535790790500088}]}
{"instance_id": "111a11189434", "generation_result": [{"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}, {"code": "public void destroy(){\n\tMS0xMS43ODctMjYuMjcxLTI2LjI3em01Mi41NDEgMzA3LjI3OGMwIDE0LjQ4My0xMS43ODMgMjYuMjctMjYuMjcgMjYuMjdzLTI2LjI3MS0xMS43ODctMjYuMjcxLTI2LjI3YzAtMTQuNDgzIDExLjc4NC0yNi4yNyAyNi4yNzEtMjYuMjdzMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3em0tMjYuMjcyLTExNy45N2MtMjAuMjA1IDAtMzYuNjQyLTE2LjQzNC0zNi42NDItMzYuNjM4IDAtMjAuMjA1IDE2LjQzNy0zNi42NDIgMzYuNjQyLTM2LjY0MiAyMC4yMDQgMCAzNi42NDEgMTYuNDM3IDM2LjY0MSAzNi42NDIgMCAyMC4yMDQtMTYuNDM3IDM2LjYzOC0zNi42NDEgMzYuNjM4em0xMzEuODMxIDY3LjE3OWMtMTQuNDg3IDAtMjYuMjctMTEuNzg4LTI2LjI3LTI2LjI3MXMxMS43ODMtMjYuMjcgMjYuMjctMjYuMjcgMjYuMjcgMTEuNzg3IDI2LjI3IDI2LjI3YzAgMTQuNDgzLTExLjc4MyAyNi4yNzEtMjYuMjcgMjYuMjcxeiIvPjwvc3ZnPg==\\\"", "CodeBleu_score": 2.299977454403141}]}
{"instance_id": "f5d2487100a9", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\timport org.thingsboard.server.gen.transport.TransportProtos;", "CodeBleu_score": 5.836799453595653}]}
{"instance_id": "886264f5ca27", "generation_result": [{"code": "private void startLogScheduling(){\n\tDuration timeout = Duration.ofMillis(kafkaSettings.getResponseTimeout());\n    long delay = timeout.toMillis() / 2;\n    statsPrintScheduler.scheduleAtFixedRate(() -> {\n        if (statsConfig.isPrintEnabled()) {\n            try {\n                Map<String, List<PartitionInfo>> topics = consumer.listTopics(timeout);\n                Map<TopicPartition, OffsetAndMetadata> offsets = consumer.committed(new ArrayList<>(topics.keySet()), timeout);\n                Map<TopicPartition, Long> endOffsets = consumer.endOffsets(new ArrayList<>(topics.keySet()), timeout);\n                for (String group : monitoredGroups) {\n                    List<TbKafkaLagInfo> laggingTopics = new ArrayList<>();\n                    for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) {\n                        TopicPartition topicPartition = entry.getKey();\n                        if (topics.containsKey(topicPartition.topic())) {\n                            long lag = endOffsets.get(topicPartition) - entry.getValue().offset();\n                            if (lag > 0) {\n                                laggingTopics.add(new TbKafkaLagInfo(topicPartition.topic(), topicPartition.partition(), lag));\n                            }\n                        }\n                    }\n                    if (!laggingTopics.isEmpty()) {\n                        log.warn(\"[{}] Lagging topics: {}\", group, laggingTopics);\n                    }\n                }\n            } catch (Exception e) {\n                log.warn(\"[{}] Failed to print consumer group stats.\", e);\n            }\n        }\n    }, delay, delay, TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 44.806870548623884}, {"code": "private void startLogScheduling(){\n\tlong timeout = kafkaSettings.getResponseTimeout();\n    statsPrintScheduler.scheduleWithFixedDelay(\n            () -> {\n                try {\n                    if (statsConfig.isEnabled()) {\n                        logStats();\n                    }\n                } catch (Exception e) {\n                    log.warn(\"[{}] Failed to print consumer group stats.\", e.getMessage(), e);\n                }\n            },\n            timeout,\n            timeout,\n            TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 15.181516531338762}, {"code": "private void startLogScheduling(){\n\tlong timeout = statsConfig.getPeriodMs() + statsConfig.getTimeoutMs();\n    long delay = statsConfig.getPeriodMs();\n    this.statsPrintScheduler.scheduleWithFixedDelay(\n            () -> {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Checking Kafka consumer group offsets...\");\n                }\n                try {\n                    if (isStatsPrintRequired()) {\n                        Map<String, List<TbKafkaConsumerGroupStats>> consumerGroupStats = getConsumerGroupStats();\n                        if (consumerGroupStats != null) {\n                            for (Map.Entry<String, List<TbKafkaConsumerGroupStats>> entry : consumerGroupStats.entrySet()) {\n                                String groupId = entry.getKey();\n                                List<TbKafkaConsumerGroupStats> statsList = entry.getValue();\n                                if (statsList != null && !statsList.isEmpty()) {\n                                    for (TbKafkaConsumerGroupStats stats : statsList) {\n                                        if (stats.getLag() > 0) {\n                                            log.warn(\"Consumer group [{}] has lagging topics: {}\", groupId, stats.toString());\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                } catch (Exception e) {\n                    log.warn(\"Failed to print Kafka consumer group offsets: {}\", e.getMessage(), e);\n                }\n            },\n            delay,\n            delay,\n            TimeUnit.MILLISECONDS);\n}", "CodeBleu_score": 35.69822076492204}]}
{"instance_id": "81553181cdc8", "generation_result": [{"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}, {"code": "protected Properties toAdminProps(){\n\t;\n\n    @Value(\\\"${queue.kafka.retries:1}\\\")\n    private int retries;\n\n    @Value(\\\"${queue.kafka.compression.type:none}\\\")\n    private String compressionType;\n\n    @Value(\\\"${queue.kafka.batch.size:16384}\\\")\n    private int batchSize;\n\n    @Value(\\\"${queue.kafka.linger.ms:1}\\\")\n    private long lingerMs;\n\n    @Value(\\\"${queue.kafka.max.request.size:1048576}\\\")\n    private int maxRequestSize;\n\n    @Value(\\\"${queue.kafka.max.in.flight.requests.per.connection:5}\\\")\n    private int maxInFlightRequestsPerConnection;\n\n    @Value(\\\"${queue.kafka.buffer.memory:33554432}\\\")\n    private long bufferMemory;\n\n    @Value(\\\"${queue.kafka.replication_factor:1}\\\")\n    @Getter\n    private short replicationFactor;\n\n    @Value(\\\"${queue.kafka.max_poll_records:8192}\\\")\n    private int maxPollRecords;\n\n    @Value(\\\"${queue.kafka.max_poll_interval_ms:300000}\\\")\n    private int maxPollIntervalMs;\n\n    @Value(\\\"${queue.kafka.max_partition_fetch_bytes:16777216}\\\")\n    private int maxPartitionFetchBytes;\n\n    @Value(\\\"${queue.kafka.fetch_max_bytes:134217728}\\\")\n    private int fetchMaxBytes;\n\n    @Value(\\\"${queue.kafka.request.timeout.ms:30000}\\\")\n    private int requestTimeoutMs;\n\n    @Value(\\\"${queue.kafka.session.timeout.ms:10000}\\\")\n    private int sessionTimeoutMs;\n\n    @Value(\\\"${queue.kafka.auto_offset_reset:earliest}\\\")\n    private String autoOffsetReset;\n\n    @Value(\\\"${queue.kafka.use_confluent_cloud:false}\\\")\n    private boolean useConfluent;\n\n    @Value(\\\"${queue.kafka.confluent.ssl.algorithm:}\\\")\n    private String sslAlgorithm;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.mechanism:}\\\")\n    private String saslMechanism;\n\n    @Value(\\\"${queue.kafka.confluent.sasl.config:}\\\")\n    private String saslConfig;\n\n    @Value(\\\"${queue.kafka.confluent.security.protocol:}\\\")\n    private String securityProtocol;\n\n    @Value(\\\"${queue.kafka.other-inline:}\\\")\n    private String otherInline;\n\n    @Deprecated\n    @Setter\n    private List<TbProperty> other;\n\n    @Setter\n    private Map<String, List<TbProperty>> consumerPropertiesPerTopic = Collections.emptyMap();\n\n    private volatile AdminClient adminClient;\n\n    public Properties toConsumerProps(String topic) {\n        Properties props = toProps();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords);\n        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, maxPartitionFetchBytes);\n        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, fetchMaxBytes);\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervalMs);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset);\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n\n        consumerPropertiesPerTopic\n                .getOrDefault(topic, Collections.emptyList())\n                .forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        return props;\n    }\n\n    public Properties toProducerProps() {\n        Properties props = toProps();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n        props.put(ProducerConfig.RETRIES_CONFIG, retries);\n        props.put(ProducerConfig.ACKS_CONFIG, acks);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, lingerMs);\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);\n        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);\n        return props;\n    }\n\n    Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\\\"ssl.endpoint.identification.algorithm\\\", sslAlgorithm);\n            props.put(\\\"sasl.mechanism\\\", saslMechanism);\n            props.put(\\\"sasl.jaas.config\\\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }\n\n    void configureSSL(Properties props) {\n        if (sslEnabled) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \\\"SSL\\\");\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);\n            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);\n            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);\n            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);\n            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);\n        }\n    }\n\n    public AdminClient getAdminClient() {\n        if (adminClient == null) {\n            synchronized (this) {\n                if (adminClient == null) {\n                    adminClient = AdminClient.create(toAdminProps());\n                }\n            }\n        }\n        return adminClient;\n    }\n\n\n\n", "CodeBleu_score": 55.871409609187594}]}
{"instance_id": "3c8bf3ff31b7", "generation_result": [{"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tString[] scopeComponents = group.getScopeComponents();\n\tString[] tags = group.getAllVariables();\n\tString scopedName = getScopedName(metricName, scopeComponents);\n\tMetricEvent event = new MetricEvent(scopedName, tags, metric.getMetricIdentifier());\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, event);\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, event);\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, event);\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, event);\n\t} else {\n\t\tLOG.warn(\"Metric type {} is not supported. Ignoring metric {}\", metric.getClass().getName(), metricName);\n\t}\n}", "CodeBleu_score": 65.92852279956567}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tString scope = getScopedName(metricName, group);\n\tMap<String, String> tags = getTags(metricName, group);\n\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, new MetricEvent(scope, tags));\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, new MetricEvent(scope, tags));\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, new MetricEvent(scope, tags));\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, new MetricEvent(scope, tags));\n\t} else {\n\t\tLOG.warn(\"Metric type {} is not supported.\", metric.getClass().getSimpleName());\n\t}\n}", "CodeBleu_score": 57.41661167524753}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tString name = getScopedName(metricName, group);\n\tMap<String, String> tags = getTags(metricName, group);\n\tMetricEvent event = new MetricEvent(name, tags);\n\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, event);\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, event);\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, event);\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, event);\n\t} else {\n\t\tLOG.warn(\"Metric type {} is not supported.\", metric.getClass().getName());\n\t}\n}", "CodeBleu_score": 67.1715190789974}]}
{"instance_id": "77c6addc9b99", "generation_result": [{"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tjvmGarbageCollector.addTags(event.getTags());\n\t\t\t\t\t\tString[] groups = name.split(\\\"GarbageCollector_\\\");\n\t\t\t\t\t\taddFields(jvmGarbageCollector, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 16.119385517538504}]}
{"instance_id": "a48971ffb3e9", "generation_result": [{"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t, groups[groups.length - 1], entry.getKey());\n\t\t\t\t\t}", "CodeBleu_score": 4.306175349302117}]}
{"instance_id": "c2080c299bb0", "generation_result": [{"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    String topic = parameter.getRequired(PropertiesConstants.METRICS_TOPIC);\n    Long time = parameter.getLong(PropertiesConstants.CONSUMER_FROM_TIME, 0L);\n    return buildSource(env, topic, time);\n}", "CodeBleu_score": 35.488986003522555}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(\n            topic,\n            new MetricSchema(),\n            props);\n    if (time > 0) {\n        Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n        List<PartitionInfo> partitionInfos = consumer.getKafkaConsumer().partitionsFor(topic);\n        for (PartitionInfo partitionInfo : partitionInfos) {\n            TopicPartition topicPartition = new TopicPartition(topic, partitionInfo.partition());\n            KafkaTopicPartition kafkaTopicPartition = new KafkaTopicPartition(topicPartition.topic(), topicPartition.partition());\n            consumer.getKafkaConsumer().assign(Collections.singletonList(topicPartition));\n            OffsetAndTimestamp offsetAndTimestamp = consumer.getKafkaConsumer().offsetsForTimes(Collections.singletonMap(topicPartition, time)).get(topicPartition);\n            if (offsetAndTimestamp != null) {\n                specificOffsets.put(kafkaTopicPartition, offsetAndTimestamp.offset());\n            }\n        }\n        if (specificOffsets.size() > 0) {\n            consumer.setStartFromSpecificOffsets(specificOffsets);\n        }\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 67.40201174674846}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameter);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n    if (time > 0) {\n        consumer.setStartFromTimestamp(time);\n    }\n    return env.addSource(consumer);\n}", "CodeBleu_score": 61.44557009092089}]}
{"instance_id": "5e828afbe8e4", "generation_result": [{"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tString groupId = parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID);\n    props.put(PropertiesConstants.KAFKA_GROUP_ID, groupId + \"-\" + time);\n\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    List<PartitionInfo> partitionInfos = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n\n    for (PartitionInfo partitionInfo : partitionInfos) {\n        TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n        OffsetAndTimestamp offsetAndTimestamp = consumer.offsetsForTimes(Collections.singletonMap(topicPartition, time)).get(topicPartition);\n        if (offsetAndTimestamp != null) {\n            partitionOffset.put(new KafkaTopicPartition(topicPartition.topic(), topicPartition.partition()), offsetAndTimestamp.offset());\n        }\n    }\n\n    consumer.close();\n    return partitionOffset;\n}", "CodeBleu_score": 55.02874797244601}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tString groupId = parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID);\n    props.put(PropertiesConstants.KAFKA_GROUP_ID, groupId + \"-\" + time);\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    List<PartitionInfo> partitionInfoList = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionInfoList) {\n        TopicPartition topicPartition = new TopicPartition(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC), partitionInfo.partition());\n        consumer.assign(Collections.singletonList(topicPartition));\n        OffsetAndTimestamp offsetAndTimestamp = consumer.offsetsForTimes(Collections.singletonMap(topicPartition, time)).get(topicPartition);\n        if (offsetAndTimestamp != null) {\n            partitionOffset.put(new KafkaTopicPartition(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC), partitionInfo.partition()), offsetAndTimestamp.offset());\n        }\n    }\n    consumer.close();\n    return partitionOffset;\n}", "CodeBleu_score": 54.907258461773345}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tprops.put(\"group.id\", parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time);\n    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    List<PartitionInfo> partitionInfoList = consumer.partitionsFor(parameterTool.get(PropertiesConstants.METRICS_TOPIC));\n    for (PartitionInfo partitionInfo : partitionInfoList) {\n        TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n        OffsetAndTimestamp offsetAndTimestamp = consumer.offsetsForTimes(Collections.singletonMap(topicPartition, time)).get(topicPartition);\n        if (offsetAndTimestamp != null) {\n            KafkaTopicPartition kafkaTopicPartition = new KafkaTopicPartition(parameterTool.get(PropertiesConstants.METRICS_TOPIC), partitionInfo.partition());\n            partitionOffset.put(kafkaTopicPartition, offsetAndTimestamp.offset());\n        }\n    }\n    consumer.close();\n    return partitionOffset;\n}", "CodeBleu_score": 54.75368292457904}]}
{"instance_id": "28da89f56f67", "generation_result": [{"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"acks\", \"all\");\n    props.put(\"retries\", 0);\n    props.put(\"batch.size\", 16384);\n    props.put(\"linger.ms\", 1);\n    props.put(\"buffer.memory\", 33554432);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    MetricEvent metricEvent = new MetricEvent();\n    metricEvent.setTimestamp(System.currentTimeMillis());\n    metricEvent.setName(\"test\");\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\"tag1\", \"value1\");\n    tags.put(\"tag2\", \"value2\");\n    metricEvent.setTags(tags);\n    Map<String, Double> fields = new HashMap<>();\n    fields.put(\"field1\", 1.0);\n    fields.put(\"field2\", 2.0);\n    metricEvent.setFields(fields);\n\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, GsonUtil.toJson(metricEvent));\n    producer.send(record);\n    producer.flush();\n\n    System.out.println(\"Sent data: \" + GsonUtil.toJson(metricEvent));\n}", "CodeBleu_score": 65.42723812436057}, {"code": "public static void writeToKafka() throws InterruptedException{\n\t// 1. \u8bbe\u7f6e Kafka \u751f\u4ea7\u8005\u53c2\u6570\n    Properties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    // 2. \u521b\u5efa KafkaProducer \u5bf9\u8c61\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    // 3. \u521b\u5efa MetricEvent \u5bf9\u8c61\uff0c\u5e76\u8d4b\u503c\n    MetricEvent metricEvent = new MetricEvent();\n    metricEvent.setName(\"zhisheng\");\n    metricEvent.setTimestamp(System.currentTimeMillis());\n    Map<String, String> tags = new HashMap<>();\n    tags.put(\"tag1\", \"value1\");\n    tags.put(\"tag2\", \"value2\");\n    metricEvent.setTags(tags);\n    Map<String, Double> fields = new HashMap<>();\n    fields.put(\"field1\", 1.0);\n    fields.put(\"field2\", 2.0);\n    metricEvent.setFields(fields);\n\n    // 4. \u521b\u5efa ProducerRecord \u5bf9\u8c61\uff0c\u5e76\u53d1\u9001\u6570\u636e\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, GsonUtil.toJson(metricEvent));\n    producer.send(record);\n\n    // 5. \u5237\u65b0\u6570\u636e\n    producer.flush();\n}", "CodeBleu_score": 63.91590202486641}, {"code": "public static void writeToKafka() throws InterruptedException{\n\tProperties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n    KafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n    MetricEvent event = new MetricEvent();\n    event.setTimestamp(System.currentTimeMillis());\n    event.setName(\"test\");\n    event.setTags(new HashMap<String, String>());\n    event.getTags().put(\"tag1\", \"value1\");\n    event.getTags().put(\"tag2\", \"value2\");\n    event.setFields(new HashMap<String, Double>());\n    event.getFields().put(\"field1\", 1.0);\n    event.getFields().put(\"field2\", 2.0);\n\n    String json = GsonUtil.toJson(event);\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, json);\n    producer.send(record);\n    producer.flush();\n\n    System.out.println(json);\n}", "CodeBleu_score": 55.05718064592345}]}
{"instance_id": "14bffe09c25a", "generation_result": [{"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (!isStarted()) {\n        return;\n    }\n    if (event.getLevel().isMoreSpecificThan(Level.getLevel(level)) && !event.getLoggerName().contains(\"org.apache.kafka\")) {\n        try {\n            LogEvent logEvent = new LogEvent();\n            logEvent.setAppId(appId);\n            logEvent.setContainerId(containerId);\n            logEvent.setContainerType(containerType);\n            logEvent.setHostname(InetAddress.getLocalHost().getHostName());\n            logEvent.setIp(nodeIp);\n            logEvent.setLevel(event.getLevel().toString());\n            logEvent.setLogger(event.getLoggerName());\n            logEvent.setMessage(event.getMessage().getFormattedMessage());\n            logEvent.setSource(source);\n            logEvent.setTaskName(taskName);\n            logEvent.setTaskId(taskId);\n            logEvent.setThreadName(event.getThreadName());\n            logEvent.setTime(event.getTimeMillis());\n            logEvent.setUuid(UUID.randomUUID().toString());\n            logEvent.setException(ExceptionUtil.getException(event.getThrown()));\n            String json = JacksonUtil.toJson(logEvent);\n            producer.send(new ProducerRecord<>(topic, json));\n        } catch (JsonProcessingException e) {\n            log.warn(\"Failed to convert LogEvent to json\", e);\n        } catch (Exception e) {\n            log.warn(\"Failed to send log to kafka\", e);\n        }\n    }\n}", "CodeBleu_score": 44.00240458858066}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event.getLevel().isMoreSpecificThan(Level.getLevel(level)) && !event.getLoggerName().contains(\"xxx\")) {\n        try {\n            producer.send(new ProducerRecord<>(topic, event.getMessage().getFormattedMessage()));\n        } catch (Exception e) {\n            log.warn(\"Error sending event to Kafka topic: {}\", topic, e);\n        }\n    }\n}", "CodeBleu_score": 53.62588448897391}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tif (event == null) {\n        return;\n    }\n    if (event.getLevel() != null && level.contains(event.getLevel().toString()) && !event.getLoggerName().contains(\"org.apache.kafka\")) {\n        LogEvent logEvent = new LogEvent();\n        logEvent.setAppId(appId);\n        logEvent.setContainerId(containerId);\n        logEvent.setContainerType(containerType);\n        logEvent.setLevel(event.getLevel().toString());\n        logEvent.setLoggerName(event.getLoggerName());\n        logEvent.setMessage(event.getMessage().getFormattedMessage());\n        logEvent.setSource(source);\n        logEvent.setTaskName(taskName);\n        logEvent.setTaskId(taskId);\n        logEvent.setNodeIp(nodeIp);\n        logEvent.setThreadName(event.getThreadName());\n        logEvent.setTimeStamp(event.getTimeMillis());\n        logEvent.setException(ExceptionUtil.getException(event.getThrown()));\n        try {\n            producer.send(new ProducerRecord<>(topic, UUID.randomUUID().toString(), JacksonUtil.toJson(logEvent)));\n        } catch (Exception e) {\n            log.warn(\"send log to kafka failed\", e);\n        }\n    }\n}", "CodeBleu_score": 51.02368401020026}]}
{"instance_id": "a9e0786fb5c1", "generation_result": [{"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, m := range ValidTopicSuffixMethods {\n\t\tif m == method {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"unknown topic suffix method %q\", method)\n}", "CodeBleu_score": 81.12543379061479}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif validMethod == method {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\"unknown topic suffix method: %s\", method)\n}", "CodeBleu_score": 87.87445529471773}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn fmt.Errorf(\"unknown topic suffix method: %s\", method)\n}", "CodeBleu_score": 92.40633567441876}]}
{"instance_id": "9b88d720d48a", "generation_result": [{"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif tag, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn tag, nil\n\t\t}\n\t}\n\n\tif k.RoutingKey == \"random\" {\n\t\treturn uuid.NewV4().String(), nil\n\t}\n\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 57.49773095504948}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif tagValue, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn tagValue, nil\n\t\t}\n\t}\n\n\tif k.RoutingKey == \"random\" {\n\t\tu, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn u.String(), nil\n\t}\n\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 86.44834969978524}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tif k.RoutingTag != \"\" {\n\t\tif routingKey, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn routingKey, nil\n\t\t}\n\t}\n\n\tif k.RoutingKey == \"random\" {\n\t\tu, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn u.String(), nil\n\t}\n\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 86.54330944132859}]}
{"instance_id": "dc35e80e7f8d", "generation_result": [{"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\t\t\\\"github.com/influxdata/telegraf/internal\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/common/kafka\\\"\n\t\\\"github.com/influxdata/telegraf/plugins/inputs\\\"", "CodeBleu_score": 8.515756470161907}]}
{"instance_id": "daf09ba144b2", "generation_result": [{"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\tsort.Strings(topicList)\n\tfingerprint := strings.Join(topicList, \\\";\\\")\n\tif fingerprint != k.fingerprint {\n\t\tk.Log.Infof(\\\"updating topics: replacing %q with %q\\\", k.allWantedTopics, topicList)\n\t}\n\tk.topicLock.Lock()\n\tk.fingerprint = fingerprint\n\tk.allWantedTopics = topicList\n\tk.topicLock.Unlock()\n\treturn nil\n}", "CodeBleu_score": 16.372749491099377}]}
{"instance_id": "8ecf2a1efead", "generation_result": [{"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\t\tk.topicLock.Unlock()\n\t\t\terr := k.consumer.Consume(ctx, topics, handler)\n\t\t\tif err != nil {\n\t\t\t\tacc.AddError(fmt.Errorf(\\\"consume: %w\\\", err))\n\t\t\t\tinternal.SleepContext(ctx, reconnectDelay) //nolint:errcheck // ignore returned error as we cannot do anything about it anyway\n\t\t\t}\n\t\t}", "CodeBleu_score": 37.72586693543955}]}
{"instance_id": "eba9dafa99ab", "generation_result": [{"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(!logkafka_id.equals(\".\"), LogkafkaIdIsDot)\n  checkCondition(!logkafka_id.equals(\"..\"), LogkafkaIdIsDotDot)\n  checkCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 84.36528019862268}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 74.29902955557642}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(logkafka_id != \".\", LogkafkaIdIsDot)\n  checkCondition(logkafka_id != \"..\", LogkafkaIdIsDotDot)\n  checkCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}", "CodeBleu_score": 81.61476592116341}]}
{"instance_id": "45d5aefd3f6c", "generation_result": [{"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tOffsetCacheMaxQueueSize = getPrefixedKey(\\\"offset-cache-max-queue-size\\\")\n  val KafkaAdminClientThreadPoolSize = getPrefixedKey(\\\"kafka-admin-client-thread-pool-size\\\")\n  val KafkaAdminClientMaxQueueSize = getPrefixedKey(\\\"kafka-admin-client-max-queue-size\\\")\n  val KafkaManagedOffsetMetadataCheckMillis = getPrefixedKey(\\\"kafka-managed-offset-metadata-check-millis\\\")\n  val KafkaManagedOffsetGroupCacheSize = getPrefixedKey(\\\"kafka-managed-offset-group-cache-size\\\")\n  val KafkaManagedOffsetGroupExpireDays = getPrefixedKey(\\\"kafka-managed-offset-group-expire-days\\\")\n\n  val DefaultConfig: Config = {\n    val defaults: Map[String, _ <: AnyRef] = Map(\n      BaseZkPath -> KafkaManagerActor.ZkRoot,\n      PinnedDispatchName -> \\\"pinned-dispatcher\\\",\n      BrokerViewUpdateSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }\n\n  private[this] val system = ActorSystem(\\\"kafka-manager-system\\\", akkaConfig)\n\n  private[this] val configWithDefaults = akkaConfig.withFallback(DefaultConfig)\n  val defaultTuning = ClusterTuning(\n    brokerViewUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , clusterManagerThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , clusterManagerThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , kafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , kafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaCommandThreadPoolSize = Option(configWithDefaults.getInt(ThreadPoolSize))\n    , logkafkaCommandThreadPoolQueueSize = Option(configWithDefaults.getInt(MaxQueueSize))\n    , logkafkaUpdatePeriodSeconds = Option(configWithDefaults.getInt(BrokerViewUpdateSeconds))\n    , partitionOffsetCacheTimeoutSecs = Option(configWithDefaults.getInt(PartitionOffsetCacheTimeoutSecs))\n    , brokerViewThreadPoolSize = Option(configWithDefaults.getInt(BrokerViewThreadPoolSize))\n    , brokerViewThreadPoolQueueSize = Option(configWithDefaults.getInt(BrokerViewMaxQueueSize))\n    , offsetCacheThreadPoolSize = Option(configWithDefaults.getInt(OffsetCacheThreadPoolSize))\n    , offsetCacheThreadPoolQueueSize = Option(configWithDefaults.getInt(OffsetCacheMaxQueueSize))\n    , kafkaAdminClientThreadPoolSize = Option(configWithDefaults.getInt(KafkaAdminClientThreadPoolSize))\n    , kafkaAdminClientThreadPoolQueueSize = Option(configWithDefaults.getInt(KafkaAdminClientMaxQueueSize))\n    , kafkaManagedOffsetMetadataCheckMillis = Option(configWithDefaults.getInt(KafkaManagedOffsetMetadataCheckMillis))\n    , kafkaManagedOffsetGroupCacheSize = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupCacheSize))\n    , kafkaManagedOffsetGroupExpireDays = Option(configWithDefaults.getInt(KafkaManagedOffsetGroupExpireDays))\n  )\n  private[this] val kafkaManagerConfig = {\n    val curatorConfig = CuratorConfig(configWithDefaults.getString(ZkHosts))\n    KafkaManagerActorConfig(\n      curatorConfig = curatorConfig\n      , baseZkPath = configWithDefaults.getString(BaseZkPath)\n      , pinnedDispatcherName = configWithDefaults.getString(PinnedDispatchName)\n      , startDelayMillis = configWithDefaults.getLong(StartDelayMillis)\n      , threadPoolSize = configWithDefaults.getInt(ThreadPoolSize)\n      , mutexTimeoutMillis = configWithDefaults.getInt(MutexTimeoutMillis)\n      , maxQueueSize = configWithDefaults.getInt(MaxQueueSize)\n      , kafkaManagerUpdatePeriod = FiniteDuration(configWithDefaults.getInt(KafkaManagerUpdateSeconds), SECONDS)\n      , deleteClusterUpdatePeriod = FiniteDuration(configWithDefaults.getInt(DeleteClusterUpdateSeconds), SECONDS)\n      , deletionBatchSize = configWithDefaults.getInt(DeletionBatchSize)\n      , clusterActorsAskTimeoutMillis = configWithDefaults.getInt(ClusterActorsAskTimeoutMillis)\n      , simpleConsumerSocketTimeoutMillis =  configWithDefaults.getInt(SimpleConsumerSocketTimeoutMillis)\n      , defaultTuning = defaultTuning\n      , consumerProperties = getConsumerPropertiesFromConfig(configWithDefaults)\n    )\n  }\n\n  private[this] val props = Props(classOf[KafkaManagerActor], kafkaManagerConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n\n\n", "CodeBleu_score": 13.234885015896749}]}
{"instance_id": "a535ce861222", "generation_result": [{"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\tSeconds -> \\\"30\\\",\n      KafkaManagerUpdateSeconds -> \\\"10\\\",\n      DeleteClusterUpdateSeconds -> \\\"10\\\",\n      DeletionBatchSize -> \\\"2\\\",\n      MaxQueueSize -> \\\"100\\\",\n      ThreadPoolSize -> \\\"2\\\",\n      MutexTimeoutMillis -> \\\"4000\\\",\n      StartDelayMillis -> \\\"1000\\\",\n      ApiTimeoutMillis -> \\\"5000\\\",\n      ClusterActorsAskTimeoutMillis -> \\\"2000\\\",\n      PartitionOffsetCacheTimeoutSecs -> \\\"5\\\",\n      SimpleConsumerSocketTimeoutMillis -> \\\"10000\\\",\n      BrokerViewThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      BrokerViewMaxQueueSize -> \\\"1000\\\",\n      OffsetCacheThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      OffsetCacheMaxQueueSize -> \\\"1000\\\",\n      KafkaAdminClientThreadPoolSize -> Runtime.getRuntime.availableProcessors().toString,\n      KafkaAdminClientMaxQueueSize -> \\\"1000\\\",\n      KafkaManagedOffsetMetadataCheckMillis -> KafkaManagedOffsetCacheConfig.defaultGroupMemberMetadataCheckMillis.toString,\n      KafkaManagedOffsetGroupCacheSize -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetMaxSize.toString,\n      KafkaManagedOffsetGroupExpireDays -> KafkaManagedOffsetCacheConfig.defaultGroupTopicPartitionOffsetExpireDays.toString\n    )\n    import scala.collection.JavaConverters._\n    ConfigFactory.parseMap(defaults.asJava)\n  }", "CodeBleu_score": 12.185481592529987}]}
{"instance_id": "ad37d3914acc", "generation_result": [{"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 24.21498397609716}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 24.21498397609716}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\tConfig)\n\n  private[this] val kafkaManagerActor: ActorPath = system.actorOf(props, \\\"kafka-manager\\\").path\n\n  private[this] val apiExecutor = new ThreadPoolExecutor(\n    kafkaManagerConfig.threadPoolSize,\n    kafkaManagerConfig.threadPoolSize,\n    0L,\n    TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue[Runnable](kafkaManagerConfig.maxQueueSize)\n  )\n\n  private[this] val apiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n\n\n", "CodeBleu_score": 24.21498397609716}]}
{"instance_id": "3ecdb2c1534f", "generation_result": [{"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\tapiExecutionContext = ExecutionContext.fromExecutor(apiExecutor)\n\n  private[this] implicit val apiTimeout: Timeout = FiniteDuration(\n    configWithDefaults.getInt(ApiTimeoutMillis),\n    MILLISECONDS\n  )\n\n  private[this] def getConsumerPropertiesFromConfig(config: Config) : Option[Properties] = {\n    if(config.hasPath(ConsumerPropertiesFile)) {\n      val filePath = config.getString(ConsumerPropertiesFile)\n      val file = new java.io.File(filePath)\n      if(file.isFile & file.canRead) {\n        val props = new Properties()\n        props.load(new java.io.FileInputStream(file))\n        return Option(props)\n      } else {\n        warn(s\\\"Failed to find consumer properties file or file is not readable : $file\\\")\n      }\n    }\n    None\n  }\n\n  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse =>\n        error(s\\\"Failed on input : $msg\\\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) =>\n            error(s\\\"Failed on input : $msg\\\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\\\"Failed on input : $msg\\\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n\n\n", "CodeBleu_score": 18.102227863402803}]}
{"instance_id": "5955ade118b9", "generation_result": [{"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\t   (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover { case t: Throwable =>\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }\n\n  private[this] def toDisjunction[T](t: Try[T]): ApiError \\/ T = {\n    t match {\n      case Failure(th) =>\n        -\\/(th)\n      case Success(tInst) =>\n        \\/-(tInst)\n    }\n  }\n\n  def shutdown(): Unit = {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).tell(KMShutdown, system.deadLetters)\n    Try(Await.ready(system.terminate(), Duration(30, TimeUnit.SECONDS)))\n    apiExecutor.shutdown()\n  }\n\n  //--------------------Commands--------------------------\n  def addCluster(clusterName: String,\n                 version: String,\n                 zkHosts: String,\n                 jmxEnabled: Boolean,\n                 jmxUser: Option[String],\n                 jmxPass: Option[String],\n                 jmxSsl: Boolean,\n                 pollConsumers: Boolean,\n                 filterConsumers: Boolean,\n                 tuning: Option[ClusterTuning],\n                 securityProtocol: String,\n                 saslMechanism: Option[String],\n                 jaasConfig: Option[String],\n                 logkafkaEnabled: Boolean = false,\n                 activeOffsetCacheEnabled: Boolean = false,\n                 displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMAddCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def updateCluster(clusterName: String,\n                    version: String,\n                    zkHosts: String,\n                    jmxEnabled: Boolean,\n                    jmxUser: Option[String],\n                    jmxPass: Option[String],\n                    jmxSsl: Boolean,\n                    pollConsumers: Boolean,\n                    filterConsumers: Boolean,\n                    tuning: Option[ClusterTuning],\n                    securityProtocol: String,\n                    saslMechanism: Option[String],\n                    jaasConfig: Option[String],\n                    logkafkaEnabled: Boolean = false,\n                    activeOffsetCacheEnabled: Boolean = false,\n                    displaySizeEnabled: Boolean = false): Future[ApiError \\/ Unit] =\n  {\n    val cc = ClusterConfig(\n      clusterName,\n      version,\n      zkHosts,\n      tuning = tuning,\n      securityProtocol = securityProtocol,\n      saslMechanism = saslMechanism,\n      jaasConfig = jaasConfig,\n      jmxEnabled = jmxEnabled,\n      jmxUser = jmxUser,\n      jmxPass = jmxPass,\n      jmxSsl = jmxSsl,\n      pollConsumers = pollConsumers,\n      filterConsumers = filterConsumers,\n      logkafkaEnabled = logkafkaEnabled,\n      activeOffsetCacheEnabled = activeOffsetCacheEnabled,\n      displaySizeEnabled = displaySizeEnabled)\n    tryWithKafkaManagerActor(KMUpdateCluster(cc)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def disableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDisableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def enableCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMEnableCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def deleteCluster(clusterName: String): Future[ApiError \\/ Unit] = {\n    tryWithKafkaManagerActor(KMDeleteCluster(clusterName)) { result: KMCommandResult =>\n      result.result.get\n    }\n  }\n\n  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }\n\n  private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n    implicit val ec = apiExecutionContext\n\n    getTopicList(clusterName).flatMap { errorOrTopicList =>\n      errorOrTopicList.fold({ e =>\n        Future.successful(-\\/(e))\n      }, { topicList =>\n        runPreferredLeaderElection(clusterName, topicList.list.toSet)\n      })\n    }\n  }\n\n  private def updateSchedulePreferredLeaderElection(clusterName: String): Unit = {\n    system.actorSelection(kafkaManagerActor).ask(KMClusterCommandRequest(\n      clusterName,\n      CMSchedulePreferredLeaderElection(\n        pleCancellable map { case (key, value) => (key, value._2) }\n      )\n    ))\n  }\n\n  def schedulePreferredLeaderElection(clusterName: String, topics: Set[String], timeIntervalMinutes: Int): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable += (clusterName ->\n      (\n        Some(\n          system.scheduler.schedule(0 seconds, Duration(timeIntervalMinutes, TimeUnit.MINUTES)) {\n            runPreferredLeaderElectionWithAllTopics(clusterName)\n          }\n        ),\n        timeIntervalMinutes\n      )\n    )\n    updateSchedulePreferredLeaderElection(clusterName)\n\n    Future(\\\"Scheduler started\\\")\n  }\n\n  def cancelPreferredLeaderElection(clusterName: String): Future[String] = {\n    implicit val ec = apiExecutionContext\n\n    pleCancellable(clusterName)._1.map(_.cancel())\n    pleCancellable -= clusterName\n    updateSchedulePreferredLeaderElection(clusterName)\n    Future(\\\"Scheduler stopped\\\")\n  }\n\n\n\n", "CodeBleu_score": 32.30124579764787}]}
{"instance_id": "26fbc2ba1d28", "generation_result": [{"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\tn                    return;\n                }", "CodeBleu_score": 2.9067510692205016}]}
{"instance_id": "adf40b68c93e", "generation_result": [{"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}, {"code": "private static Validator forKafka(final Validator validator){\n\t'{}' should be '{}' but is '{}'\\\", topicName, RETENTION_BYTES_NAME, UNLIMITED_VALUE,\n                            retentionBytes);\n                    return;\n                }", "CodeBleu_score": 13.767520100688197}]}
{"instance_id": "07ff1ca41c93", "generation_result": [{"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\t// if the topic.creation.enable property is set to false, return false\n    if (config.containsKey(\"topic.creation.enable\") && Boolean.FALSE.equals(config.get(\"topic.creation.enable\"))) {\n        return false;\n    }\n\n    // if the topic.creation.enable property is set to true, return true\n    if (config.containsKey(\"topic.creation.enable\") && Boolean.TRUE.equals(config.get(\"topic.creation.enable\"))) {\n        return true;\n    }\n\n    // if the topic.creation.enable property is not set, check the Kafka Connect version\n    final Version kafkaVersion = parseVersion(AppInfoParser.getVersion());\n    if (kafkaVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0) {\n        return true;\n    }\n\n    // if the Kafka Connect version is not compatible with topic creation, return false\n    return false;\n}", "CodeBleu_score": 42.31543018321899}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\t// Kafka Connect versions >= 2.6.0 support topic creation\n    Version currentVersion = parseVersion(AppInfoParser.getVersion());\n    if (currentVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0) {\n        // if the property is not set, default to true\n        return config.containsKey(\"topic.creation.enable\") ? Boolean.parseBoolean(config.get(\"topic.creation.enable\").toString()) : true;\n    }\n    return false;\n}", "CodeBleu_score": 36.47290823187298}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tVersion kafkaVersion = parseVersion(AppInfoParser.getVersion());\n    if (kafkaVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0) {\n        Object topicCreationEnabled = config.getOrDefault(\"topic.creation.enable\", true);\n        return Boolean.TRUE.equals(topicCreationEnabled);\n    }\n    return false;\n}", "CodeBleu_score": 42.375915864852374}]}
{"instance_id": "e2cc420e1e63", "generation_result": [{"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tString clientId = config.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(\"Missing required configuration property \" + CLIENT_ID_CONFIG);\n    }\n\n    Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, clientId + \"-admin\");\n    SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n\n    Converter converter = converterForOffsetStore();\n    return new KafkaOffsetBackingStore(admin, clientId, converter);\n}", "CodeBleu_score": 38.171409076503885}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tMap<String, Object> adminConfig = new HashMap<>(config);\n    adminConfig.put(CLIENT_ID_CONFIG, \"debezium-offset-admin\");\n    adminConfig.put(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, DistributedConfig.DEFAULT_OFFSET_STORAGE_TOPIC);\n    SharedTopicAdmin admin = new SharedTopicAdmin(adminConfig);\n    String clientId = config.get(CLIENT_ID_CONFIG);\n    if (clientId == null) {\n        throw new DebeziumException(\"Kafka Connect client ID is not set\");\n    }\n    return new KafkaOffsetBackingStore(admin, clientId, converterForOffsetStore());\n}", "CodeBleu_score": 35.964005050799464}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tfinal String clientId = DistributedConfig.CLIENT_ID_CONFIG.defaultValue();\n    final Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, clientId);\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n    return new KafkaOffsetBackingStore(admin, clientId, converterForOffsetStore());\n}", "CodeBleu_score": 33.44534734881259}]}
{"instance_id": "4ecb2279231e", "generation_result": [{"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        JsonNode payload = node.get(PAYLOAD_FIELD);\n\n        // Schema + payload format\n        if (payload != null) {\n            node = payload;\n        }\n\n        // Debezium envelope\n        if (config.asEnvelope()) {\n            return reader.readValue(node);\n        }\n        else if (node.has(Envelope.FieldName.SOURCE) && node.has(config.sourceField())) {\n            return reader.readValue(node.get(config.sourceField()));\n        }\n        // Extracted format\n        else {\n            return reader.readValue(node);\n        }\n    }\n    else {\n        return reader.readValue(node);\n    }\n}", "CodeBleu_score": 40.565499636124386}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        JsonNode payload = node.get(PAYLOAD_FIELD);\n\n        // Schema + payload format\n        if (payload != null) {\n            node = payload;\n        }\n        // Debezium envelope\n        else if (config.asEnvelope()) {\n            return reader.readValue(node);\n        }\n        else if (node.has(Envelope.FieldName.SOURCE) && node.has(config.sourceField())) {\n            return reader.readValue(node.get(config.sourceField()));\n        }\n        // Extracted format\n        else {\n            return reader.readValue(node);\n        }\n    }\n\n    return null;\n}", "CodeBleu_score": 40.87326655122756}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tif (node.isObject()) {\n        JsonNode payload = node.get(PAYLOAD_FIELD);\n\n        // Schema + payload format\n        if (payload != null) {\n            node = payload;\n        }\n        // Debezium envelope\n        if (config.asEnvelope()) {\n            return reader.readValue(node);\n        }\n        else if (node.has(Envelope.FieldName.SOURCE) && node.has(config.sourceField())) {\n            return reader.readValue(node.get(config.sourceField()));\n        }\n        // Extracted format\n        else {\n            return reader.readValue(node);\n        }\n    }\n    return null;\n}", "CodeBleu_score": 41.04340385312044}]}
{"instance_id": "b8ba1bfc7b5e", "generation_result": [{"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < topicName.length(); i++) {\n        char c = topicName.charAt(i);\n        if (isInvalidTopicNameChar(c)) {\n            sb.append(REPLACEMENT_CHAR);\n            LOGGER.warn(\"Replacing invalid topic name character '{}' with '{}'\", c, REPLACEMENT_CHAR);\n        }\n        else {\n            sb.append(c);\n        }\n    }\n    if (sb.length() > 0) {\n        LOGGER.warn(\"Sanitizing topic name '{}' to '{}'\", topicName, sb.toString());\n        return sb.toString();\n    }\n    return topicName;\n}", "CodeBleu_score": 54.831582026197204}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sanitizedTopicName = new StringBuilder();\n    for (int i = 0; i < topicName.length(); i++) {\n        char c = topicName.charAt(i);\n        if (isValidTopicNameChar(c)) {\n            sanitizedTopicName.append(c);\n        }\n        else {\n            sanitizedTopicName.append(REPLACEMENT_CHAR);\n            LOGGER.warn(\"Replacing invalid character '{}' in topic name '{}' with '{}'\", c, topicName, REPLACEMENT_CHAR);\n        }\n    }\n    if (sanitizedTopicName.length() == topicName.length()) {\n        return topicName;\n    }\n    else {\n        return sanitizedTopicName.toString();\n    }\n}", "CodeBleu_score": 57.41726924769852}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sanitizedName = new StringBuilder();\n    boolean replaced = false;\n    for (char c : topicName.toCharArray()) {\n        if (isValidTopicNameCharacter(c)) {\n            sanitizedName.append(c);\n        } else {\n            replaced = true;\n            sanitizedName.append(REPLACEMENT_CHAR);\n        }\n    }\n    if (replaced) {\n        LOGGER.warn(\"Topic name '{}' contains invalid characters and was sanitized to '{}'\", topicName, sanitizedName);\n        return sanitizedName.toString();\n    }\n    return topicName;\n}", "CodeBleu_score": 48.53433639768588}]}
{"instance_id": "9cf0d74c098d", "generation_result": [{"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n    this.queue = queue;\n    this.producerThrowable = new AtomicReference<>();\n    this.maxRetries = connectorConfig.getMaxRetries();\n    if (replacedErrorHandler != null) {\n        this.retries = replacedErrorHandler.retries;\n    }\n}", "CodeBleu_score": 67.79357980006915}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n        this.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n\n        if (replacedErrorHandler != null) {\n            this.maxRetries = replacedErrorHandler.maxRetries;\n            this.retries = replacedErrorHandler.retries;\n        }\n        else {\n            this.maxRetries = connectorConfig.getInteger(ErrorHandler.CONNECTOR_MAX_RETRIES);\n            this.retries = 0;\n        }\n\n        if (maxRetries == RETRIES_DISABLED) {\n            LOGGER.info(\\\"Retries are disabled\\\");\n        }\n        else if (maxRetries == RETRIES_UNLIMITED) {\n            LOGGER.info(\\\"Retries are unlimited\\\");\n        }\n        else {\n            LOGGER.info(\\\"Maximum number of retries is {}\\\", maxRetries);\n        }\n    }", "CodeBleu_score": 61.74109298774938}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\n        this.queue = queue;\n        this.producerThrowable = new AtomicReference<>();\n        this.maxRetries = connectorConfig.getInt(ErrorHandler.CONNECTOR_MAX_RETRIES_CONFIG);\n        if (replacedErrorHandler != null) {\n            this.retries = replacedErrorHandler.retries;\n        }\n    }", "CodeBleu_score": 65.11209324552763}]}
{"instance_id": "8ab4bf3a4dd8", "generation_result": [{"code": "public void validate(Configuration configuration, Field.Set fields){\n\tfinal ConfigValue.ConfigValueValidity validity = configuration.validate(fields);\n    if (!validity.isValid()) {\n        final Map<String, ConfigValue> configValues = validity.getConfigValues();\n        final ConfigValue firstInvalidValue = configValues.values().stream()\n                .filter(configValue -> !configValue.errorMessages().isEmpty())\n                .findFirst()\n                .orElseThrow(() -> new RuntimeException(\"No invalid config values found\"));\n        throw new ConfigException(firstInvalidValue.name(), firstInvalidValue.errorMessages().get(0));\n    }\n}", "CodeBleu_score": 39.67448566009486}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tfinal Map<String, ConfigValue> configValues = configuration.valuesWithPrefix(\"transforms.\" + getName());\n    final ConfigValue configValue = configValues.get(\"transforms.\" + getName());\n    final ConfigException configException = new ConfigException(configValue, \"\");\n    for (Field field : fields) {\n        final ConfigValue configValueForField = configValues.get(field.name());\n        if (configValueForField != null) {\n            for (String errorMessage : configValueForField.errorMessages()) {\n                configException.addErrorMessage(errorMessage);\n            }\n        }\n    }\n    if (configException.errorMessages().size() > 0) {\n        throw configException;\n    }\n}", "CodeBleu_score": 39.10712304054653}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\tfinal Map<String, ConfigValue> configValues = configuration.validate(fields);\n    for (Map.Entry<String, ConfigValue> entry : configValues.entrySet()) {\n        final ConfigValue configValue = entry.getValue();\n        if (configValue.errorMessages().size() > 0) {\n            throw new ConfigException(configValue.name(), configValue.errorMessages());\n        }\n    }\n}", "CodeBleu_score": 54.32695154523815}]}
{"instance_id": "860405dc9abb", "generation_result": [{"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t.withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\\\"Header names in the record whose values are to be copied or moved to record value.\\\")\n            .required();\n\n    public static final Field FIELDS_FIELD = Field.create(FIELDS_CONF)\n            .withDisplayName(\\\"Field names list\\\")\n            .withType(ConfigDef.Type.LIST)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withValidation(\n                    Field::notContainSpaceInAnyElement,\n                    Field::notContainEmptyElements)\n            .withDescription(\n                    \\\"Field names, in the same order as the header names listed in the headers configuration property. Supports Struct nesting using dot notation.\\\")\n            .required();\n\n    public static final Field OPERATION_FIELD = Field.create(OPERATION_CONF)\n            .withDisplayName(\\\"Operation: mover or copy\\\")\n            .withType(ConfigDef.Type.STRING)\n            .withEnum(Operation.class)\n            .withImportance(ConfigDef.Importance.HIGH)\n            .withDescription(\\\"Either <code>move</code> if the fields are to be moved to the value (removed from the headers), \\\" +\n                    \\\"or <code>copy</code> if the fields are to be copied to the value (retained in the headers).\\\")\n            .required();\n\n    private List<String> fields;\n\n    private List<String> headers;\n\n    private Operation operation;\n\n    private final BoundedConcurrentHashMap<Schema, Schema> schemaUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n    private final BoundedConcurrentHashMap<Headers, Headers> headersUpdateCache = new BoundedConcurrentHashMap<>(CACHE_SIZE);\n\n    @Override\n    public ConfigDef config() {\n\n        final ConfigDef config = new ConfigDef();\n        Field.group(config, null, HEADERS_FIELD, FIELDS_FIELD, OPERATION_FIELD);\n        return config;\n    }\n\n    @Override\n    public void configure(Map<String, ?> props) {\n\n        final Configuration config = Configuration.from(props);\n        SmtManager<R> smtManager = new SmtManager<>(config);\n        smtManager.validate(config, Field.setOf(FIELDS_FIELD, HEADERS_FIELD, OPERATION_FIELD));\n\n        fields = config.getList(FIELDS_FIELD);\n        headers = config.getList(HEADERS_FIELD);\n\n        validateConfiguration();\n\n        operation = Operation.fromName(config.getString(OPERATION_FIELD));\n    }\n\n    private void validateConfiguration() {\n\n        if (headers.size() != fields.size()) {\n            throw new ConfigException(format(\\\"'%s' config must have the same number of elements as '%s' config.\\\",\n                    FIELDS_FIELD, HEADERS_FIELD));\n        }\n    }\n\n    @Override\n    public R apply(R record) {\n\n        if (record.value() == null) {\n            LOGGER.trace(\\\"Tombstone {} arrived and will be skipped\\\", record.key());\n            return record;\n        }\n\n        final Struct value = requireStruct(record.value(), \\\"Header field insertion\\\");\n\n        LOGGER.trace(\\\"Processing record {}\\\", value);\n        Map<String, Header> headerToProcess = StreamSupport.stream(record.headers().spliterator(), false)\n                .filter(header -> headers.contains(header.key()))\n                .collect(Collectors.toMap(Header::key, Function.identity()));\n\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.trace(\\\"Header to be processed: {}\\\", headersToString(headerToProcess));\n        }\n\n        if (headerToProcess.isEmpty()) {\n            return record;\n        }\n\n        Schema updatedSchema = schemaUpdateCache.computeIfAbsent(value.schema(), valueSchema -> makeNewSchema(valueSchema, headerToProcess));\n\n        LOGGER.trace(\\\"Updated schema fields: {}\\\", updatedSchema.fields());\n\n        Struct updatedValue = makeUpdatedValue(value, headerToProcess, updatedSchema);\n\n        LOGGER.trace(\\\"Updated value: {}\\\", updatedValue);\n\n        Headers updatedHeaders = record.headers();\n        if (MOVE.equals(operation)) {\n            updatedHeaders = headersUpdateCache.computeIfAbsent(record.headers(), this::removeHeaders);\n        }\n\n        return record.newRecord(\n                record.topic(),\n                record.kafkaPartition(),\n                record.keySchema(),\n                record.key(),\n                updatedSchema,\n                updatedValue,\n                record.timestamp(),\n                updatedHeaders);\n    }\n\n    private Headers removeHeaders(Headers originalHeaders) {\n\n        Headers updatedHeaders = originalHeaders.duplicate();\n        headers.forEach(updatedHeaders::remove);\n\n        return updatedHeaders;\n    }\n\n    private Struct makeUpdatedValue(Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildUpdatedValue(ROOT_FIELD_NAME, originalValue, headerToProcess, updatedSchema, nestedFields, 0);\n    }\n\n    private Struct buildUpdatedValue(String fieldName, Struct originalValue, Map<String, Header> headerToProcess, Schema updatedSchema, List<String> nestedFields,\n                                     int level) {\n\n        Struct updatedValue = new Struct(updatedSchema);\n        for (org.apache.kafka.connect.data.Field field : originalValue.schema().fields()) {\n            if (originalValue.get(field) != null) {\n                if (isContainedIn(field.name(), nestedFields)) {\n                    Struct nestedField = requireStruct(originalValue.get(field), \\\"Nested field\\\");\n                    updatedValue.put(field.name(),\n                            buildUpdatedValue(field.name(), nestedField, headerToProcess, updatedSchema.field(field.name()).schema(), nestedFields, ++level));\n                }\n                else {\n                    updatedValue.put(field.name(), originalValue.get(field));\n                }\n            }\n        }\n\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n\n            if (currentHeader != null) {\n                Optional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n                fieldNameToAdd.ifPresent(s -> updatedValue.put(s, currentHeader.value()));\n            }\n        }\n\n        return updatedValue;\n    }\n\n    private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }\n\n    private Schema makeNewSchema(Schema oldSchema, Map<String, Header> headerToProcess) {\n\n        List<String> nestedFields = fields.stream().filter(field -> field.contains(NESTING_SEPARATOR)).collect(Collectors.toList());\n\n        return buildNewSchema(ROOT_FIELD_NAME, oldSchema, headerToProcess, nestedFields, 0);\n    }\n\n\n\n", "CodeBleu_score": 24.71173767990796}]}
{"instance_id": "681a0415df83", "generation_result": [{"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\n        return null;\n    }\n    try {\n        String snapshotString = source.getString(SNAPSHOT_KEY);\n        return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n    }\n    catch (Exception e) {\n        return null;\n    }\n}", "CodeBleu_score": 30.723807120057927}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source.schema().field(SNAPSHOT_KEY) == null) {\n        return null;\n    }\n    String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 32.71169980772373}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source.schema().field(SNAPSHOT_KEY) == null) {\n        return null;\n    }\n    String snapshotString = source.getString(SNAPSHOT_KEY);\n    if (snapshotString == null) {\n        return null;\n    }\n    return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n}", "CodeBleu_score": 32.71169980772373}]}
{"instance_id": "85ef1162e38a", "generation_result": [{"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\ttry {\n    String[] versionParts = version.split(\"\\\\.\");\n    if (versionParts.length > 2) {\n      versionParts = Arrays.copyOfRange(versionParts, 0, 2);\n    }\n    String[] versionPart = versionParts[0].split(\"-\");\n    return Optional.of(Float.parseFloat(versionPart[0]));\n  } catch (Exception e) {\n    return Optional.empty();\n  }\n}", "CodeBleu_score": 62.49925697314755}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tif (version == null) {\n    return Optional.empty();\n  }\n  String[] splitVersion = version.split(\"\\\\.\");\n  if (splitVersion.length > 2) {\n    splitVersion = Arrays.copyOf(splitVersion, 2);\n  }\n  String[] splitHyphen = splitVersion[1].split(\"-\");\n  return Optional.of(Float.parseFloat(splitHyphen[0]));\n}", "CodeBleu_score": 49.623255878583386}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\tString[] versionSplit = version.split(\"\\\\.\");\n  if (versionSplit.length > 2) {\n    versionSplit = new String[] {versionSplit[0], versionSplit[1]};\n  }\n  String[] hyphenSplit = versionSplit[1].split(\"-\");\n  return Optional.of(Float.parseFloat(hyphenSplit[0]));\n}", "CodeBleu_score": 46.373630877102286}]}
{"instance_id": "64346d69eba0", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.flatMap(c -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, c)))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 71.40535793143783}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.flatMap(c -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, c)))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 71.40535793143783}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context)\n        .then(connector.flatMap(connectorDTO -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, connectorDTO)))\n        .map(ResponseEntity::ok)\n        .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 70.65443506466787}]}
{"instance_id": "4c446fd64db6", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n        .connector(connectorName)\n        .operationName(\"deleteConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 81.14430364663718}]}
{"instance_id": "d34a49f3ca1b", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t;", "CodeBleu_score": 6.683033777000043}]}
{"instance_id": "199cfe41b374", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\tConnectService.getConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 24.87195720779858}]}
{"instance_id": "88380ae300c4", "generation_result": [{"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder()\n      .sources(KafkaUiApplication.class)\n      .initializers(new DynamicConfigOperations())\n      .run(args);\n}", "CodeBleu_score": 35.36337410242643}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n      .initializers(new DynamicConfigOperations())\n      .run(args);\n}", "CodeBleu_score": 40.8226072471737}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\tSpringApplicationBuilder builder = new SpringApplicationBuilder(KafkaUiApplication.class);\n  builder.initializers(new DynamicConfigOperations());\n  return builder.run(args);\n}", "CodeBleu_score": 44.03273262771223}]}
{"instance_id": "3f022876f737", "generation_result": [{"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tif (search == null) {\n    return (connector) -> true;\n  }\n  return (connector) -> {\n    String connectorName = connector.getConnector().getName();\n    String connectorConfig = connector.getConfig().getConfig();\n    String connectorTasks = connector.getTasks().stream().map(TaskDTO::getId).collect(Collectors.joining());\n    String connectorTopics = connector.getTopics().stream().map(ConnectorTopics::getTopic).collect(Collectors.joining());\n    return Stream.of(connectorName, connectorConfig, connectorTasks, connectorTopics)\n        .anyMatch(str -> StringUtils.containsIgnoreCase(str, search));\n  };\n}", "CodeBleu_score": 47.80142210294278}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tif (search == null) {\n    return (connector) -> true;\n  } else {\n    return (connector) -> {\n      String connectorName = connector.getName();\n      String connectorClass = connector.getConnector().getConnectorClass();\n      String connectorTopics = connector.getTopics().toString();\n      String connectorConfig = connector.getConfig().toString();\n      return StringUtils.containsIgnoreCase(connectorName, search)\n          || StringUtils.containsIgnoreCase(connectorClass, search)\n          || StringUtils.containsIgnoreCase(connectorTopics, search)\n          || StringUtils.containsIgnoreCase(connectorConfig, search);\n    };\n  }\n}", "CodeBleu_score": 51.634920902429535}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\treturn search == null\n      ? (connectorInfo) -> true\n      : (connectorInfo) -> {\n    final String connectorName = connectorInfo.getName();\n    final String connectorType = connectorInfo.getType();\n    final String connectorTopic = connectorInfo.getTopics().stream()\n        .map(topic -> topic.getName())\n        .collect(Collectors.joining(\", \"));\n    final String connectorConfig = connectorInfo.getConfig().entrySet().stream()\n        .map(entry -> entry.getKey() + \"=\" + entry.getValue())\n        .collect(Collectors.joining(\", \"));\n    return StringUtils.containsIgnoreCase(connectorName, search)\n        || StringUtils.containsIgnoreCase(connectorType, search)\n        || StringUtils.containsIgnoreCase(connectorTopic, search)\n        || StringUtils.containsIgnoreCase(connectorConfig, search);\n  };\n}", "CodeBleu_score": 40.01405810254148}]}
{"instance_id": "9754aa1ec96c", "generation_result": [{"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return null;\n    }\n    Map<String, Object> flattened = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      String newKey = prefix == null ? key : prefix + \".\" + key;\n      if (value instanceof Map) {\n        flattened.putAll(flattenClusterProperties(newKey, (Map<String, Object>) value));\n      } else {\n        flattened.put(newKey, value);\n      }\n    }\n    return flattened;\n  }", "CodeBleu_score": 56.37154432732704}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tif (propertiesMap == null) {\n      return new HashMap<>();\n    }\n\n    Map<String, Object> flattenedProperties = new HashMap<>();\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String key = entry.getKey();\n      Object value = entry.getValue();\n      String newKey = prefix == null ? key : prefix + \".\" + key;\n      if (value instanceof Map) {\n        flattenedProperties.putAll(flattenClusterProperties(newKey, (Map<String, Object>) value));\n      } else {\n        flattenedProperties.put(newKey, value);\n      }\n    }\n    return flattenedProperties;\n  }", "CodeBleu_score": 52.210267763444186}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\tMap<String, Object> flattened = new HashMap<>();\n    if (propertiesMap == null) {\n      return flattened;\n    }\n    for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n      String newKey = prefix == null ? entry.getKey() : prefix + \".\" + entry.getKey();\n      if (entry.getValue() instanceof Map) {\n        flattened.putAll(flattenClusterProperties(newKey, (Map<String, Object>) entry.getValue()));\n      } else {\n        flattened.put(newKey, entry.getValue());\n      }\n    }\n    return flattened;\n  }", "CodeBleu_score": 50.35720430842665}]}
{"instance_id": "99266677c846", "generation_result": [{"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n    configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_KINIT_CMD);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER);\n    configDef.define(SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN);\n    configDef.define(SaslConfigs.SASL_MECHANISM);\n    configDef.define(SaslConfigs.SASL_KERBEROS_SERVICE_NAME);\n    configDef.define(SaslConfigs.SASL_KERBEROS_KINIT_CMD);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER);\n    configDef.define(SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN);\n    configDef.define(SaslConfigs.SASL_MECHANISM);\n    configDef.define(SaslConfigs.S", "CodeBleu_score": 43.02902539062665}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tConfigDef configDef = new ConfigDef();\n    configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);\n    configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n    configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_KINIT_CMD);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN);\n    configDef.define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_RETRY_BACKOFF_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_PRINCIPAL_TO_LOCAL_RULES_CONFIG);\n    configDef.define(SaslConfigs.SASL_KERBEROS_KINIT_MAX_ATTEMPTS);\n    configDef.define(SaslConfigs.SASL_KERBEROS_SERVICE_NAME);\n    configDef.define(SaslConfigs.SASL_KERBEROS_KINIT_TIMEOUT);\n    configDef.define(SaslConfigs.SASL_KERBEROS_RELOGIN_MAX_BACKOFF_MS);\n    configDef.define(SaslConfigs.SASL_KERBEROS_RELOGIN_MAX_TIMEOUT_MS);\n    configDef.define(SaslConfigs.SASL_KERBEROS_RELOGIN_TIMEOUT_MS);", "CodeBleu_score": 43.07048653995367}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tvar configDef = new ConfigDef();\n  configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);\n  configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_JAAS_CONFIG);\n  configDef.define(SaslConfigs.SASL_", "CodeBleu_score": 40.91043043556957}]}
{"instance_id": "deebb6fcb0a8", "generation_result": [{"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n  return adminClient.isAclViewEnabled() &&\n    authorizedOperations.contains(AclOperation.ALL) &&\n    authorizedOperations.contains(AclOperation.ALTER)\n    ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT)\n    : Mono.empty();\n}", "CodeBleu_score": 42.58093370760034}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n  return adminClient.isAclViewEnabled() &&\n      authorizedOperations.contains(AclOperation.ALL) &&\n      authorizedOperations.contains(AclOperation.ALTER)\n      ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT)\n      : Mono.empty();\n}", "CodeBleu_score": 43.01710160652679}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n  return adminClient.isAclViewEnabled() &&\n      authorizedOperations.contains(AclOperation.ALTER) &&\n      authorizedOperations.contains(AclOperation.ALL)\n      ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT)\n      : Mono.empty();\n}", "CodeBleu_score": 42.97713027577839}]}
{"instance_id": "a5d8cc77955f", "generation_result": [{"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tMap<TopicPartition, Long> offsets = new TreeMap<>(prevRange.comparator());\n  Map<TopicPartition, Long> fromOffsets = new TreeMap<>(prevRange.comparator());\n  Map<TopicPartition, Long> toOffsets = new TreeMap<>(prevRange.comparator());\n  Map<TopicPartition, Long> fromOffsetsToSeek = new TreeMap<>(prevRange.comparator());\n\n  for (Map.Entry<TopicPartition, FromToOffset> entry : prevRange.entrySet()) {\n    TopicPartition topicPartition = entry.getKey();\n    FromToOffset prevRangeForPartition = entry.getValue();\n    long fromOffset = prevRangeForPartition.from();\n    long toOffset = prevRangeForPartition.to();\n    long messagesToPoll = messagesPerPage;\n\n    if (fromOffset == 0) {\n      // This is the first time we poll this partition.\n      // We need to get the beginning and end offsets of the partition to calculate the number of messages to poll.\n      long beginningOffset = consumer.beginningOffsets(List.of(topicPartition)).get(topicPartition);\n      long endOffset = consumer.endOffsets(List.of(topicPartition)).get(topicPartition);\n\n      if (beginningOffset == 0) {\n        // This partition is empty.\n        // We don't need to poll it.\n        continue;\n      }\n\n      long rangeSize = endOffset - beginningOffset;\n      messagesToPoll = Math.min(rangeSize, messagesToPoll);\n\n      // We need to start from the end of the partition and go backwards.\n      // This is the offset to start from.\n      fromOffset = endOffset;\n\n      // This is the offset to stop at.\n      toOffset = endOffset - messagesToPoll;\n\n      // If the beginning offset is greater than the number of messages to poll, we need to start from the beginning offset.\n      if (beginningOffset > messagesToPoll) {\n        fromOffset = beginningOffset;\n        toOffset = beginningOffset - messagesToPoll;\n      }\n    } else {\n      //", "CodeBleu_score": 33.02097879693253}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tvar pollingRange = new TreeMap<TopicPartition, FromToOffset>();\n    for (var topicPartition : prevRange.keySet()) {\n        var prevPollingRange = prevRange.get(topicPartition);\n        var fromOffset = prevPollingRange.getFromOffset();\n        var toOffset = prevPollingRange.getToOffset();\n        var messagesPerPartition = messagesPerPage;\n        if (fromOffset == toOffset) {\n            var seekOffset = seekOperations.getSeekOffset(topicPartition);\n            if (seekOffset != null) {\n                fromOffset = seekOffset;\n            }\n            var endOffset = consumer.endOffsets(List.of(topicPartition)).get(topicPartition);\n            if (fromOffset > endOffset) {\n                fromOffset = endOffset;\n            }\n            toOffset = fromOffset;\n            messagesPerPartition = 1;\n        } else {\n            var newFromOffset = fromOffset - messagesPerPartition;\n            if (newFromOffset < 0) {\n                newFromOffset = 0;\n            }\n            fromOffset = newFromOffset;\n            var newToOffset = toOffset - messagesPerPartition;\n            if (newToOffset < fromOffset) {\n                toOffset = fromOffset;\n            } else {\n                toOffset = newToOffset;\n            }\n        }\n        pollingRange.put(topicPartition, new FromToOffset(fromOffset, toOffset));\n    }\n    return pollingRange;\n}", "CodeBleu_score": 28.09822011147798}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tMap<TopicPartition, FromToOffset> range = new TreeMap<>(Comparator.comparing(TopicPartition::topic));\n  for (TopicPartition topicPartition : seekOperations.getSeekOperations().keySet()) {\n    FromToOffset prevOffsetRange = prevRange.get(topicPartition);\n    FromToOffset seekOffsetRange = seekOperations.getSeekOperations().get(topicPartition);\n    long from = prevOffsetRange != null ? prevOffsetRange.getTo() : seekOffsetRange.getFrom();\n    long to = prevOffsetRange != null ? prevOffsetRange.getTo() : seekOffsetRange.getTo();\n    long messagesToPoll = Math.min(\n        messagesPerPage - (to - from),\n        to - from\n    );\n    long startOffset = Math.max(from - messagesToPoll, 0);\n    range.put(topicPartition, new FromToOffset(startOffset, to));\n  }\n  return new TreeMap<>(range);\n}", "CodeBleu_score": 39.26258145637706}]}
{"instance_id": "c4c657caeba5", "generation_result": [{"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}, {"code": "public void doLaunch(final MessageInput input){\n\tializer;", "CodeBleu_score": 2.390111398990128}]}
{"instance_id": "dffe85dbb47b", "generation_result": [{"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}, {"code": "private void insertCustomProperties(Properties props){\n\t       }", "CodeBleu_score": 3.0487516405477018}]}
{"instance_id": "ea146a7bc30c", "generation_result": [{"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries.isEmpty()) {\n        return;\n    }\n\n    final List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(this::toJournalEntry)\n            .collect(Collectors.toList());\n\n    if (journalEntries.isEmpty()) {\n        return;\n    }\n\n    final AtomicLong totalBytes = new AtomicLong();\n    final List<Journal.Entry> filteredEntries = journalEntries.stream()\n            .filter(entry -> {\n                final boolean result = entry.getPayload().length > 0;\n                if (!result) {\n                    metrics.filteredMessages().mark();\n                }\n                return result;\n            })\n            .peek(entry -> totalBytes.addAndGet(entry.getPayload().length))\n            .collect(Collectors.toList());\n\n    try {\n        writeRetryer.call(() -> kafkaJournal.write(filteredEntries));\n        metrics.writtenMessages().mark(filteredEntries.size());\n        metrics.writtenBytes().mark(totalBytes.get());\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n}", "CodeBleu_score": 57.60448480863916}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries == null || entries.isEmpty()) {\n        return;\n    }\n\n    final List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(RawMessageEvent::toJournalEntry)\n            .collect(Collectors.toList());\n\n    final AtomicLong totalBytes = new AtomicLong();\n    journalEntries.forEach(entry -> totalBytes.addAndGet(entry.getEvent().length()));\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(totalBytes.get());\n}", "CodeBleu_score": 51.26353684175247}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tif (entries == null || entries.isEmpty()) {\n        return;\n    }\n    final List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(this::toJournalEntry)\n            .collect(Collectors.toList());\n\n    try {\n        writeRetryer.call(() -> {\n            kafkaJournal.write(journalEntries);\n            return null;\n        });\n    } catch (ExecutionException | RetryException e) {\n        throw new MessageQueueException(\"Unable to write to journal\", e);\n    }\n\n    metrics.writtenMessages().mark(entries.size());\n    metrics.writtenBytes().mark(totalMessageBytes.getAndSet(0));\n}", "CodeBleu_score": 46.508886209055234}]}
{"instance_id": "944e77614042", "generation_result": [{"code": "public void acknowledge(List<Message> messages){\n\tfinal Optional<Long> maxValidMessageQueueId = messages.stream()\n            .map(Message::getQueueId)\n            .filter(queueId -> queueId != null)\n            .max(Long::compare);\n\n    maxValidMessageQueueId.ifPresent(queueId -> {\n        kafkaJournal.acknowledge(queueId);\n        metrics.markAcknowledged(messages.size());\n    });\n}", "CodeBleu_score": 44.57795482209593}, {"code": "public void acknowledge(List<Message> messages){\n\tfinal Optional<Long> maxMessageQueueId = messages.stream().map(Message::getMessageQueueId).max(Long::compareTo);\n    maxMessageQueueId.ifPresent(this::acknowledge);\n    updateMetrics(messages.size());\n}", "CodeBleu_score": 36.568721313650904}, {"code": "public void acknowledge(List<Message> messages){\n\tfinal Optional<Long> maxValidMessageQueueId = messages.stream()\n            .map(Message::getMessageQueueId)\n            .filter(Objects::nonNull)\n            .max(Long::compareTo);\n\n    maxValidMessageQueueId.ifPresent(kafkaJournal::acknowledge);\n\n    metrics.acknowledged(messages.size());\n}", "CodeBleu_score": 45.20252896418362}]}
{"instance_id": "981280879490", "generation_result": [{"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}, {"code": "private void registerLegacyMetrics(){\n\tnumber of bytes between index entries\n                .put(LogConfig.IndexIntervalBytesProp(), 4096)\n                // fileDeleteDelayMs: The time to wait before deleting a file from the filesystem\n                .put(LogConfig.FileDeleteDelayMsProp(), MINUTES.toMillis(1L))\n                // deleteRetentionMs: The time to retain delete markers in the log. Only applicable for logs that are being compacted.\n                .put(LogConfig.DeleteRetentionMsProp(), DAYS.toMillis(1L))\n                // minCleanableRatio: The ratio of bytes that are available for cleaning to the bytes already cleaned\n                .put(LogConfig.MinCleanableDirtyRatioProp(), 0.5)\n                // compact: Should old segments in this log be deleted or de-duplicated?\n                .put(LogConfig.Compact(), false)\n                // uncleanLeaderElectionEnable Indicates whether unclean leader election is enabled; actually a controller-level property\n                //                             but included here for topic-specific configuration validation purposes\n                .put(LogConfig.UncleanLeaderElectionEnableProp(), true)\n                // minInSyncReplicas If number of insync replicas drops below this number, we stop accepting writes with -1 (or all) required acks\n                .put(LogConfig.MinInSyncReplicasProp(), 1)\n                .build();\n        final LogConfig defaultConfig = new LogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 26.82476090250743}]}
{"instance_id": "3ca34bfd2d64", "generation_result": [{"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\tLogConfig(config);\n\n        // these are the default values as per kafka 0.8.1.1, except we don't turn on the cleaner\n        // Cleaner really is log compaction with respect to \\\"deletes\\\" in the log.\n        // we never insert a message twice, at least not on purpose, so we do not \\\"clean\\\" logs, ever.\n        final CleanerConfig cleanerConfig =\n                new CleanerConfig(\n                        1,\n                        Size.megabytes(4L).toBytes(),\n                        0.9d,\n                        Ints.saturatedCast(Size.megabytes(1L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 12.601502712237522}]}
{"instance_id": "e1d07a1fa986", "generation_result": [{"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t.megabytes(32L).toBytes()),\n                        Ints.saturatedCast(Size.megabytes(5L).toBytes()),\n                        SECONDS.toMillis(15L),\n                        false,\n                        \\\"MD5\\\");\n\n        if (!java.nio.file.Files.exists(journalDirectory)) {\n            try {\n                java.nio.file.Files.createDirectories(journalDirectory);\n            } catch (IOException e) {\n                LOG.error(\\\"Cannot create journal directory at {}, please check the permissions\\\", journalDirectory.toAbsolutePath());\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        // TODO add check for directory, etc\n        committedReadOffsetFile = new File(journalDirectory.toFile(), \\\"graylog2-committed-read-offset\\\");\n        try {\n            if (!committedReadOffsetFile.createNewFile()) {\n                final String line = Files.asCharSource(committedReadOffsetFile, StandardCharsets.UTF_8).readFirstLine();\n                // the file contains the last offset graylog2 has successfully processed.\n                // thus the nextReadOffset is one beyond that number\n                if (line != null) {\n                    committedOffset.set(Long.parseLong(line.trim()));\n                    nextReadOffset = committedOffset.get() + 1;\n                }\n            }\n        } catch (IOException e) {\n            LOG.error(\\\"Cannot access offset file: {}\\\", e.getMessage());\n            final AccessDeniedException accessDeniedException = new AccessDeniedException(committedReadOffsetFile.getAbsolutePath(), null, e.getMessage());\n            throw new RuntimeException(accessDeniedException);\n        }\n        try {\n            final BrokerState brokerState = new BrokerState();\n            brokerState.newState(RunningAsBroker.state());\n            kafkaScheduler = new KafkaScheduler(2, \\\"kafka-journal-scheduler-\\\", false); // TODO make thread count configurable\n            kafkaScheduler.startup();\n            logManager = new LogManager(\n                    new File[]{journalDirectory.toFile()},\n                    Map$.MODULE$.<String, LogConfig>empty(),\n                    defaultConfig,\n                    cleanerConfig,\n                    NUM_IO_THREADS,\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    SECONDS.toMillis(60L),\n                    kafkaScheduler, // Broker state\n                    brokerState,\n                    JODA_TIME);\n\n            final TopicAndPartition topicAndPartition = new TopicAndPartition(\\\"messagejournal\\\", 0);\n            final Option<Log> messageLog = logManager.getLog(topicAndPartition);\n            if (messageLog.isEmpty()) {\n                kafkaLog = logManager.createLog(topicAndPartition, logManager.defaultConfig());\n            } else {\n                kafkaLog = messageLog.get();\n            }\n\n            // Set up more metrics\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_SIZE), (Gauge<Long>) kafkaLog::size);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LOG_END_OFFSET), (Gauge<Long>) kafkaLog::logEndOffset);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_NUMBER_OF_SEGMENTS), (Gauge<Integer>) kafkaLog::numberOfSegments);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_UNFLUSHED_MESSAGES), (Gauge<Long>) kafkaLog::unflushedMessages);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_RECOVERY_POINT), (Gauge<Long>) kafkaLog::recoveryPoint);\n            this.metricRegistry.register(name(metricPrefix, METRIC_NAME_LAST_FLUSH_TIME), (Gauge<Long>) kafkaLog::lastFlushTime);\n\n            // must not be a lambda, because the serialization cannot determine the proper Metric type :(\n            this.metricRegistry.register(getOldestSegmentMetricName(), (Gauge<Date>) new Gauge<Date>() {\n                @Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }\n            });\n\n            LOG.info(\\\"Initialized Kafka based journal at {}\\\", journalDirectory);\n\n            offsetFlusher = new OffsetFileFlusher();\n            dirtyLogFlusher = new DirtyLogFlusher();\n            recoveryCheckpointFlusher = new RecoveryCheckpointFlusher();\n            logRetentionCleaner = new LogRetentionCleaner();\n        } catch (KafkaException e) {\n            // most likely failed to grab lock\n            LOG.error(\\\"Unable to start logmanager.\\\", e);\n            throw new RuntimeException(e);\n        }\n\n        if (LocalKafkaJournal.class.getName().equals(metricPrefix)) {\n            registerLegacyMetrics();\n        }\n    }", "CodeBleu_score": 20.978178617442346}]}
{"instance_id": "8b30a387069f", "generation_result": [{"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tmin, i), max);\n    }", "CodeBleu_score": 4.722092116426301}]}
{"instance_id": "ee338b61e75d", "generation_result": [{"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        LOGGER.info(\"[KafkaAdminClient] start to close kafka admin client for cluster: \" + clusterPhyId);\n        for (AdminClient adminClient : adminClientList) {\n            try {\n                adminClient.close(Duration.ofSeconds(10));\n            } catch (Exception e) {\n                LOGGER.error(\"[KafkaAdminClient] close kafka admin client failed, clusterId: \" + clusterPhyId, e);\n            }\n        }\n        LOGGER.info(\"[KafkaAdminClient] close kafka admin client for cluster: \" + clusterPhyId + \" success\");\n        KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n    }\n}", "CodeBleu_score": 41.35404284385276}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tList<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n    if (adminClientList == null) {\n        return;\n    }\n\n    LOGGER.info(\"start to close kafka admin-client, clusterPhyId: {}\", clusterPhyId);\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close();\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka admin-client failed, clusterPhyId: {}, errMsg: {}\", clusterPhyId, e.getMessage());\n        }\n    }\n    LOGGER.info(\"close kafka admin-client success, clusterPhyId: {}\", clusterPhyId);\n\n    KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n}", "CodeBleu_score": 47.50844777829511}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n\n        LOGGER.info(\"[KafkaAdminClient-closeKafkaAdminClient] start to close kafka admin-client, clusterPhyId={}\", clusterPhyId);\n        for (AdminClient adminClient : adminClientList) {\n            try {\n                adminClient.close();\n            } catch (Exception e) {\n                LOGGER.error(\"[KafkaAdminClient-closeKafkaAdminClient] close kafka admin-client failed, clusterPhyId={}, e={}\", clusterPhyId, e);\n            }\n        }\n        LOGGER.info(\"[KafkaAdminClient-closeKafkaAdminClient] close kafka admin-client success, clusterPhyId={}\", clusterPhyId);\n        KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n    }\n}", "CodeBleu_score": 51.902780410663}]}
{"instance_id": "6f0b935b971c", "generation_result": [{"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean allSuccess = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            allSuccess = false;\n        }\n    }\n\n    return allSuccess;\n}", "CodeBleu_score": 79.9206672992389}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean allSuccess = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            allSuccess = false;\n        }\n    }\n\n    return allSuccess;\n}", "CodeBleu_score": 79.9206672992389}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tif (adminClientList == null) {\n        return true;\n    }\n\n    boolean allSuccess = true;\n    for (AdminClient adminClient : adminClientList) {\n        try {\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}, adminClient:{}\", clusterPhyId, adminClient, e);\n            allSuccess = false;\n        }\n    }\n\n    return allSuccess;\n}", "CodeBleu_score": 79.9206672992389}]}
{"instance_id": "b5475aa94c1a", "generation_result": [{"code": "public void stop() {\n\tif (app != null) {\n\t\tapp.stop();\n\t\tfor (int i = 0; i < 5; i++) {\n\t\t\ttry {\n\t\t\t\tapp.close();\n\t\t\t\tapp = null;\n\t\t\t\treturn;\n\t\t\t} catch (Exception e) {\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\tthrow new IllegalStateException(\"Interrupted while waiting for app to stop\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 48.75505298874542}, {"code": "public void stop() {\n\tif (app != null) {\n\t\tapp.stop();\n\t\tint attempts = 0;\n\t\twhile (app.isRunning() && attempts < 5) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t\tattempts++;\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 69.85311857674677}, {"code": "public void stop() {\n\tif (app == null) {\n\t\treturn;\n\t}\n\n\tapp.stop();\n\tint attempts = 0;\n\twhile (app.isRunning()) {\n\t\ttry {\n\t\t\tThread.sleep(1000);\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\tthrow new IllegalStateException(\"Interrupted while waiting for app to stop\");\n\t\t}\n\t\tattempts++;\n\t\tif (attempts > 5) {\n\t\t\tthrow new IllegalStateException(\"App failed to stop\");\n\t\t}\n\t}\n\tapp = null;\n}", "CodeBleu_score": 53.36007928373279}]}
{"instance_id": "5da405de9e3d", "generation_result": [{"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner runner = new EurekaServerRunner(config);\n\tconfigureTLS(runner);\n\trunner.start();\n\treturn runner;\n}", "CodeBleu_score": 27.482183664653974}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner runner = new EurekaServerRunner(config);\n    runner.enableSelfPreservation();\n    runner.setExpectedServerUpTime(20000);\n    runner.setExpectedHeartbeatLatency(30000);\n    runner.setExpectedResponseLatency(30000);\n    runner.setExpectedNumberOfClients(1);\n    runner.setExpectedNumberOfApps(1);\n    runner.setExpectedNumberOfInstancesPerApp(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyHeartbeat(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQuery(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryFullRegistryFetchWithoutDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryDeltaWithoutFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutFullRegistryFetchWithoutDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutFullRegistryFetchWithoutDelta(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutDeltaWithoutFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutDeltaWithoutFullRegistryFetch(1);\n    runner.setExpectedNumberOfInstancesPerAppToSatisfyQueryWithoutDeltaWithoutFullRegistryFetch(1);", "CodeBleu_score": 28.851600084210265}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\tEurekaServerRunner runner = new EurekaServerRunner(config);\n\trunner.enableTls(true);\n\trunner.setKeyStore(caCert, KEY_STORE_PASSWORD);\n\trunner.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\trunner.start();\n\treturn runner;\n}", "CodeBleu_score": 52.24193698509124}]}
{"instance_id": "19cbf70751b2", "generation_result": [{"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner client = new EurekaClientRunner(config, server.getServiceUrl(), \"testservice\");\n\tclient.enableTls();\n\tclient.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tclient.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tclient.start();\n\treturn client;\n}", "CodeBleu_score": 39.4871063723634}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner service = new EurekaClientRunner(server.getTransportConfig(), config, \"testservice\");\n\tservice.enableTls();\n\tservice.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tservice.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tservice.start();\n\treturn service;\n}", "CodeBleu_score": 52.279034917943335}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\tEurekaClientRunner service = new EurekaClientRunner(server.getServiceUrl(), config, \"testservice\");\n\tservice.enableTls();\n\tservice.setKeyStore(clientCert, KEY_STORE_PASSWORD, \"client\", KEY_PASSWORD);\n\tservice.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\tservice.start();\n\treturn service;\n}", "CodeBleu_score": 52.46894054944199}]}
{"instance_id": "d6ccff5c0536", "generation_result": [{"code": "static void createCertificates() throws Exception{\n\t// Create a CA certificate\n\tKeyStore caKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tcaKeyStore.load(null, null);\n\tKeyTool caKeyTool = new KeyTool(caKeyStore, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tcaKeyTool.createSelfSignedCert(\"test-ca\", \"CN=test-ca\");\n\tsaveCert(caCert, caKeyTool.getCertificate(\"test-ca\"));\n\n\t// Create a server certificate\n\tKeyStore serverKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tserverKeyStore.load(null, null);\n\tKeyTool serverKeyTool = new KeyTool(serverKeyStore, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tserverKeyTool.createCert(\"test-server\", caKeyTool, \"CN=test-server\");\n\tsaveKeyAndCert(serverCert, serverKeyTool.getPrivateKey(\"test-server\"), serverKeyTool.getCertificate(\"test-server\"));\n\n\t// Create a client certificate\n\tKeyStore clientKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tclientKeyStore.load(null, null);\n\tKeyTool clientKeyTool = new KeyTool(clientKeyStore, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\tclientKeyTool.createCert(\"test-client\", caKeyTool, \"CN=test-client\");\n\tsaveKeyAndCert(clientCert, clientKeyTool.getPrivateKey(\"test-client\"), clientKeyTool.getCertificate(\"test-client\"));\n\n\t// Create a second CA certificate\n\tKeyStore wrongCaKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\twrongCaKeyStore.load(null, null);\n\tKeyTool wrongCaKeyTool = new KeyTool(wrongCaKeyStore, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\twrongCaKeyTool.createSelfSignedCert(\"test-wrong-ca\", \"CN=test-wrong-ca\");\n\tsaveCert(wrongCaCert, wrongCaKeyTool.getCertificate(\"test-wrong-ca\"));\n\n\t// Create a second client certificate", "CodeBleu_score": 26.8779410098755}, {"code": "static void createCertificates() throws Exception{\n\tlog.info(\"Creating certificates...\");\n\n\t// Create a CA certificate and key\n\tKeyStore caKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tcaKeyStore.load(null, null);\n\tcaKeyStore.setKeyEntry(\"ca\", createKeyPair(\"ca\"), KEY_STORE_PASSWORD.toCharArray(), null);\n\tcaCert = saveCert(caKeyStore, \"ca\");\n\n\t// Create a server certificate and key\n\tKeyStore serverKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tserverKeyStore.load(null, null);\n\tserverKeyStore.setKeyEntry(\"server\", createKeyPair(\"server\"), KEY_STORE_PASSWORD.toCharArray(), null);\n\tserverKeyStore.setCertificateEntry(\"ca\", caKeyStore.getCertificate(\"ca\"));\n\tserverCert = saveCert(serverKeyStore, \"server\");\n\n\t// Create a client certificate and key\n\tKeyStore clientKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\tclientKeyStore.load(null, null);\n\tclientKeyStore.setKeyEntry(\"client\", createKeyPair(\"client\"), KEY_STORE_PASSWORD.toCharArray(), null);\n\tclientKeyStore.setCertificateEntry(\"ca\", caKeyStore.getCertificate(\"ca\"));\n\tclientCert = saveCert(clientKeyStore, \"client\");\n\n\t// Create a second CA certificate and key\n\tKeyStore wrongCaKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\twrongCaKeyStore.load(null, null);\n\twrongCaKeyStore.setKeyEntry(\"ca\", createKeyPair(\"ca\"), KEY_STORE_PASSWORD.toCharArray(), null);\n\twrongCaCert = saveCert(wrongCaKeyStore, \"ca\");\n\n\t// Create a second client certificate and key\n\tKeyStore wrongClientKeyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\twrongClientKeyStore.load(null, null);\n\twrongClientKeyStore.setKeyEntry(\"client\", createKeyPair(\"client\"), KEY_STORE_PASSWORD.toCharArray(), null);\n\twrong", "CodeBleu_score": 27.885502898221286}, {"code": "static void createCertificates() throws Exception{\n\tcreateCertificateAuthority();\n\tcreateServerCertificate();\n\tcreateClientCertificate();\n}", "CodeBleu_score": 2.198959066098788}]}
{"instance_id": "d2eeaa6a6bbe", "generation_result": [{"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong timeLimit = System.currentTimeMillis() + seconds * 1000;\n    while (System.currentTimeMillis() < timeLimit) {\n        if (assertion.getAsBoolean()) {\n            return;\n        }\n        try {\n            Thread.sleep(200);\n        }\n        catch (InterruptedException e) {\n            throw new RuntimeException(e);\n        }\n    }\n    throw new RuntimeException(\"Assertion failed\");\n}", "CodeBleu_score": 42.418135848522205}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong start = System.currentTimeMillis();\n\tlong timeLimit = start + (seconds * 1000);\n\twhile (System.currentTimeMillis() < timeLimit) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t\ttry {\n\t\t\tThread.sleep(100);\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\tthrow new RuntimeException(\"Assertion not met within \" + seconds + \" seconds\");\n}", "CodeBleu_score": 44.64727772475797}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\tlong timeLimit = System.currentTimeMillis() + (seconds * 1000);\n    while (System.currentTimeMillis() < timeLimit) {\n        if (assertion.getAsBoolean()) {\n            return;\n        }\n        try {\n            Thread.sleep(100);\n        }\n        catch (InterruptedException e) {\n            throw new RuntimeException(e);\n        }\n    }\n    throw new RuntimeException(\"Assertion failed to evaluate within the time limit\");\n}", "CodeBleu_score": 41.55140571614429}]}
{"instance_id": "6bb261b0e4e1", "generation_result": [{"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\treturn createCert(keyPair.getPublic(), keyPair.getPrivate(), ca, ca);\n}", "CodeBleu_score": 15.198387402404595}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX500Name subject = new X500Name(\\\"CN=\\\" + ca + \\\"\\\");\n\tX500Name issuer = new X500Name(\\\"CN=\\\" + ca + \\\"\\\");\n\tX509Certificate certificate = createCert(keyPair.getPublic(), keyPair.getPrivate(), issuer, subject);\n\treturn certificate;\n}", "CodeBleu_score": 16.577389975893208}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\tX509Certificate certificate = createCert(keyPair.getPublic(), keyPair.getPrivate(), ca, ca);\n\treturn certificate;\n}", "CodeBleu_score": 29.931721155769587}]}
{"instance_id": "e4f9c32a7c9e", "generation_result": [{"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient client = this.eurekaHttpClient.get();\n\tif (client == null) {\n\t\ttry {\n\t\t\tObject eurekaTransport = this.eurekaTransportField.get(this);\n\t\t\tclient = (EurekaHttpClient) ReflectionUtils.getField(\n\t\t\t\t\tReflectionUtils.findField(eurekaTransport.getClass(), \"registrationClient\"),\n\t\t\t\t\teurekaTransport);\n\t\t\tthis.eurekaHttpClient.set(client);\n\t\t}\n\t\tcatch (IllegalAccessException ex) {\n\t\t\tlog.error(\"Cannot access EurekaHttpClient\", ex);\n\t\t}\n\t}\n\treturn client;\n}", "CodeBleu_score": 55.559002855358465}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient client = eurekaHttpClient.get();\n\tif (client == null) {\n\t\ttry {\n\t\t\tObject eurekaTransport = eurekaTransportField.get(this);\n\t\t\tclient = (EurekaHttpClient) ReflectionUtils.getField(\n\t\t\t\t\tReflectionUtils.findField(eurekaTransport.getClass(), \"registrationClient\"),\n\t\t\t\t\teurekaTransport);\n\t\t\teurekaHttpClient.set(client);\n\t\t}\n\t\tcatch (IllegalAccessException e) {\n\t\t\tlog.error(\"Error retrieving EurekaHttpClient\", e);\n\t\t}\n\t}\n\treturn client;\n}", "CodeBleu_score": 54.07386944774159}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient client = eurekaHttpClient.get();\n\tif (client == null) {\n\t\tObject eurekaTransport = ReflectionUtils.getField(eurekaTransportField, this);\n\t\tif (eurekaTransport != null) {\n\t\t\ttry {\n\t\t\t\tclient = (EurekaHttpClient) ReflectionUtils.getField(ReflectionUtils.findField(eurekaTransport.getClass(), \"client\"), eurekaTransport);\n\t\t\t\teurekaHttpClient.compareAndSet(null, client);\n\t\t\t}\n\t\t\tcatch (IllegalAccessException e) {\n\t\t\t\tlog.error(\"Failed to get EurekaHttpClient\", e);\n\t\t\t}\n\t\t}\n\t}\n\treturn client;\n}", "CodeBleu_score": 49.92614309666158}]}
{"instance_id": "5c404f04e34c", "generation_result": [{"code": "public List<String> getServices(){\n\tApplications applications = this.eurekaClient.getApplications();\n\tif (applications == null || applications.getRegisteredApplications() == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications.getRegisteredApplications()) {\n\t\tif (application.getInstances() != null && !application.getInstances().isEmpty()) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 66.55202667688104}, {"code": "public List<String> getServices(){\n\tApplications applications = this.eurekaClient.getApplications();\n\tif (applications == null || applications.getRegisteredApplications() == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications.getRegisteredApplications()) {\n\t\tif (application.getInstances() != null && !application.getInstances().isEmpty()) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 66.55202667688104}, {"code": "public List<String> getServices(){\n\tApplications applications = this.eurekaClient.getApplications();\n\tif (applications == null || applications.getRegisteredApplications().isEmpty()) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application app : applications.getRegisteredApplications()) {\n\t\tif (!app.getInstances().isEmpty()) {\n\t\t\tservices.add(app.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 68.14633045185072}]}
{"instance_id": "a21251337ade", "generation_result": [{"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\thealthContributors.forEach((key, value) -> {\n\t\tif (value instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor compositeHealthContributor = (DiscoveryCompositeHealthContributor) value;\n\t\t\tcompositeHealthContributor.getContributors().forEach(indicator -> {\n\t\t\t\tif (!(indicator instanceof EurekaHealthIndicator)) {\n\t\t\t\t\tthis.healthContributors.put(key, indicator);\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\telse {\n\t\t\tthis.healthContributors.put(key, value);\n\t\t}\n\t});\n}", "CodeBleu_score": 35.959401659710345}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\thealthContributors.forEach((key, value) -> {\n\t\tif (value instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor discoveryCompositeHealthContributor = (DiscoveryCompositeHealthContributor) value;\n\t\t\tSet<HealthContributor> indicators = new HashSet<>(discoveryCompositeHealthContributor.getContributors());\n\t\t\tindicators.removeIf(EurekaHealthIndicator.class::isInstance);\n\t\t\tindicators.forEach(indicator -> this.healthContributors.put(key, indicator));\n\t\t}\n\t\telse {\n\t\t\tthis.healthContributors.put(key, value);\n\t\t}\n\t});\n}", "CodeBleu_score": 31.28772670067174}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\thealthContributors.forEach((name, contributor) -> {\n\t\tif (contributor instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor discoveryCompositeHealthContributor = (DiscoveryCompositeHealthContributor) contributor;\n\t\t\tSet<HealthContributor> indicators = discoveryCompositeHealthContributor.getContributors();\n\t\t\tSet<HealthContributor> nonEurekaIndicators = indicators.stream().filter(healthContributor -> !(healthContributor instanceof EurekaHealthIndicator)).collect(Collectors.toSet());\n\t\t\tnonEurekaIndicators.forEach(healthContributor -> this.healthContributors.put(healthContributor.getName(), healthContributor));\n\t\t}\n\t\telse {\n\t\t\tthis.healthContributors.put(name, contributor);\n\t\t}\n\t});\n}", "CodeBleu_score": 28.62216319575793}]}
{"instance_id": "998fee829929", "generation_result": [{"code": "public String hello(){\n\t// 1. \u901a\u8fc7\u670d\u52a1\u540d\u67e5\u627e\u670d\u52a1\n    List<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No provider-application service instance found.\");\n    }\n    // 2. \u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u9009\u62e9\u4e00\u4e2a\u5b9e\u4f8b\n    ServiceInstance instance = loadBalancerClient.choose(\"provider-application\");\n    URI uri = instance.getUri();\n    // 3. \u4f7f\u7528RestTemplate\u53d1\u9001\u8bf7\u6c42\n    return restTemplate.postForObject(uri + \"/echo\", \"Hello\", String.class);\n}", "CodeBleu_score": 42.33299887127219}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No service instance found for provider-application\");\n    }\n    ServiceInstance serviceInstance = instances.get(0);\n    URI uri = serviceInstance.getUri();\n    String url = uri.toString() + \"/echo\";\n    return restTemplate.postForObject(url, \"hello\", String.class);\n}", "CodeBleu_score": 61.52897603652694}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n    if (instances.isEmpty()) {\n        throw new RuntimeException(\"No provider-application found.\");\n    }\n    ServiceInstance serviceInstance = instances.get(0);\n    URI uri = serviceInstance.getUri();\n    return restTemplate.postForObject(uri + \"/echo\", \"Hello\", String.class);\n}", "CodeBleu_score": 51.337799595497536}]}
{"instance_id": "31166ae7d3dc", "generation_result": [{"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 76.19490484366067}, {"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 76.19490484366067}, {"code": "public String world(){\n\tList<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = loadBalancedRestTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 76.19490484366067}]}
{"instance_id": "d7714f9cf0e9", "generation_result": [{"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service: \" + serviceName + \" with IP: \" + ip + \" and port: \" + port);\n    if (serverMap.containsKey(serviceName)) {\n        Set<Server> servers = serverMap.get(serviceName);\n        synchronized (servers) {\n            servers.add(new Server(ip, port));\n        }\n    } else {\n        Set<Server> servers = Collections.synchronizedSet(new HashSet<>());\n        servers.add(new Server(ip, port));\n        serverMap.put(serviceName, servers);\n    }\n    return true;\n}", "CodeBleu_score": 52.76968691937959}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service \" + serviceName + \" with IP \" + ip + \" and port \" + port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.put(serviceName, Collections.synchronizedSet(new HashSet<>()));\n    }\n    Server server = new Server(ip, port);\n    serverMap.get(serviceName).add(server);\n    return true;\n}", "CodeBleu_score": 67.08110777195522}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"Registering service: \" + serviceName + \" with IP: \" + ip + \" and port: \" + port);\n    if (!serverMap.containsKey(serviceName)) {\n        serverMap.putIfAbsent(serviceName, Collections.synchronizedSet(new HashSet<>()));\n    }\n    Server server = new Server(ip, port);\n    return serverMap.get(serviceName).add(server);\n}", "CodeBleu_score": 61.95700306702523}]}
{"instance_id": "45f91ed1c560", "generation_result": [{"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    serverMap.putIfAbsent(serviceName.toLowerCase(), Collections.synchronizedSet(new HashSet<>()));\n    Server server = new Server(ip, port);\n    serverMap.get(serviceName).remove(server);\n    return true;\n}", "CodeBleu_score": 65.41892418121358}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Server server = new Server(ip, port);\n    Set<Server> serverSet = serverMap.get(serviceName);\n    if (serverSet != null) {\n        serverSet.remove(server);\n    }\n    return true;\n}", "CodeBleu_score": 78.96862191717557}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    serverMap.putIfAbsent(serviceName.toLowerCase(), Collections.synchronizedSet(new HashSet<>()));\n    Server server = new Server(ip, port);\n    serverMap.get(serviceName).remove(server);\n    return true;\n}", "CodeBleu_score": 65.41892418121358}]}
{"instance_id": "f671d9564e40", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null) {\n        String defaultConfiguration = (String) attributes.get(\"defaultConfiguration\");\n        if (defaultConfiguration != null && !defaultConfiguration.isEmpty()) {\n            BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\n            builder.addPropertyValue(\"name\", defaultConfiguration);\n            builder.addPropertyValue(\"configuration\", defaultConfiguration);\n            registry.registerBeanDefinition(defaultConfiguration, builder.getBeanDefinition());\n        }\n    }\n}", "CodeBleu_score": 43.569180650050335}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    if (attributes != null) {\n        Object[] defaultConfiguration = (Object[]) attributes.get(\"defaultConfiguration\");\n        if (defaultConfiguration != null) {\n            registerClientConfiguration(defaultConfiguration, registry);\n        }\n    }\n}", "CodeBleu_score": 51.24482162006076}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\n    Object[] clients = (Object[]) attributes.get(\"value\");\n    if (clients != null && clients.length > 0) {\n        for (Object client : clients) {\n            Map<String, Object> clientAttributes = ((AnnotationMetadata) client).getAnnotationAttributes(RibbonClient.class.getName());\n            if (clientAttributes != null) {\n                String name = (String) clientAttributes.get(\"name\");\n                String configuration = (String) clientAttributes.get(\"configuration\");\n                if (name != null && configuration != null) {\n                    BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientSpecification.class);\n                    builder.addConstructorArgValue(name);\n                    builder.addConstructorArgValue(configuration);\n                    registry.registerBeanDefinition(name, builder.getBeanDefinition());\n                }\n            }\n        }\n    }\n}", "CodeBleu_score": 42.885046433436855}]}
{"instance_id": "dbc92d8f146f", "generation_result": [{"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tif (StrUtil.isBlank(serviceId)) {\n        return null;\n    }\n    ILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n    if (loadBalancer == null) {\n        return null;\n    }\n    Server server = loadBalancer.chooseServer(serviceId);\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort());\n}", "CodeBleu_score": 55.095744162619575}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getILoadBalancer(serviceId);\n    if (loadBalancer == null) {\n        return null;\n    }\n    Server server = loadBalancer.chooseServer(new TutuLoadBalancerRequest(serviceId, request));\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort(), server.getZone());\n}", "CodeBleu_score": 56.60263434620414}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\tILoadBalancer loadBalancer = clientFactory.getLoadBalancer(serviceId);\n    if (loadBalancer == null) {\n        return null;\n    }\n    Server server = loadBalancer.chooseServer(new RibbonServer(serviceId));\n    if (server == null) {\n        return null;\n    }\n    return new TutuServiceInstance(serviceId, server.getHost(), server.getPort(), server.getZone(), server.getMetadata());\n}", "CodeBleu_score": 57.31961747600074}]}
{"instance_id": "362eadbd3c5e", "generation_result": [{"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\tString host = server.getHost();\n    int port = server.getPort();\n    String scheme = original.getScheme();\n\n    try {\n        return new URI(scheme, null, host, port, original.getPath(), original.getQuery(), original.getFragment());\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 34.32143368860909}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n        String host = server.getHost();\n        int port = server.getPort();\n        String scheme = original.getScheme();\n        String path = original.getPath();\n        String query = original.getQuery();\n        String fragment = original.getFragment();\n        String userInfo = original.getUserInfo();\n\n        String hostHeader = host;\n        if (port > 0 && port != 80 && port != 443) {\n            hostHeader = host + \":\" + port;\n        }\n\n        StringBuilder builder = new StringBuilder(scheme).append(\"://\").append(hostHeader);\n        if (StrUtil.isNotBlank(path)) {\n            builder.append(path);\n        }\n\n        if (StrUtil.isNotBlank(query)) {\n            builder.append(\"?\").append(query);\n        }\n\n        if (StrUtil.isNotBlank(fragment)) {\n            builder.append(\"#\").append(fragment);\n        }\n\n        return new URI(builder.toString());\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 50.93886297067846}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n        return new URI(\n                original.getScheme(),\n                null,\n                server.getHost(),\n                server.getPort(),\n                original.getPath(),\n                original.getQuery(),\n                original.getFragment()\n        );\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}", "CodeBleu_score": 31.95376432181086}]}
{"instance_id": "6801d2b2e4c8", "generation_result": [{"code": "private List<TutuServer> getServer(){\n\tString serviceName = discoveryProperties.getServiceName();\n    String serverAddress = discoveryProperties.getServerAddress();\n\n    String response = HttpUtil.get(serverAddress + \"/list?serviceName=\" + serviceName);\n    JSONObject jsonObject = JSON.parseObject(response);\n    List<JSONObject> data = jsonObject.getJSONArray(\"data\").toJavaList(JSONObject.class);\n    return data.stream().map(d -> {\n        String ip = d.getString(\"ip\");\n        Integer port = d.getInteger(\"port\");\n        return new TutuServer(ip, port);\n    }).collect(Collectors.toList());\n}", "CodeBleu_score": 48.72943514631655}, {"code": "private List<TutuServer> getServer(){\n\tString url = discoveryProperties.getUrl();\n    String serviceName = discoveryProperties.getServiceName();\n    String serverAddress = url + \"/list?serviceName=\" + serviceName;\n    String result = HttpUtil.get(serverAddress);\n    JSONObject jsonObject = JSON.parseObject(result);\n    List<JSONObject> jsonList = jsonObject.getJSONArray(\"data\").toJavaList(JSONObject.class);\n    return jsonList.stream().map(json -> {\n        String ip = json.getString(\"ip\");\n        int port = json.getInteger(\"port\");\n        return new TutuServer(ip, port);\n    }).collect(Collectors.toList());\n}", "CodeBleu_score": 46.09231017022134}, {"code": "private List<TutuServer> getServer(){\n\tString url = discoveryProperties.getServerAddress() + \"/list\";\n    String response = HttpUtil.get(url, new HashMap<>());\n    JSONObject jsonObject = JSON.parseObject(response);\n    List<JSONObject> jsonList = jsonObject.getJSONArray(\"list\").toJavaList(JSONObject.class);\n    return jsonList.stream().map(json -> {\n        String host = json.getString(\"host\");\n        int port = json.getIntValue(\"port\");\n        return new TutuServer(host, port);\n    }).collect(Collectors.toList());\n}", "CodeBleu_score": 44.74814452987609}]}
{"instance_id": "ab2fb77484fd", "generation_result": [{"code": "public Route getMatchingRoute(String path){\n\tMap<String, ZuulRoute> routes = zuulProperties.getRoutes();\n\tfor (Map.Entry<String, ZuulRoute> entry : routes.entrySet()) {\n\t\tString pattern = entry.getValue().getPath();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString targetPath = path.substring(path.indexOf(pattern));\n\t\t\treturn new Route(entry.getKey(), targetPath);\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 53.73743394264097}, {"code": "public Route getMatchingRoute(String path){\n\tMap<String, ZuulRoute> routes = zuulProperties.getRoutes();\n\tfor (Map.Entry<String, ZuulRoute> entry : routes.entrySet()) {\n\t\tZuulRoute zuulRoute = entry.getValue();\n\t\tString pattern = zuulRoute.getPath();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString targetPath = pathMatcher.extractPathWithinPattern(pattern, path);\n\t\t\treturn new Route(targetPath, zuulRoute.getServiceId());\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 58.10998032013362}, {"code": "public Route getMatchingRoute(String path){\n\tfor (Map.Entry<String, ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n\t\tString pattern = entry.getKey();\n\t\tZuulRoute zuulRoute = entry.getValue();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString targetPath = pathMatcher.extractPathWithinPattern(pattern, path);\n\t\t\treturn new Route(targetPath, zuulRoute.getServiceId());\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 60.77162367858511}]}
{"instance_id": "822071925513", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tClassPathFeignClientsScanner scanner = new ClassPathFeignClientsScanner(registry);\n    AnnotationAttributes attributes = AnnotationAttributes.fromMap(importingClassMetadata.getAnnotationAttributes(EnableFeignClients.class.getName(), false));\n    Class<?>[] clients = attributes.getClassArray(\"clients\");\n    if (clients.length == 0) {\n        scanner.registerDefaultFilters();\n        scanner.addIncludeFilter(new AnnotationTypeFilter(FeignClient.class));\n    }\n    else {\n        scanner.registerFilters();\n    }\n    scanner.setAnnotationClass(FeignClient.class);\n    Set<BeanDefinitionHolder> beanDefinitions = scanner.doScan(clients);\n    if (beanDefinitions.isEmpty()) {\n        return;\n    }\n    processBeanDefinitions(beanDefinitions, registry);\n}", "CodeBleu_score": 35.1462198589054}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\tSet<String> packagesToScan = getPackagesToScan(importingClassMetadata);\n    if (packagesToScan.isEmpty()) {\n        return;\n    }\n\n    // \u83b7\u53d6\u6240\u6709\u5e26\u6709@FeignClient\u6ce8\u89e3\u7684\u7c7b\n    Set<Class<?>> feignClientClasses = new Reflections(packagesToScan.toArray(new String[0]))\n            .getTypesAnnotatedWith(FeignClient.class, true);\n\n    for (Class<?> clazz : feignClientClasses) {\n        // \u83b7\u53d6\u6ce8\u89e3\u4fe1\u606f\n        FeignClient annotation = clazz.getAnnotation(FeignClient.class);\n        String className = annotation.name();\n        if (className.isEmpty()) {\n            className = clazz.getSimpleName();\n        }\n        // \u521b\u5efabean\u5b9a\u4e49\n        GenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n        beanDefinition.setBeanClass(FeignClientFactoryBean.class);\n        beanDefinition.setLazyInit(false);\n        beanDefinition.getPropertyValues().add(\"type\", clazz);\n        beanDefinition.getPropertyValues().add(\"name\", className);\n        beanDefinition.getPropertyValues().add(\"url\", annotation.url());\n        beanDefinition.getPropertyValues().add(\"path\", annotation.path());\n        beanDefinition.getPropertyValues().add(\"primary\", annotation.primary());\n        beanDefinition.getPropertyValues().add(\"configuration\", annotation.configuration());\n        beanDefinition.getPropertyValues().add(\"decode404\", annotation.decode404());\n        beanDefinition.getPropertyValues().add(\"fallback\", annotation.fallback());\n        beanDefinition.getPropertyValues().add(\"fallbackFactory\", annotation.fallbackFactory());\n        beanDefinition.getPropertyValues().add(\"defaultFallback\", annotation.defaultFallback());\n        beanDefinition.getPropertyValues().add(\"contextId\", className);\n        // \u6ce8\u518cbean\n        registry.registerBeanDefinition(className, beanDefinition);\n    }\n}", "CodeBleu_score": 51.680047787015596}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\t// \u83b7\u53d6\u6ce8\u89e3\u7c7b\u4e0a\u7684\u6ce8\u89e3\n    Map<String, Object> attributes = importingClassMetadata.getAnnotationAttributes(EnableFeignClients.class.getName());\n    // \u83b7\u53d6\u6ce8\u89e3\u7c7b\u4e0a\u7684\u6ce8\u89e3\u7684\u5c5e\u6027\u503c\n    Class<?>[] clients = (Class[]) attributes.get(\"clients\");\n    Class<?>[] defaultClients = (Class[]) attributes.get(\"defaultClients\");\n    Class<?>[] basePackageClasses = (Class[]) attributes.get(\"basePackageClasses\");\n    String[] basePackages = (String[]) attributes.get(\"basePackages\");\n    Class<?>[] defaultConfiguration = (Class<?>[]) attributes.get(\"defaultConfiguration\");\n    Class<?>[] clientsConfiguration = (Class<?>[]) attributes.get(\"clientsConfiguration\");\n\n    // \u83b7\u53d6\u6ce8\u89e3\u7c7b\u4e0a\u7684\u6ce8\u89e3\u7684\u5c5e\u6027\u503c\n    if (clients != null && clients.length > 0) {\n        for (Class<?> clazz : clients) {\n            registerFeignClient(clazz, registry);\n        }\n    }\n\n    if (basePackages != null && basePackages.length > 0) {\n        for (String pkg : basePackages) {\n            // \u626b\u63cf\u6307\u5b9a\u5305\u4e0b\u7684\u7c7b\n            registerFeignClientsFromPackage(pkg, registry);\n        }\n    }\n\n    if (basePackageClasses != null && basePackageClasses.length > 0) {\n        for (Class<?> clazz : basePackageClasses) {\n            // \u626b\u63cf\u6307\u5b9a\u7c7b\u6240\u5728\u7684\u5305\u4e0b\u7684\u7c7b\n            registerFeignClientsFromClass(clazz, registry);\n        }\n    }\n\n    if (defaultConfiguration != null && defaultConfiguration.length > 0) {\n        registerDefaultConfiguration(defaultConfiguration, registry);\n    }\n\n    if (clientsConfiguration != null && clientsConfiguration.length > 0) {\n        registerFeignConfiguration(clientsConfiguration, registry);\n    }\n\n    if (defaultClients != null && defaultClients.length > 0) {\n        for (Class<?> clazz : defaultClients) {\n            registerFeignClient(", "CodeBleu_score": 34.84783602539007}]}
{"instance_id": "3d678e3bf3c8", "generation_result": [{"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}]}
