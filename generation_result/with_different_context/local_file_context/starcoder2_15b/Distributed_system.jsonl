{"instance_id": "ba8707e206f9", "generation_result": [{"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\tt_time, result = self._orig_kazoo_connect(*args)\n    self.loop_wait = max(2, self.loop_wait - 2)\n    return wait_time, result\n", "CodeBleu_score": 52.719580440989944}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\this function, _kazoo_connect, calls an original connection method with any given arguments, captures its return value, and then returns a tuple. The tuple consists of a calculated wait time in milliseconds and the second element of the original method's return value. The wait time is determined by subtracting 2 from the loop_wait attribute, ensuring it does not go below 2, and then multiplying by 1000.\n    wait_time, result = self._orig_kazoo_connect(*args)\n    wait_time = max(2, self.handler.loop_wait - 2) * 1000\n    return wait_time, result\n\n", "CodeBleu_score": 44.72776613320634}, {"code": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]\n\this function, _kazoo_connect, calls an original connection method with any given arguments, captures its return value, and then returns a tuple. The tuple consists of a calculated wait time in milliseconds and the second element of the original method's return value. The wait time is determined by subtracting 2 from the loop_wait attribute, ensuring it does not go below 2, and then multiplying by 1000.\n    wait, result = self._orig_kazoo_connect(*args)\n    self.handler.set_connect_timeout(self.handler._loop_wait)\n    return max(2.0, wait - 2.0) * 1000, result\n\n", "CodeBleu_score": 44.651120410879756}]}
{"instance_id": "88e24a7c26a4", "generation_result": [{"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\t\n    if isinstance(config, dict):\n        config = Config(config)\n    self.config = config\n    self.retry_timeout = self.config.get('retry_timeout', 10)\n    self.loop_wait = self.config.get('loop_wait', 10)\n    self.ttl = self.config.get('ttl', 30)\n    if self._client and isinstance(self._client._connection._handler, PatroniSequentialThreadingHandler):\n        self._client._connection._handler.set_connect_timeout(self.loop_wait)\n    try:\n        self.set_ttl(self.ttl)\n    except ZooKeeperError:\n        if self.loop_wait!= self.config.get('loop_wait', 10):\n            self._client.stop()\n            self._client.start()\n\n", "CodeBleu_score": 39.794027595780065}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\t\n    if isinstance(config, dict):\n        config = Config(config)\n    self.config = config\n    self.retry_timeout = self.config.get('retry_timeout', 10)\n    self.loop_wait = self.config.get('loop_wait', 10)\n    self.ttl = self.config.get('ttl', 30)\n    if self.client_handler and isinstance(self.client_handler, PatroniSequentialThreadingHandler):\n        self.client_handler.set_connect_timeout(self.loop_wait)\n    try:\n        self.set_ttl(self.ttl)\n    except Exception:\n        if self.loop_wait!= self.config.get('loop_wait', 10):\n            self.client.stop()\n            self.client.start()\n\n", "CodeBleu_score": 38.38241228639407}, {"code": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None\n\t\n    if isinstance(config, dict):\n        config = Config(config)\n    self._config = config\n    self._client.handler.set_connect_timeout(self._config.retry_timeout)\n    self._client.set_retry(KazooRetry(max_delay=1, max_tries=-1, sleep_func=time.sleep))\n    self._client.command.retry = KazooRetry(max_delay=1, max_tries=-1, deadline=self._config.retry_timeout, sleep_func=time.sleep)\n    self._client.set_default_retry(KazooRetry(max_delay=1, max_tries=-1, sleep_func=time.sleep))\n    if self._config.loop_wait!= self._client.handler.loop_wait:\n        self._client.handler.loop_wait = self._config.loop_wait\n        self._client.handler.set_connect_timeout(self._config.retry_timeout)\n        try:\n            self._client.set_ttl(self._config.ttl)\n        except Exception:\n            self._client.handler.loop_wait = self._config.loop_wait\n            self._client.stop()\n            self._client.start()\n\n", "CodeBleu_score": 34.88112019567654}]}
{"instance_id": "2af7d7b3c887", "generation_result": [{"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\t\n    # If the cluster is not initialized, we need to create a new one.\n    if not self.is_initialized(path):\n        return Cluster.from_status(self.get_status(path, None), self.get_members(path, None))\n\n    # If the cluster is initialized, we need to load the configuration.\n    config = self.get_node(path + self._CONFIG)\n    if not config:\n        return Cluster.from_status(self.get_status(path, None), self.get_members(path, None))\n\n    # We need to process the configuration to get the timeline history and synchronization state.\n    timeline_history = self.get_timeline_history(config[0])\n    sync_state = self.get_sync_state(config[0])\n\n    # We need to load the members and leader information.\n    members = self.get_members(path, timeline_history)\n    leader = self.get_leader(path, members)\n\n    # We need to process the cluster status to get the cluster status, failover state, and failsafe configuration.\n    status = self.get_status(path, leader)\n    failover = self.get_failover(path, leader)\n    failsafe = self.get_failsafe(path, leader)\n\n    # We need to check if the cluster is in the recovery state.\n    recovery = self.is_recovering(path)\n\n    # We need to check if the cluster is in the restart state.\n    restart = self.is_restarting(path)\n\n    # We need to check if the cluster is in the rewind state.\n    rewind = self.is_rewinding(path)\n\n    # We need to check if the cluster is in the demote state.\n    demote = self.is_demoting(path)\n\n    # We need to check if the cluster is in the reinitialize state.\n    reinitialize = self.is_reinitializing(path)\n\n    # We need to check if the cluster is in the reinitialize state.\n    reinitialize_force = self.is_reinitialize_forcing(path)\n\n    # We need to check if the cluster is in the reinitialize state.\n    reinitialize_wait = self.is_reinitialize_waiting(path)\n\n    # We need to check if the cluster is in the reinitialize state.\n    reinitialize_cancel = self.is_reinitialize", "CodeBleu_score": 25.561027804842883}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\t\n    if self.get_node(path + self._INITIALIZATION) is not None:\n        return Cluster.from_node(self.get_node(path + self._INITIALIZATION)[0])\n\n    cluster = Cluster.from_node(self.get_node(path + self._CONFIG)[0])\n    cluster.history = self.get_node(path + self._TIMELINE_HISTORY)\n    cluster.sync = self.get_node(path + self._SYNC_STATE)\n    cluster.members = self.load_members(path)\n    cluster.leader = self.get_node(path + self._LEADER)\n    cluster.status = self.get_status(path, cluster.leader)\n    cluster.failover = self.get_node(path + self._FAILOVER)\n    cluster.failsafe = self.get_node(path + self._FAILSAFE)\n    return cluster\n\n", "CodeBleu_score": 16.40133567071676}, {"code": "def _postgresql_cluster_loader(self, path: str) -> Cluster\n\t\n    cluster = Cluster(self.name, self.namespace, self.scope, self.config.data_dir, self.config.use_unix_socket)\n\n    def _get_node(key: str, watch: Optional[Callable[[WatchedEvent], None]] = None) -> Optional[Tuple[str, ZnodeStat]]:\n        \"\"\"This function retrieves a node from a given key in the ZooKeeper cluster. It attempts to retrieve the node using the get_node method of the DCS object. If the node is found, it returns the node data and its znode stat. If the node is not found, it returns None. The watch parameter is an optional callable that is used to watch for changes to the node.\"\"\"\n        try:\n            ret = self.get_node(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None\n\n    def _get_children(self, key: str) -> List[str]:\n        \"\"\"This function retrieves the children of a given key in the ZooKeeper cluster. It attempts to retrieve the children using the get_children method of the DCS object. If the key is found, it returns the list of children. If the key is not found, it returns an empty list. This function is used to retrieve the list of members in a PostgreSQL cluster.\"\"\"\n        try:\n            return self.get_children(key)\n        except NoNodeError:\n            return []\n\n    def _get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        \"\"\"This function retrieves the status of a given PostgreSQL cluster. It attempts to retrieve the status using the get_status method of the DCS object. If the status is found, it returns the status object. If the status is not found, it returns a Status object with the given leader. This function is used to retrieve the status of a PostgreSQL cluster.\"\"\"\n        status = _get_node(path + self._STATUS)\n        if not status:\n            status = _get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])\n\n    def _get_member(self, name: str, value: str, znode: ZnodeStat) -> Member:\n        \"\"\"This function retrieves a member of a given PostgreSQL cluster. It creates a Member object with the given name, value, and", "CodeBleu_score": 23.176749525280325}]}
{"instance_id": "43e1bc044d8e", "generation_result": [{"code": "def attempt_to_acquire_leader(self) -> bool\n\t\n    if self._client.state!= KazooState.CONNECTED:\n        raise ZooKeeperError('ZooKeeper connection is closed')\n\n    try:\n        return self._client.retry(self._client.create, self._path, b'', ephemeral=True, makepath=True)\n    except NodeExistsError:\n        logger.info('Leader lock is already acquired')\n        return False\n    except Exception:\n        logger.exception('Failed to acquire leader lock')\n        raise ZooKeeperError('ZooKeeper in not responding properly')\n\n\n", "CodeBleu_score": 44.45866042693542}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\t\n    try:\n        self._client.retry(self._client.create, self._path, ephemeral=True)\n        return True\n    except Exception as e:\n        logger.exception('Failed to acquire leader lock at %s', self._path)\n        if isinstance(e, NodeExistsError):\n            logger.info('Leader lock is already acquired')\n        elif isinstance(e, NoNodeError):\n            logger.info('Leader lock parent node does not exist')\n        else:\n            raise ZooKeeperError('ZooKeeper in not responding properly') from e\n    return False\n\n", "CodeBleu_score": 48.730105079488304}, {"code": "def attempt_to_acquire_leader(self) -> bool\n\t\n    try:\n        return self._client.retry(self._client.create, self._path, ephemeral=True, sequence=True)\n    except NodeExistsError:\n        logger.warning('Leader key already exists, another instance must be running')\n        return False\n    except Exception:\n        logger.exception('Failed to acquire leader lock')\n        raise ZooKeeperError('ZooKeeper in not responding properly')\n", "CodeBleu_score": 35.10950688358739}]}
{"instance_id": "93d126ab8a22", "generation_result": [{"code": "def _update_leader(self, leader: Leader) -> bool\n\t\n    if self._client.client_id is not None and leader.session!= self._client.client_id[0]:\n        logger.warning('Recreating the leader ZNode due to ownership mismatch')\n        try:\n            self._client.delete_async(self.leader_path).get(timeout=1)\n        except (NoNodeError, ConnectionClosedError, RetryFailedError):\n            pass\n        except Exception:\n            return False\n    try:\n        self._client.create_async(self.leader_path, self._name.encode('utf-8'), makepath=True, ephemeral=True).get(timeout=1)\n        return True\n    except Exception as e:\n        if not isinstance(e, NodeExistsError):\n            logger.exception('update_leader')\n            return False\n\n", "CodeBleu_score": 44.22743784178128}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\t\n    if self._client.client_id is not None and leader.session!= self._client.client_id[0]:\n        logger.warning('Recreating the leader ZNode due to ownership mismatch')\n        try:\n            self._client.delete_async(self.leader_path).get(timeout=1)\n        except NoNodeError:\n            pass\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception:\n            return False\n    try:\n        self._client.create_async(self.leader_path, self._name.encode('utf-8'), makepath=True, ephemeral=True).get(timeout=1)\n        return True\n    except Exception as e:\n        if not isinstance(e, NodeExistsError):\n            logger.exception('take_leader')\n            return False\n\n", "CodeBleu_score": 55.4663297800121}, {"code": "def _update_leader(self, leader: Leader) -> bool\n\t\n    if self._client.client_id is not None and leader.session!= self._client.client_id[0]:\n        logger.warning('Recreating the leader ZNode due to ownership mismatch')\n        try:\n            self._client.delete_async(self.leader_path).get(timeout=1)\n        except (NoNodeError, ConnectionClosedError, RetryFailedError):\n            pass\n        except Exception:\n            return False\n    try:\n        self._client.create_async(self.leader_path, self._name.encode('utf-8'), makepath=True, ephemeral=True).get(timeout=1)\n        return True\n    except Exception as e:\n        if not isinstance(e, NodeExistsError):\n            logger.exception('update_leader')\n            return False\n\n", "CodeBleu_score": 44.22743784178128}]}
{"instance_id": "f68719982812", "generation_result": [{"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() > self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        for exhibitor in self._exhibitors:\n            try:\n                response = requests_get(uri(exhibitor, self._exhibitor_port, self._uri_path), timeout=self.TIMEOUT)\n                response.raise_for_status()\n                data = json.loads(response.text)\n                if data and data['servers']:\n                    self._exhibitors = data['servers']\n                    self._zookeeper_hosts = ','.join(data['servers'])\n                    logger.info('exhibitor ensemble changed: %s', self._zookeeper_hosts)\n                    return True\n            except Exception as e:\n                logger.info('exhibitor ensemble poll failed: %s', e)\n                pass\n        return False\n\n    return False\n\n", "CodeBleu_score": 40.68670508525746}, {"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() > self._next_poll:\n        logger.debug('polling exhibitor')\n        self._next_poll = time.time() + self._poll_interval\n        for exhibitor in self._exhibitors:\n            try:\n                status = self._get_exhibitor_status(exhibitor)\n            except Exception:\n                logger.exception('unable to get exhibitor status')\n                continue\n            if not status:\n                continue\n            logger.debug('exhibitor status: %s', status)\n            if not self._update_zookeeper_hosts(status):\n                continue\n            logger.info('exhibitor status changed to %s', status)\n            return True\n        return False\n\n    def _get_exhibitor_status(self, exhibitor: str) -> Union[Dict[str, Any], None]:\n    \"\"\"The function queries the status of the exhibitor at the given URL and returns a dictionary containing the server and port information if the status is valid, or None otherwise.\"\"\"\n    url = uri('http', exhibitor, self._exhibitor_port, self._uri_path)\n    try:\n        response = requests_get(url, timeout=self.TIMEOUT)\n    except Exception:\n        logger.exception('unable to get exhibitor status')\n        return None\n    if response.status_code!= 200:\n        logger.debug('exhibitor status code: %s', response.status_code)\n        return None\n    try:\n        return json.loads(response.text)\n    except Exception:\n        logger.exception('unable to parse exhibitor status')\n        return None\n\n    def _update_zookeeper_hosts(self, status: Dict[str, Any]) -> bool:\n    \"\"\"The function updates the ZooKeeper connection string if it has changed, logging the change and updating internal state. It returns True if a change occurred, and False otherwise.\"\"\"\n    hosts = ','.join([f'{server}:{port}' for server, port in status['servers'] if server!= 'localhost'])\n    if self._zookeeper_hosts!= hosts:\n        logger.info('updating zookeeper hosts to %s', hosts", "CodeBleu_score": 36.49616858843841}, {"code": "def poll(self) -> bool\n\tif self._next_poll is None or time.time() >= self._next_poll:\n        self._next_poll = time.time() + self._poll_interval\n        for exhibitor in self._exhibitors:\n            try:\n                response = requests_get(uri(exhibitor, self._exhibitor_port, self._uri_path), timeout=self.TIMEOUT)\n                if response.status_code == 200:\n                    data = json.loads(response.text)\n                    if data:\n                        self._exhibitors = data\n                        if self._zookeeper_hosts!= self._get_zookeeper_hosts():\n                            self._zookeeper_hosts = self._get_zookeeper_hosts()\n                            logger.info('exhibitor ensemble changed: %s', self._zookeeper_hosts)\n                            return True\n            except Exception as e:\n                logger.debug('exhibitor ensemble polling failed: %s', e)\n        return False\n\n    return False\n\n", "CodeBleu_score": 37.982259586995866}]}
{"instance_id": "022ed3721913", "generation_result": [{"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\t self._ensemble_provider.poll():\n        self._client.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)", "CodeBleu_score": 82.04568684693892}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\t self._ensemble_provider.poll():\n        self._client.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n", "CodeBleu_score": 82.04568684693892}, {"code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]\n\t self._ensemble_provider and self._ensemble_provider.poll():\n        self._client.hosts = self._ensemble_provider.zookeeper_hosts\n    return super(Exhibitor, self)._load_cluster(path, loader)\n\n", "CodeBleu_score": 78.8720169684002}]}
{"instance_id": "c7d2e1db0cda", "generation_result": [{"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\nthis.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 78.49526490509582}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\nthis.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 78.49526490509582}, {"code": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener){\n\tthis.nodeCache = nodeCache;\nthis.nodeCacheListener = nodeCacheListener;\n}", "CodeBleu_score": 78.49526490509582}]}
{"instance_id": "c779f567d3e2", "generation_result": [{"code": "public void destroy(){\n\tf (zookeeperListener == null) {\nreturn;\n}\n\nString group = getGroup();\nString dataId = getDataId();\n\nlogUnsubscribeStarted();\n\ntry {\nzookeeperListener.unsubscribeConfig(group, dataId);\n} catch (Exception e) {\nlogUnsubscribeFailed(e);\n}\n}", "CodeBleu_score": 73.79315909628242}, {"code": "public void destroy(){\n\tf (zookeeperListener == null) {\nreturn;\n}\n\nString group = getGroup();\nString dataId = getDataId();\n\nlogUnsubscribeStarted();\n\ntry {\nzookeeperListener.unsubscribeConfig(group, dataId);\n} catch (Exception e) {\nlogUnsubscribeFailed(e);\n}\n}", "CodeBleu_score": 73.79315909628242}, {"code": "public void destroy(){\n\tf (zookeeperListener == null) {\nreturn;\n}\n\nString group = getGroup();\nString dataId = getDataId();\n\nlogUnsubscribeStarted();\n\ntry {\nzookeeperListener.unsubscribeConfig(group, dataId);\n} catch (Exception e) {\nlogUnsubscribeFailed(e);\n}\n}", "CodeBleu_score": 73.79315909628242}]}
{"instance_id": "7f0ce4db487b", "generation_result": [{"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\nZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\nreturn zookeeperServer.getPayload();\n}\n\nreturn null;\n}", "CodeBleu_score": 58.81707376954307}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\nZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\nreturn zookeeperServer.getPayload();\n}\n\nreturn null;\n}", "CodeBleu_score": 58.81707376954307}, {"code": "public Map<String, String> getServerMetadata(Server server){\n\tif (server instanceof ZookeeperServer) {\nZookeeperServer zookeeperServer = (ZookeeperServer) server;\n\nreturn zookeeperServer.getPayload();\n}\n\nreturn null;\n}", "CodeBleu_score": 58.81707376954307}]}
{"instance_id": "7365d194b7ee", "generation_result": [{"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\nNodeCache nodeCache = new NodeCache(curatorFramework, path);\nnodeCache.start();\n\nNodeCacheListener nodeCacheListener = new NodeCacheListener() {\n@Override\npublic void nodeChanged() throws Exception {\nChildData childData = nodeCache.getCurrentData();\nif (childData!= null) {\nString config = new String(childData.getData());\nzookeeperSubscribeCallback.onSubscribe(config);\n}\n}\n};\n\nnodeCache.getListenable().addListener(nodeCacheListener);\n\nZookeeperListener zookeeperListener = new ZookeeperListener();\nzookeeperListener.setNodeCache(nodeCache);\nzookeeperListener.setNodeCacheListener(nodeCacheListener);\n\nreturn zookeeperListener;\n}", "CodeBleu_score": 70.8442709104538}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\tString path = getPath(group, serviceId);\n\nboolean hasPath = hasPath(path);\nif (!hasPath) {\nreturn null;\n}\n\nNodeCache nodeCache = new NodeCache(curatorFramework, path);\nnodeCache.start();\n\nNodeCacheListener nodeCacheListener = new NodeCacheListener() {\n@Override\npublic void nodeChanged() throws Exception {\nChildData childData = nodeCache.getCurrentData();\nif (childData == null) {\nreturn;\n}\n\nString config = new String(childData.getData());\nzookeeperSubscribeCallback.onSubscribe(config);\n}\n};\n\nnodeCache.getListenable().addListener(nodeCacheListener);\n\nZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\nzookeeperListener.addZookeeperListener();\n\nreturn zookeeperListener;\n}", "CodeBleu_score": 68.37727376513637}, {"code": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception{\n\t\tString path = getPath(group, serviceId);\n\n\tNodeCache nodeCache = new NodeCache(curatorFramework, path);\n\tnodeCache.start();\n\n\tNodeCacheListener nodeCacheListener = new NodeCacheListener() {\n\t\t@Override\n\t\tpublic void nodeChanged() throws Exception {\n\t\t\tChildData childData = nodeCache.getCurrentData();\n\t\t\tif (childData!= null) {\n\t\t\t\tString config = new String(childData.getData());\n\t\t\t\tzookeeperSubscribeCallback.onSubscribe(config);\n\t\t\t} else {\n\t\t\t\tzookeeperSubscribeCallback.onSubscribe(null);\n\t\t\t}\n\t\t}\n\t};\n\n\tZookeeperListener zookeeperListener = new ZookeeperListener();\n\tzookeeperListener.setNodeCache(nodeCache);\n\tzookeeperListener.setNodeCacheListener(nodeCacheListener);\n\tzookeeperListener.add();\n\n\treturn zookeeperListener;\n}", "CodeBleu_score": 69.50780988880675}]}
{"instance_id": "6f492670162b", "generation_result": [{"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\nserverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\nserverList.setServiceId(config.getClientName());\nreturn serverList;\n}", "CodeBleu_score": 72.30778809199214}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\nserverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\nserverList.setServiceId(config.getClientName());\n\nreturn serverList;\n}", "CodeBleu_score": 72.30778809199214}, {"code": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery){\n\tZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\nserverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\nserverList.setServiceId(config.getClientName());\n\nreturn serverList;\n}", "CodeBleu_score": 72.30778809199214}]}
{"instance_id": "17c01a5f8ef8", "generation_result": [{"code": "public void register(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onRegister(registration);\n} catch (BeansException e) {\n// Ignore\n}\nserviceRegistry.register(registration);\n}", "CodeBleu_score": 87.72512978610983}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onRegister(registration);\n} catch (BeansException e) {\n// Ignore\n}\nserviceRegistry.register(registration);\n}", "CodeBleu_score": 87.72512978610983}, {"code": "public void register(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onRegister(registration);\n} catch (BeansException e) {\n// Ignore\n}\nserviceRegistry.register(registration);\n}", "CodeBleu_score": 87.72512978610983}]}
{"instance_id": "99392dfa2335", "generation_result": [{"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onDeregister(registration);\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\n\nserviceRegistry.deregister(registration);\n}", "CodeBleu_score": 87.78401070371797}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onDeregister(registration);\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\n\nserviceRegistry.deregister(registration);\n}", "CodeBleu_score": 87.78401070371797}, {"code": "public void deregister(ZookeeperRegistration registration){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onDeregister(registration);\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\n\nserviceRegistry.deregister(registration);\n}", "CodeBleu_score": 87.78401070371797}]}
{"instance_id": "02a835946e90", "generation_result": [{"code": "public void close(){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onClose();\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\n\nserviceRegistry.close();\n}", "CodeBleu_score": 95.55281258320267}, {"code": "public void close(){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onClose();\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\nserviceRegistry.close();\n}", "CodeBleu_score": 95.55281258320267}, {"code": "public void close(){\n\ttry {\nRegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\nregisterListenerExecutor.onClose();\n} catch (BeansException e) {\n// LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n}\n\nserviceRegistry.close();\n}", "CodeBleu_score": 95.55281258320267}]}
{"instance_id": "64cf817f5945", "generation_result": [{"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = globalConfig? globalGroup : partialGroup;\nString dataId = globalConfig? globalDataId : partialDataId;\n\nlog.info(\\\"Subscribing to Zookeeper configuration. group={}, dataId={}\\\", group, dataId);\n\ntry {\n    zookeeperOperation.subscribeConfig(group, dataId, new ZookeeperSubscribeCallback() {\n        @Override\n        public void onSubscribe(String content) {\n            try {\n                onSubscribe(content, globalConfig);\n            } catch (Exception e) {\n                log.error(\\\"Failed to subscribe to Zookeeper configuration. group={}, dataId={}\\\", group, dataId, e);\n            }\n        }\n    });\n} catch (Exception e) {\n    log.error(\\\"Failed to subscribe to Zookeeper configuration. group={}, dataId={}\\\", group, dataId, e);\n    return null;\n}\n\nlog.info(\\\"Subscribed to Zookeeper configuration. group={}, dataId={}\\\", group, dataId);\n\nreturn new ZookeeperListener(group, dataId);\n}", "CodeBleu_score": 51.768375555400304}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\tString group = globalConfig? getGlobalGroup() : getPartialGroup();\nString dataId = globalConfig? getGlobalDataId() : getPartialDataId();\n\nlog.info(\\\"Subscribing to Zookeeper configuration, group: {}, dataId: {}\\\", group, dataId);\n\ntry {\nreturn zookeeperOperation.subscribe(group, dataId, new ZookeeperSubscribeCallback() {\n@Override\npublic void onSubscribe(String config) {\nlog.info(\\\"Received Zookeeper configuration update, group: {}, dataId: {}, config: {}\\\", group, dataId, config);\n\ntry {\nConfigType configType = globalConfig? ConfigType.GLOBAL : ConfigType.PARTIAL;\nsetConfig(config, configType);\n} catch (Exception e) {\nlog.error(\\\"Failed to set Zookeeper configuration, group: {}, dataId: {}, config: {}\\\", group, dataId, config, e);\n}\n}\n});\n} catch (Exception e) {\nlog.error(\\\"Failed to subscribe to Zookeeper configuration, group: {}, dataId: {}\\\", group, dataId, e);\n}\n\nreturn null;\n}", "CodeBleu_score": 44.074192420932846}, {"code": "private ZookeeperListener subscribeConfig(boolean globalConfig){\n\t\tString group = globalConfig? getGlobalGroup() : getPartialGroup();\n\tString dataId = globalConfig? getGlobalDataId() : getPartialDataId();\n\n\tlog.info(\"Start to subscribe Zookeeper configuration, group={}, dataId={}\", group, dataId);\n\n\ttry {\n\t\treturn zookeeperOperation.subscribeConfig(group, dataId, new ZookeeperSubscribeCallback() {\n\t\t\t@Override\n\t\t\tpublic void onConfigChange(String group, String dataId, String config) {\n\t\t\t\tlog.info(\"Zookeeper configuration updated, group={}, dataId={}, config={}\", group, dataId, config);\n\t\t\t\tnotifyConfigChange(config, globalConfig? ConfigType.GLOBAL : ConfigType.PARTIAL);\n\t\t\t}\n\t\t});\n\t} catch (Exception e) {\n\t\tlog.error(\"Zookeeper configuration subscription failed, group={}, dataId={}\", group, dataId, e);\n\t}\n\n\treturn null;\n}", "CodeBleu_score": 43.254746301188646}]}
{"instance_id": "be408acc71f6", "generation_result": [{"code": "public CuratorFramework curatorFramework(){\n\t\tString connectionString = environment.getProperty(ZookeeperConstant.SPRING_CLOUD_ZOOKEEPER_CONNECT_STRING);\n\\tif (StringUtils.isEmpty(connectionString)) {\n\\t\\tthrow new DiscoveryException(\\\"Zookeeper connection string cannot be empty\\\");\n\\t}\n\n\\tint retryCount = environment.getProperty(ZookeeperConstant.SPRING_CLOUD_ZOOKEEPER_CONNECT_RETRY_COUNT, Integer.class, 3);\n\\tint sleepTime = environment.getProperty(ZookeeperConstant.SPRING_CLOUD_ZOOKEEPER_CONNECT_SLEEP_TIME, Integer.class, 1000);\n\n\\tCuratorFramework client = CuratorFrameworkFactory.newClient(connectionString, new ExponentialBackoffRetry(retryCount, sleepTime));\n\\tclient.start();\n\n\\treturn client;\n}", "CodeBleu_score": 43.78660522908184}, {"code": "public CuratorFramework curatorFramework(){\n\t\tString connectionString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECTION_STRING);\n\\tif (StringUtils.isEmpty(connectionString)) {\n\t\tthrow new DiscoveryException(\\\"Zookeeper connection string is empty\\\");\n\t}\n\n\tint retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class, 3);\n\tint sleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, Integer.class, 1000);\n\n\tCuratorFramework client = CuratorFrameworkFactory.builder()\n\t\t\t.connectString(connectionString)\n\t\t\t.retryPolicy(new ExponentialBackoffRetry(retryCount, sleepTime))\n\t\t\t.build();\n\n\tclient.start();\n\n\treturn client;\n}", "CodeBleu_score": 59.657396443871804}, {"code": "public CuratorFramework curatorFramework(){\n\t", "CodeBleu_score": 0.9803101440509456}]}
{"instance_id": "3628d7b6f0a5", "generation_result": [{"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t\n/**\n * The gatewayStrategyRouteZookeeperProcessor method returns a new instance of GatewayStrategyRouteZookeeperProcessor.\n * It is annotated with @Bean, indicating that it should be registered as a bean in the Spring context.\n * The method is conditional on the property \"spring.cloud.gateway.discovery.locator.enabled\" being set to \"false\" or missing.\n * This means that the bean will only be created if that property is set to \"false\" or is missing.\n * The method also has the @ConditionalOnProperty annotation, which specifies that the bean should only be created if the property \"spring.cloud.gateway.discovery.locator.enabled\" is set to \"false\" or is missing.\n * This is to ensure that the bean is only created if the gateway is not using the service discovery locator.\n * The method also has the @ConditionalOnClass annotation, which specifies that the bean should only be created if the class ZookeeperProcessor is present on the classpath.\n * This is to ensure that the bean is only created if the ZookeeperProcessor class is available, which is required for the gateway route to work with Zookeeper.\n * The method also has the @ConditionalOnProperty annotation, which specifies that the bean should only be created if the property \"spring.application.strategy.gateway.dynamic-route.enabled\" is set to \"true\".\n * This is to ensure that the bean is only created if dynamic route is enabled for the gateway.\n * The method returns a new instance of GatewayStrategyRouteZookeeperProcessor, which is a class that implements the GatewayStrategyRoute interface and provides functionality for dynamically routing gateway requests based on Zookeeper.\n */\n@Bean\n@ConditionalOnProperty(value = \"spring.cloud.gateway.discovery.locator.enabled\", havingValue = \"false\", matchIfMissing = true)\npublic ZookeeperProcessor gatewayStrategyRouteZookeeperProcessor() {\nreturn new GatewayStrategyRouteZookeeperProcessor();\n}\n}", "CodeBleu_score": 72.06432740825706}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t\t@Bean\n\t@ConditionalOnProperty(value = \"spring.cloud.gateway.discovery.locator.enabled\", havingValue = \"false\", matchIfMissing = true)\n\tpublic ZookeeperProcessor gatewayStrategyRouteZookeeperProcessor() {\n\t\treturn new GatewayStrategyRouteZookeeperProcessor();\n\t}\n}", "CodeBleu_score": 72.10890884244438}, {"code": "protected static class GatewayRouteZookeeperConfiguration{\n\t\n/**\n * The gatewayStrategyRouteZookeeperProcessor() method is a conditional bean method that returns a new instance of GatewayStrategyRouteZookeeperProcessor.\n * It is annotated with @ConditionalOnProperty, which specifies that the bean should only be created if the value of the property \"spring.cloud.gateway.discovery.locator.enabled\" is set to \"false\" or is missing.\n * The @ConditionalOnProperty annotation is used to conditionally create a bean based on the value of a specific property.\n * In this case, the bean will only be created if the value of the property \"spring.cloud.gateway.discovery.locator.enabled\" is set to \"false\" or is missing.\n * If the property is set to any other value, the bean will not be created.\n * The @ConditionalOnProperty annotation is useful for creating beans that are dependent on specific configuration settings.\n */\n@Bean\n@ConditionalOnProperty(value = \"spring.cloud.gateway.discovery.locator.enabled\", havingValue = \"false\", matchIfMissing = true)\npublic ZookeeperProcessor gatewayStrategyRouteZookeeperProcessor() {\nreturn new GatewayStrategyRouteZookeeperProcessor();\n}\n}", "CodeBleu_score": 72.06432740825706}]}
{"instance_id": "68a13b54ad7c", "generation_result": [{"code": "public  CuratorFramework createCuratorFramework(){\n\t\t// Create a retry policy with exponential backoff\n\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n\n\t// Create a CuratorFramework instance with the specified properties and retry policy\n\tCuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n\t\t\t.connectString(zooKeeperClientProperties.getHosts())\n\t\t\t.sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeout())\n\t\t\t.namespace(zooKeeperClientProperties.getNamespace())\n\t\t\t.retryPolicy(retryPolicy)\n\t\t\t.build();\n\n\t// Start the CuratorFramework instance\n\tcuratorFramework.start();\n\n\treturn curatorFramework;\n}", "CodeBleu_score": 64.4103734499318}, {"code": "public  CuratorFramework createCuratorFramework(){\n\t// Configure the connection settings\nCuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n.connectString(zooKeeperClientProperties.getConnectString())\n.sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeoutMs())\n.namespace(zooKeeperClientProperties.getNamespace());\n\n// Configure the retry policy\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getBaseSleepTimeMs(), zooKeeperClientProperties.getMaxRetries());\nbuilder.retryPolicy(retryPolicy);\n\n// Start the CuratorFramework and return the instance\nCuratorFramework curatorFramework = builder.build();\ncuratorFramework.start();\nreturn curatorFramework;\n}", "CodeBleu_score": 58.21874434738099}, {"code": "public  CuratorFramework createCuratorFramework(){\n\t// Create a retry policy\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(zooKeeperClientProperties.getRetryPolicy().getBaseSleepTimeMs(), zooKeeperClientProperties.getRetryPolicy().getMaxRetries());\n\n// Create a CuratorFramework instance\nCuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n       .connectString(zooKeeperClientProperties.getHosts())\n       .sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeout())\n       .namespace(zooKeeperClientProperties.getNamespace())\n       .retryPolicy(retryPolicy)\n       .build();\n\n// Start the CuratorFramework\ncuratorFramework.start();\n\n// Return the CuratorFramework instance\nreturn curatorFramework;\n}", "CodeBleu_score": 64.16123502802836}]}
{"instance_id": "018a3d7b5f59", "generation_result": [{"code": "public void lock(String path) throws DistributedLockException{\n\tInterProcessMutex lock = locksMap.get(Thread.currentThread());\nif (lock == null) {\ntry {\nlock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\nif (lock.acquire(10, TimeUnit.SECONDS)) {\nlocksMap.put(Thread.currentThread(), lock);\n} else {\nthrow new DistributedLockException(\"Failed to acquire lock: \" + path);\n}\n} catch (Exception e) {\nthrow new DistributedLockException(\"Failed to acquire lock: \" + path, e);\n}\n} else {\ntry {\nlock.acquire();\n} catch (Exception e) {\nthrow new DistributedLockException(\"Failed to re-acquire lock: \" + path, e);\n}\n}\n}", "CodeBleu_score": 47.88953959838056}, {"code": "public void lock(String path) throws DistributedLockException{\n\tInterProcessMutex lock = locksMap.get(Thread.currentThread());\nif (lock == null) {\nlock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\nlocksMap.put(Thread.currentThread(), lock);\n}\ntry {\nif (!lock.acquire(30, TimeUnit.SECONDS)) {\nthrow new DistributedLockException(\"Failed to acquire lock for path: \" + path);\n}\n} catch (Exception e) {\nthrow new DistributedLockException(\"Failed to acquire lock for path: \" + path, e);\n}\n}", "CodeBleu_score": 40.25714763228091}, {"code": "public void lock(String path) throws DistributedLockException{\n\t\tInterProcessMutex lock = locksMap.get(Thread.currentThread());\n\tif (lock == null) {\n\t\tlock = new InterProcessMutex(curatorFrameworkClient.getCuratorFramework(), path);\n\t\ttry {\n\t\t\tlock.acquire();\n\t\t} catch (Exception e) {\n\t\t\tthrow new DistributedLockException(e);\n\t\t}\n\t\tlocksMap.put(Thread.currentThread(), lock);\n\t} else {\n\t\ttry {\n\t\t\tlock.acquire();\n\t\t} catch (Exception e) {\n\t\t\tthrow new DistributedLockException(e);\n\t\t}\n\t}\n}", "CodeBleu_score": 42.13097954764383}]}
{"instance_id": "2068af09c973", "generation_result": [{"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\t\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n\tString nodePath = nodePath(dtpProperties);\n\tif (dtpProperties.getConfigFileType().equals(PROPERTIES)) {\n\t\treturn genPropertiesTypeMap(curatorFramework, nodePath);\n\t} else if (dtpProperties.getConfigFileType().equals(JSON)) {\n\t\tString configKey = dtpProperties.getConfigKey();\n\t\tString path = ZKPaths.makePath(nodePath, configKey);\n\t\tString value = getCuratorFramework(dtpProperties).getData().forPath(path);\n\t\treturn ConfigHandler.parse(value, dtpProperties.getConfigFileType());\n\t} else {\n\t\tthrow new RuntimeException(\"Unsupported configuration type.\");\n\t}\n}", "CodeBleu_score": 47.57817194846831}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\t\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\n\tString nodePath = nodePath(dtpProperties);\n\tif (PROPERTIES.equals(dtpProperties.getConfigType())) {\n\t\treturn genPropertiesTypeMap(curatorFramework, nodePath);\n\t}\n\tif (JSON.equals(dtpProperties.getConfigType())) {\n\t\tString configKey = dtpProperties.getConfigKey();\n\t\tString configValue = getConfigValue(curatorFramework, ZKPaths.makePath(nodePath, configKey));\n\t\treturn ConfigHandler.parseJsonToMap(configValue);\n\t}\n\treturn Collections.emptyMap();\n}", "CodeBleu_score": 42.64625807139368}, {"code": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties){\n\tCuratorFramework curatorFramework = getCuratorFramework(dtpProperties);\nString nodePath = nodePath(dtpProperties);\nif (PROPERTIES.equals(dtpProperties.getConfigType())) {\nreturn genPropertiesTypeMap(curatorFramework, nodePath);\n}\nString configKey = ZKPaths.makePath(nodePath, dtpProperties.getConfigKey());\nString value = new String(curatorFramework.getData().forPath(configKey), StandardCharsets.UTF_8);\nreturn ConfigHandler.parse(value);\n}", "CodeBleu_score": 42.88218547682791}]}
{"instance_id": "3d3c9020d299", "generation_result": [{"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\t\tDtpProperties dtpProperties = new DtpProperties();\n\tBinderHelper.bind(environment.getPropertySources(), \"dtp\", dtpProperties);\n\tMap<String, Object> properties = dtpProperties.toProperties();\n\tif (!environment.containsProperty(DtpProperties.DTP_ZK_ADDRESS)) {\n\t\tenvironment.getPropertySources().addFirst(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties));\n\t}\n}", "CodeBleu_score": 40.1450770695381}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\tDtpProperties dtpProperties = BinderHelper.bind(DtpProperties.class, environment);\n\nMap<String, Object> properties = BinderHelper.bind(Map.class, environment);\n\nif (dtpProperties.isEnableZkConfig()) {\nif (properties.containsKey(DtpProperties.DTP_ZK_CONFIG_NAMESPACE)) {\nString zkConfigNamespace = dtpProperties.getZkConfigNamespace();\n\nif (zkConfigNamespace == null || zkConfigNamespace.isEmpty()) {\nlog.error(\"zkConfigNamespace is empty, please check the configuration\");\n\nreturn;\n}\n\nString zkConfigPath = zkConfigNamespace + DtpProperties.ZK_CONFIG_PATH_SEPARATOR + dtpProperties.getAppName();\n\nMap<String, Object> zkProperties = CuratorUtil.getZkProperties(zkConfigPath);\n\nif (zkProperties == null || zkProperties.isEmpty()) {\nlog.error(\"zkProperties is empty, please check the configuration\");\n\nreturn;\n}\n\nMutablePropertySources propertySources = environment.getPropertySources();\n\npropertySources.addFirst(new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, zkProperties));\n} else {\nlog.error(\"dtp.zk-config.namespace is empty, please check the configuration\");\n}\n}\n}", "CodeBleu_score": 42.611950337799286}, {"code": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application){\n\t", "CodeBleu_score": 5.918508044412982}]}
{"instance_id": "0a934e7885be", "generation_result": [{"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\t.Register(provider => new ServiceRouteFactory(provider.Resolve<ISerializer<string>>(), provider.Resolve<ILogger<ServiceRouteFactory>>()))\n.SingleInstance();\nbuilder.Register(provider => new ZookeeperServiceRouteManager(provider.Resolve<ISerializer<string>>(), provider.Resolve<IServiceRouteFactory>(), provider.Resolve<ILogger<ZookeeperServiceRouteManager>>(), provider.Resolve<IZookeeperClientProvider>(), configInfo))\n.SingleInstance();\nreturn this;\n}", "CodeBleu_score": 52.75100558519059}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\t.Register(provider => {\nvar serializer = provider.GetRequiredService<ISerializer<string>>();\nvar routeFactory = provider.GetRequiredService<IServiceRouteFactory>();\nvar logger = provider.GetRequiredService<ILogger<ServiceRouteManager>>();\nvar clientProvider = provider.GetRequiredService<IZookeeperClientProvider>();\nvar serviceRouteManager = new ServiceRouteManager(serializer, routeFactory, logger, clientProvider, configInfo);\nreturn serviceRouteManager;\n}).InstancePerLifetimeScope();\nreturn this;\n}", "CodeBleu_score": 36.703741643969096}, {"code": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tializer = new KryoSerializer();\nvar routeFactory = new ZookeeperServiceRouteFactory(serializer);\nvar logger = builder.GetLogger<ZookeeperServiceRouteManager>();\nvar clientProvider = builder.GetRequiredService<IZookeeperClientProvider>();\nvar routeManager = new ZookeeperServiceRouteManager(routeFactory, logger, clientProvider, configInfo);\nUseRouteManager(builder, provider => routeManager);\nreturn this;\n}", "CodeBleu_score": 34.45992624609686}]}
{"instance_id": "c30a17587331", "generation_result": [{"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tmandManager(builder, provider => {\n var result = new ZooKeeperServiceCommandManager(GetConfigInfo(configInfo), provider.GetRequiredService<ISerializer<byte[]>>(), provider.GetRequiredService<ISerializer<string>>(), provider.GetRequiredService<IServiceCommandFactory>(), provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(), provider.GetRequiredService<IZookeeperClientProvider>());\n return result;\n });\n return this;\n }", "CodeBleu_score": 67.89030872166589}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tmandManager(builder, provider => {\n var result = new ZooKeeperServiceCommandManager(\n GetConfigInfo(configInfo),\n provider.GetRequiredService<ISerializer<byte[]>>(),\n provider.GetRequiredService<ISerializer<string>>(),\n provider.GetRequiredService<IServiceCommandFactory>(),\n provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(),\n provider.GetRequiredService<IZookeeperClientProvider>());\n return result;\n });\n return this;\n }", "CodeBleu_score": 69.15656589084026}, {"code": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo){\n\tmandManager(builder, provider => {\n var result = new ZooKeeperServiceCommandManager(GetConfigInfo(configInfo), provider.GetRequiredService<ISerializer<byte[]>>(), provider.GetRequiredService<ISerializer<string>>(), provider.GetRequiredService<IServiceCommandFactory>(), provider.GetRequiredService<ILogger<ZooKeeperServiceCommandManager>>(), provider.GetRequiredService<IZookeeperClientProvider>());\n return result;\n });\n return this;\n }", "CodeBleu_score": 67.89030872166589}]}
{"instance_id": "213cc4aa9b19", "generation_result": [{"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\t\tif (builder == null) {\n\t\tthrow new ArgumentNullException(nameof(builder));\n\t}\n\tif (string.IsNullOrEmpty(path)) {\n\t\tthrow new ArgumentException(nameof(path));\n\t}\n\tif (!optional &&!File.Exists(path)) {\n\t\tthrow new FileNotFoundException(path);\n\t}\n\tif (provider == null && Path.IsPathRooted(path)) {\n\t\tprovider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n\t}\n\tvar source = new ZookeeperConfigurationSource() {\n\t\tFileProvider = provider,\n\t\tPath = path,\n\t\tOptional = optional,\n\t\tReloadOnChange = reloadOnChange\n\t};\n\tbuilder.Add(source);\n\treturn builder;\n}", "CodeBleu_score": 59.37481239749894}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\t\tif (builder == null) {\n\t\tthrow new ArgumentNullException(nameof(builder));\n\t}\n\tif (string.IsNullOrEmpty(path)) {\n\t\tthrow new ArgumentException(\"File path must be a non-empty string.\", nameof(path));\n\t}\n\tif (!optional &&!File.Exists(path)) {\n\t\tthrow new FileNotFoundException($\"Zookeeper file does not exist: {path}\");\n\t}\n\tif (provider == null && Path.IsPathRooted(path)) {\n\t\tprovider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n\t}\n\tvar source = new ZookeeperConfigurationSource {\n\t\tFileProvider = provider,\n\t\tPath = path,\n\t\tOptional = optional,\n\t\tReloadOnChange = reloadOnChange\n\t};\n\tbuilder.Add(source);\n\treturn builder;\n}", "CodeBleu_score": 60.55257600978226}, {"code": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange){\n\t\tif (builder == null) {\n\t\tthrow new ArgumentNullException(nameof(builder));\n\t}\n\tif (string.IsNullOrEmpty(path)) {\n\t\tthrow new ArgumentException(\"File path must be a non-empty string.\", nameof(path));\n\t}\n\tif (!optional &&!File.Exists(path)) {\n\t\tthrow new FileNotFoundException(\n\t\t\t$\"Zookeeper file does not exist: {path}\",\n\t\t\tpath);\n\t}\n\tif (provider == null && Path.IsPathRooted(path)) {\n\t\tprovider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n\t}\n\tbuilder.Add(new ZookeeperConfigurationSource {\n\t\tFileProvider = provider,\n\t\tPath = path,\n\t\tOptional = optional,\n\t\tReloadOnChange = reloadOnChange\n\t});\n\treturn builder;\n}", "CodeBleu_score": 56.40070819633783}]}
{"instance_id": "15bbffa82a13", "generation_result": [{"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tf (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u79fb\u9664\u670d\u52a1\u8def\u7531\u5730\u5740\u3002\");\nvar routes = await GetRoutesAsync();\nvar descriptors = new List<ServiceRouteDescriptor>();\nforeach (var route in routes)\n{\nvar addresses = route.Address.Where(p =>!Address.Any(a => a.ToString() == p.ToString())).ToArray();\ndescriptors.Add(new ServiceRouteDescriptor(route.ServiceDescriptor, addresses));\n}\nawait SetRoutesAsync(descriptors);\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u670d\u52a1\u8def\u7531\u5730\u5740\u79fb\u9664\u6210\u529f\u3002\");\n}", "CodeBleu_score": 48.525699497472466}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\tar routes = await GetRoutesAsync();\n\tvar serviceRoutes = routes as ServiceRoute[]?? routes.ToArray();\n\tforeach (var serviceRoute in serviceRoutes) {\n\t\tserviceRoute.AddressDescriptors = serviceRoute.AddressDescriptors.Where(p =>!Address.Any(a => a.ToString() == p.Address)).ToArray();\n\t}\n\tawait SetRoutesAsync(serviceRoutes);\n}", "CodeBleu_score": 45.86157541367844}, {"code": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address){\n\t (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u79fb\u9664\u670d\u52a1\u8def\u7531\u5730\u5740\u3002\");\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers)\n{\nvar path = _configInfo.RoutePath;\nif (!path.EndsWith(\"/\"))\npath += \"/\";\n\nvar routes = await GetRoutesAsync();\nforeach (var route in routes)\n{\nvar nodePath = $\"{path}{route.ServiceDescriptor.Id}\";\nvar nodeData = _serializer.Serialize(route);\n\nif (await zooKeeper.Item2.existsAsync(nodePath)!= null)\n{\nvar onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\nif (DataEquals(nodeData, onlineData))\n{\nvar serviceRoute = _serviceRouteFactory.CreateServiceRoute(route.ServiceDescriptor,\n_serializer.Deserialize<ServiceRouteDescriptor>(onlineData).AddressDescriptors.Where(p =>!Address.Any(a => a.Equals(p.Address))).ToArray());\n\nnodeData = _serializer.Serialize(serviceRoute);\nawait zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n}\n}\n}\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u670d\u52a1\u8def\u7531\u5730\u5740\u79fb\u9664\u6210\u529f\u3002\");\n}", "CodeBleu_score": 53.52485656035439}]}
{"instance_id": "125d23de2a60", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\t routePath = _configInfo.RoutePath;\nif (!routePath.EndsWith(\"/\")) {\nroutePath += \"/\";\n}\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar result = await zooKeeper.Item2.getChildrenAsync(routePath);\nvar childrens = result.Children;\nif (childrens!= null) {\nforeach (var child in childrens) {\nvar childPath = $\"{routePath}{child}\";\nvar data = await zooKeeper.Item2.getDataAsync(childPath);\nvar serviceRoute = _serializer.Deserialize<ServiceRoute>(data.Data);\nif (serviceRoute!= null) {\nvar exceptRoutes = routes.Where(p => p.ServiceDescriptor.Id == serviceRoute.ServiceDescriptor.Id);\nif (exceptRoutes.Count() == 0) {\nif (serviceRoute.Address.Contains(hostAddr)) {\nawait zooKeeper.Item2.deleteAsync(childPath);\n}\n}\n}\n}\n}\n}\n}", "CodeBleu_score": 44.27103647397651}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\t", "CodeBleu_score": 4.511260546062985}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr){\n\t routePath = _configInfo.RoutePath;\nif (!routePath.EndsWith(\"/\"))\nroutePath += \"/\";\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers)\n{\nvar childrens = await zooKeeper.Item2.getChildrenAsync(routePath);\nif (childrens!= null && childrens.Children!= null)\n{\nforeach (var child in childrens.Children)\n{\nvar childPath = $\"{routePath}{child}\";\nvar childData = (await zooKeeper.Item2.getDataAsync(childPath)).Data;\nvar serviceRoute = _serializer.Deserialize<ServiceRoute>(childData);\nvar exceptRoutes = routes.Where(p => p.ServiceDescriptor.Id == serviceRoute.ServiceDescriptor.Id);\nif (exceptRoutes.Count() == 0)\n{\nif (serviceRoute.Address.Contains(hostAddr))\nawait zooKeeper.Item2.deleteAsync(childPath);\n}\nelse\n{\nvar exceptRoute = exceptRoutes.FirstOrDefault();\nvar addresses = serviceRoute.Address.Except(exceptRoute.Address);\nforeach (var address in addresses)\n{\nvar nodePath = $\"{routePath}{serviceRoute.ServiceDescriptor.Id}/{address}\";\nif (await zooKeeper.Item2.existsAsync(nodePath)!= null)\nawait zooKeeper.Item2.deleteAsync(nodePath);\n}\n}\n}\n}\n}\n}", "CodeBleu_score": 53.82231258811942}]}
{"instance_id": "b020a235d57f", "generation_result": [{"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\t\tif (data == null) {\n\t\treturn null;\n\t}\n\tif (_logger.IsEnabled(LogLevel.Debug)) {\n\t\t_logger.LogDebug(\"Route data: \" + Encoding.UTF8.GetString(data));\n\t}\n\tvar descriptor = _serializer.Deserialize<ServiceRouteDescriptor>(data);\n\treturn _serviceRouteFactory.Create(descriptor);\n}", "CodeBleu_score": 33.55229503301725}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\tif (data == null) return null;\nif (_logger.IsEnabled(LogLevel.Debug)) _logger.LogDebug($\"Route data: {Encoding.UTF8.GetString(data)}\");\nvar descriptor = _serializer.Deserialize<ServiceRouteDescriptor>(data);\nvar route = _serviceRouteFactory.Create(descriptor);\nreturn route;\n}", "CodeBleu_score": 41.45348654218311}, {"code": "private async Task<ServiceRoute> GetRoute(byte[] data){\n\t\tif (_logger.IsEnabled(LogLevel.Debug))\n\t\t_logger.LogDebug($\"\u83b7\u53d6\u8def\u7531\u6570\u636e\uff1a{Encoding.UTF8.GetString(data)}\u3002\");\n\tif (data == null)\n\t\treturn null;\n\tvar route = _serviceRouteFactory.Create(await _serializer.DeserializeAsync<ServiceRouteDescriptor>(data));\n\treturn route;\n}", "CodeBleu_score": 47.29990653216994}]}
{"instance_id": "169ed4cc2ad4", "generation_result": [{"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tif (DataEquals(oldData, newData))\nreturn;\nvar newRoute = await GetRoute(newData);\nif (newRoute == null)\nreturn;\nvar oldRoute = _routes.Where(i => i.ServiceDescriptor.Id == newRoute.ServiceDescriptor.Id).FirstOrDefault();\nif (oldRoute == null)\nreturn;\nlock (_routes)\n{\nvar index = _routes.IndexOf(oldRoute);\n_routes[index] = newRoute;\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation($\\\"\u670d\u52a1\u8def\u7531\uff1a{newRoute.ServiceDescriptor.Id}\uff0c\u66f4\u65b0\u6210\u529f\u3002\\\");\nOnChanged(newRoute, oldRoute);\n}", "CodeBleu_score": 43.27900746542298}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tif (DataEquals(oldData, newData))\nreturn;\nvar oldRoute = await GetRoute(oldData);\nvar newRoute = await GetRoute(newData);\nif (oldRoute == null || newRoute == null)\nreturn;\nlock (_routes) {\nvar oldIndex = _routes.IndexOf(oldRoute);\nif (oldIndex < 0)\nreturn;\n_routes[oldIndex] = newRoute;\n}\nOnChanged(newRoute, oldRoute);\n}", "CodeBleu_score": 31.127357727848352}, {"code": "public async Task NodeChange(byte[] oldData, byte[] newData){\n\tif (DataEquals(oldData, newData))\nreturn;\nvar newRoute = await GetRoute(newData);\nvar oldRoute = _routes.Where(p => p.ServiceDescriptor.Id == newRoute.ServiceDescriptor.Id).FirstOrDefault();\nlock (_routes) {\n_routes.Remove(oldRoute);\n_routes.Add(newRoute);\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation($\"\u670d\u52a1\u8def\u7531\u5df2\u66f4\u65b0\uff1a{newRoute.ServiceDescriptor.Id}\u3002\");\nOnChanged(new RouteChangedEventArgs(newRoute, oldRoute));\n}", "CodeBleu_score": 46.480714967825996}]}
{"instance_id": "af01261d3199", "generation_result": [{"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\t existingSubscribers = await GetSubscribersAsync();\nforeach (var subscriber in subscribers) {\nvar existingSubscriber = existingSubscribers.FirstOrDefault(i => i.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id);\nif (existingSubscriber!= null) {\nsubscriber.AddressDescriptors = subscriber.AddressDescriptors.Concat(existingSubscriber.AddressDescriptors).Distinct();\n}\n}\nawait base.SetSubscribersAsync(subscribers);\n}", "CodeBleu_score": 52.74311722828361}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\t existingSubscribers = await GetSubscribersAsync();\nvar subscriberList = subscribers.ToList();\nforeach (var subscriber in subscriberList) {\nvar existingSubscriber = existingSubscribers.FirstOrDefault(i => i.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id);\nif (existingSubscriber!= null) {\nvar addressList = existingSubscriber.AddressDescriptors.Select(i => i.Address).ToList();\naddressList.AddRange(subscriber.AddressDescriptors.Select(i => i.Address));\nsubscriber.AddressDescriptors = addressList.Distinct().Select(i => new AddressModel { Address = i }).ToList();\n}\n}\nawait base.SetSubscribersAsync(subscriberList);\n}", "CodeBleu_score": 45.08856569352285}, {"code": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers){\n\t existingSubscribers = await GetSubscribersAsync();\nvar updatedSubscribers = new List<ServiceSubscriber>();\nforeach (var subscriber in subscribers) {\nvar existingSubscriber = existingSubscribers.FirstOrDefault(i => i.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id);\nif (existingSubscriber!= null) {\nvar addresses = subscriber.Addresses.Union(existingSubscriber.Addresses);\nupdatedSubscribers.Add(new ServiceSubscriber(subscriber.ServiceDescriptor, addresses));\n} else {\nupdatedSubscribers.Add(subscriber);\n}\n}\nawait base.SetSubscribersAsync(updatedSubscribers);\n}", "CodeBleu_score": 43.377612435388514}]}
{"instance_id": "fda02cec3296", "generation_result": [{"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tvar zooKeeper = await GetZooKeeper();\nvar path = _configInfo.SubscriberPath;\nif (!path.EndsWith(\"/\"))\npath += \"/\";\n\nvar subscribers = new List<ServiceSubscriber>();\nforeach (var children in childrens) {\nvar nodePath = $\"{path}{children}\";\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u83b7\u53d6\u670d\u52a1\u8ba2\u9605\u8005\uff1a{nodePath}\u3002\");\n\nvar subscriber = await GetSubscriber(nodePath);\nif (subscriber!= null)\nsubscribers.Add(subscriber);\n}\nreturn subscribers.ToArray();\n}", "CodeBleu_score": 59.85589430662327}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u83b7\u53d6\u670d\u52a1\u8ba2\u9605\u8005\uff0c\u8282\u70b9\uff1a{_configInfo.SubscriberPath}\u3002\");\n\nvar zooKeeper = await GetZooKeeper();\nvar subscribers = new List<ServiceSubscriber>();\n\nvar rootPath = _configInfo.SubscriberPath;\nif (!rootPath.EndsWith(\"/\"))\nrootPath += \"/\";\n\nforeach (var children in childrens) {\nvar nodePath = $\"{rootPath}{children}\";\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u83b7\u53d6\u670d\u52a1\u8ba2\u9605\u8005\uff0c\u8282\u70b9\uff1a{nodePath}\u3002\");\n\nvar data = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\nvar subscriber = await GetSubscriber(data);\nif (subscriber!= null)\nsubscribers.Add(subscriber);\n}\nreturn subscribers.ToArray();\n}", "CodeBleu_score": 68.28368047488338}, {"code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens){\n\tif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug(\"\u51c6\u5907\u83b7\u53d6\u670d\u52a1\u8ba2\u9605\u8005\u3002\");\n\nvar result = new List<ServiceSubscriber>();\nvar zooKeeper = await GetZooKeeper();\nvar path = _configInfo.SubscriberPath;\nif (!path.EndsWith(\"/\"))\npath += \"/\";\n\nforeach (var children in childrens) {\nvar nodePath = $\"{path}{children}\";\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u83b7\u53d6\u670d\u52a1\u8ba2\u9605\u8005\uff1a{nodePath}\u3002\");\n\nvar subscriber = await GetSubscriber(nodePath);\nif (subscriber!= null)\nresult.Add(subscriber);\n}\nreturn result.ToArray();\n}", "CodeBleu_score": 64.6705112311635}]}
{"instance_id": "6c3018ff0c0d", "generation_result": [{"code": "private async Task EnterSubscribers(){\n\tif (_subscribers!= null) {\nreturn;\n}\nvar zooKeeper = await GetZooKeeper();\nif (await zooKeeper.Item2.existsAsync(_configInfo.SubscriberPath)!= null) {\nvar children = (await zooKeeper.Item2.getChildrenAsync(_configInfo.SubscriberPath)).Children;\nif (children!= null && children.Count > 0) {\n_subscribers = await GetSubscribers(children);\n} else {\n_subscribers = new ServiceSubscriber[0];\n}\n} else {\n_logger.LogWarning($\"The subscriber path {_configInfo.SubscriberPath} does not exist.\");\n_subscribers = new ServiceSubscriber[0];\n}\n}", "CodeBleu_score": 50.97952833542719}, {"code": "private async Task EnterSubscribers(){\n\tif (_subscribers!= null) {\nreturn;\n}\n\nvar zooKeeper = await GetZooKeeper();\nawait zooKeeper.Item1.WaitOneAsync();\n\nvar path = _configInfo.SubscriberPath;\nif (await zooKeeper.Item2.existsAsync(path)!= null) {\nvar result = await zooKeeper.Item2.getChildrenAsync(path);\nif (result?.Children!= null) {\n_subscribers = await GetSubscribers(result.Children);\n} else {\n_subscribers = new ServiceSubscriber[0];\n}\n} else {\n_logger.LogWarning(\"Subscriber path does not exist.\");\n_subscribers = new ServiceSubscriber[0];\n}\n}", "CodeBleu_score": 54.46630707987501}, {"code": "private async Task EnterSubscribers(){\n\tif (_subscribers!= null) {\nreturn;\n}\nvar zooKeeper = await GetZooKeeper();\nif (await zooKeeper.Item2.existsAsync(_configInfo.SubscriberPath)!= null) {\nvar result = await zooKeeper.Item2.getChildrenAsync(_configInfo.SubscriberPath);\nif (result?.Children!= null) {\n_subscribers = await GetSubscribers(result.Children);\nreturn;\n}\n}\n_subscribers = new ServiceSubscriber[0];\nif (_logger.IsEnabled(LogLevel.Warning)) {\n_logger.LogWarning($\"Subscriber path {_configInfo.SubscriberPath} does not exist.\");\n}\n}", "CodeBleu_score": 55.37369317683905}]}
{"instance_id": "60ff973ba60a", "generation_result": [{"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\t// Check if the data has changed\nif (!DataEquals(oldData, newData)) {\n// Get the new service command\nvar newCommand = GetServiceCommand(newData);\n\n// Replace the old command with the new one in the service commands list\nlock (_serviceCommands) {\n_serviceCommands = _serviceCommands.Where(c => c.ServiceId!= newCommand.ServiceId).Concat(new[] { newCommand }).ToArray();\n}\n\n// Trigger the OnChanged event with the new and old commands\nOnChanged(new ServiceCommandChangedEventArgs(newCommand, GetServiceCommand(oldData)));\n}\n}", "CodeBleu_score": 58.06892466637158}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\t\t// If the data has changed, retrieve the new service command.\n\tif (!DataEquals(oldData, newData)) {\n\t\tvar newCommand = GetServiceCommand(newData);\n\t\tif (newCommand!= null) {\n\t\t\t// Update the service commands list.\n\t\t\tlock (_serviceCommands) {\n\t\t\t\tvar oldCommand = _serviceCommands.FirstOrDefault(i => i.ServiceId == newCommand.ServiceId);\n\t\t\t\t_serviceCommands = _serviceCommands.Where(i => i.ServiceId!= newCommand.ServiceId).Concat(new[] { newCommand }).ToArray();\n\t\t\t}\n\n\t\t\t// Trigger the OnChanged event.\n\t\t\tOnChanged(new ServiceCommandChangedEventArgs(newCommand, oldCommand));\n\t\t}\n\t}\n}", "CodeBleu_score": 70.6779000859805}, {"code": "public void NodeChange(byte[] oldData, byte[] newData){\n\t\tif (DataEquals(oldData, newData)) {\n\t\treturn;\n\t}\n\tvar oldCommand = GetServiceCommand(oldData);\n\tvar newCommand = GetServiceCommand(newData);\n\tif (newCommand == null) {\n\t\treturn;\n\t}\n\tlock (_serviceCommands) {\n\t\t_serviceCommands = _serviceCommands.Where(i => i.ServiceId!= newCommand.ServiceId).Concat(new[] { newCommand }).ToArray();\n\t}\n\tOnChanged(new ServiceCommandChangedEventArgs(newCommand, oldCommand));\n}", "CodeBleu_score": 67.55321124184219}]}
{"instance_id": "4597b75f2b3d", "generation_result": [{"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tif (watchedEvent.Type == EventType.NodeDataChanged) {\nvar (wait, client) = await _zooKeeperCall.Invoke();\nvar data = await client.GetDataAsync(Path, this, null);\n_action.Invoke(_currentData, data);\n_currentData = data;\nwait.Set();\n}\n}", "CodeBleu_score": 44.588242333941885}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tif (watchedEvent.Type == EventType.NodeDataChanged) {\nvar (resetEvent, zooKeeper) = await _zooKeeperCall();\ntry {\nvar data = await zooKeeper.GetDataAsync(Path, this, resetEvent);\n_action(_currentData, data);\n_currentData = data;\n}\nfinally {\nresetEvent.Set();\n}\n}\n}", "CodeBleu_score": 48.31152420143824}, {"code": "protected override async Task ProcessImpl(WatchedEvent watchedEvent){\n\tif (watchedEvent.Type == EventType.NodeDataChanged) {\nvar (wait, zooKeeper) = await _zooKeeperCall();\nvar data = await zooKeeper.GetDataAsync(Path, this, wait);\n_action(_currentData, data);\n_currentData = data;\n}\n}", "CodeBleu_score": 46.92004584505774}]}
{"instance_id": "cb4f0e22a0d5", "generation_result": [{"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\t.UseZookeeperClientProvider(provider => {\nvar result = new DefaultZookeeperClientProvider(\nGetConfigInfo(configInfo),\nprovider.GetRequiredService<IHealthCheckService>(),\nprovider.GetRequiredService<IZookeeperAddressSelector>(),\nprovider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>());\nreturn result;\n});\n}", "CodeBleu_score": 47.95566365403264}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tces.AddSingleton(provider => {\nvar healthCheckService = provider.GetRequiredService<IHealthCheckService>();\nvar addressSelector = provider.GetRequiredService<IZookeeperAddressSelector>();\nvar logger = provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>();\nvar result = new DefaultZookeeperClientProvider(GetConfigInfo(configInfo), healthCheckService, addressSelector, logger);\nreturn result;\n});\n\nreturn builder;\n}", "CodeBleu_score": 44.60569019939014}, {"code": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo){\n\tces.AddSingleton<IZookeeperClientProvider, DefaultZookeeperClientProvider>(provider => new DefaultZookeeperClientProvider(configInfo, provider.GetRequiredService<IHealthCheckService>(), provider.GetRequiredService<IZookeeperAddressSelector>(), provider.GetRequiredService<ILogger<DefaultZookeeperClientProvider>>()));\n\treturn builder;\n}", "CodeBleu_score": 51.74671814349345}]}
{"instance_id": "80709a4b2b69", "generation_result": [{"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\t (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\u914d\u7f6e\u3002\");\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar path = _configInfo.MqttRoutePath;\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\nawait zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\nforeach (var route in routes) {\nvar routePath = $\"{path}/{route.Id}\";\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u6dfb\u52a0\uff1a{routePath}\u3002\");\nvar data = _serializer.Serialize(route);\nif (await zooKeeper.Item2.existsAsync(routePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{routePath}\u3002\");\nawait zooKeeper.Item2.createAsync(routePath, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n} else {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{routePath}\u3002\");\nawait zooKeeper.Item2.setDataAsync(routePath, data);\n}\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u8def\u7531\u914d\u7f6e\u6dfb\u52a0\u5b8c\u6210\u3002\");\n}\n}", "CodeBleu_score": 63.569971129609925}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\t (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0mqtt\u8def\u7531\u914d\u7f6e\u3002\");\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar path = _configInfo.MqttRoutePath;\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\nawait zooKeeper.Item2.createAsync(nodePath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\nforeach (var route in routes) {\nvar routePath = $\"{path}/{route.Id}\";\nif (await zooKeeper.Item2.existsAsync(routePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{routePath}\u3002\");\nawait zooKeeper.Item2.createAsync(routePath, _serializer.Serialize(route), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n} else {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{routePath}\u3002\");\nawait zooKeeper.Item2.setDataAsync(routePath, _serializer.Serialize(route));\n}\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u8def\u7531\u914d\u7f6e\u6dfb\u52a0\u5b8c\u6210\u3002\");\n}\n}", "CodeBleu_score": 60.62331328557015}, {"code": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes){\n\t (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0MQTT\u8def\u7531\u914d\u7f6e\u3002\");\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar path = _configInfo.MqttRoutePath;\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\nawait zooKeeper.Item2.createAsync(nodePath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\nforeach (var route in routes) {\nvar routePath = $\"{path}/{route.Descriptor.Id}\";\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u6dfb\u52a0\uff1a{routePath}\u3002\");\nvar routeData = _serializer.Serialize(route);\nvar result = await zooKeeper.Item2.existsAsync(routePath);\nif (result == null) {\nawait zooKeeper.Item2.createAsync(routePath, routeData, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nelse {\nawait zooKeeper.Item2.setDataAsync(routePath, routeData);\n}\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u8def\u7531\u914d\u7f6e\u6dfb\u52a0\u5b8c\u6210\u3002\");\n}\n}", "CodeBleu_score": 61.510243748009955}]}
{"instance_id": "69ff4d3427cd", "generation_result": [{"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\troutePath = _configInfo.MqttRoutePath;\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar childrens = routePath.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath)!= null) {\nvar result = await zooKeeper.Item2.getChildrenAsync(nodePath);\nif (result?.Children!= null) {\nforeach (var child in result.Children) {\nvar childPath = $\"{nodePath}/{child}\";\nvar data = (await zooKeeper.Item2.getDataAsync(childPath)).Data;\nvar serviceRoute = _serializer.Deserialize<MqttServiceRoute>(data);\nif (serviceRoute!= null && serviceRoute.MqttEndpoint!= null) {\nvar addresses = serviceRoute.MqttEndpoint.Where(p => p.ToString()!= hostAddr.ToString()).ToArray();\nif (addresses.Length == 0) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\nawait zooKeeper.Item2.deleteAsync(childPath);\n} else {\nserviceRoute.MqttEndpoint = addresses;\nvar nodeData = _serializer.Serialize(serviceRoute);\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\u8282\u70b9\uff1a{childPath}\u7684\u6570\u636e\u3002\");\nawait zooKeeper.Item2.setDataAsync(childPath, nodeData);\n}\n}\n}\n}\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\n}\n}", "CodeBleu_score": 43.911433744191726}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\tpath = _configInfo.MqttRoutePath;\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath)!= null) {\nvar result = await zooKeeper.Item2.getChildrenAsync(nodePath);\nif (result?.Children!= null) {\nforeach (var child in result.Children) {\nvar childPath = $\"{nodePath}/{child}\";\nvar serviceRoute = routes.Where(p => p.MqttDescriptor.Topic == child).FirstOrDefault();\nif (serviceRoute!= null) {\nvar addresses = serviceRoute.MqttEndpoint.Select(p => p.ToString()).ToArray();\nif (addresses.Contains(hostAddr.ToString())) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\nawait zooKeeper.Item2.deleteAsync(childPath);\n}\n}\n}\n}\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\n}\n}", "CodeBleu_score": 48.69327221399005}, {"code": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr){\n\t path = _configInfo.MqttRoutePath;\n\tvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n\tforeach (var zooKeeper in zooKeepers) {\n\t\tvar childrens = await zooKeeper.Item2.getChildrenAsync(path);\n\t\tvar children = childrens.Children;\n\t\tforeach (var child in children) {\n\t\t\tvar childPath = $\"{path}/{child}\";\n\t\t\tvar childData = await zooKeeper.Item2.getDataAsync(childPath);\n\t\t\tvar childRoute = _serializer.Deserialize<MqttServiceRoute>(childData.Data);\n\t\t\tif (childRoute.MqttEndpoint.Contains(hostAddr)) {\n\t\t\t\tvar route = routes.Where(p => p.MqttDescriptor.Topic == childRoute.MqttDescriptor.Topic).FirstOrDefault();\n\t\t\t\tif (route == null) {\n\t\t\t\t\tif (_logger.IsEnabled(LogLevel.Debug)) {\n\t\t\t\t\t\t_logger.LogDebug($\"\u51c6\u5907\u5220\u9664\uff1a{childPath}\u3002\");\n\t\t\t\t\t}\n\t\t\t\t\tawait zooKeeper.Item2.deleteAsync(childPath);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 43.99243422374198}]}
{"instance_id": "8856055e4872", "generation_result": [{"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\t (_logger.IsEnabled(LogLevel.Information)) {\n_logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u3002\");\n}\nvar cachePath = _configInfo.CachePath;\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers) {\nvar path = cachePath;\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nawait zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\nforeach (var cacheDescriptor in cacheDescriptors) {\nvar nodePath = $\"{cachePath}/{cacheDescriptor.Id}\";\nvar data = _serializer.Serialize(cacheDescriptor);\nvar stat = await zooKeeper.Item2.existsAsync(nodePath);\nif (stat == null) {\nawait zooKeeper.Item2.createAsync(nodePath, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n} else {\nvar result = await zooKeeper.Item2.getDataAsync(nodePath);\nif (result.Data!= null &&!result.Data.SequenceEqual(data)) {\nawait zooKeeper.Item2.setDataAsync(nodePath, data);\n}\n}\n}\n}\nif (_logger.IsEnabled(LogLevel.Information)) {\n_logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u66f4\u65b0\u5b8c\u6210\u3002\");\n}\n}", "CodeBleu_score": 55.22120761141329}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\t (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u3002\");\nvar path = _configInfo.CachePath;\nvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\nforeach (var zooKeeper in zooKeepers)\n{\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1)\n{\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null)\n{\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{nodePath}\u3002\");\nawait zooKeeper.Item2.createAsync(nodePath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\nforeach (var cacheDescriptor in cacheDescriptors)\n{\nvar cachePath = $\"{path}/{cacheDescriptor.Id}\";\nvar data = _serializer.Serialize(cacheDescriptor);\nif (await zooKeeper.Item2.existsAsync(cachePath) == null)\n{\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u521b\u5efa\uff1a{cachePath}\u3002\");\nawait zooKeeper.Item2.createAsync(cachePath, data, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nelse\n{\nvar result = await zooKeeper.Item2.getDataAsync(cachePath);\nif (!data.SequenceEqual(result.Data))\n{\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u51c6\u5907\u66f4\u65b0\uff1a{cachePath}\u3002\");\nawait zooKeeper.Item2.setDataAsync(cachePath, data);\n}\n}\n}\n}\nif (_logger.IsEnabled(LogLevel.Information))\n_logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u66f4\u65b0\u5b8c\u6210\u3002\");\n}", "CodeBleu_score": 62.65833626660257}, {"code": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors){\n\tf (_logger.IsEnabled(LogLevel.Information)) {\n\t\t_logger.LogInformation(\"\u51c6\u5907\u66f4\u65b0\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u3002\");\n\t}\n\n\tvar path = _configInfo.CachePath;\n\tvar zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n\tforeach (var zooKeeper in zooKeepers) {\n\t\tvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\t\tvar index = 0;\n\t\twhile (childrens.Count() > 1) {\n\t\t\tvar nodePath = \"/\" + string.Join(\"/\", childrens);\n\t\t\tif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\n\t\t\t\tawait zooKeeper.Item2.createAsync(nodePath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t}\n\t\t\tindex++;\n\t\t\tchildrens = childrens.Take(childrens.Length - index).ToArray();\n\t\t}\n\n\t\tforeach (var cacheDescriptor in cacheDescriptors) {\n\t\t\tvar cacheDescriptorBytes = _serializer.Serialize(cacheDescriptor);\n\t\t\tvar cacheDescriptorPath = $\"{path}/{cacheDescriptor.Id}\";\n\t\t\tvar stat = await zooKeeper.Item2.existsAsync(cacheDescriptorPath);\n\t\t\tif (stat == null) {\n\t\t\t\tawait zooKeeper.Item2.createAsync(cacheDescriptorPath, cacheDescriptorBytes, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tawait zooKeeper.Item2.setDataAsync(cacheDescriptorPath, cacheDescriptorBytes);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (_logger.IsEnabled(LogLevel.Information)) {\n\t\t_logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u914d\u7f6e\u66f4\u65b0\u5b8c\u6210\u3002\");\n\t}\n}", "CodeBleu_score": 53.48977001292224}]}
{"instance_id": "57d92c17de83", "generation_result": [{"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t eventWaitHandle = zooKeeper.Item1;\nvar client = zooKeeper.Item2;\n\nif (!path.EndsWith(\"/\"))\npath += \"/\";\n\nvar childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\n\nif (await client.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\nawait client.createAsync(nodePath, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\n\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\n}", "CodeBleu_score": 48.19213045369646}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t pathParts = path.Split('/');\nvar nodePath = \"/\";\n\nfor (var i = 1; i < pathParts.Length; i++) {\nnodePath += pathParts[i] + \"/\";\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n\nawait zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\n}\n}", "CodeBleu_score": 47.01563960083996}, {"code": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path){\n\t childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\nvar index = 0;\nwhile (childrens.Count() > 1) {\nvar nodePath = \"/\" + string.Join(\"/\", childrens);\nif (await zooKeeper.Item2.existsAsync(nodePath) == null) {\nif (_logger.IsEnabled(LogLevel.Debug))\n_logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\nawait zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n}\nindex++;\nchildrens = childrens.Take(childrens.Length - index).ToArray();\n}\n}", "CodeBleu_score": 53.293521533299206}]}
{"instance_id": "b276a07da453", "generation_result": [{"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\t result = new List<(ManualResetEvent, ZooKeeper)>();\n\tvar address = new List<AddressModel>();\n\tforeach (var addressModel in _config.Addresses) {\n\t\t_healthCheckService.Monitor(addressModel);\n\t\tvar task = _healthCheckService.IsHealth(addressModel);\n\t\tif (!(task.IsCompletedSuccessfully? task.Result : await task)) {\n\t\t\tcontinue;\n\t\t}\n\t\taddress.Add(addressModel);\n\t}\n\tif (address.Count == 0) {\n\t\tif (_logger.IsEnabled(Level.Warning))\n\t\t\t_logger.LogWarning($\"\u627e\u4e0d\u5230\u53ef\u7528\u7684\u6ce8\u518c\u4e2d\u5fc3\u5730\u5740\u3002\");\n\t\treturn result;\n\t}\n\n\tvar vt = _zookeeperAddressSelector.SelectAsync(new AddressSelectContext {\n\t\tDescriptor = new ServiceDescriptor { Id = nameof(DefaultZookeeperClientProvider) },\n\t\tAddress = address\n\t});\n\tvar addr = vt.IsCompletedSuccessfully? vt.Result : await vt;\n\tif (addr!= null) {\n\t\tvar ipAddress = addr as IpAddressModel;\n\t\tresult.Add(CreateZooKeeper(ipAddress));\n\t}\n\treturn result;\n}", "CodeBleu_score": 61.11079369163266}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\t", "CodeBleu_score": 7.549424158321882}, {"code": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers(){\n\tresult = new List<(ManualResetEvent, ZooKeeper)>();\nforeach (var address in _config.Addresses) {\nif (!await _healthCheckService.IsHealth(address)) {\ncontinue;\n}\nresult.Add(CreateZooKeeper(address));\n}\nreturn result;\n}", "CodeBleu_score": 67.9082722122626}]}
{"instance_id": "dbb6463e7ee8", "generation_result": [{"code": "ZooKeeper getZooKeeper() throws Exception{\n\tetZooKeeper() == null) {\nsynchronized (this) {\nif (helper.getZooKeeper() == null) {\nhelper = new Helper(helper.getData());\n}\n}\n}\n\nreturn helper.getZooKeeper();\n}", "CodeBleu_score": 26.82636471900758}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\tetZooKeeper() == null) {\nsynchronized (this) {\nif (helper.getZooKeeper() == null) {\nhelper = new Helper(helper.data, zookeeperFactory, watcher, ensembleProvider, sessionTimeout, canBeReadOnly);\n}\n}\n}\nreturn helper.getZooKeeper();\n}", "CodeBleu_score": 37.81552837280988}, {"code": "ZooKeeper getZooKeeper() throws Exception{\n\t (data) {\nif (data.zk == null) {\ndata.zk = zookeeperFactory.newZooKeeper(\nensembleProvider.getConnectionString(),\nsessionTimeout,\nwatcher,\ncanBeReadOnly);\ndata.connectionString = ensembleProvider.getConnectionString();\ndata.negotiatedSessionTimeoutMs = data.zk.getSessionTimeout();\n}\nreturn data.zk;\n}\n}", "CodeBleu_score": 37.62945185088936}]}
{"instance_id": "0d9f311aa482", "generation_result": [{"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\tooKeeper = null;\n\tif (helper!= null) {\n\t\tzooKeeper = helper.getZooKeeper();\n\t}\n\tif (zooKeeper!= null) {\n\t\t// register a dummy watcher to clear the default watcher, preventing new events from being processed\n\t\tzooKeeper.register(new Watcher() {\n\t\t\t@Override\n\t\t\tpublic void process(WatchedEvent event) {\n\t\t\t}\n\t\t});\n\t\tif (waitForShutdownTimeoutMs > 0) {\n\t\t\ttry {\n\t\t\t\tzooKeeper.close();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t} else {\n\t\t\tzooKeeper.close();\n\t\t}\n\t}\n}", "CodeBleu_score": 35.5878550128944}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\t= null) {\n\t\tZooKeeper zooKeeper = helper.getZooKeeper();\n\t\tif (zooKeeper!= null) {\n\t\t\ttry {\n\t\t\t\tzooKeeper.register(new Watcher() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void process(WatchedEvent event) {\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t} catch (Exception e) {\n\t\t\t}\n\n\t\t\tif (waitForShutdownTimeoutMs > 0) {\n\t\t\t\tzooKeeper.close();\n\t\t\t} else {\n\t\t\t\ttry {\n\t\t\t\t\tzooKeeper.close();\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tThread.currentThread().interrupt();\n\t\t\t\t\tthrow e;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 38.12386807730279}, {"code": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception{\n\t= null) {\n\t\tZooKeeper zooKeeper = helper.getZooKeeper();\n\t\tif (zooKeeper!= null) {\n\t\t\ttry {\n\t\t\t\tzooKeeper.register(new Watcher() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void process(WatchedEvent event) {\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t} catch (Exception e) {\n\t\t\t\t// ignore\n\t\t\t}\n\n\t\t\tif (waitForShutdownTimeoutMs > 0) {\n\t\t\t\tzooKeeper.close();\n\t\t\t} else {\n\t\t\t\ttry {\n\t\t\t\t\tzooKeeper.close();\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tThread.currentThread().interrupt();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 38.708807647024486}]}
{"instance_id": "d57ef4592303", "generation_result": [{"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\t\tPreconditions.checkState(started.get(), \"Client is not started\");\n\n\tlog.debug(\\\"blockUntilConnectedOrTimedOut\\\");\n\tOperationTrace trace = tracer.get().start(\\\"blockUntilConnectedOrTimedOut\\\");\n\ttry {\n\t\treturn state.blockUntilConnectedOrTimedOut(connectionTimeoutMs);\n\t} finally {\n\t\ttrace.commit();\n\t}\n}", "CodeBleu_score": 49.488509565933725}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\t\tPreconditions.checkState(started.get(), \"Client is not started\");\n\tlog.debug(\\\"blockUntilConnectedOrTimedOut() start\\\");\n\ttry {\n\t\tif (state.blockUntilConnectedOrTimedOut(connectionTimeoutMs)) {\n\t\t\tlog.debug(\\\"blockUntilConnectedOrTimedOut() end - connected\\\");\n\t\t\treturn true;\n\t\t} else {\n\t\t\tlog.debug(\\\"blockUntilConnectedOrTimedOut() end - not connected\\\");\n\t\t\treturn false;\n\t\t}\n\t} catch (InterruptedException e) {\n\t\tlog.debug(\\\"blockUntilConnectedOrTimedOut() end - interrupted\\\");\n\t\tthrow e;\n\t}\n}", "CodeBleu_score": 43.11907303049966}, {"code": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException{\n\t\tPreconditions.checkState(started.get(), \"Client is not started\");\n\tlog.debug(\"blockUntilConnectedOrTimedOut\");\n\ttracer.get().start(\"blockUntilConnectedOrTimedOut\");\n\ttry {\n\t\tif (state.blockUntilConnectedOrTimedOut(connectionTimeoutMs)) {\n\t\t\treturn true;\n\t\t} else {\n\t\t\tlog.warn(\"Unable to connect to ZooKeeper with connection timeout of \" + connectionTimeoutMs + \" ms\");\n\t\t\treturn false;\n\t\t}\n\t} finally {\n\t\ttracer.get().stop(\"blockUntilConnectedOrTimedOut\");\n\t}\n}", "CodeBleu_score": 42.38204106588251}]}
{"instance_id": "d722f6d0beb6", "generation_result": [{"code": "public void close(int waitForShutdownTimeoutMs){\n\t\tlog.debug(\"Closing\");\n\n\tif (!started.compareAndSet(true, false)) {\n\t\tthrow new IllegalStateException(\"Not started\");\n\t}\n\n\ttry {\n\t\tstate.close(waitForShutdownTimeoutMs);\n\t} catch (IOException e) {\n\t\tif (Thread.interrupted()) {\n\t\t\tlog.error(\"Interrupted while closing\", e);\n\t\t} else {\n\t\t\tlog.error(\"Error while closing\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 52.760389568846854}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\tlog.debug(\"Closing\");\n\nif (!started.compareAndSet(true, false)) {\nthrow new IllegalStateException(\"Not started\");\n}\n\ntry {\nstate.close(waitForShutdownTimeoutMs);\n} catch (IOException e) {\nif (Thread.currentThread().isInterrupted()) {\nlog.warn(\"Interrupted while closing\", e);\n} else {\nlog.error(\"Error closing\", e);\n}\n}\n}", "CodeBleu_score": 53.14210454865758}, {"code": "public void close(int waitForShutdownTimeoutMs){\n\t\tlog.debug(\"Closing\");\n\tif (!started.compareAndSet(true, false)) {\n\t\tlog.debug(\"Already closed\");\n\t\treturn;\n\t}\n\tlog.debug(\"Shutting down\");\n\ttry {\n\t\tstate.close(waitForShutdownTimeoutMs);\n\t} catch (IOException e) {\n\t\tif (Thread.currentThread().isInterrupted()) {\n\t\t\tlog.debug(\"Interrupted while closing\", e);\n\t\t} else {\n\t\t\tlog.error(\"Error while closing\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 61.11600024935859}]}
{"instance_id": "8492e37ce46e", "generation_result": [{"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() start\");\n\tfinal CountDownLatch latch = new CountDownLatch(1);\n\tfinal AtomicBoolean connected = new AtomicBoolean(false);\n\tfinal AtomicBoolean watcherAdded = new AtomicBoolean(false);\n\tfinal AtomicBoolean timedOut = new AtomicBoolean(false);\n\tfinal Watcher watcher = new Watcher() {\n\t\t@Override\n\t\tpublic void process(WatchedEvent event) {\n\t\t\tif (event.getState() == Event.KeeperState.SyncConnected) {\n\t\t\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() watcher: connected\");\n\t\t\t\tconnected.set(true);\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}\n\t};\n\n\tlong remainingWaitTime = connectionTimeoutMs;\n\twhile (!connected.get() &&!timedOut.get()) {\n\t\ttry {\n\t\t\tif (!watcherAdded.getAndSet(true)) {\n\t\t\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() adding watcher\");\n\t\t\t\tstate.addParentWatcher(watcher);\n\t\t\t}\n\t\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() waiting for connection\");\n\t\t\tif (!latch.await(remainingWaitTime, TimeUnit.MILLISECONDS)) {\n\t\t\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() timed out\");\n\t\t\t\ttimedOut.set(true);\n\t\t\t}\n\t\t} finally {\n\t\t\tif (watcherAdded.getAndSet(false)) {\n\t\t\t\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() removing watcher\");\n\t\t\t\tstate.removeParentWatcher(watcher);\n\t\t\t}\n\t\t}\n\t\tremainingWaitTime = connectionTimeoutMs - (System.currentTimeMillis() - state.getStartTimeMs());\n\t\tif (remainingWaitTime <= 0) {\n\t\t\ttimedOut.set(true);\n\t\t}\n\t}\n\tlog.debug(\"internalBlockUntilConnectedOrTimedOut() end. connected: \" + connected.get());\n}", "CodeBleu_score": 37.54027143981207}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t\tint remainingWaitTime = connectionTimeoutMs;\n\tCountDownLatch latch = new CountDownLatch(1);\n\tWatcher watcher = new Watcher() {\n\t\t@Override\n\t\tpublic void process(WatchedEvent event) {\n\t\t\tif (event.getState() == Event.KeeperState.SyncConnected) {\n\t\t\t\tlatch.countDown();\n\t\t\t}\n\t\t}\n\t};\n\taddParentWatcher(watcher);\n\ttry {\n\t\twhile (remainingWaitTime > 0) {\n\t\t\tif (state.isConnected()) {\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlatch.await(remainingWaitTime, TimeUnit.MILLISECONDS);\n\t\t\tremainingWaitTime = connectionTimeoutMs - (int) (System.currentTimeMillis() - state.getLastConnectTime());\n\t\t}\n\t} finally {\n\t\tremoveParentWatcher(watcher);\n\t}\n}", "CodeBleu_score": 41.97310721969371}, {"code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n\t", "CodeBleu_score": 1.4312056341607657}]}
{"instance_id": "4a01f5365731", "generation_result": [{"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tf (doProtected() && createMode.isEphemeral()) {\nlong currentSessionId = client.getZooKeeper().getSessionId();\nif (currentSessionId!= sessionId) {\nlog.warn(\\\"Session ID changed from {} to {}\\\", sessionId, currentSessionId);\nclient.getZooKeeper().delete(foundNode, -1);\nsessionId = currentSessionId;\n}\n}\nreturn foundNode;\n}", "CodeBleu_score": 36.481228393398155}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tif (doProtected() && (sessionId!= 0) && createMode.isEphemeral()) {\n\t\tlong currentSessionId = client.getZooKeeper().getSessionId();\n\t\tif (currentSessionId!= sessionId) {\n\t\t\tlog.warn(\\\"Session changed from {} to {} during protected mode create\\\", sessionId, currentSessionId);\n\t\t\tclient.getZooKeeper().delete(foundNode, -1);\n\t\t\tsessionId = currentSessionId;\n\t\t}\n\t}\n\treturn foundNode;\n}", "CodeBleu_score": 39.120477909030434}, {"code": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception{\n\tf (doProtected() && (sessionId!= 0) && createMode.isEphemeral()) {\nlong currentSessionId = client.getZooKeeper().getSessionId();\nif (sessionId!= currentSessionId) {\nlog.warn(\\\"Session ID changed during protected mode operation. Deleting old node\\\");\nclient.getZooKeeper().delete(foundNode, -1);\nsessionId = currentSessionId;\n}\n}\nreturn foundNode;\n}", "CodeBleu_score": 34.83416994539913}]}
{"instance_id": "126d0922a699", "generation_result": [{"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tal OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n\ttry {\n\t\tfinal RetryLoop.RetrySleeper sleeper = client.getZookeeperClient().newRetrySleeper();\n\t\tRetryLoop.callWithRetry(client.getZookeeperClient(), new Callable<Stat>() {\n\t\t\t@Override\n\t\t\tpublic Stat call() throws Exception {\n\t\t\t\ttrace.setPath(path).commit();\n\t\t\t\treturn client.getZooKeeper().setACL(path, aclList, version);\n\t\t\t}\n\t\t}, client.getZookeeperClient().getZooKeeperSleeper(), sleeper);\n\t} catch (Throwable e) {\n\t\ttrace.setException(e).commit();\n\t\tthrow e;\n\t}\n\treturn trace.getResultStat();\n}", "CodeBleu_score": 58.049343998606396}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\tl OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\nfinal RetryLoop loop = client.newRetryLoop();\nStat resultStat = null;\nwhile (true) {\ntry {\nresultStat = client.getZooKeeper().setACL(path, aclList, version);\ntrace.setReturnCode(0).setPath(path).setStat(resultStat).commit();\nbreak;\n} catch (KeeperException.NoNodeException e) {\nif (!loop.shouldContinue(e)) {\nthrow e;\n}\n}\n}\nreturn resultStat;\n}", "CodeBleu_score": 48.28595005181586}, {"code": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception{\n\trationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n\tRetryLoop.callWithRetry(client.getZookeeperClient(), new Callable<Stat>() {\n\t\t@Override\n\t\tpublic Stat call() throws Exception {\n\t\t\treturn client.getZooKeeper().setACL(path, aclList, version);\n\t\t}\n\t});\n\ttrace.setPath(path).commit();\n\treturn trace.getResult();\n}", "CodeBleu_score": 63.37803775490405}]}
{"instance_id": "e07b079ff61f", "generation_result": [{"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\t\tthis.thisId = Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n\tthis.membershipPath = membershipPath;\n\tthis.pen = new PersistentNode(client, CreateMode.EPHEMERAL, membershipPath + \"/\" + thisId, payload);\n\tthis.cache = new CuratorCacheBridge(client, membershipPath, parentPathFilter());\n}", "CodeBleu_score": 54.82319223595498}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\t", "CodeBleu_score": 9.224495547481085}, {"code": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload){\n\tthis(client, membershipPath, thisId, payload, CreateMode.EPHEMERAL);\n}", "CodeBleu_score": 12.170290392892221}]}
{"instance_id": "d11c76b2b912", "generation_result": [{"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n\t// create a node at the given path asynchronously\n\tasync.create().forPath(path).whenComplete((name, exception) -> {\n\t\tif (exception!= null) {\n\t\t\t// there was a problem\n\t\t\texception.printStackTrace();\n\t\t} else {\n\t\t\t// node was created successfully, now set up a watch\n\t\t\tasync.watches().watch().forPath(path, (client, event) -> {\n\t\t\t\tSystem.out.println(\\\"Event received: \\\" + event);\n\t\t\t}).whenComplete((watch, exception) -> {\n\t\t\t\tif (exception!= null) {\n\t\t\t\t\t// there was a problem\n\t\t\t\t\texception.printStackTrace();\n\t\t\t\t} else {\n\t\t\t\t\tSystem.out.println(\\\"Watch set up successfully\\\");\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t});\n}", "CodeBleu_score": 70.4276271372416}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n\t// create a node at the given path with the given payload asynchronously\n\tasync.create().forPath(path).whenComplete((name, exception) -> {\n\t\tif (exception!= null) {\n\t\t\t// there was a problem\n\t\t\texception.printStackTrace();\n\t\t} else {\n\t\t\t// set up a watch on the node to handle future events\n\t\t\tasync.watches().watch().forPath(path).whenComplete((watch, watchException) -> {\n\t\t\t\tif (watchException!= null) {\n\t\t\t\t\t// there was a problem\n\t\t\t\t\twatchException.printStackTrace();\n\t\t\t\t} else {\n\t\t\t\t\t// handle future events\n\t\t\t\t\twatch.handleWatchedEvent(new WatchedEvent());\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t});\n}", "CodeBleu_score": 73.78811953444139}, {"code": "public static void createThenWatch(CuratorFramework client, String path) {\n\tAsyncCuratorFramework async = AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\tasync.create().forPath(path).whenComplete((name, exception) -> {\n\t\tif (exception!= null) {\n\t\t\texception.printStackTrace();\n\t\t} else {\n\t\t\t// watch the node for changes\n\t\t\tasync.watched().getData().forPath(path).whenComplete((data, ex) -> {\n\t\t\t\tif (ex!= null) {\n\t\t\t\t\tex.printStackTrace();\n\t\t\t\t} else {\n\t\t\t\t\tSystem.out.println(\\\"Node data is: \\\" + new String(data));\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t});\n}", "CodeBleu_score": 73.95775365435267}]}
{"instance_id": "4571de52ad5d", "generation_result": [{"code": "private List<OpResult> doOperation() throws Exception{\n\tList<Op> ops = transaction.getOperations();\n\tList<OpResult> resultList = client.getZooKeeper().multi(ops);\n\tif (resultList.size() > 0) {\n\t\tOpResult firstResult = resultList.get(0);\n\t\tif (firstResult.getType() == OpResult.Type.error) {\n\t\t\tOpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n\t\t\tint errorCode = errorResult.getErr();\n\t\t\tthrow KeeperException.create(KeeperException.Code.get(errorCode));\n\t\t}\n\t}\n\treturn resultList;\n}", "CodeBleu_score": 51.648434350811414}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<Op> opList = transaction.toOpList();\n\tList<OpResult> resultList = client.getZooKeeper().multi(opList);\n\tif (resultList.size() > 0 && resultList.get(0).getType() == OpResult.Type.error) {\n\t\tOpResult.ErrorResult errorResult = (OpResult.ErrorResult) resultList.get(0);\n\t\tint errorCode = errorResult.getErr();\n\t\tthrow KeeperException.create(KeeperException.Code.get(errorCode));\n\t}\n\treturn resultList;\n}", "CodeBleu_score": 41.42335360390008}, {"code": "private List<OpResult> doOperation() throws Exception{\n\tList<Op> ops = transaction.getOperations();\n\tList<OpResult> resultList = client.getZooKeeper().multi(ops);\n\tif (resultList.size() > 0) {\n\t\tOpResult firstResult = resultList.get(0);\n\t\tif (firstResult.getType() == OpResult.Type.error) {\n\t\t\tOpResult.ErrorResult errorResult = (OpResult.ErrorResult) firstResult;\n\t\t\tint errorCode = errorResult.getErr();\n\t\t\tthrow KeeperException.create(KeeperException.Code.get(errorCode));\n\t\t}\n\t}\n\treturn resultList;\n}", "CodeBleu_score": 51.648434350811414}]}
{"instance_id": "99263700ce6d", "generation_result": [{"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\tmethod lists all the services and their instances\n\t// This is useful for debugging\n\n\tSystem.out.println(\\\"Services:\\\");\n\tCollection<String> serviceNames = serviceDiscovery.queryForNames();\n\tfor (String serviceName : serviceNames) {\n\t\tSystem.out.println(\\\"  \\\" + serviceName + \\\":\\\");\n\t\tCollection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n\t\tfor (ServiceInstance<InstanceDetails> instance : instances) {\n\t\t\toutputInstance(instance);\n\t\t}\n\t}\n}", "CodeBleu_score": 40.979910408957856}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\tmethod lists all the services and their instances\n\n\t// Get all the service names\n\tCollection<String> serviceNames = serviceDiscovery.queryForNames();\n\n\t// Iterate through the service names and get their instances\n\tfor (String serviceName : serviceNames) {\n\t\tCollection<ServiceInstance<InstanceDetails>> instances = serviceDiscovery.queryForInstances(serviceName);\n\n\t\t// If there are no instances, print an error message\n\t\tif (instances.isEmpty()) {\n\t\t\tSystem.err.println(\"No instances for service: \" + serviceName);\n\t\t} else {\n\t\t\t// Print the service name and its instances\n\t\t\tSystem.out.println(\"Service: \" + serviceName);\n\t\t\tfor (ServiceInstance<InstanceDetails> instance : instances) {\n\t\t\t\toutputInstance(instance);\n\t\t\t}\n\t\t}\n\t}\n\n\t// Close the ServiceDiscovery instance quietly\n\tCloseableUtils.closeQuietly(serviceDiscovery);\n}", "CodeBleu_score": 54.43100673411783}, {"code": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception{\n\tfunction retrieves and prints the service names and their instances from the given ServiceDiscovery instance.\n\t// It queries for all service names and their corresponding instances, printing each service name and its instances.\n\t// If no instances are registered, it catches the NoNodeException and prints an error message.\n\t// Finally, it closes the ServiceDiscovery instance quietly.\n\n\ttry {\n\t\tfor (String serviceName : serviceDiscovery.queryForNames()) {\n\t\t\tfor (ServiceInstance<InstanceDetails> instance : serviceDiscovery.queryForInstances(serviceName)) {\n\t\t\t\toutputInstance(instance);\n\t\t\t}\n\t\t}\n\t} catch (KeeperException.NoNodeException e) {\n\t\tSystem.err.println(\\\"No instances found\\\");\n\t} finally {\n\t\tCloseableUtils.closeQuietly(serviceDiscovery);\n\t}\n}", "CodeBleu_score": 49.6284512320794}]}
{"instance_id": "891a390fa04d", "generation_result": [{"code": "private List<String> filterAndSortChildren(List<String> children){\n\ting> filteredChildren = Lists.newArrayList();\n\tfor (String child : children) {\n\t\tif (!child.equals(READY_NODE)) {\n\t\t\tfilteredChildren.add(child);\n\t\t}\n\t}\n\tCollections.sort(filteredChildren);\n\treturn filteredChildren;\n}", "CodeBleu_score": 28.18502824644164}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\ting> filteredChildren = new ArrayList<String>();\n\tfor (String child : children) {\n\t\tif (!child.equals(READY_NODE)) {\n\t\t\tfilteredChildren.add(child);\n\t\t}\n\t}\n\tCollections.sort(filteredChildren);\n\treturn filteredChildren;\n}", "CodeBleu_score": 28.39474533381756}, {"code": "private List<String> filterAndSortChildren(List<String> children){\n\ting> filteredChildren = Lists.newArrayList(Iterables.filter(children, new Predicate<String>() {\n\t\t@Override\n\t\tpublic boolean apply(String input) {\n\t\t\treturn!input.equals(READY_NODE);\n\t\t}\n\t}));\n\tCollections.sort(filteredChildren);\n\treturn filteredChildren;\n}", "CodeBleu_score": 59.701481619046085}]}
{"instance_id": "f25af95a13b6", "generation_result": [{"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\t\tif (zk!= null) {\n\t\tzk.close();\n\t}\n\tzk = new ZooKeeper(newHost, timeout, this);\n\tif (readOnly) {\n\t\tzk.addAuthInfo(\"digest\", \"guest:guest\".getBytes());\n\t}\n}", "CodeBleu_score": 23.270388095220323}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\t\tif (zk!= null) {\n\t\tzk.close();\n\t}\n\tzk = new ZooKeeper(newHost, timeout, this);\n\treadOnly = false;\n}", "CodeBleu_score": 19.32029786567175}, {"code": "protected void connectToZK(String newHost) throws InterruptedException, IOException{\n\t\tif (zk!= null) {\n\t\tzk.close();\n\t}\n\tzk = new ZooKeeper(newHost, timeout, this);\n\tif (readOnly) {\n\t\tzk.setReadOnly();\n\t}\n}", "CodeBleu_score": 20.518259528243515}]}
{"instance_id": "269eaac5cec6", "generation_result": [{"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\t\tList<ACL> acl = new ArrayList<ACL>();\n\tif (aclString == null || aclString.length() == 0) {\n\t\treturn acl;\n\t}\n\tString[] strings = aclString.split(\",\");\n\tfor (String s : strings) {\n\t\ts = s.trim();\n\t\tString[] parts = s.split(\":\");\n\t\tif (parts.length!= 3) {\n\t\t\tSystem.err.println(\"ACL string \\\"\" + s + \"\\\" is not valid\");\n\t\t\tcontinue;\n\t\t}\n\t\tId id = new Id(parts[0], parts[1]);\n\t\tint perm = 0;\n\t\tfor (int i = 0; i < parts[2].length(); i++) {\n\t\t\tchar c = parts[2].charAt(i);\n\t\t\tswitch (c) {\n\t\t\tcase 'r':\n\t\t\t\tperm |= ZooDefs.Perms.READ;\n\t\t\t\tbreak;\n\t\t\tcase 'w':\n\t\t\t\tperm |= ZooDefs.Perms.WRITE;\n\t\t\t\tbreak;\n\t\t\tcase 'c':\n\t\t\t\tperm |= ZooDefs.Perms.CREATE;\n\t\t\t\tbreak;\n\t\t\tcase 'd':\n\t\t\t\tperm |= ZooDefs.Perms.DELETE;\n\t\t\t\tbreak;\n\t\t\tcase 'a':\n\t\t\t\tperm |= ZooDefs.Perms.ADMIN;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tSystem.err.println(\"ACL string \\\"\" + s + \"\\\" is not valid\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tacl.add(new ACL(perm, id));\n\t}\n\treturn acl;\n}", "CodeBleu_score": 28.46646939056012}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\t\tList<ACL> acl = new ArrayList<ACL>();\n\tString[] segments = aclString.split(\\\"\\\\,\\\\s*\\\");\n\tfor (String segment : segments) {\n\t\tACL newAcl = new ACL();\n\t\tString[] parts = segment.split(\\\"\\\\:\\\\s*\\\");\n\t\tif (parts.length!= 3) {\n\t\t\tSystem.err.println(\\\"Invalid ACL segment: \\\" + segment);\n\t\t\tcontinue;\n\t\t}\n\t\tnewAcl.setId(new Id(parts[0], parts[1]));\n\t\tnewAcl.setPerms(getPerms(parts[2]));\n\t\tacl.add(newAcl);\n\t}\n\treturn acl;\n}", "CodeBleu_score": 24.47600479188896}, {"code": "private List<ACL> parseACLs(String aclString) throws IOException{\n\t\n        List<ACL> acls = new ArrayList<ACL>();\n        String[] strings = aclString.split(\\\",\\\");\n        for (String s : strings) {\n            String[] parts = s.split(\\\":\\\");\n            if (parts.length!= 3) {\n                System.err.println(\\\"Invalid ACL string: \\\" + s);\n                continue;\n            }\n            Id id;\n            try {\n                id = new Id(parts[0], parts[1]);\n            } catch (IllegalArgumentException e) {\n                System.err.println(\\\"Invalid ACL string: \\\" + s);\n                continue;\n            }\n            int perms = 0;\n            try {\n                perms = Integer.parseInt(parts[2]);\n            } catch (NumberFormatException e) {\n                System.err.println(\\\"Invalid ACL string: \\\" + s);\n                continue;\n            }\n            acls.add(new ACL(perms, id));\n        }\n        return acls;\n    }", "CodeBleu_score": 22.68891211037866}]}
{"instance_id": "c25db637a176", "generation_result": [{"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\t\tassertZookeeperExists(serverId);\n\tif (pathList == null || pathList.isEmpty()) {\n\t\treturn;\n\t}\n\tZookeeper zookeeper = zookeeperMap.get(serverId);\n\tif (pathList.size() <= 20) {\n\t\tfor (String path : pathList) {\n\t\t\tzookeeper.delete(path);\n\t\t}\n\t} else {\n\t\tzookeeper.deleteAsync(pathList);\n\t}\n}", "CodeBleu_score": 60.1364972242302}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\t\tassertZookeeperExists(serverId);\n\tif (pathList == null || pathList.isEmpty()) {\n\t\tthrow new IllegalArgumentException(\"path list is empty\");\n\t}\n\tZookeeper zookeeper = zookeeperMap.get(serverId);\n\tif (pathList.size() < 20) {\n\t\tfor (String path : pathList) {\n\t\t\tzookeeper.delete(path);\n\t\t}\n\t} else {\n\t\tzookeeper.deleteAsync(pathList);\n\t}\n}", "CodeBleu_score": 61.30900441409119}, {"code": "public void delete(String serverId, List<String> pathList) throws Exception{\n\t\tassertZookeeperExists(serverId);\n\tif (pathList == null || pathList.isEmpty()) {\n\t\treturn;\n\t}\n\tif (pathList.size() < 20) {\n\t\tfor (String path : pathList) {\n\t\t\tzookeeperMap.get(serverId).delete(path);\n\t\t}\n\t} else {\n\t\tfor (String path : pathList) {\n\t\t\tzookeeperMap.get(serverId).delete(path, true);\n\t\t}\n\t}\n}", "CodeBleu_score": 80.06068561592284}]}
{"instance_id": "3ecb1ce78f90", "generation_result": [{"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\tecting to \" + url + \"...\");\n\tvar factory = new CuratorZookeeperConnectionFactory();\n\tvar params = ZookeeperParams.builder()\n\t\t\t.id(id)\n\t\t\t.url(url)\n\t\t\t.maxRetries(10)\n\t\t\t.connectionTimeout(10000)\n\t\t\t.retryIntervalTime(1000)\n\t\t\t.sessionTimeout(10000)\n\t\t\t.build();\n\tvar connection = factory.createAsync(params);\n\twriter.write(\"success\");\n\tvar zooKeeperMain = new ZooKeeperMain(connection);\n\treturn new Terminal(id, url, connection, zooKeeperMain);\n}", "CodeBleu_score": 37.30887648604584}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\tecting to \" + url + \"...\n\");\n\tvar factory = new CuratorZookeeperConnectionFactory();\n\tvar params = ZookeeperParams.builder()\n\t\t\t.id(id)\n\t\t\t.url(url)\n\t\t\t.maxRetries(10)\n\t\t\t.connectionTimeout(10000)\n\t\t\t.retryIntervalTime(1000)\n\t\t\t.sessionTimeout(10000)\n\t\t\t.build();\n\tvar connection = factory.createAsync(params, null);\n\twriter.write(\"Connected to \" + url + \"!\n\");\n\tvar zk = new ZooKeeperMain(connection);\n\treturn new Terminal(id, url, connection, zk);\n}", "CodeBleu_score": 37.24245092240145}, {"code": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception{\n\tcting to \" + url + \"...\");\nvar connection = new CuratorZookeeperConnectionFactory().create(url);\nwriter.write(\"Success\");\nvar zk = new ZooKeeperMain(connection);\nreturn new Terminal(id, url, connection, zk);\n}", "CodeBleu_score": 22.066154121740382}]}
{"instance_id": "a3606e777eb5", "generation_result": [{"code": "public void blockUntilConnected(){\n\t retryCount = 0;\n\twhile (true) {\n\t\ttry {\n\t\t\tsshClient.connect();\n\t\t\treturn;\n\t\t} catch (IOException e) {\n\t\t\tif (e.getClass().getSimpleName().contains(\"Timeout\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n\t\t\t}\n\t\t\tif (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n\t\t\t}\n\t\t\tlog.error(\"create ssh-tunnel failed\", e);\n\t\t\tthis.close();\n\t\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 31.333907206872762}, {"code": "public void blockUntilConnected(){\n\t retries = 0;\n\twhile (retries < 6) {\n\t\ttry {\n\t\t\tsshClient.connect(getSshHost(), getSshPort());\n\t\t\tif (getSshPassword()!= null &&!getSshPassword().isBlank()) {\n\t\t\t\tlog.info(\"use password auth to create ssh-tunnel\");\n\t\t\t\tsshClient.authPassword(getSshUsername(), getSshPassword());\n\t\t\t} else if (getSshKeyFilePath()!= null &&!getSshKeyFilePath().isBlank()) {\n\t\t\t\tlog.info(\"use key file {} auth to create ssh-tunnel\", getSshKeyFilePath());\n\t\t\t\tsshClient.authPublickey(getSshUsername(), getSshKeyFilePath());\n\t\t\t} else {\n\t\t\t\tlog.info(\"use default key auth to create ssh-tunnel\");\n\t\t\t\tsshClient.authPublickey(getSshUsername());\n\t\t\t}\n\t\t\tbreak;\n\t\t} catch (IOException e) {\n\t\t\tif (e.getClass().getSimpleName().contains(\"Timeout\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n\t\t\t}\n\t\t\tif (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n\t\t\t}\n\t\t\tlog.error(\"create ssh-tunnel failed\", e);\n\t\t\tthis.close();\n\t\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\", e);\n\t\t}\n\t}\n\n\tif (retries == 6) {\n\t\tthis.close();\n\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\");\n\t}\n}", "CodeBleu_score": 33.277975990745915}, {"code": "public void blockUntilConnected(){\n\t attempts = 0;\n\twhile (attempts < 6) {\n\t\ttry {\n\t\t\tsshClient.connect(getSshHost(), getSshPort());\n\t\t\tif (getSshPassword()!= null &&!getSshPassword().isBlank()) {\n\t\t\t\tlog.info(\"use password auth to create ssh-tunnel\");\n\t\t\t\tsshClient.authPassword(getSshUsername(), getSshPassword());\n\t\t\t} else if (getSshKeyFilePath()!= null &&!getSshKeyFilePath().isBlank()) {\n\t\t\t\tlog.info(\"use key file {} auth to create ssh-tunnel\", getSshKeyFilePath());\n\t\t\t\tsshClient.authPublickey(getSshUsername(), getSshKeyFilePath());\n\t\t\t} else {\n\t\t\t\tlog.info(\"use default key auth to create ssh-tunnel\");\n\t\t\t\tsshClient.authPublickey(getSshUsername());\n\t\t\t}\n\t\t\treturn;\n\t\t} catch (IOException e) {\n\t\t\tif (e.getClass().getSimpleName().contains(\"Timeout\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by timeout: \" + sshHost, e);\n\t\t\t}\n\t\t\tif (e.getClass().getSimpleName().contains(\"UnknownHost\")) {\n\t\t\t\tthrow new IllegalStateException(\"SSH connect error by Unknown host \" + sshHost, e);\n\t\t\t}\n\t\t\tlog.error(\"create ssh-tunnel failed\", e);\n\t\t\tthis.close();\n\t\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\", e);\n\t\t} catch (Exception e) {\n\t\t\tlog.error(\"create ssh-tunnel failed\", e);\n\t\t\tthis.close();\n\t\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\", e);\n\t\t} finally {\n\t\t\tattempts++;\n\t\t\tlog.info(\"attempt {} to connect ssh-tunnel\", attempts);\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t} catch (InterruptedException ex) {\n\t\t\t\tlog.error(\"create ssh-tunnel failed\", ex);\n\t\t\t\tthis.close();\n\t\t\t\tthrow new IllegalStateException(\"create ssh-tunnel failed\", ex);\n\t\t\t}\n\t\t}\n\t}\n\tthis.close();\n\tthrow new IllegalStateException(\"create ssh-tunnel failed\");\n}", "CodeBleu_score": 35.21620341562402}]}
{"instance_id": "a4e5b886b72b", "generation_result": [{"code": "public void deleteNode(String serverId, List<String> pathList){\n\t\ttry {\n\t\tzookeeperDomainService.delete(serverId, pathList);\n\t} catch (Exception e) {\n\t\tlog.error(\"Delete node error, serverId: {}, pathList: {}\", serverId, pathList, e);\n\t\tthrow new IllegalStateException(e);\n\t}\n}", "CodeBleu_score": 73.44802083151448}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\t", "CodeBleu_score": 11.635939287176233}, {"code": "public void deleteNode(String serverId, List<String> pathList){\n\t\ttry {\n\t\tzookeeperDomainService.delete(serverId, pathList);\n\t} catch (Exception e) {\n\t\tlog.error(e.getMessage());\n\t\tthrow new IllegalStateException(e);\n\t}\n}", "CodeBleu_score": 67.57333208215431}]}
{"instance_id": "4fdbc9ec2e75", "generation_result": [{"code": "public void disconnect(String id){\n\t\tPlatform.runLater(() -> {\n\t\tvar serverConfig = configurationDomainService.getById(id).orElseThrow();\n\t\tzookeeperDomainService.disconnect(serverConfig.getId());\n\t\ttreeItemCache.remove(id);\n\t});\n}", "CodeBleu_score": 68.32489278508822}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\nvar serverConfig = configurationDomainService.getById(id).orElseThrow();\nzookeeperDomainService.disconnect(serverConfig.getId());\ntreeItemCache.removeServer(id);\n});\n}", "CodeBleu_score": 67.15200409550891}, {"code": "public void disconnect(String id){\n\tPlatform.runLater(() -> {\nvar serverConfig = configurationDomainService.getById(id).orElseThrow();\nzookeeperDomainService.disconnect(serverConfig);\ntreeItemCache.remove(id);\n});\n}", "CodeBleu_score": 55.58088909474035}]}
{"instance_id": "6bf0d7ac9390", "generation_result": [{"code": "public void startTerminal(String serverId, StringWriter stream){\n\t\tfinal ServerConfiguration serverConfiguration = getServerConfigurationById(serverId);\n\tfinal String connectionUrl = serverConfiguration.isSshTunnelEnabled()?\n\t\t\tserverConfiguration.getSshTunnel().getConnectUrl() :\n\t\t\tserverConfiguration.getConnectUrl();\n\tzookeeperDomainService.startTerminal(serverId, connectionUrl, stream);\n}", "CodeBleu_score": 38.228267901354684}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\t\tServerConfiguration serverConfiguration = getServerConfigurationById(serverId);\n\tString connectionUrl = serverConfiguration.isSshTunnelEnabled()?\n\t\t\t\"localhost:\" + serverConfiguration.getSshTunnel().getLocalPort() :\n\t\t\tserverConfiguration.getHost() + \":\" + serverConfiguration.getPort();\n\tzookeeperDomainService.startTerminal(serverId, connectionUrl, stream);\n}", "CodeBleu_score": 45.15764211249356}, {"code": "public void startTerminal(String serverId, StringWriter stream){\n\t\tServerConfiguration configuration = getServerConfigurationById(serverId);\n\tString connectionUrl;\n\tif (configuration.isSshTunnelEnabled()) {\n\t\tconnectionUrl = String.format(\"127.0.0.1:%d\", configuration.getSshTunnel().getLocalPort());\n\t} else {\n\t\tconnectionUrl = String.format(\"%s:%d\", configuration.getHost(), configuration.getPort());\n\t}\n\tzookeeperDomainService.startTerminal(serverId, connectionUrl, stream);\n}", "CodeBleu_score": 45.58124448934283}]}
{"instance_id": "635dea120e55", "generation_result": [{"code": "public void executeCommand(String id, String command){\n\t\tTry.of(() -> zookeeperDomainService.executeCommand(id, command))\n\t\t.onFailure(e -> {\n\t\t\tVToast.error(String.format(\"Failed to execute command on server %s: %s\", id, command));\n\t\t\tlogger.error(String.format(\"Failed to execute command on server %s: %s\", id, command), e);\n\t\t});\n}", "CodeBleu_score": 40.963575348466094}, {"code": "public void executeCommand(String id, String command){\n\t\ttry {\n\t\tzookeeperDomainService.executeCommand(id, command);\n\t} catch (Exception e) {\n\t\tlog.error(\"Failed to execute command {} on server {}. {}\", command, id, e.getMessage());\n\t\tVToast.error(I18n.get(\\\"error.command.execute.failed\\\", id, command, e.getMessage()));\n\t}\n}", "CodeBleu_score": 51.67496995381593}, {"code": "public void executeCommand(String id, String command){\n\t\tTry.of(() -> zookeeperDomainService.executeCommand(id, command))\n\t\t\t.onFailure(e -> {\n\t\t\t\tlog.error(\"Failed to execute command {} on server {}\", command, id, e);\n\t\t\t\tVToast.error(e.getMessage());\n\t\t\t});\n}", "CodeBleu_score": 38.409146475443556}]}
{"instance_id": "8627d161f5fc", "generation_result": [{"code": "public Properties loadZookeeperSystemProperties(){\n\t\tvar file = new File(System.getProperty(\"user.home\") + \"/.prettyZoo/conf/zoo.properties\");\n\tif (file.exists()) {\n\t\ttry (var reader = new FileReader(file)) {\n\t\t\tvar properties = new Properties();\n\t\t\tproperties.load(reader);\n\t\t\tlog.info(\"loaded system properties: {}\", properties);\n\t\t\treturn properties;\n\t\t} catch (IOException e) {\n\t\t\tlog.error(\"error loading system properties from file: {}\", e.getMessage());\n\t\t}\n\t} else {\n\t\tlog.warn(\"system properties file not found: {}\", file.getAbsolutePath());\n\t}\n\treturn new Properties();\n}", "CodeBleu_score": 45.240477856737215}, {"code": "public Properties loadZookeeperSystemProperties(){\n\t\tvar properties = new Properties();\n\tvar path = System.getProperty(\"user.home\") + \"/.prettyZoo/conf/zookeeper.properties\";\n\tvar file = new File(path);\n\tif (file.exists()) {\n\t\ttry (var inputStream = new FileInputStream(file)) {\n\t\t\tproperties.load(inputStream);\n\t\t\tlog.info(\"zookeeper system properties loaded: {}\", properties);\n\t\t} catch (IOException e) {\n\t\t\tlog.error(\"error loading zookeeper system properties, msg:{}\", e.getMessage(), e);\n\t\t}\n\t} else {\n\t\tlog.info(\"zookeeper system properties file not found at {}\", path);\n\t}\n\treturn properties;\n}", "CodeBleu_score": 42.374527796812856}, {"code": "public Properties loadZookeeperSystemProperties(){\n\t\tvar properties = new Properties();\n\tvar path = Paths.get(System.getProperty(\"user.home\"), \".prettyZoo\", \"zookeeper.properties\");\n\tif (Files.exists(path)) {\n\t\ttry (var inputStream = Files.newInputStream(path)) {\n\t\t\tproperties.load(inputStream);\n\t\t\tlog.info(\"load zookeeper system properties: {}\", properties);\n\t\t} catch (IOException e) {\n\t\t\tlog.error(\"load zookeeper system properties failed, msg:{}\", e.getMessage(), e);\n\t\t}\n\t} else {\n\t\tlog.warn(\"can't find zookeeper system properties file in {}\", path);\n\t}\n\treturn properties;\n}", "CodeBleu_score": 43.33999208168543}]}
{"instance_id": "dce20e143e04", "generation_result": [{"code": "public void saveZookeeperSystemProperties(String content){\n\t\tString sysPropPath = PrettyZooConfigRepository.SYS_PROP_PATH;\n\ttry {\n\t\tProperties properties = new Properties();\n\t\tproperties.load(new StringReader(content));\n\t\tlog.info(\"save system properties success ->\n{}\", properties);\n\t\tFiles.write(Paths.get(sysPropPath), content.getBytes(StandardCharsets.UTF_8));\n\t} catch (IOException e) {\n\t\tlog.error(\"save system properties failed\", e);\n\t\tVToast.error(\"\");\n\t}\n}", "CodeBleu_score": 29.89258044486326}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t\tvar properties = new Properties();\n\ttry {\n\t\tproperties.load(new StringReader(content));\n\t} catch (IOException e) {\n\t\tlog.error(\"load system properties failed\", e);\n\t\tVToast.error(\"load system properties failed\");\n\t\treturn;\n\t}\n\n\ttry {\n\t\tvar sysPropPath = PrettyZooConfigRepository.SYS_PROP_PATH;\n\t\tFiles.createDirectories(Paths.get(sysPropPath).getParent());\n\t\tFiles.writeString(Paths.get(sysPropPath), content);\n\t} catch (IOException e) {\n\t\tlog.error(\"save system properties failed\", e);\n\t\tVToast.error(\"save system properties failed\");\n\t}\n}", "CodeBleu_score": 42.323033505610184}, {"code": "public void saveZookeeperSystemProperties(String content){\n\t\tProperties properties = new Properties();\n\ttry {\n\t\tproperties.load(new StringReader(content));\n\t} catch (IOException e) {\n\t\tlog.error(\"load system properties failed\", e);\n\t\tVToast.error(\"load system properties failed\");\n\t\treturn;\n\t}\n\ttry {\n\t\tproperties.store(new FileWriter(PrettyZooConfigRepository.SYS_PROP_PATH), \"zookeeper system properties\");\n\t} catch (IOException e) {\n\t\tlog.error(\"save system properties failed\", e);\n\t\tVToast.error(\"save system properties failed\");\n\t\treturn;\n\t}\n\tlog.info(\"save system properties success\");\n\tVToast.success(\"save system properties success\");\n}", "CodeBleu_score": 32.51273349435229}]}
{"instance_id": "d89f8e6bfb31", "generation_result": [{"code": "private String response(Socket client){\n\t\ttry {\n\t\tBufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\tStringBuilder sb = new StringBuilder();\n\t\tString line;\n\t\twhile ((line = reader.readLine())!= null) {\n\t\t\tsb.append(line).append(\"\n\");\n\t\t}\n\t\tclient.close();\n\t\treturn sb.toString();\n\t} catch (IOException e) {\n\t\tthrow new IllegalStateException(e);\n\t}\n}", "CodeBleu_score": 54.22801545629812}, {"code": "private String response(Socket client){\n\t\tStringBuilder sb = new StringBuilder();\n\ttry {\n\t\tBufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\tString line = null;\n\t\twhile ((line = reader.readLine())!= null) {\n\t\t\tsb.append(line);\n\t\t}\n\t\tclient.close();\n\t\treturn sb.toString();\n\t} catch (IOException e) {\n\t\tthrow new IllegalStateException(e);\n\t}\n}", "CodeBleu_score": 50.883355691514375}, {"code": "private String response(Socket client){\n\t\ttry {\n\t\tBufferedReader reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\tStringBuilder sb = new StringBuilder();\n\t\tString line = null;\n\t\twhile ((line = reader.readLine())!= null) {\n\t\t\tsb.append(line).append(\"\n\");\n\t\t}\n\t\treader.close();\n\t\tclient.close();\n\t\treturn sb.toString();\n\t} catch (IOException e) {\n\t\tthrow new IllegalStateException(e);\n\t}\n}", "CodeBleu_score": 54.24293758597385}]}
{"instance_id": "b0a277c5e3c0", "generation_result": [{"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\t//        CuratorFramework client = CuratorFrameworkFactory.builder()\n//               .connectString(params.getConnectString())\n//               .sessionTimeoutMs(params.getSessionTimeout())\n//               .connectionTimeoutMs(params.getConnectionTimeout())\n//               .retryPolicy(new ExponentialBackoffRetry(1000, 3))\n//               .build();\n//\n//        client.getConnectionStateListenable().addListener((client1, newState) -> {\n            if (newState == ConnectionState.LOST) {\n                log.warn(\\\"Lost connection to Zookeeper\\\");\n            } else if (newState == ConnectionState.CONNECTED) {\n                log.info(\\\"Connected to Zookeeper\\\");\n            }\n        });\n//\n//        client.start();\n//\n//        try {\n            client.blockUntilConnected(5, TimeUnit.SECONDS);\n        } catch (InterruptedException e) {\n            client.close();\n            throw new RuntimeException(\\\"Failed to connect to Zookeeper\\\", e);\n        }\n//\n//        return new CuratorZookeeperConnection(params, client);\n//    }", "CodeBleu_score": 39.52395765497206}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\t", "CodeBleu_score": 3.373706656875291}, {"code": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params){\n\t", "CodeBleu_score": 3.373706656875291}]}
{"instance_id": "b144bcd67462", "generation_result": [{"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tString connectionString = getClient().getZookeeperClient().getCurrentConnectionString();\n\tif (isSync.compareAndSet(false, true)) {\n\t\tlog.info(\"Start syncing zookeeper nodes for connection string: {}\", connectionString);\n\t\ttreeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\n\t\ttry {\n\t\t\ttreeCache.start();\n\t\t} catch (Exception e) {\n\t\t\tlog.error(\"Failed to start tree cache for connection string: {}\", connectionString, e);\n\t\t\ttreeCache.close();\n\t\t}\n\t} else {\n\t\tlog.info(\"Synchronization is already in progress for connection string: {}\", connectionString);\n\t}\n}", "CodeBleu_score": 54.48913551796822}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tync.compareAndSet(false, true)) {\nlog.info(\"Zookeeper sync start\");\ntreeCache.getListenable().addListener((client, event) -> {\ntry {\nfor (ZookeeperNodeListener listener : listeners) {\nlistener.onEvent(event);\n}\n} catch (Exception e) {\nlog.error(\"Zookeeper sync error\", e);\n}\n});\ntry {\ntreeCache.start();\n} catch (Exception e) {\nlog.error(\"Zookeeper sync error\", e);\ntreeCache.close();\n}\n} else {\nlog.info(\"Zookeeper sync already in progress\");\n}\n}", "CodeBleu_score": 39.859860602350636}, {"code": "public void sync(List<ZookeeperNodeListener> listeners){\n\tync.get()) {\nlog.info(\"Zookeeper connection {} is already in sync\", id);\nreturn;\n}\nlog.info(\"Start to sync zookeeper connection {}\", id);\ntreeCache.getListenable().addListener(new CuratorTreeCacheListener(listeners));\ntry {\ntreeCache.start();\n} catch (Exception e) {\nlog.error(\"Failed to start TreeCache\", e);\ntreeCache.close();\n}\nisSync.set(true);\n}", "CodeBleu_score": 36.09983403505429}]}
{"instance_id": "6452f272f2d7", "generation_result": [{"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tChildrenCache cache = new PathChildrenCache(client, path, true);\n\tcache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n\tcache.getListenable().addListener(listener);\n}", "CodeBleu_score": 71.31256554899097}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tChildrenCache cache = new PathChildrenCache(client, path, true);\n\tcache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n\tcache.getListenable().addListener(listener);\n}", "CodeBleu_score": 71.31256554899097}, {"code": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception{\n\tChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n\tpathChildrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n\tpathChildrenCache.getListenable().addListener(listener);\n}", "CodeBleu_score": 86.08951142453473}]}
{"instance_id": "17f786e147c3", "generation_result": [{"code": "public void unregisterService(){\n\t\tlogger.info(\"Unregister all services\");\n\tfor (String path : pathList) {\n\t\ttry {\n\t\t\tcuratorClient.deletePath(path);\n\t\t} catch (Exception e) {\n\t\t\tlogger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n\t\t}\n\t}\n\tcuratorClient.close();\n}", "CodeBleu_score": 59.78285543221661}, {"code": "public void unregisterService(){\n\t\tlogger.info(\"Unregister service start\");\n\tfor (String path : pathList) {\n\t\ttry {\n\t\t\tthis.curatorClient.deletePath(path);\n\t\t} catch (Exception e) {\n\t\t\tlogger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n\t\t}\n\t}\n\tthis.curatorClient.close();\n}", "CodeBleu_score": 69.5051575035248}, {"code": "public void unregisterService(){\n\t\tlogger.info(\"Start to unregister service\");\n\tfor (String path : pathList) {\n\t\ttry {\n\t\t\tthis.curatorClient.deletePath(path);\n\t\t} catch (Exception e) {\n\t\t\tlogger.error(\"Unregister service fail, exception: {}\", e.getMessage());\n\t\t}\n\t}\n\tthis.curatorClient.close();\n}", "CodeBleu_score": 70.2158463670586}]}
{"instance_id": "32fec6cfa353", "generation_result": [{"code": "public void addListener(IZkStateListener listener){\n\tcompletableFuture.whenComplete((client, throwable) -> {\nif (throwable == null) {\nclient.subscribeStateChanges(listener);\n}\n});\n}", "CodeBleu_score": 40.90530245682146}, {"code": "public void addListener(IZkStateListener listener){\n\t\ttry {\n\t\tmakeClientReady().thenAccept(client -> client.subscribeStateChanges(listener));\n\t} catch (Throwable t) {\n\t\tlogger.error(\"Failed to add listener\", t);\n\t}\n}", "CodeBleu_score": 22.240905311573165}, {"code": "public void addListener(IZkStateListener listener){\n\t\tif (client == null) {\n\t\tcompletableFuture.whenComplete(this::makeClientReady);\n\t}\n\tclient.subscribeStateChanges(listener);\n}", "CodeBleu_score": 21.625805133270507}]}
{"instance_id": "619d57562a76", "generation_result": [{"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tclient = CuratorFrameworkFactory.builder()\n.connectString(serverAddr)\n.retryPolicy(new RetryNTimes(10, 1000))\n.connectionTimeoutMs(5000)\n.authorization(\"digest\", authority.getBytes())\n.build();\nclient.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n@Override\npublic void stateChanged(CuratorFramework client, ConnectionState newState) {\nswitch (newState) {\ncase LOST:\nif (CuratorZookeeperClient.this.getState()!= State.DISCONNECTED) {\nCuratorZookeeperClient.this.setState(State.DISCONNECTED);\n}\nbreak;\ncase CONNECTED:\ncase RECONNECTED:\nif (CuratorZookeeperClient.this.getState()!= State.CONNECTED) {\nCuratorZookeeperClient.this.setState(State.CONNECTED);\n}\nbreak;\ndefault:\nbreak;\n}\n}\n});\ntry {\nclient.start();\n} catch (Exception e) {\nthrow new IllegalStateException(e.getMessage(), e);\n}\n}", "CodeBleu_score": 58.26748757656568}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\tclient = CuratorFrameworkFactory.newClient(serverAddr, new RetryNTimes(10, 1000), 1000,\nnew CuratorFrameworkFactory.Builder().authorization(\"digest\", authority.getBytes()).build());\nclient.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n@Override\npublic void stateChanged(CuratorFramework client, ConnectionState newState) {\nswitch (newState) {\ncase LOST:\nCuratorZookeeperClient.this.stateChanged(StateListener.State.DISCONNECTED);\nbreak;\ncase CONNECTED:\nCuratorZookeeperClient.this.stateChanged(StateListener.State.CONNECTED);\nbreak;\ncase RECONNECTED:\nCuratorZookeeperClient.this.stateChanged(StateListener.State.RECONNECTED);\nbreak;\ndefault:\nbreak;\n}\n}\n});\nclient.start();\ntry {\nclient.blockUntilConnected();\n} catch (InterruptedException e) {\nthrow new IllegalStateException(e.getMessage(), e);\n}\n}", "CodeBleu_score": 47.431287569997174}, {"code": "public CuratorZookeeperClient(String serverAddr, String authority){\n\t\n        this.client = CuratorFrameworkFactory.builder()\n               .connectString(serverAddr)\n               .retryPolicy(new RetryNTimes(10, 1000))\n               .connectionTimeoutMs(5000)\n               .build();\n        if (authority!= null) {\n            this.client.addAuthInfo(\\\"digest\\\", authority.getBytes());\n        }\n        this.client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n            @Override\n            public void stateChanged(CuratorFramework client, ConnectionState newState) {\n                switch (newState) {\n                    case LOST:\n                        CuratorZookeeperClient.this.stateChanged(StateListener.State.LOST);\n                        break;\n                    case CONNECTED:\n                        CuratorZookeeperClient.this.stateChanged(StateListener.State.CONNECTED);\n                        break;\n                    case RECONNECTED:\n                        CuratorZookeeperClient.this.stateChanged(StateListener.State.RECONNECTED);\n                        break;\n                }\n            }\n        });\n        try {\n            this.client.start();\n        } catch (Exception e) {\n            throw new IllegalStateException(e.getMessage(), e);\n        }\n    }", "CodeBleu_score": 56.856074582111425}]}
{"instance_id": "9e7fb18946e4", "generation_result": [{"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\nif (listeners!= null) {\nTargetChildListener targetListener = listeners.remove(listener);\nif (targetListener!= null) {\nremoveTargetChildListener(path, targetListener);\n}\n}\n}", "CodeBleu_score": 80.36479814632517}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\nif (listeners!= null) {\nTargetChildListener targetListener = listeners.remove(listener);\nif (targetListener!= null) {\nremoveTargetChildListener(path, targetListener);\n}\n}\n}", "CodeBleu_score": 80.36479814632517}, {"code": "public void removeChildListener(String path, ChildListener listener){\n\tConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\nif (listeners!= null) {\nTargetChildListener targetListener = listeners.remove(listener);\nif (targetListener!= null) {\nremoveTargetChildListener(path, targetListener);\n}\n}\n}", "CodeBleu_score": 80.36479814632517}]}
{"instance_id": "2c3fae642654", "generation_result": [{"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\t\tstring idc_host;\n\tunsigned long htkey = reinterpret_cast<unsigned long>(zh);\n\tif (QCONF_OK!= lock_ht_find(_ht_handle_idchost, _ht_hi_mutex, htkey, idc_host)) {\n\t\treturn QCONF_ERR_OTHER;\n\t}\n\tstring idc, host;\n\tdeserialize_from_idc_host(idc_host, idc, host);\n\t// close the old handle\n\tzookeeper_close(zh);\n\tzh = zookeeper_init(host.c_str(), global_watcher, _recv_timeout, NULL, NULL, 0);\n\tif (NULL == zh) {\n\t\tLOG_ERR(\"Failed to initial zookeeper. host:%s timeout:%d\", host.c_str(), _recv_timeout);\n\t\treturn QCONF_ERR_OTHER;\n\t}\n\tinit_env_for_zk(zh, idc_host, idc);\n\t_finish_process_tbl_sleep_setting = true;\n\treturn QCONF_OK;\n}", "CodeBleu_score": 49.77054110956365}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\t", "CodeBleu_score": 1.8025725052776684}, {"code": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh){\n\t\tif (zh == NULL) {\n\t\treturn QCONF_ERR_OTHER;\n\t}\n\tstring idc_host;\n\tunsigned long htkey = reinterpret_cast<unsigned long>(zh);\n\tif (QCONF_OK!= lock_ht_find(_ht_handle_idchost, _ht_hi_mutex, htkey, idc_host)) {\n\t\treturn QCONF_ERR_OTHER;\n\t}\n\tstring idc, host;\n\tdeserialize_from_idc_host(idc_host, idc, host);\n\tzookeeper_close(zh);\n\tzh = zookeeper_init(host.c_str(), global_watcher, _recv_timeout, NULL, NULL, 0);\n\tif (zh == NULL) {\n\t\tLOG_ERR(\"Failed to initial zookeeper. host:%s timeout:%d\", host.c_str(), _recv_timeout);\n\t\treturn QCONF_ERR_OTHER;\n\t}\n\tinit_env_for_zk(zh, idc_host, idc);\n\t// reset the table watcher\n\t_finish_process_tbl_sleep_setting = true;\n\treturn QCONF_OK;\n}", "CodeBleu_score": 50.18394263594126}]}
{"instance_id": "0b5ab34e1dd9", "generation_result": [{"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\t\tif (NULL == zh) return;\n\n\t// update the table\n\tlock_ht_insert(_ht_handle_idchost, _ht_hi_mutex, reinterpret_cast<unsigned long>(zh), idc_host);\n\tlock_ht_insert(_ht_idchost_handle, _ht_ih_mutex, idc_host, reinterpret_cast<unsigned long>(zh));\n\n\t// check the connection state\n\tint state = zoo_state(zh);\n\tif (ZOO_CONNECTED_STATE!= state) {\n\t\tLOG_ERR(\"The zookeeper handle is not connected. host:%s\", idc_host.c_str());\n\t\treturn;\n\t}\n\n\t// register the current host on the Zookeeper server\n\tstring host_path = _zk_root_path + \"/\" + idc_host;\n\tint ret = zk_create_node(zh, host_path, NULL, 0, &ZOO_OPEN_ACL_UNSAFE, 0);\n\tif (QCONF_OK!= ret) {\n\t\tLOG_ERR(\"Failed to create node on Zookeeper server. path:%s\", host_path.c_str());\n\t\treturn;\n\t}\n\n\t// set the watcher on the notify node\n\tstring notify_path = _zk_root_path + \"/\" + _notify_node;\n\tret = zk_set_watcher(zh, notify_path, global_watcher, NULL);\n\tif (QCONF_OK!= ret) {\n\t\tLOG_ERR(\"Failed to set watcher on Zookeeper server. path:%s\", notify_path.c_str());\n\t\treturn;\n\t}\n\n\t// if the watcher is set successfully, add the IDC to a gray list\n\tadd_gray_idc(idc);\n}", "CodeBleu_score": 50.22765946635992}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\t\tif (NULL == zh) {\n\t\treturn;\n\t}\n\n\t// Update internal tables with the handle and host information\n\tlock_ht_insert(_ht_handle_idchost, _ht_hi_mutex, reinterpret_cast<unsigned long>(zh), idc_host);\n\tlock_ht_insert(_ht_idchost_handle, _ht_ih_mutex, idc_host, zh);\n\n\t// Check the connection state of the Zookeeper handle\n\tint state = zoo_state(zh);\n\tif (state!= ZOO_CONNECTED_STATE) {\n\t\treturn;\n\t}\n\n\t// Register the current host on the Zookeeper server\n\tstring host;\n\tget_local_host(host);\n\tstring host_path = _zk_root_path + \"/\" + idc + \"/hosts/\" + host;\n\tint ret = zk_create(zh, host_path, \"\", 0, &ZOO_OPEN_ACL_UNSAFE, 0);\n\tif (ret!= ZOK) {\n\t\tLOG_ERR(\"Failed to register host on Zookeeper. host_path:%s\", host_path.c_str());\n\t\treturn;\n\t}\n\n\t// Set a watcher on the notify node\n\tstring notify_path = _zk_root_path + \"/\" + idc + \"/notify\";\n\tret = zk_exists(zh, notify_path.c_str(), 1, NULL);\n\tif (ret == ZOK) {\n\t\t// Add the IDC to a gray list if the watcher is successfully set\n\t\tadd_gray_idc(idc);\n\t} else if (ret == ZNONODE) {\n\t\t// Log an error if the watcher is not set\n\t\tLOG_ERR(\"Failed to set watcher on notify node. notify_path:%s\", notify_path.c_str());\n\t} else {\n\t\tLOG_ERR(\"Failed to check the existence of notify node. notify_path:%s\", notify_path.c_str());\n\t}\n}", "CodeBleu_score": 56.43239654836015}, {"code": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc){\n\t\tif (NULL == zh) {\n\t\treturn;\n\t}\n\n\t// Update internal tables with the handle and host information\n\tlock_ht_insert(_ht_handle_idchost, _ht_hi_mutex, reinterpret_cast<unsigned long>(zh), idc_host);\n\tlock_ht_insert(_ht_idchost_handle, _ht_ih_mutex, idc_host, zh);\n\n\t// Check the connection state of the Zookeeper handle\n\tint state = zoo_state(zh);\n\tif (state!= ZOO_CONNECTED_STATE) {\n\t\treturn;\n\t}\n\n\t// Register the current host on the Zookeeper server\n\tstring host;\n\tif (QCONF_OK!= get_host_conf(host)) {\n\t\tLOG_ERR(\"Failed to get host conf.\");\n\t\treturn;\n\t}\n\n\tstring path = \"/qconf/host/\" + host;\n\tint ret = zk_create_node(zh, path, \"\", 0);\n\tif (QCONF_OK!= ret) {\n\t\tLOG_ERR(\"Failed to create host node. path:%s\", path.c_str());\n\t\treturn;\n\t}\n\n\t// Set a watcher on the notify node\n\tstring notify_node = \"/qconf/notify\";\n\tret = zk_watch_node(zh, notify_node, global_watcher);\n\tif (QCONF_OK!= ret) {\n\t\tLOG_ERR(\"Failed to set watcher on notify node. path:%s\", notify_node.c_str());\n\t\treturn;\n\t}\n\n\t// Add the IDC to a gray list if the watcher is set successfully\n\tif (QCONF_OK == ret) {\n\t\tadd_gray_idc(idc);\n\t} else {\n\t\tLOG_ERR(\"Failed to set watcher on notify node. path:%s\", notify_node.c_str());\n\t}\n}", "CodeBleu_score": 56.437098686847165}]}
{"instance_id": "19fc5847a7c5", "generation_result": [{"code": "def make_digest_acl_credential(username, password)\n\t\n    return f\"{username}:{b64encode(hashlib.sha1(f'{username}:{password}'.encode()).digest()).decode()}\"\n\n", "CodeBleu_score": 3.237872572936826}, {"code": "def make_digest_acl_credential(username, password)\n\t\n    return f\"{username}:{b64encode(hashlib.sha1(f'{username}:{password}'.encode()).digest()).decode()}\"\n\n", "CodeBleu_score": 3.237872572936826}, {"code": "def make_digest_acl_credential(username, password)\n\t\n    return \"%s:%s\" % (username, b64encode(hashlib.sha1((username + \":\" + password).encode()).digest()).decode())\n\n", "CodeBleu_score": 21.099958043009913}]}
{"instance_id": "4a062391deb0", "generation_result": [{"code": "def holds_lock(self)\n\t\n    if self.processing_element is None:\n        return False\n    else:\n        lock_path = self.processing_element[0]\n        try:\n            lock_value = self.client.retry(self.client.get, lock_path)[0]\n        except NoNodeError:\n            return False\n        else:\n            return lock_value == self.id\n\n", "CodeBleu_score": 46.48766186469449}, {"code": "def holds_lock(self)\n\t\n    if self.processing_element is None:\n        return False\n    lock_path = self.processing_element[0]\n    lock_value = self.client.retry(self.client.get, lock_path)[0]\n    return lock_value == self.id\n\n", "CodeBleu_score": 43.5838604540094}, {"code": "def holds_lock(self)\n\t\n    if self.processing_element is None:\n        return False\n    lock_path = self.processing_element[0]\n    try:\n        lock_value = self.client.retry(self.client.get, lock_path)[0]\n    except NoNodeError:\n        return False\n    return lock_value == self.id\n\n    def _inner_get(self, timeout):\n        start_time = time.time()\n        while True:\n            if self.processing_element is not None:\n                return self.processing_element[1]\n            try:\n                self.processing_element = self.client.retry(\n                    self._get_next_element\n                )\n            except NoNodeError:\n                return None\n            if self.processing_element is None:\n                if timeout is None:\n                    continue\n                elif time.time() - start_time > timeout:\n                    return None\n                else:\n                    time.sleep(0.01)\n            else:\n                return self.processing_element[1]\n\n    def _get_next_element(self):\n        children = self.client.retry(self.client.get_children, self._entries_path)\n        children = sorted(children)\n        if not children:\n            return None\n        name = children[0]\n        lock_path = self._lock_path + \\\"/\\\" + name\n        try:\n            self.client.create(lock_path, self.id, ephemeral=True)\n        except NodeExistsError:\n            # Someone else got the lock before us, try again\n            return None\n        return (lock_path, self.client.get(self._entries_path + \\\"/\\\" + name)[0])\n\n    def consume(self):\n        \\\"\\\"\\\"Consumes the entry that was previously retrieved with the\n        :meth:`LockingQueue.get` method. If no entry was retrieved, this\n        method will do nothing.\n        \\\"\\\"\\\"\n        if self.processing_element is None:\n            return\n        lock_path, _ = self.processing_element\n        self.client.delete(lock_path)\n        self.processing_", "CodeBleu_score": 44.1004385421862}]}
{"instance_id": "aaac7523d2b5", "generation_result": [{"code": "def consume(self)\n\t\n    if self.processing_element is None:\n        return False\n    lock_id, _ = self.processing_element\n    lock_path = \"{path}/{id}\".format(path=self._lock_path, id=lock_id)\n    self.client.sync(lock_path)\n    value, stat = self.client.retry(self.client.get, lock_path)\n    if value!= self.id:\n        return False\n    with self.client.transaction() as transaction:\n        transaction.delete(lock_path)\n        transaction.delete(self.processing_element[1])\n    self.processing_element = None\n    return True\n\n", "CodeBleu_score": 57.12937894113929}, {"code": "def consume(self)\n\t\n    if self.processing_element is None or not self.holds_lock():\n        return False\n    lock_id, entry_path = self.processing_element\n    lock_path = \"{path}/{id}\".format(path=self._lock_path, id=lock_id)\n    with self.client.transaction() as transaction:\n        transaction.delete(lock_path)\n        transaction.delete(entry_path)\n    self.processing_element = None\n    return True\n\n", "CodeBleu_score": 56.699055166229726}, {"code": "def consume(self)\n\t\n    if self.processing_element is None:\n        return False\n    lock_id, _ = self.processing_element\n    lock_path = \"{path}/{id}\".format(path=self._lock_path, id=lock_id)\n    self.client.sync(lock_path)\n    value, stat = self.client.retry(self.client.get, lock_path)\n    if value!= self.id:\n        return False\n    with self.client.transaction() as transaction:\n        transaction.delete(lock_path)\n        transaction.delete(self.processing_element[1])\n    self.processing_element = None\n    return True\n\n", "CodeBleu_score": 57.12937894113929}]}
{"instance_id": "bed0c998ac4c", "generation_result": [{"code": "def _read_socket(self, read_timeout)\n\t\n    header, buffer, offset = self._read_header(read_timeout)\n    if header.xid == PING_XID:\n        self.logger.log(BLATHER, \"Received ping\")\n        self.outstanding_pings.clear()\n    elif header.xid == AUTH_XID:\n        if header.err:\n            self.logger.debug(\"Received error(xid=%s) %r\", AUTH_XID, header.err)\n            self.auth_failed = True\n        else:\n            self.auth_failed = False\n            self.auth_complete.set()\n    elif header.xid == WATCH_XID:\n        self._read_watch_event(buffer, offset)\n    else:\n        self.logger.log(BLATHER, \"Received response(xid=%s)\", header.xid)\n        self._read_response(header, buffer, offset)\n\n", "CodeBleu_score": 48.49336817911813}, {"code": "def _read_socket(self, read_timeout)\n\t\n    # Read the header\n    header = self._read_header(read_timeout)\n\n    # If its a ping, just log it and return\n    if header.xid == PING_XID:\n        self.logger.log(BLATHER, \\\"Read ping response\\\")\n        self._outstanding_pings.clear()\n        return\n\n    # If its an auth response, handle it\n    if header.xid == AUTH_XID:\n        if header.err:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", AUTH_XID, callback_exception\n            )\n            self._auth_failed.set()\n            return\n        self._auth_failed.clear()\n        return\n\n    # If its a watch event, handle it\n    if header.xid == WATCH_XID:\n        buffer = self._read(header.length, read_timeout)\n        self._read_watch_event(buffer, 0)\n        return\n\n    # If its a response to a request, handle it\n    if header.xid == PING_XID:\n        self.logger.log(BLATHER, \\\"Read ping response\\\")\n        self._outstanding_pings.clear()\n        return\n\n    # If its an auth response, handle it\n    if header.xid == AUTH_XID:\n        if header.err:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \\\"Received error(xid=%s) %r\\\", AUTH_XID, callback_exception\n            )\n            self._auth_failed.set()\n            return\n        self._auth_failed.clear()\n        return\n\n    # If its a watch event, handle it\n    if header.xid == WATCH_XID:\n        buffer = self._read(header.length, read_timeout)\n        self._read_watch_event(buffer, 0)\n        return\n\n    # If its a response to a request, handle it\n   ", "CodeBleu_score": 42.086804588547345}, {"code": "def _read_socket(self, read_timeout)\n\t\n    self.logger.log(BLATHER, \"Reading socket\")\n    header, buffer, offset = self._read_header(read_timeout)\n    if header.xid == PING_XID:\n        self.logger.log(BLATHER, \"Received ping\")\n        self._outstanding_pings.clear()\n    elif header.xid == AUTH_XID:\n        self.logger.log(BLATHER, \"Received auth response\")\n        if header.err:\n            self.logger.error(\"Authentication failed: %s\", header.err)\n            self._auth_failed.set()\n        else:\n            self._auth_failed.clear()\n    elif header.xid == WATCH_XID:\n        self._read_watch_event(buffer, offset)\n    else:\n        self.logger.log(BLATHER, \"Received response(xid=%s)\", header.xid)\n        self._read_response(header, buffer, offset)\n\n", "CodeBleu_score": 62.753590972914864}]}
{"instance_id": "b10e2c2c410b", "generation_result": [{"code": "def zk_loop(self)\n\t\n    self.logger.log(BLATHER, \"Starting connection handler\")\n    self._connection_stopped.clear()\n    while not self._stopped.is_set():\n        try:\n            self.logger.log(BLATHER, \"Connecting to Zookeeper\")\n            self._connect()\n        except (SessionExpiredError, ConnectionDropped) as e:\n            self.logger.log(BLATHER, \"Connection dropped, reconnecting\")\n            self.logger.debug(\"Exception: %r\", e)\n        except KazooException as e:\n            self.logger.log(\n                BLATHER,\n                \"Connection attempt to Zookeeper failed: %s\",\n                e,\n            )\n            self.logger.debug(\"Exception: %r\", e)\n            self._connection_stopped.set()\n            self._session_callback(KeeperState.LOST)\n            self._stopped.wait()\n        else:\n            self.logger.log(BLATHER, \"Connected to Zookeeper\")\n            self._connection_stopped.set()\n            self._session_callback(KeeperState.CONNECTED)\n            self._stopped.wait()\n    self.logger.log(BLATHER, \"Stopping connection handler\")\n    self._connection_stopped.set()\n    self._session_callback(KeeperState.CLOSED)\n    self._stopped.set()\n\n", "CodeBleu_score": 44.024743665395405}, {"code": "def zk_loop(self)\n\t\n    self.logger.log(BLATHER, \"Starting ZK loop\")\n    self.connection_stopped.clear()\n    while True:\n        if self._stopped.is_set():\n            break\n        try:\n            self.connect(self.hosts, self.timeout)\n        except (ConnectionRefusedError, TimeoutError) as e:\n            if self._stopped.is_set():\n                break\n            self.logger.warning(\n                \"Unable to connect to Zookeeper: %s, retrying in %s seconds\",\n                e,\n                self.retry_policy.sleep_time,\n            )\n            self.retry_policy.sleep()\n            continue\n        except Exception as e:\n            if self._stopped.is_set():\n                break\n            self.logger.warning(\"Unable to connect to Zookeeper: %s\", e)\n            continue\n        break\n    self.logger.log(BLATHER, \"Stopping ZK loop\")\n    self.connection_stopped.set()\n    self._session_callback(KeeperState.CLOSED)\n\n", "CodeBleu_score": 47.01665266721907}, {"code": "def zk_loop(self)\n\t\n    self.logger.log(BLATHER, \"Starting ZK loop\")\n    self.connection_stopped.clear()\n    while not self._stopped.is_set():\n        try:\n            self._connect()\n        except (ConnectionLoss, OperationTimeoutError, SessionExpiredError):\n            self.logger.warn(\"Connection attempt to Zookeeper failed\")\n            if not self._stopped.is_set():\n                self._retry_policy.sleep_for()\n        else:\n            break\n    self.logger.log(BLATHER, \"Stopped ZK loop\")\n    self.connection_stopped.set()\n    self._session_callback(KeeperState.CLOSED)\n\n", "CodeBleu_score": 47.40862300910624}]}
{"instance_id": "42b3d26a3540", "generation_result": [{"code": "def _partitioner(self, identifier, members, partitions)\n\t\n    if len(members) == 0:\n        return []\n    if len(partitions) == 0:\n        return []\n    members.sort()\n    partitions.sort()\n    index = members.index(identifier)\n    return partitions[index::len(members)]\n\n", "CodeBleu_score": 45.41790567719023}, {"code": "def _partitioner(self, identifier, members, partitions)\n\t\n    members = list(members)\n    partitions = list(partitions)\n    members.sort()\n    partitions.sort()\n    index = members.index(identifier)\n    return [partitions[i] for i in range(index, len(partitions), len(members))]\n\n", "CodeBleu_score": 35.440506186234636}, {"code": "def _partitioner(self, identifier, members, partitions)\n\t\n    members = sorted(members)\n    partitions = sorted(partitions)\n    index = members.index(identifier)\n    return partitions[index::len(members)]\n\n", "CodeBleu_score": 39.65957835883439}]}
{"instance_id": "738974d4e9c0", "generation_result": [{"code": "def leave(self)\n\t   try:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        self._best_effort_cleanup()\n        self.participating = True\n\n", "CodeBleu_score": 61.0884883672711}, {"code": "def leave(self)\n\t   try:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        # We failed to leave, best effort cleanup\n        self._best_effort_cleanup()\n        self.participating = True\n\n", "CodeBleu_score": 61.0884883672711}, {"code": "def leave(self)\n\t   try:\n        self.client.retry(self._inner_leave)\n        self.participating = False\n    except KazooException:\n        self._best_effort_cleanup()\n        self.participating = False\n\n", "CodeBleu_score": 62.589749951211495}]}
{"instance_id": "a321fef7d100", "generation_result": [{"code": "public Result start(){\n\t\tMap map = new HashMap<>();\n\ttry {\n\t\tboolean status = hdfsOverFtpServer.statusServer();\n\t\tif (!status) {\n\t\t\thdfsOverFtpServer.startServer();\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n\t\t} else {\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u5df2\u542f\u52a8\");\n\t\t}\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\tmap.put(\"is_running\", false);\n\t\treturn new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 48.51014140569358}, {"code": "public Result start(){\n\t\tMap map = new HashMap<>();\n\ttry {\n\t\tboolean status = hdfsOverFtpServer.statusServer();\n\t\tif (!status) {\n\t\t\thdfsOverFtpServer.startServer();\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n\t\t} else {\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u5df2\u7ecf\u542f\u52a8\");\n\t\t}\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\tmap.put(\"is_running\", false);\n\t\treturn new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 48.48249831494788}, {"code": "public Result start(){\n\t\tMap map = new HashMap<>();\n\ttry {\n\t\tboolean status = hdfsOverFtpServer.statusServer();\n\t\tif (!status) {\n\t\t\thdfsOverFtpServer.startServer();\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n\t\t} else {\n\t\t\tmap.put(\"is_running\", true);\n\t\t\treturn new Result(true, map, \"FTP \u670d\u52a1\u5df2\u542f\u52a8\");\n\t\t}\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\tmap.put(\"is_running\", false);\n\t\treturn new Result(false, map, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 48.51014140569358}]}
{"instance_id": "24b088689114", "generation_result": [{"code": "public Result stop(){\n\t\ttry {\n\t\tboolean status = hdfsOverFtpServer.statusServer();\n\t\tif (status) {\n\t\t\thdfsOverFtpServer.stopServer();\n\t\t}\n\t\treturn new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\treturn new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 84.86209319187434}, {"code": "public Result stop(){\n\t\ttry {\n\t\tboolean status = hdfsOverFtpServer.statusServer();\n\t\tif (status) {\n\t\t\thdfsOverFtpServer.stopServer();\n\t\t}\n\t\treturn new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\treturn new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 84.86209319187434}, {"code": "public Result stop(){\n\ttry {\nboolean status = hdfsOverFtpServer.statusServer();\nif(status) {\nhdfsOverFtpServer.stopServer();\n}\nreturn new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n}catch (Exception e) {\nlog.error(e);\nreturn new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n}\n}", "CodeBleu_score": 89.60593044836757}]}
{"instance_id": "af0884ffda66", "generation_result": [{"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tf (authorities == null) {\n\t\treturn null;\n\t}\n\tfor (Authority authority : authorities) {\n\t\trequest = authority.authorize(request);\n\t\tif (request == null) {\n\t\t\treturn null;\n\t\t}\n\t}\n\treturn request;\n}", "CodeBleu_score": 35.25199708872136}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tf (authorities == null || authorities.isEmpty()) {\n\t\treturn null;\n\t}\n\tfor (Authority authority : authorities) {\n\t\trequest = authority.authorize(request);\n\t\tif (request == null) {\n\t\t\treturn null;\n\t\t}\n\t}\n\treturn request;\n}", "CodeBleu_score": 36.10015852321412}, {"code": "public AuthorizationRequest authorize(AuthorizationRequest request){\n\tf (authorities == null) {\n\t\treturn null;\n\t}\n\n\tfor (Authority authority : authorities) {\n\t\trequest = authority.authorize(request);\n\t\tif (request == null) {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\treturn request;\n}", "CodeBleu_score": 35.25199708872136}]}
{"instance_id": "920d28a0fe76", "generation_result": [{"code": "public boolean mkdir(){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\treturn dfs.mkdirs(fullPath);\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.16999616034119}, {"code": "public boolean mkdir(){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.59138464273558}, {"code": "public boolean mkdir(){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.mkdirs(fullPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.59138464273558}]}
{"instance_id": "a2b04232d387", "generation_result": [{"code": "public boolean delete(){\n\try {\n\t\tFileSystem fs = HdfsOverFtpSystem.getDfs();\n\t\tfs.delete(fullPath, true);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.98458301107836}, {"code": "public boolean delete(){\n\try {\n\t\tFileSystem fs = HdfsOverFtpSystem.getDfs();\n\t\tfs.delete(fullPath, true);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.98458301107836}, {"code": "public boolean delete(){\n\try {\n\t\tFileSystem fs = HdfsOverFtpSystem.getDfs();\n\t\tfs.delete(fullPath, true);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 72.98458301107836}]}
{"instance_id": "3407087c9af6", "generation_result": [{"code": "public boolean move(FtpFile ftpFile){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tPath newPath = new Path(homePath + ftpFile.getAbsolutePath());\n\t\tdfs.rename(fullPath, newPath);\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 60.62246649783629}, {"code": "public boolean move(FtpFile ftpFile){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.rename(fullPath, new Path(homePath + ftpFile.getAbsolutePath()));\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 59.84246794971462}, {"code": "public boolean move(FtpFile ftpFile){\n\try {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tdfs.rename(fullPath, new Path(ftpFile.getAbsolutePath()));\n\t\treturn true;\n\t} catch (IOException e) {\n\t\te.printStackTrace();\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 59.17679903300025}]}
{"instance_id": "91227000a139", "generation_result": [{"code": "public List<FtpFile> listFiles(){\n\ttry {\nFileSystem dfs = HdfsOverFtpSystem.getDfs();\nFileStatus[] fileStatus = dfs.listStatus(fullPath);\nString base = getAbsolutePath();\nif (base.length() > 0 && base.charAt(base.length() - 1)!= '/') {\nbase += \"/\";\n}\nFtpFile[] files = new FtpFile[fileStatus.length];\nfor (int i = 0; i < fileStatus.length; i++) {\nfiles[i] = new HdfsFileObject(homePath, base + fileStatus[i].getPath().getName(), user);\n}\nreturn Collections.unmodifiableList(Arrays.asList(files));\n} catch (IOException e) {\nlog.error(\"listFiles: \" + fullPath, e);\nreturn null;\n}\n}", "CodeBleu_score": 53.37000446204686}, {"code": "public List<FtpFile> listFiles(){\n\t\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tFileStatus[] files = dfs.listStatus(fullPath);\n\t\tList<FtpFile> fileList = new ArrayList<FtpFile>();\n\t\tfor (FileStatus file : files) {\n\t\t\tString fileName = file.getPath().getName();\n\t\t\tif (fileName.equals(\\\".\\\") || fileName.equals(\\\"..\\\") || fileName.equals(\\\".Trash\\\")) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tHdfsFileObject fileObj = new HdfsFileObject(homePath, path.toString() + File.separator + fileName, user);\n\t\t\tfileList.add(fileObj);\n\t\t}\n\t\treturn Collections.unmodifiableList(fileList);\n\t} catch (IOException e) {\n\t\tlog.error(fullPath + \\\" : \\\" + e.getMessage());\n\t\treturn null;\n\t}\n}", "CodeBleu_score": 39.604532490594}, {"code": "public List<FtpFile> listFiles(){\n\t\ttry {\n\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\tFileStatus[] fs = dfs.listStatus(fullPath);\n\t\tList<FtpFile> files = new ArrayList<FtpFile>();\n\t\tfor (FileStatus f : fs) {\n\t\t\tString name = f.getPath().getName();\n\t\t\tString virtualName = path.toString() + File.separator + name;\n\t\t\tfiles.add(new HdfsFileObject(homePath.toString(), virtualName, user));\n\t\t}\n\t\treturn Collections.unmodifiableList(files);\n\t} catch (IOException e) {\n\t\tlog.error(e.getMessage(), e);\n\t\treturn null;\n\t}\n}", "CodeBleu_score": 35.48820868607242}]}
{"instance_id": "aec4623134b5", "generation_result": [{"code": "public void configure(){\n\t\tisConfigured = true;\n\n\tuserDataProp = new BaseProperties();\n\n\tif (userDataFile.exists()) {\n\t\ttry {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\\\"Cannot load user data file '\\\" + userDataFile.getAbsolutePath() + \\\"'\\\", e);\n\t\t}\n\t}\n\n\t// convert deprecated properties\n\tfor (Object key : userDataProp.keySet()) {\n\t\tString keyStr = (String) key;\n\nif (keyStr.startsWith(DEPRECATED_PREFIX)) {\n\nString newKey = PREFIX + keyStr.substring(DEPRECATED_PREFIX.length());\n\nuserDataProp.setProperty(newKey, userDataProp.getProperty(keyStr));\n\nuserDataProp.remove(keyStr);\n\n}\n\n}\n\n}", "CodeBleu_score": 47.699695893788444}, {"code": "public void configure(){\n\t\tisConfigured = true;\n\n\ttry {\n\n\t\tif (userDataFile.exists()) {\n\t\t\tuserDataProp = new BaseProperties(new FileInputStream(userDataFile));\n\t\t} else {\n\t\t\tuserDataProp = new BaseProperties();\n\t\t}\n\n\t\t// convert deprecated names\n\t\tMap<String, String> props = userDataProp.getProperties();\n\t\tfor (Map.Entry<String, String> entry : props.entrySet()) {\n\t\t\tString key = entry.getKey();\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX + key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, entry.getValue());\n\t\t\t\tuserDataProp.removeProperty(key);\n\t\t\t}\n\t\t}\n\t} catch (IOException e) {\n\t\tthrow new FtpServerConfigurationException(\\\"Could not load user data file '\\\" + userDataFile + \\\"'\\\", e);\n\t}\n}", "CodeBleu_score": 46.31753424493992}, {"code": "public void configure(){\n\t\tisConfigured = true;\n\tuserDataProp = new BaseProperties();\n\tif (userDataFile.exists()) {\n\t\ttry {\n\t\t\tuserDataProp.load(new FileInputStream(userDataFile));\n\t\t} catch (IOException e) {\n\t\t\tthrow new FtpServerConfigurationException(\\\"Failed to load user data file '\\\" + userDataFile.getAbsolutePath() + \\\"'\\\", e);\n\t\t}\n\t}\n\tconvertDeprecatedProperties();\n}", "CodeBleu_score": 41.57696999528617}]}
{"instance_id": "b5a2587f14d0", "generation_result": [{"code": "private void saveUserData() throws FtpException{\n\te parent = userDataFile.getParentFile();\n\tif (parent!= null &&!parent.exists()) {\n\t\tif (!parent.mkdirs()) {\n\t\t\tthrow new FtpServerConfigurationException(\"Failed to create directory: \" + parent);\n\t\t}\n\t}\n\n\tFileOutputStream fos = null;\n\ttry {\n\t\tfos = new FileOutputStream(userDataFile);\n\t\tuserDataProp.store(fos);\n\t} catch (IOException e) {\n\t\tLOG.error(\"Failed to save user data\", e);\n\t\tthrow new FtpException(\"Failed to save user data\", e);\n\t} finally {\n\t\tIoUtils.close(fos);\n\t}\n}", "CodeBleu_score": 51.137819257368}, {"code": "private void saveUserData() throws FtpException{\n\t(!userDataFile.getParentFile().exists()) {\n\t\tif (!userDataFile.getParentFile().mkdirs()) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\"Failed to create user data directory: \"\n\t\t\t\t\t\t\t+ userDataFile.getParentFile().getAbsolutePath());\n\t\t}\n\t}\n\n\tFileOutputStream fos = null;\n\ttry {\n\t\tfos = new FileOutputStream(userDataFile);\n\t\tuserDataProp.store(fos, \"User data\");\n\t} catch (IOException e) {\n\t\tLOG.error(\"Failed to save user data\", e);\n\t\tthrow new FtpException(\"Failed to save user data\", e);\n\t} finally {\n\t\tIoUtils.close(fos);\n\t}\n}", "CodeBleu_score": 48.17277288597661}, {"code": "private void saveUserData() throws FtpException{\n\te parent = userDataFile.getParentFile();\n\tif (parent!= null &&!parent.exists()) {\n\t\tif (!parent.mkdirs()) {\n\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\"Failed to create parent directory for user data file : \"\n\t\t\t\t\t\t\t+ parent.getAbsolutePath());\n\t\t}\n\t}\n\n\tFileOutputStream fos = null;\n\ttry {\n\t\tfos = new FileOutputStream(userDataFile);\n\t\tuserDataProp.store(fos);\n\t} catch (IOException e) {\n\t\tLOG.error(e.getMessage(), e);\n\t\tthrow new FtpException(e.getMessage());\n\t} finally {\n\t\tIoUtils.close(fos);\n\t}\n}", "CodeBleu_score": 49.23215257823621}]}
{"instance_id": "5cf2989cf513", "generation_result": [{"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\t\ttry {\n\t\tHdfsUserManager userManagerFactory = new HdfsUserManager();\n\t\tuserManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n\t\tuserManagerFactory.delete(user);\n\t\treturn new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\treturn new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 77.74143874051694}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\t\ttry {\n\t\tHdfsUserManager userManagerFactory = new HdfsUserManager();\n\t\tuserManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n\t\tuserManagerFactory.delete(user);\n\t\treturn new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n\t} catch (Exception e) {\n\t\tlog.error(e);\n\t\treturn new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n\t}\n}", "CodeBleu_score": 77.74143874051694}, {"code": "public Result delete(@PathVariable(value = \"user\") String user){\n\ttry {\nHdfsUserManager userManagerFactory = new HdfsUserManager();\nuserManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\nuserManagerFactory.delete(user);\nreturn new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n} catch (Exception e) {\nlog.error(e);\nreturn new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n}\n}", "CodeBleu_score": 77.60398801197302}]}
{"instance_id": "ce383a9a6497", "generation_result": [{"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tG.info(\\\"Opening file {} with buffer size {}\\\", path, bufferSize);\nPath qualifiedPath = makeQualified(path);\ntry {\nInputStream inputStream = seaweedFileSystemStore.open(qualifiedPath, bufferSize);\nreturn new FSDataInputStream(new BufferedInputStream(inputStream, bufferSize));\n} catch (FileNotFoundException e) {\nLOG.warn(\\\"File {} not found\\\", path);\nreturn null;\n}\n}", "CodeBleu_score": 33.829056247907936}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tG.info(\\\"Opening file: \\\" + path + \\\" with buffer size: \\\" + bufferSize);\nPath qualifiedPath = makeQualified(path);\ntry {\nInputStream inputStream = seaweedFileSystemStore.open(qualifiedPath, bufferSize);\nreturn new FSDataInputStream(new BufferedInputStream(inputStream, bufferSize));\n} catch (FileNotFoundException e) {\nLOG.warn(\\\"File not found: \\\" + path, e);\nreturn null;\n}\n}", "CodeBleu_score": 21.10116677117271}, {"code": "public FSDataInputStream open(Path path, int bufferSize) throws IOException{\n\tG.info(\\\"Opening file {} with buffer size {}\\\", path, bufferSize);\nPath qualifiedPath = makeQualified(path);\ntry {\nreturn new FSDataInputStream(new BufferedFSInputStream(seaweedFileSystemStore.open(qualifiedPath), bufferSize));\n} catch (FileNotFoundException e) {\nLOG.warn(\\\"File {} not found\\\", qualifiedPath, e);\nreturn null;\n}\n}", "CodeBleu_score": 24.92569349034656}]}
{"instance_id": "f0fc12072c02", "generation_result": [{"code": "public boolean rename(Path src, Path dst) throws IOException{\n\t.debug(\"rename src: {} dst:{}\", src, dst);\n\nif (isRoot(src)) {\nreturn false;\n}\n\nif (src.equals(dst)) {\nreturn true;\n}\n\nPath adjustedDst = dst;\nif (exists(adjustedDst)) {\nFileStatus dstStatus = getFileStatus(adjustedDst);\nif (dstStatus.isDirectory()) {\nadjustedDst = new Path(adjustedDst, src.getName());\n}\n}\n\nsrc = qualify(src);\nadjustedDst = qualify(adjustedDst);\n\nreturn seaweedFileSystemStore.rename(src, adjustedDst);\n}", "CodeBleu_score": 46.76277834985022}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tG.debug(\"rename src: {} dst:{}\", src, dst);\n\nsrc = qualify(src);\ndst = qualify(dst);\n\ntry {\nreturn seaweedFileSystemStore.renameFile(src, dst);\n} catch (Exception ex) {\nLOG.warn(\"rename src: {} dst:{}\", src, dst, ex);\nreturn false;\n}\n}", "CodeBleu_score": 18.966677887985032}, {"code": "public boolean rename(Path src, Path dst) throws IOException{\n\tG.debug(\"rename src: {} dst:{}\", src, dst);\n\nif (src.isRoot()) {\nreturn false;\n}\n\nif (src.equals(dst)) {\nreturn true;\n}\n\nPath adjustedDst = dst;\nif (exists(dst)) {\nFileStatus dstStatus = getFileStatus(dst);\nif (dstStatus.isDirectory()) {\nadjustedDst = new Path(dst, src.getName());\n}\n}\n\nsrc = qualify(src);\nadjustedDst = qualify(adjustedDst);\n\nreturn seaweedFileSystemStore.rename(src, adjustedDst);\n}", "CodeBleu_score": 51.129910824310734}]}
{"instance_id": "5104c071af46", "generation_result": [{"code": "public boolean hasCapability(String capability){\n\treturn supportFlush && \"hsync\".equalsIgnoreCase(capability) || \"hflush\".equalsIgnoreCase(capability);\n}", "CodeBleu_score": 32.77214272075999}, {"code": "public boolean hasCapability(String capability){\n\tif (capability == null) {\nreturn false;\n}\nString cap = capability.toLowerCase(Locale.ROOT);\nif (cap.equals(\"hsync\") || cap.equals(\"hflush\")) {\nreturn supportFlush;\n}\nreturn false;\n}", "CodeBleu_score": 35.93635593576692}, {"code": "public boolean hasCapability(String capability){\n\treturn supportFlush && (capability.equalsIgnoreCase(StreamCapabilities.HSYNC) ||\ncapability.equalsIgnoreCase(StreamCapabilities.HFLUSH));\n}", "CodeBleu_score": 35.60721595745966}]}
{"instance_id": "6989c860c79b", "generation_result": [{"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\the last flush offset is greater than or equal to the specified offset, do nothing.\nif (lastFlushOffset >= offset) {\nreturn;\n}\n\n// Synchronize the operation.\nsynchronized (writeOperations) {\n// If the last flush offset is greater than or equal to the specified offset, do nothing.\nif (lastFlushOffset >= offset) {\nreturn;\n}\n\n// If the last flush offset is less than the specified offset, update the last flush offset.\nlastFlushOffset = offset;\n}\n\n// Write metadata to the filer client.\nif (shouldSaveMetadata) {\ntry {\nfilerClient.writeMetadata(path, entry.build());\n} catch (IOException e) {\nthrow new IOException(e);\n}\n}\n}", "CodeBleu_score": 41.68355815539012}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\t   LOG.debug(\\\"flushWrittenBytesToServiceInternal offset: {}\\\", offset);\n        if (offset == lastFlushOffset) {\n            return;\n        }\n\n        if (offset < lastFlushOffset) {\n            throw new IOException(\\\"Invalid offset for flush: \\\" + offset);\n        }\n\n        final long flushOffset = offset;\n\n        if (flushOffset == 0) {\n            // This is the first flush operation. We need to create the file.\n            // If the file already exists, it is an error.\n            if (entry.getIsDirectory()) {\n                throw new IOException(\\\"Cannot create file: \\\" + path + \\\" because it is a directory.\\\");\n            }\n\n            if (filerClient.lookup(path, false)!= null) {\n                throw new IOException(\\\"Cannot create file: \\\" + path + \\\" because it already exists.\\\");\n            }\n\n            try {\n                filerClient.create(getParentDirectory(path), entry.build(), shouldSaveMetadata);\n            } catch (IOException e) {\n                throw new IOException(\\\"Cannot create file: \\\" + path, e);\n            }\n        }\n\n        // If there is any data to flush, we need to flush it.\n        if (flushOffset > lastFlushOffset) {\n            // Flush the data.\n            try {\n                flushData(flushOffset);\n            } catch (IOException e) {\n                throw new IOException(\\\"Error while flushing data for file: \\\" + path, e);\n            }\n        }\n\n        // Write the metadata.\n        try {\n            filerClient.update(path, entry.build(), shouldSaveMetadata);\n        } catch (IOException e) {\n            throw new IOException(\\\"Cannot update metadata for file: \\\" + path, e);\n        }\n\n        // Update the last flush offset.\n        lastFlushOffset = flushOffset;\n    }", "CodeBleu_score": 27.255103025542443}, {"code": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException{\n\tset > lastFlushOffset) {\nflushWrittenBytesToServiceInternal(offset, false);\n}\n}", "CodeBleu_score": 25.32971837143424}]}
{"instance_id": "56e8fb9de5da", "generation_result": [{"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\tToWrite.flip();\n\nint bytesToWrite = bufferToWrite.remaining();\n\n// Ensure the task queue is not overloaded\nwhile (writeOperations.size() >= maxConcurrentRequestCount) {\ntry {\n// Wait for a task to complete\ncompletionService.take().get();\n} catch (InterruptedException e) {\nthrow new IOException(e);\n} catch (ExecutionException e) {\nthrow new IOException(e);\n}\n}\n\n// Create a write task and submit it to the completion service\nWriteOperation writeOperation = new WriteOperation(bufferToWrite, writePosition);\ncompletionService.submit(writeOperation);\n\n// Record the write operation and manage the operation queue\nwriteOperations.add(writeOperation);\nmanageWriteOperations();\n\n// Return the number of bytes written\nreturn bytesToWrite;\n}", "CodeBleu_score": 29.97492583190342}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\tp the buffer to prepare for reading\n\tbufferToWrite.flip();\n\n\t// calculate the byte length\n\tfinal int byteLength = bufferToWrite.remaining();\n\n\t// ensure the task queue is not overloaded\n\twhile (writeOperations.size() >= maxConcurrentRequestCount) {\n\t\ttry {\n\t\t\t// wait for a task to complete\n\t\t\tcompletionService.take();\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new IOException(e);\n\t\t}\n\t}\n\n\t// submit a write task to the completion service\n\tcompletionService.submit(new WriteOperation(bufferToWrite, writePosition, byteLength));\n\n\t// record the write operation\n\twriteOperations.add(new WriteOperation(bufferToWrite, writePosition, byteLength));\n\n\t// manage the operation queue\n\tmanageOperationQueue();\n\n\t// return the number of bytes written\n\treturn byteLength;\n}", "CodeBleu_score": 32.35480820890194}, {"code": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n\toWrite.flip();\nint numberOfBytesToWrite = bufferToWrite.remaining();\n\nif (numberOfBytesToWrite == 0) {\nreturn 0;\n}\n\nif (writeOperations.size() >= maxConcurrentRequestCount) {\ntry {\ncompletionService.take();\n} catch (InterruptedException e) {\nthrow new IOException(e);\n}\n}\n\nWriteOperation writeOperation = new WriteOperation(bufferToWrite, writePosition);\nwriteOperations.add(writeOperation);\n\ncompletionService.submit(new Callable<Void>() {\n@Override\npublic Void call() throws Exception {\ntry {\nSeaweedWrite.write(filerClient, path, bufferToWrite, writePosition, replication, collection);\n} catch (Exception e) {\nlastError = new IOException(e);\n} finally {\nwriteOperations.remove(writeOperation);\n}\nreturn null;\n}\n});\n\nreturn numberOfBytesToWrite;\n}", "CodeBleu_score": 35.10650417342772}]}
{"instance_id": "bf27d955c38b", "generation_result": [{"code": "private void waitForTaskToComplete() throws IOException{\n\t\twhile (true) {\n\t\ttry {\n\t\t\tFuture<Void> completedTask = completionService.take();\n\t\t\tif (completedTask == null) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} catch (InterruptedException e) {\n\t\t\tthrow new IOException(e);\n\t\t}\n\t}\n}", "CodeBleu_score": 24.094402768979307}, {"code": "private void waitForTaskToComplete() throws IOException{\n\twhile (true) {\ntry {\ncompletionService.take();\nbreak;\n} catch (InterruptedException ex) {\nthrow new IOException(ex);\n}\n}\n}", "CodeBleu_score": 16.353815039023324}, {"code": "private void waitForTaskToComplete() throws IOException{\n\ttry {\nwhile (true) {\nFuture<Void> completedJob = completionService.poll(100, TimeUnit.MILLISECONDS);\nif (completedJob == null) {\nreturn;\n}\ncompletedJob.get();\n}\n} catch (InterruptedException e) {\nthrow new IOException(e);\n} catch (ExecutionException e) {\nthrow new IOException(e);\n}\n}", "CodeBleu_score": 30.38672478567841}]}
{"instance_id": "5c7a311273ae", "generation_result": [{"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t", "CodeBleu_score": 1.7963923232153831}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\tif (writeOperations.isEmpty()) {\n\t\treturn;\n\t}\n\n\twhile (true) {\n\t\tfinal WriteOperation writeOperation = writeOperations.peek();\n\t\tif (writeOperation == null) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (writeOperation.isDone()) {\n\t\t\ttry {\n\t\t\t\twriteOperation.get();\n\t\t\t} catch (Exception ex) {\n\t\t\t\tlastError = (IOException) new IOException(ex.toString()).initCause(ex);\n\t\t\t\tthrow lastError;\n\t\t\t}\n\n\t\t\twriteOperations.poll();\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n}", "CodeBleu_score": 32.93360918254965}, {"code": "private synchronized void shrinkWriteOperationQueue() throws IOException{\n\t\t// System.out.println(path + \\\" shrinkWriteOperationQueue\\\");\n\twhile (true) {\n\t\tfinal WriteOperation op = writeOperations.peek();\n\t\tif (op == null) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (op.isDone()) {\n\t\t\ttry {\n\t\t\t\top.get();\n\t\t\t\tposition += op.getLength();\n\t\t\t\twriteOperations.remove();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tlastError = (IOException) new InterruptedIOException(e.toString()).initCause(e);\n\t\t\t\tthrow lastError;\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\tlastError = (IOException) new IOException(e.toString()).initCause(e);\n\t\t\t\tthrow lastError;\n\t\t\t}\n\t\t} else {\n\t\t\treturn;\n\t\t}\n\t}\n}", "CodeBleu_score": 33.69014155896535}]}
{"instance_id": "d8f07e66f690", "generation_result": [{"code": "protected HdfsFileSystemView(User user){\n\t\tif (user == null) {\n\t\tthrow new IllegalArgumentException(\"User cannot be null\");\n\t}\n\n\tif (user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"User's home directory cannot be null\");\n\t}\n\n\tthis.homePath = user.getHomeDirectory();\n\tthis.user = user;\n}", "CodeBleu_score": 74.8405099801611}, {"code": "protected HdfsFileSystemView(User user){\n\t\tif (user == null || user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"user or home directory is null\");\n\t}\n\tthis.homePath = user.getHomeDirectory();\n\tthis.user = user;\n}", "CodeBleu_score": 54.01106412982757}, {"code": "protected HdfsFileSystemView(User user){\n\t\tif (user == null || user.getHomeDirectory() == null) {\n\t\tthrow new IllegalArgumentException(\"User or user's home directory is null.\");\n\t}\n\tthis.homePath = user.getHomeDirectory();\n\tthis.user = user;\n}", "CodeBleu_score": 52.81061832542521}]}
{"instance_id": "27807db0eb0f", "generation_result": [{"code": "public boolean changeWorkingDirectory(String dir){\n\t\tPath path;\n\tif (dir.startsWith(File.separator)) {\n\t\tpath = new Path(homePath + dir);\n\t} else {\n\t\tpath = new Path(currPath + File.separator + dir);\n\t}\n\n\tif (path.getParent().equals(new Path(homePath))) {\n\t\tcurrPath = File.separator;\n\t\treturn true;\n\t}\n\n\tif (HdfsFileObject.isDirectory(path)) {\n\t\tcurrPath = path.toString();\n\t\treturn true;\n\t}\n\n\treturn false;\n}", "CodeBleu_score": 38.46330650414473}, {"code": "public boolean changeWorkingDirectory(String dir){\n\t\tPath newPath;\n\\tif (dir.startsWith(File.separator)) {\n\t\tnewPath = new Path(homePath, dir);\n\t} else {\n\t\tnewPath = new Path(currPath, dir);\n\t}\n\\tif (newPath.toString().equals(File.separator)) {\n\t\tcurrPath = File.separator;\n\t\treturn true;\n\t}\n\\tif (newPath.getParent().toString().equals(File.separator)) {\n\t\treturn false;\n\t}\n\\tif (newPath.isAbsolute()) {\n\t\tcurrPath = newPath.toString();\n\t\treturn true;\n\t}\n\\treturn false;\n}", "CodeBleu_score": 33.436591386905086}, {"code": "public boolean changeWorkingDirectory(String dir){\n\t\tPath newPath;\n\tif (dir.startsWith(File.separator)) {\n\t\tnewPath = new Path(homePath + dir);\n\t} else {\n\t\tnewPath = new Path(currPath + File.separator + dir);\n\t}\n\n\tif (newPath.getParent().toString().equals(homePath)) {\n\t\tcurrPath = newPath.toString();\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}", "CodeBleu_score": 32.61616954728279}]}
{"instance_id": "be7aba23d184", "generation_result": [{"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\nthis.filerClient = new FilerClient(host, port, grpcPort);\nString accessMode = conf.get(ACCESS_MODE, ACCESS_MODE_DIRECT);\nif (accessMode.equals(ACCESS_MODE_PUBLIC_URL)) {\nthis.filerClient.setAccessMode(FilerClient.AccessMode.PublicUrl);\n} else if (accessMode.equals(ACCESS_MODE_FILER_PROXY)) {\nthis.filerClient.setAccessMode(FilerClient.AccessMode.FilerProxy);\n}\n}", "CodeBleu_score": 45.37358348486561}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\nthis.filerClient = new FilerClient(host, port, grpcPort);\nthis.filerClient.setAccessMode(conf.get(ACCESS_MODE, ACCESS_MODE_DEFAULT));\n}", "CodeBleu_score": 27.023762220977414}, {"code": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf){\n\tthis.conf = conf;\nthis.filerClient = new FilerClient(host, port, grpcPort);\nif (conf.getBoolean(ACCESS_MODE_KEY, false)) {\nthis.filerClient.setAccessMode(FilerClient.AccessMode.DIRECT);\n} else if (conf.getBoolean(PUBLIC_URL_KEY, false)) {\nthis.filerClient.setAccessMode(FilerClient.AccessMode.PUBLIC_URL);\n} else {\nthis.filerClient.setAccessMode(FilerClient.AccessMode.FILER_PROXY);\n}\n}", "CodeBleu_score": 37.72158490800205}]}
{"instance_id": "7943a1723c23", "generation_result": [{"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\tLOG.debug(\"openFileForRead path: {}\", path);\n\tFilerProto.Entry entry = lookupEntry(path);\n\tif (entry == null) {\n\t\tthrow new FileNotFoundException(\"File does not exist: \" + path);\n\t}\n\treturn new SeaweedHadoopInputStream(filerClient, path.toString(), entry, statistics);\n}", "CodeBleu_score": 78.02019090010046}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\tLOG.debug(\"openFileForRead path: {}\", path);\n\tFilerProto.Entry entry = lookupEntry(path);\n\tif (entry == null) {\n\t\tthrow new FileNotFoundException(\"File does not exist: \" + path);\n\t}\n\treturn new SeaweedHadoopInputStream(filerClient, path.toString(), entry, statistics);\n}", "CodeBleu_score": 78.02019090010046}, {"code": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException{\n\t\tLOG.debug(\"openFileForRead path: {}\", path);\n\n\tFilerProto.Entry entry = lookupEntry(path);\n\tif (entry == null) {\n\t\tthrow new FileNotFoundException(\"File does not exist: \" + path);\n\t}\n\n\treturn new SeaweedHadoopInputStream(filerClient, path.toString(), entry, statistics);\n}", "CodeBleu_score": 78.02019090010046}]}
{"instance_id": "36634cee9ed3", "generation_result": [{"code": "public void configure(JobConf conf){\n\t\tsetConf(conf);\n\ttry {\n\t\thostName = InetAddress.getLocalHost().getHostName();\n\t} catch (IOException e) {\n\t\thostName = \"localhost\";\n\t}\n\tthreadsPerMap = conf.getInt(\"threadsPerMap\", 1);\n\tfilesPerThread = conf.getInt(\"filesPerThread\", 1);\n\tpool = Executors.newFixedThreadPool(threadsPerMap);\n\tfor (int i = 0; i < threadsPerMap; i++) {\n\t\tpool.execute(new Thread(pool, \"Thread-\" + i, true));\n\t}\n}", "CodeBleu_score": 56.33370610056673}, {"code": "public void configure(JobConf conf){\n\tsetConf(conf);\ntry {\nhostName = InetAddress.getLocalHost().getHostName();\n} catch (IOException e) {\nhostName = \"localhost\";\n}\nthreadsPerMap = conf.getInt(\"test.threads_per_map\", 1);\nfilesPerThread = conf.getInt(\"test.files_per_thread\", 1);\npool = Executors.newFixedThreadPool(threadsPerMap);\nfor (int i = 0; i < threadsPerMap; i++) {\npool.execute(new Thread(new Runnable() {\n@Override\npublic void run() {\ntry {\nrunTest();\n} catch (Throwable e) {\nLOG.error(e);\n}\n}\n}));\n}\n}", "CodeBleu_score": 53.68172824315447}, {"code": "public void configure(JobConf conf){\n\tsetConf(conf);\ntry {\nhostName = InetAddress.getLocalHost().getHostName();\n} catch (IOException e) {\nhostName = \"localhost\";\n}\nthreadsPerMap = conf.getInt(\"mapreduce.map.threads\", 1);\nfilesPerThread = conf.getInt(\"mapreduce.map.files.per.thread\", 1);\npool = Executors.newFixedThreadPool(threadsPerMap);\nfor (int i = 0; i < threadsPerMap; i++) {\npool.submit(new Runnable() {\n@Override\npublic void run() {\ntry {\nfor (int j = 0; j < filesPerThread; j++) {\nrunTest();\n}\n} catch (IOException e) {\nLOG.error(\"Error running test\", e);\n}\n}\n});\n}\n}", "CodeBleu_score": 51.85158729113568}]}
{"instance_id": "53cd9065153d", "generation_result": [{"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tString host = uri.getHost();\n\tif (BgTaskUtil.isTaskRunning(host)) {\n\t\treturn;\n\t}\n\tUserGroupInformation superUser = UserGroupInformation.createRemoteUser(conf.get(\"fs.trash.emptier.user\", \"root\"));\n\tFileSystem fs = superUser.doAs(new PrivilegedExceptionAction<FileSystem>() {\n\t\t@Override\n\t\tpublic FileSystem run() throws Exception {\n\t\t\treturn new JuiceFileSystemImpl(conf);\n\t\t}\n\t});\n\tif (fs instanceof JuiceFileSystemImpl) {\n\t\tBgTaskUtil.startTask(host, new TrashEmptier((JuiceFileSystemImpl) fs, conf));\n\t}\n}", "CodeBleu_score": 39.62547978864558}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tif (BgTaskUtil.isTaskRunning(uri.getHost())) {\n\t\treturn;\n\t}\n\tfinal UserGroupInformation superUser = UserGroupInformation.createRemoteUser(\"root\");\n\tsuperUser.doAs(new PrivilegedExceptionAction<Void>() {\n\t\t@Override\n\t\tpublic Void run() throws Exception {\n\t\t\tFileSystem fs = new JuiceFileSystemImpl();\n\t\t\tfs.initialize(uri, conf);\n\t\t\tBgTaskUtil.schedule(uri.getHost(), new TrashEmptier(fs, conf), 10, TimeUnit.MINUTES);\n\t\t\treturn null;\n\t\t}\n\t});\n}", "CodeBleu_score": 36.376055331896936}, {"code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException{\n\tString host = uri.getHost();\n\tif (BgTaskUtil.isTaskRunning(host, \"trash-emptier\")) {\n\t\treturn;\n\t}\n\tUserGroupInformation superUser = UserGroupInformation.createRemoteUser(host);\n\ttry {\n\t\tsuperUser.doAs(new PrivilegedExceptionAction<Void>() {\n\t\t\t@Override\n\t\t\tpublic Void run() throws Exception {\n\t\t\t\tFileSystem fs = FileSystem.get(URI.create(\"juicefs://\" + host), conf);\n\t\t\t\tif (fs instanceof JuiceFileSystemImpl) {\n\t\t\t\t\t((JuiceFileSystemImpl) fs).startTrashEmptier(conf);\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t});\n\t} catch (Exception e) {\n\t\tthrow new IOException(\"Failed to start trash emptier: \" + e.getMessage());\n\t}\n}", "CodeBleu_score": 41.24883617740994}]}
{"instance_id": "ede797f6a8f2", "generation_result": [{"code": "public static void setContext(String context) throws Exception{\n\t\tString currentContext = CallerContext.getCurrentContext();\n\tif (currentContext == null || currentContext.isEmpty()) {\n\t\tCallerContext.setCurrentContext(new CallerContext(context));\n\t} else {\n\t\tCallerContext callerContext = CallerContext.parse(currentContext);\n\t\tif (callerContext.getSignature() == null) {\n\t\t\tCallerContext.setCurrentContext(new CallerContext(context));\n\t\t} else {\n\t\t\tCallerContext.setCurrentContext(callerContext.appendContext(context));\n\t\t}\n\t}\n}", "CodeBleu_score": 34.732016416492726}, {"code": "public static void setContext(String context) throws Exception{\n\t\tCallerContext callerContext = CallerContext.getCurrentContext();\n\tif (callerContext == null ||!callerContext.isValid()) {\n\t\tCallerContext.setCurrentContext(CallerContext.create(context));\n\t} else if (!callerContext.hasSignature() &&!callerContext.hasContext(context)) {\n\t\tCallerContext.setCurrentContext(CallerContext.create(callerContext.getContext() + \",\" + context));\n\t}\n}", "CodeBleu_score": 34.27788792561581}, {"code": "public static void setContext(String context) throws Exception{\n\t\tString currentContext = CallerContext.getCurrentContext();\n\tif (currentContext == null || currentContext.isEmpty()) {\n\t\tCallerContext.setCurrentContext(context);\n\t} else {\n\t\tCallerContext.setCurrentContext(currentContext + \";\" + context);\n\t}\n}", "CodeBleu_score": 25.361823734337147}]}
{"instance_id": "56968a526f6a", "generation_result": [{"code": "Status HDFS::remove_file(const URI& uri){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\tint ret = libhdfs_->hdfsDelete(fs, uri.to_path().c_str(), 0);\n\tif (ret < 0) {\n\t\treturn LOG_STATUS(Status_HDFSError(\"Cannot remove file: \" + uri.to_string()));\n\t}\n\treturn Status::Ok();\n}", "CodeBleu_score": 81.75439910598475}, {"code": "Status HDFS::remove_file(const URI& uri){\n\thdfsFS fs = nullptr;\nRETURN_NOT_OK(connect(&fs));\nint ret = libhdfs_->hdfsDelete(fs, uri.to_path().c_str(), 0);\nif (ret < 0) {\nreturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot remove file \") + uri.to_string()));\n}\nreturn Status::Ok();\n}", "CodeBleu_score": 91.45310387058738}, {"code": "Status HDFS::remove_file(const URI& uri){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\tint ret = libhdfs_->hdfsDelete(fs, uri.to_path().c_str(), 0);\n\tif (ret < 0) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot remove file \") + uri.to_string()));\n\t}\n\treturn Status::Ok();\n}", "CodeBleu_score": 91.50574359030097}]}
{"instance_id": "f33adb3e8cb6", "generation_result": [{"code": "Status HDFS::sync(const URI& uri){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\tbool file_exists = false;\n\tRETURN_NOT_OK(is_file(uri, &file_exists));\n\tif (!file_exists) {\n\t\treturn Status::Ok();\n\t}\n\thdfsFile write_file = libhdfs_->hdfsOpenFile(fs, uri.to_path().c_str(), O_WRONLY | O_APPEND, 0, 0, 0);\n\tif (!write_file) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File opening error\"));\n\t}\n\t// Flush data to HDFS\n\tif (libhdfs_->hdfsHFlush(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File flushing error\"));\n\t}\n\t// Close file\n\tif (libhdfs_->hdfsCloseFile(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File closing error\"));\n\t}\n\treturn Status::Ok();\n}", "CodeBleu_score": 82.57743653068566}, {"code": "Status HDFS::sync(const URI& uri){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\tbool file_exists = false;\n\tRETURN_NOT_OK(is_file(uri, &file_exists));\n\tif (!file_exists) {\n\t\treturn Status::Ok();\n\t}\n\thdfsFile write_file = libhdfs_->hdfsOpenFile(fs, uri.to_path().c_str(), O_WRONLY | O_APPEND, 0, 0, 0);\n\tif (!write_file) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File opening error\"));\n\t}\n\tif (libhdfs_->hdfsHFlush(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File flushing error\"));\n\t}\n\tif (libhdfs_->hdfsCloseFile(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File closing error\"));\n\t}\n\treturn Status::Ok();\n}", "CodeBleu_score": 82.57743653068566}, {"code": "Status HDFS::sync(const URI& uri){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\tbool file_exists = false;\n\tRETURN_NOT_OK(is_file(uri, &file_exists));\n\tif (!file_exists) {\n\t\treturn Status::Ok();\n\t}\n\thdfsFile write_file = libhdfs_->hdfsOpenFile(fs, uri.to_path().c_str(), O_WRONLY | O_APPEND, 0, 0, 0);\n\tif (!write_file) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File opening error\"));\n\t}\n\tif (libhdfs_->hdfsHFlush(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File flushing error\"));\n\t}\n\tif (libhdfs_->hdfsCloseFile(fs, write_file)) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot sync file \") + uri.to_string() + \"; File closing error\"));\n\t}\n\treturn Status::Ok();\n}", "CodeBleu_score": 82.57743653068566}]}
{"instance_id": "893c46ab9675", "generation_result": [{"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\n\thdfsFileInfo* fileInfo = libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n\tif (fileInfo == NULL) {\n\t\tif (errno) {\n\t\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot get file info for '\") + uri.to_string() + \"'\"));\n\t\t}\n\t}\n\n\tif (fileInfo->mKind!= kObjectKindFile) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot get file size for '\") + uri.to_string() + \"'; Not a file\"));\n\t}\n\n\t*nbytes = fileInfo->mSize;\n\tlibhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n\n\treturn Status::Ok();\n}", "CodeBleu_score": 66.8787378464216}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\n\thdfsFileInfo* fileInfo = libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n\tif (fileInfo == NULL) {\n\t\tif (errno) {\n\t\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot get file info for \") + uri.to_string()));\n\t\t}\n\t}\n\n\tif (fileInfo->mKind!= kObjectKindFile) {\n\t\treturn LOG_STATUS(Status_HDFSError(std::string(\"Cannot get file size for \") + uri.to_string()));\n\t}\n\n\t*nbytes = fileInfo->mSize;\n\tlibhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n\n\treturn Status::Ok();\n}", "CodeBleu_score": 71.3681832190816}, {"code": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes){\n\t\thdfsFS fs = nullptr;\n\tRETURN_NOT_OK(connect(&fs));\n\n\thdfsFileInfo* fileInfo = libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n\tif (!fileInfo) {\n\t\treturn LOG_STATUS(Status_HDFSError(\n\t\t\tstd::string(\"Cannot get file info for '\") + uri.to_string() + \"'\"));\n\t}\n\n\tif (fileInfo->mKind!= kObjectKindFile) {\n\t\tlibhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n\t\treturn LOG_STATUS(Status_HDFSError(\n\t\t\tstd::string(\"Cannot get file size for '\") + uri.to_string() +\n\t\t\t\"'; not a file\"));\n\t}\n\n\t*nbytes = fileInfo->mSize;\n\tlibhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n\treturn Status::Ok();\n}", "CodeBleu_score": 69.22092253275535}]}
{"instance_id": "3da0b4d8ae16", "generation_result": [{"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\t\t// Parse the HBase configuration string\n\tMap<String, String> hbaseCfg = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, String>>() {});\n\n\t// Extract the Zookeeper quorum and znode parent values\n\tString quorum = hbaseCfg.get(HBaseConstant.HBASE_ZOOKEEPER_QUORUM);\n\tString znodeParent = hbaseCfg.get(HBaseConstant.HBASE_ZOOKEEPER_ZNODE_PARENT);\n\n\t// Ensure the quorum includes the port number, defaulting to 2181 if not specified\n\tif (quorum!= null &&!quorum.contains(\":\")) {\n\t\tquorum += \":2181\";\n\t}\n\n\t// If the znode parent is not provided, default to a predefined value\n\tif (znodeParent == null) {\n\t\tznodeParent = \"/hbase\";\n\t}\n\n\t// Return the quorum and znode parent as a pair\n\treturn Pair.newPair(quorum, znodeParent);\n}", "CodeBleu_score": 48.89120254867502}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\t// Parse the HBase configuration string using JSON\nMap<String, Object> hbaseCfg = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, Object>>() {});\n\n// Extract the Zookeeper quorum and znode parent values\nString quorum = (String) hbaseCfg.getOrDefault(\"hbase.zookeeper.quorum\", \"\");\nString znodeParent = (String) hbaseCfg.getOrDefault(\"hbase.zookeeper.znode.parent\", \"/hbase\");\n\n// Ensure the quorum includes the port number, defaulting to 2181 if not specified\nif (!quorum.contains(\":\")) {\nquorum += \":2181\";\n}\n\n// If the znode parent is not provided, default to a predefined value\nif (znodeParent.isEmpty()) {\nznodeParent = \"/hbase\";\n}\n\n// Return the quorum and znode parent as a pair\nreturn new Pair<>(quorum, znodeParent);\n}", "CodeBleu_score": 43.384533079465456}, {"code": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString){\n\t\tMap<String, String> hbaseCfgMap = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, String>>() {});\n\tString quorum = hbaseCfgMap.get(HBaseKey.HBASE_ZOOKEEPER_QUORUM);\n\tString znodeParent = hbaseCfgMap.get(HBaseKey.HBASE_ZOOKEEPER_ZNODE_PARENT);\n\tif (quorum == null || quorum.isEmpty()) {\n\t\tthrow new AddaxException(\"hbase config must contain quorum\");\n\t}\n\tif (znodeParent == null || znodeParent.isEmpty()) {\n\t\tznodeParent = HBaseConstant.DEFAULT_ZNODE_PARENT;\n\t}\n\tif (!quorum.contains(\":\")) {\n\t\tquorum += \":\" + HBaseConstant.DEFAULT_ZOOKEEPER_PORT;\n\t}\n\treturn new Pair<>(quorum, znodeParent);\n}", "CodeBleu_score": 59.172357997866}]}
{"instance_id": "860cae2bb7f4", "generation_result": [{"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\t/ Establish a JDBC connection\n\tConnection conn = null;\n\ttry {\n\t\tconn = getConnection(cfg);\n\t\t// Check the specified table's existence in the given namespace\n\t\tif (!isTableExists(conn, cfg.getHbaseSchema(), cfg.getHbaseTable())) {\n\t\t\tthrow new AddaxException(\"Hbase table \" + cfg.getHbaseSchema() + \".\" + cfg.getHbaseTable() + \" does not exist.\");\n\t\t}\n\t\t// Retrieve the table schema\n\t\tMap<String, String> columnType = getColumnTypes(conn, cfg.getHbaseSchema(), cfg.getHbaseTable());\n\t\t// Ensure that all configured columns exist in the target table\n\t\tfor (Map.Entry<String, String> entry : cfg.getColumns().entrySet()) {\n\t\t\tString columnName = entry.getKey();\n\t\t\tString columnTypeStr = entry.getValue();\n\t\t\tif (!columnType.containsKey(columnName)) {\n\t\t\t\tthrow new AddaxException(\"Column \" + columnName + \" does not exist in the target table.\");\n\t\t\t}\n\t\t\t// Check the column type\n\t\t\tif (!columnType.get(columnName).equalsIgnoreCase(columnTypeStr)) {\n\t\t\t\tthrow new AddaxException(\"Column \" + columnName + \" type is not compatible with the target table.\");\n\t\t\t}\n\t\t}\n\t} catch (SQLException e) {\n\t\tthrow new AddaxException(\"Error occurred while validating HbaseSQLWriter configuration.\", e);\n\t} finally {\n\t\t// Close the connection\n\t\tif (conn!= null) {\n\t\t\ttry {\n\t\t\t\tconn.close();\n\t\t\t} catch (SQLException e) {\n\t\t\t\tLOG.error(\"Error occurred while closing the JDBC connection.\", e);\n\t\t\t}\n\t\t}\n\t}\n}", "CodeBleu_score": 40.26952244849377}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\t", "CodeBleu_score": 0.8577865379463421}, {"code": "public static void validateConfig(HbaseSQLWriterConfig cfg){\n\t", "CodeBleu_score": 0.8577865379463421}]}
{"instance_id": "61721c327cf6", "generation_result": [{"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\t\tif (haveKerberos &&!StringUtils.isBlank(kerberosPrincipal) &&!StringUtils.isBlank(kerberosKeytabFilePath)) {\n\t\thadoopConf.set(HBaseConstant.KERBEROS_PRINCIPAL, kerberosPrincipal);\n\t\thadoopConf.set(HBaseConstant.KERBEROS_KEYTAB, kerberosKeytabFilePath);\n\t\ttry {\n\t\t\tUserGroupInformation.setConfiguration(hadoopConf);\n\t\t\tUserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Kerberos login error: {}\", e.getMessage());\n\t\t\tthrow AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, \"Kerberos login error\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 52.96017853460421}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\tif (haveKerberos &&!StringUtils.isBlank(kerberosPrincipal) &&!StringUtils.isBlank(kerberosKeytabFilePath)) {\ntry {\nUserGroupInformation.setConfiguration(hadoopConf);\nUserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n} catch (IOException e) {\nLOG.error(\"Kerberos login failed: \" + e.getMessage());\nthrow new RuntimeException(\"Kerberos login failed\", e);\n}\n}\n}", "CodeBleu_score": 39.727362180456474}, {"code": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath){\n\t\tif (haveKerberos &&!StringUtils.isBlank(kerberosPrincipal) &&!StringUtils.isBlank(kerberosKeytabFilePath)) {\n\t\thadoopConf.set(\"hadoop.security.authentication\", \"Kerberos\");\n\t\ttry {\n\t\t\tUserGroupInformation.setConfiguration(hadoopConf);\n\t\t\tUserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n\t\t} catch (IOException e) {\n\t\t\tLOG.error(\"Kerberos login error: {}\", e.getMessage());\n\t\t\tthrow AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, \"Kerberos login error\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 45.16055713795889}]}
{"instance_id": "9ceb7f0bb717", "generation_result": [{"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t", "CodeBleu_score": 3.6144534202708902}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\tMap<String, ThinClientPTable.ThinClientPColumn> colTypeMap = new HashMap<>();\nwhile (rs.next()) {\nString colName = rs.getString(\"COLUMN_NAME\");\nString colType = rs.getString(\"TYPE_NAME\");\nif (colName == null || colType == null) {\nthrow new SQLException(\"column name or type is null\");\n}\ncolTypeMap.put(colName, new ThinClientPTable.ThinClientPColumn(colType));\n}\nreturn colTypeMap;\n}", "CodeBleu_score": 25.318396387469623}, {"code": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException{\n\t", "CodeBleu_score": 3.6144534202708902}]}
{"instance_id": "247fc8882f6f", "generation_result": [{"code": "public static void truncateTable(Connection conn, String tableName){\n\t\tPhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n\tAdmin admin = hconn.getQueryServices().getAdmin();\n\tif (admin.tableExists(Bytes.toBytes(tableName))) {\n\t\tadmin.disableTable(Bytes.toBytes(tableName));\n\t\tadmin.truncateTable(Bytes.toBytes(tableName), true);\n\t\tLOG.debug(\"Truncated table {}\", tableName);\n\t} else {\n\t\tthrow AddaxException.asAddaxException(HbaseSQLWriterErrorCode.TRUNCATE_TABLE_ERROR,\n\t\t\t\t\"The table \" + tableName + \" does not exist.\");\n\t}\n\ttry {\n\t\tadmin.close();\n\t} catch (IOException e) {\n\t\tLOG.error(\"Failed to close the Admin instance.\", e);\n\t}\n}", "CodeBleu_score": 41.182635817845465}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\t\tPhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n\tAdmin admin = hconn.getQueryServices().getAdmin();\n\ttry {\n\t\tif (admin.tableExists(TableName.valueOf(tableName))) {\n\t\t\tadmin.disableTable(TableName.valueOf(tableName));\n\t\t\tadmin.truncateTable(TableName.valueOf(tableName), true);\n\t\t\tLOG.debug(\"Truncated table {}.\", tableName);\n\t\t}\n\t} catch (Exception e) {\n\t\tthrow AddaxException.asAddaxException(HbaseSQLWriterErrorCode.TRUNCATE_TABLE_ERROR,\n\t\t\t\t\"Unable to truncate table \" + tableName, e);\n\t} finally {\n\t\tIOUtils.closeQuietly(admin);\n\t}\n}", "CodeBleu_score": 40.46460575699975}, {"code": "public static void truncateTable(Connection conn, String tableName){\n\t\tPhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n\tAdmin admin = hconn.getQueryServices().getAdmin();\n\ttry {\n\t\tif (admin.tableExists(TableName.valueOf(tableName))) {\n\t\t\tadmin.disableTable(TableName.valueOf(tableName));\n\t\t\tadmin.truncateTable(TableName.valueOf(tableName), true);\n\t\t\tLOG.debug(\"Truncated table: \" + tableName);\n\t\t}\n\t} catch (Exception e) {\n\t\tthrow AddaxException.asAddaxException(HbaseSQLWriterErrorCode.TRUNCATE_TABLE_ERROR,\n\t\t\t\t\"Unable to truncate table: \" + tableName, e);\n\t} finally {\n\t\ttry {\n\t\t\tadmin.close();\n\t\t} catch (IOException e) {\n\t\t\tLOG.error(\"Unable to close admin instance\", e);\n\t\t}\n\t}\n}", "CodeBleu_score": 41.05256974309579}]}
{"instance_id": "ac87e1b9a54e", "generation_result": [{"code": "private void addSourceFileByType(String filePath){\n\t\tif (specifiedFileType!= null) {\n\t\tString fileType = getFileType(filePath);\n\t\tif (fileType.equals(specifiedFileType)) {\n\t\t\tLOG.info(\"Add file [{}] to source files list.\", filePath);\n\t\t\tsourceHDFSAllFilesList.add(filePath);\n\t\t} else {\n\t\t\tString message = String.format(\"The file type [%s] of file [%s] is not supported, ignore it.\", fileType, filePath);\n\t\t\tLOG.error(message);\n\t\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.UNSUPPORTED_FILE_TYPE, message);\n\t\t}\n\t} else {\n\t\tLOG.info(\"Add file [{}] to source files list.\", filePath);\n\t\tsourceHDFSAllFilesList.add(filePath);\n\t}\n}", "CodeBleu_score": 41.76757528919231}, {"code": "private void addSourceFileByType(String filePath){\n\tif (specifiedFileType == null) {\nsourceHDFSAllFilesList.add(filePath);\n} else {\nString fileType = filePath.substring(filePath.lastIndexOf(\".\") + 1);\nif (fileType.equalsIgnoreCase(specifiedFileType)) {\nsourceHDFSAllFilesList.add(filePath);\n} else {\nString message = String.format(\"The file [%s] type is [%s], not [%s], ignore it.\", filePath, fileType, specifiedFileType);\nLOG.error(message);\nthrow AddaxException.asAddaxException(HdfsReaderErrorCode.UNSUPPORTED_FILE_TYPE, message);\n}\n}\n}", "CodeBleu_score": 40.11838838443038}, {"code": "private void addSourceFileByType(String filePath){\n\t\tString fileType = getFileType(filePath);\n\tif (specifiedFileType.equalsIgnoreCase(fileType)) {\n\t\tLOG.info(\"Add file [{}] to source files list.\", filePath);\n\t\tsourceHDFSAllFilesList.add(filePath);\n\t} else {\n\t\tString message = String.format(\"The file type of [%s] is [%s], but the specified file type is [%s].\", filePath, fileType, specifiedFileType);\n\t\tLOG.error(message);\n\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.PATH_CONFIG_ERROR, message);\n\t}\n}", "CodeBleu_score": 42.97402920820178}]}
{"instance_id": "5e6a66d06162", "generation_result": [{"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tLOG.info(\"start read sequence file from [{}]\", sourceSequenceFilePath);\ntry {\nFileSystem fs = FileSystem.get(readerSliceConfig);\nPath path = new Path(sourceSequenceFilePath);\nSequenceFile.Reader reader = new SequenceFile.Reader(fs, path, readerSliceConfig);\nWritable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), readerSliceConfig);\nWritable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), readerSliceConfig);\nwhile (reader.next(key, value)) {\nif (value instanceof BytesWritable) {\nBytesWritable bytesWritable = (BytesWritable) value;\nif (bytesWritable.getLength() > 0) {\nStorageReaderUtil.sendRecord(recordSender, bytesWritable.getBytes(), taskPluginCollector);\n}\n}\n}\n} catch (IOException e) {\nString message = String.format(\"IO exception occurred while reading the file [%s].\", sourceSequenceFilePath);\nthrow AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message, e);\n}\n}", "CodeBleu_score": 53.05568614779803}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\tLOG.info(\"start to read sequence file [{}]\", sourceSequenceFilePath);\nPath path = new Path(sourceSequenceFilePath);\ntry (SequenceFile.Reader reader = new SequenceFile.Reader(hadoopConf, SequenceFile.Reader.file(path))) {\n    Text key = new Text();\n    BytesWritable value = new BytesWritable();\n    while (reader.next(key, value)) {\n        if (value.getLength() > 0) {\n            StorageReaderUtil.sendRecord(value.getBytes(), recordSender);\n        }\n    }\n} catch (IOException e) {\n    String message = String.format(\"IO exception occurred while reading the sequence file [%s].\", sourceSequenceFilePath);\n    throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message, e);\n}\n}", "CodeBleu_score": 47.49035285226416}, {"code": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector){\n\t\nLOG.info(\"HDFS SequenceFile Reader start reading file: \" + sourceSequenceFilePath);\n\nFileSystem fs = null;\nSequenceFile.Reader reader = null;\ntry {\n    fs = FileSystem.get(readerSliceConfig);\n    reader = new SequenceFile.Reader(fs, new Path(sourceSequenceFilePath), readerSliceConfig);\n\n    Text key = new Text();\n    BytesWritable value = new BytesWritable();\n    while (reader.next(key, value)) {\n        if (value.getLength() > 0) {\n            StorageReaderUtil.sendRecord(recordSender, key.toString(), value.getBytes(), taskPluginCollector);\n        }\n    }\n} catch (Exception e) {\n    String message = String.format(\"HDFS SequenceFile Reader read file [%s] failed.\", sourceSequenceFilePath);\n    LOG.error(message, e);\n    throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message, e);\n} finally {\n    if (reader!= null) {\n        try {\n            reader.close();\n        } catch (IOException e) {\n            LOG.error(\"HDFS SequenceFile Reader close reader failed.\", e);\n        }\n    }\n    if (fs!= null) {\n        try {\n            fs.close();\n        } catch (IOException e) {\n            LOG.error(\"HDFS SequenceFile Reader close FileSystem failed.\", e);\n        }\n    }\n}\n}", "CodeBleu_score": 56.66358604414058}]}
{"instance_id": "c450f959616c", "generation_result": [{"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t\ttry {\n\t\tPath path = new Path(filepath);\n\t\tFileSystem fs = path.getFileSystem(hadoopConf);\n\t\tFileStatus fileStatus = fs.getFileStatus(path);\n\t\tif (fileStatus.isDir()) {\n\t\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, \"The specified file is a directory.\");\n\t\t}\n\n\t\tif (fileStatus.getLen() == 0) {\n\t\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, \"The specified file is empty.\");\n\t\t}\n\n\t\tswitch (specifiedFileType) {\n\t\t\tcase ORC:\n\t\t\t\tif (OrcFile.isOrcFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase RC:\n\t\t\t\tif (OrcFile.isRCFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase SEQUENCE:\n\t\t\t\tif (OrcFile.isSequenceFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase PARQUET:\n\t\t\t\tif (ParquetFileReader.isParquetFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase CSV:\n\t\t\t\tif (TextFile.isTextFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TEXT:\n\t\t\t\tif (TextFile.isTextFile(hadoopConf, path)) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn false;\n\t\t}\n\n\t\treturn false;\n\t} catch (IOException e) {\n\t\tLOG.error(\"Error occurred while checking file type: \" + e.getMessage());\n\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, \"Error occurred while checking file type: \" + e.getMessage());\n\t}\n}", "CodeBleu_score": 35.42755544421517}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t", "CodeBleu_score": 2.382402701443833}, {"code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType){\n\t\ttry {\n\t\tFileSystem fs = FileSystem.get(URI.create(filepath), hadoopConf);\n\t\tPath path = new Path(filepath);\n\t\tif (fs.isFile(path)) {\n\t\t\tFileStatus fileStatus = fs.getFileStatus(path);\n\t\t\tif (fileStatus.getLen() == 0) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (specifiedFileType.equalsIgnoreCase(\"orc\")) {\n\t\t\t\treturn isOrcFile(path);\n\t\t\t} else if (specifiedFileType.equalsIgnoreCase(\"rc\")) {\n\t\t\t\treturn isRcFile(path);\n\t\t\t} else if (specifiedFileType.equalsIgnoreCase(\"sequence\")) {\n\t\t\t\treturn isSequenceFile(path);\n\t\t\t} else if (specifiedFileType.equalsIgnoreCase(\"parquet\")) {\n\t\t\t\treturn isParquetFile(path);\n\t\t\t} else if (specifiedFileType.equalsIgnoreCase(\"csv\")) {\n\t\t\t\treturn isCsvFile(path);\n\t\t\t} else if (specifiedFileType.equalsIgnoreCase(\"text\")) {\n\t\t\t\treturn isTextFile(path);\n\t\t\t} else {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t} catch (IOException e) {\n\t\tString message = String.format(\"Error occurred while checking the file type of [%s]\", filepath);\n\t\tLOG.error(message, e);\n\t\tthrow AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message);\n\t}\n}", "CodeBleu_score": 38.98195974342579}]}
{"instance_id": "1bf487d2cc64", "generation_result": [{"code": "public static void closeConnection(Connection hConnection){\n\t\tif (hConnection!= null) {\n\t\ttry {\n\t\t\thConnection.close();\n\t\t} catch (IOException e) {\n\t\t\tthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.HBASE_CONNECTION_ERROR, e);\n\t\t}\n\t}\n}", "CodeBleu_score": 54.53814153771641}, {"code": "public static void closeConnection(Connection hConnection){\n\tif (hConnection!= null) {\ntry {\nhConnection.close();\n} catch (IOException e) {\nthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.HBASE_CONNECTION_ERROR, e);\n}\n}\n}", "CodeBleu_score": 54.20142439495024}, {"code": "public static void closeConnection(Connection hConnection){\n\t\tif (hConnection!= null) {\n\t\ttry {\n\t\t\thConnection.close();\n\t\t} catch (IOException e) {\n\t\t\tthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.HBASE_CONNECTION_ERROR, e);\n\t\t}\n\t}\n}", "CodeBleu_score": 54.53814153771641}]}
{"instance_id": "680728cbc3ec", "generation_result": [{"code": "public static void closeAdmin(Admin admin){\n\t\ttry {\n\t\tif (admin!= null) {\n\t\t\tadmin.close();\n\t\t}\n\t} catch (IOException e) {\n\t\tthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.CLOSE_HBASE_ADMIN_ERROR, e);\n\t}\n}", "CodeBleu_score": 58.528163646521215}, {"code": "public static void closeAdmin(Admin admin){\n\t\ttry {\n\t\tif (admin!= null) {\n\t\t\tadmin.close();\n\t\t}\n\t} catch (IOException e) {\n\t\tthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.CLOSE_HBASE_ADMIN_ERROR, e);\n\t}\n}", "CodeBleu_score": 58.528163646521215}, {"code": "public static void closeAdmin(Admin admin){\n\t\ttry {\n\t\tif (admin!= null) {\n\t\t\tadmin.close();\n\t\t}\n\t} catch (IOException e) {\n\t\tthrow AddaxException.asAddaxException(Hbase11xWriterErrorCode.CLOSE_HBASE_ADMIN_ERROR, e);\n\t}\n}", "CodeBleu_score": 58.528163646521215}]}
{"instance_id": "ce50bd53abef", "generation_result": [{"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\t", "CodeBleu_score": 2.0379993148048974}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\t", "CodeBleu_score": 2.0379993148048974}, {"code": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception{\n\t\n// Extract the row key, timestamp, column family, qualifier, and value from the cell\nbyte[] rowKey = CellUtil.cloneRow(cell);\nlong timestamp = cell.getTimestamp();\nString family = Bytes.toString(CellUtil.cloneFamily(cell));\nString qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));\nbyte[] value = CellUtil.cloneValue(cell);\n\n// Determine the types and formats for the row key and column value based on predefined mappings\nString rowKeyType = HBaseConstant.DEFAULT_ROWKEY_TYPE;\nString rowKeyFormat = HBaseConstant.DEFAULT_ROWKEY_FORMAT;\nString columnValueFormat = HBaseConstant.DEFAULT_COLUMN_VALUE_FORMAT;\n\nif (this.familyQualifierMap.containsKey(family)) {\n    Map<String, String> qualifierMap = this.familyQualifierMap.get(family);\n    if (qualifierMap.containsKey(qualifier)) {\n        String columnType = qualifierMap.get(qualifier);\n        String[] typeFormat = StringUtils.split(columnType, \":\");\n        if (typeFormat.length == 2) {\n            rowKeyType = typeFormat[0];\n            rowKeyFormat = typeFormat[1];\n        } else if (typeFormat.length == 1) {\n            rowKeyType = typeFormat[0];\n        }\n    }\n}\n\n// Convert the extracted data to the appropriate types and add them as columns to the record\nrecord.addColumn(new LongColumn(timestamp));\nrecord.addColumn(new LongColumn(rowKey));\nrecord.addColumn(new LongColumn(family));\nrecord.addColumn(new LongColumn(qualifier));\nrecord.addColumn(new LongColumn(columnValueFormat));\n}", "CodeBleu_score": 46.259058054516935}]}
{"instance_id": "29e8f3a99f0c", "generation_result": [{"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t\tString startRowkey = configuration.getString(HBaseKey.START_ROWKEY);\n\tif (StringUtils.isBlank(startRowkey)) {\n\t\treturn new byte[0];\n\t}\n\tif (configuration.getBool(HBaseKey.ROWKEY_IS_BINARY)) {\n\t\treturn Bytes.toBytesBinary(startRowkey);\n\t} else {\n\t\treturn Bytes.toBytes(startRowkey);\n\t}\n}", "CodeBleu_score": 49.17588469833783}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\t\tString startRowkey = configuration.getString(HBaseKey.START_ROWKEY);\n\tif (StringUtils.isBlank(startRowkey)) {\n\t\treturn new byte[0];\n\t} else {\n\t\tif (isBinaryRowkey(configuration)) {\n\t\t\treturn Bytes.toBytesBinary(startRowkey);\n\t\t} else {\n\t\t\treturn Bytes.toBytes(startRowkey);\n\t\t}\n\t}\n}", "CodeBleu_score": 46.83007291949751}, {"code": "public static byte[] convertUserStartRowkey(Configuration configuration){\n\tString startRowkey = configuration.getString(HBaseKey.START_ROWKEY);\nif (StringUtils.isBlank(startRowkey)) {\nreturn new byte[0];\n}\n\nif (configuration.getBool(HBaseKey.START_ROWKEY_IS_BINARY)) {\nreturn Bytes.toBytesBinary(startRowkey);\n} else {\nreturn Bytes.toBytes(startRowkey);\n}\n}", "CodeBleu_score": 49.2588693665782}]}
{"instance_id": "f5ad51545b10", "generation_result": [{"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\tString startRowkey = configuration.getString(HBaseKey.START_ROW_KEY);\nif (StringUtils.isBlank(startRowkey)) {\nreturn HConstants.EMPTY_BYTE_ARRAY;\n} else {\nreturn Hbase20xHelper.stringToBytes(startRowkey);\n}\n}", "CodeBleu_score": 75.86680831008007}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t\tString startRowkey = configuration.getString(HBaseKey.START_ROW_KEY);\n\tif (StringUtils.isBlank(startRowkey)) {\n\t\treturn HConstants.EMPTY_BYTE_ARRAY;\n\t} else {\n\t\treturn Hbase20xHelper.stringToBytes(startRowkey);\n\t}\n}", "CodeBleu_score": 75.6280989131165}, {"code": "public static byte[] convertInnerStartRowkey(Configuration configuration){\n\t\tString startRowkey = configuration.getString(HBaseKey.START_ROW_KEY);\n\tif (StringUtils.isBlank(startRowkey)) {\n\t\treturn HConstants.EMPTY_BYTE_ARRAY;\n\t} else {\n\t\treturn Hbase20xHelper.stringToBytes(startRowkey);\n\t}\n}", "CodeBleu_score": 75.6280989131165}]}
{"instance_id": "8618eef8b0e9", "generation_result": [{"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\nthis.headers = new DefaultTbQueueMsgHeaders(record.headers());\nthis.data = record.value();\n}", "CodeBleu_score": 42.571804455549525}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\nthis.headers = new DefaultTbQueueMsgHeaders(record.headers());\nthis.data = record.value();\n}", "CodeBleu_score": 42.571804455549525}, {"code": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record){\n\tthis.key = UUID.fromString(record.key());\nthis.headers = new DefaultTbQueueMsgHeaders(record.headers());\nthis.data = record.value();\n}", "CodeBleu_score": 42.571804455549525}]}
{"instance_id": "46484df184dd", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tTbKafkaProducerTemplate<TbProtoQueueMsg<ToTransportMsg>> producerTemplate = new TbKafkaProducerTemplate<>(kafkaSettings, serviceInfoProvider.getServiceId().getId(), transportNotificationSettings.getDefaultTopicName());\nproducerTemplate.setAdmin(notificationAdmin);\nreturn producerTemplate;\n}", "CodeBleu_score": 38.40631756502002}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\treturn new TbKafkaProducerTemplate<>(kafkaSettings, serviceInfoProvider.getServiceId(), transportNotificationSettings.getTopicName(), coreAdmin);\n}", "CodeBleu_score": 13.111991912265983}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer(){\n\tTbKafkaProducerTemplate<TbProtoQueueMsg<ToTransportMsg>> producerTemplate = new TbKafkaProducerTemplate<>(kafkaSettings,\nserviceInfoProvider.getServiceId(),\ntransportNotificationSettings.getDefaultTopicName());\nproducerTemplate.setAdmin(notificationAdmin);\nreturn producerTemplate;\n}", "CodeBleu_score": 38.02068247417756}]}
{"instance_id": "4ac56ef9f89a", "generation_result": [{"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\t\tTbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n\tconsumerBuilder.settings(kafkaSettings);\n\tconsumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\n\tconsumerBuilder.clientId(\"tb-core-notifications-consumer-\" + serviceInfoProvider.getServiceId() + \"-\" + consumerCount.incrementAndGet());\n\tconsumerBuilder.groupId(topicService.buildTopicName(\"tb-core-notifications-node\"));\n\tconsumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreNotificationMsg.parseFrom(msg.getData()), msg.getHeaders()));\n\tconsumerBuilder.admin(notificationAdmin);\n\tconsumerBuilder.statsService(consumerStatsService);\n\treturn consumerBuilder.build();\n}", "CodeBleu_score": 75.67403560175535}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tTbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\nconsumerBuilder.settings(kafkaSettings);\nconsumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\nconsumerBuilder.clientId(\"tb-core-consumer-notifications-\" + serviceInfoProvider.getServiceId() + \"-\" + consumerCount.incrementAndGet());\nconsumerBuilder.groupId(topicService.buildTopicName(\"tb-core-node\"));\nconsumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreNotificationMsg.parseFrom(msg.getData()), msg.getHeaders()));\nconsumerBuilder.admin(notificationAdmin);\nconsumerBuilder.statsService(consumerStatsService);\nreturn consumerBuilder.build();\n}", "CodeBleu_score": 74.73598804676844}, {"code": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer(){\n\tTbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\nconsumerBuilder.settings(kafkaSettings);\nconsumerBuilder.topic(topicService.buildTopicName(coreSettings.getTopic()));\nconsumerBuilder.clientId(\"tb-core-consumer-\" + serviceInfoProvider.getServiceId() + \"-\" + consumerCount.incrementAndGet());\nconsumerBuilder.groupId(topicService.buildTopicName(\"tb-core-node\"));\nconsumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreNotificationMsg.parseFrom(msg.getData()), msg.getHeaders()));\nconsumerBuilder.admin(notificationAdmin);\nconsumerBuilder.statsService(consumerStatsService);\nreturn consumerBuilder.build();\n}", "CodeBleu_score": 74.32093329241887}]}
{"instance_id": "e13a82b26338", "generation_result": [{"code": "protected void doSubscribe(List<String> topicNames){\n\t (!topicNames.isEmpty()) {\nfor (String topicName : topicNames) {\nadmin.ensureTopicExists(topicName);\n}\nconsumer.subscribe(topicNames);\nlog.info(\\\"Subscribed to topics: {}\\\", topicNames);\n\n} else {\n\nconsumer.unsubscribe();\nlog.info(\\\"Unsubscribed from all topics.\\\");\n}\n}", "CodeBleu_score": 53.751630314463604}, {"code": "protected void doSubscribe(List<String> topicNames){\n\t (!topicNames.isEmpty()) {\nadmin.ensureTopics(topicNames);\nconsumer.subscribe(topicNames);\n} else {\nlog.info(\\\"Unsubscribing from all topics\\\");\nconsumer.unsubscribe();\n}\n}", "CodeBleu_score": 56.2800881322494}, {"code": "protected void doSubscribe(List<String> topicNames){\n\t (!topicNames.isEmpty()) {\nadmin.ensureTopics(topicNames);\nconsumer.subscribe(topicNames);\n} else {\nlog.info(\\\"[{}] Unsubscribing from all topics\\\", groupId);\nconsumer.unsubscribe();\n}\n}", "CodeBleu_score": 56.66748552495996}]}
{"instance_id": "8a20d0b48703", "generation_result": [{"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\topWatch stopWatch = new StopWatch();\nstopWatch.start();\nlog.trace(\\\"polling topic {} for {} ms\\\", topic, durationInMillis);\nConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\nstopWatch.stop();\nlog.trace(\\\"polling topic {} took {} ms\\\", topic, stopWatch.getTotalTimeMillis());\nif (records.isEmpty()) {\nreturn Collections.emptyList();\n}\nList<ConsumerRecord<String, byte[]>> result = new ArrayList<>();\nfor (ConsumerRecord<String, byte[]> record : records) {\nresult.add(record);\n}\nreturn result;\n}", "CodeBleu_score": 67.3369023807641}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\topWatch stopWatch = new StopWatch();\nstopWatch.start();\nlog.trace(\\\"polling topic [{}] for [{}] ms\\\", topic, durationInMillis);\nConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\nstopWatch.stop();\nlog.trace(\\\"polling topic [{}] took [{}] ms\\\", topic, stopWatch.getTotalTimeMillis());\nif (records.isEmpty()) {\nreturn Collections.emptyList();\n}\nList<ConsumerRecord<String, byte[]>> result = new ArrayList<>();\nrecords.forEach(result::add);\nreturn result;\n}", "CodeBleu_score": 65.79412997683147}, {"code": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis){\n\topWatch stopWatch = new StopWatch();\nstopWatch.start();\nlog.trace(\\\"[{}] poll for {} ms\\\", getTopic(), durationInMillis);\nConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\nstopWatch.stop();\nlog.trace(\\\"[{}] poll for {} ms took {} ms\\\", getTopic(), durationInMillis, stopWatch.getTotalTimeMillis());\nif (records.isEmpty()) {\nreturn Collections.emptyList();\n}\nList<ConsumerRecord<String, byte[]>> result = new ArrayList<>();\nfor (ConsumerRecord<String, byte[]> record : records) {\nresult.add(record);\n}\nreturn result;\n}", "CodeBleu_score": 67.7104711354022}]}
{"instance_id": "328e4c39a8e1", "generation_result": [{"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tif (initError!= null) {\nlog().error(\\\"[{}] Failed to initialize: {}\\\", ctx.getSelfId(), initError.getMessage(), initError);\nctx.notifyError(initError);\nreturn;\n}\nString topic = config.getTopic();\nif (topic == null) {\ntopic = msg.getMetadata().getTopic();\n}\nif (topic == null) {\nlog().error(\\\"[{}] Failed to publish message: topic is not defined in the configuration or metadata!\\\", ctx.getSelfId());\nctx.notifyError(new ThingsboardKafkaClientError(\\\"Failed to publish message: topic is not defined in the configuration or metadata!\\\"));\nreturn;\n}\nString key = config.getKey();\nif (key == null) {\nkey = msg.getMetadata().getKey();\n}\nif (key == null) {\nlog().error(\\\"[{}] Failed to publish message: key is not defined in the configuration or metadata!\\\", ctx.getSelfId());\nctx.notifyError(new ThingsboardKafkaClientError(\\\"Failed to publish message: key is not defined in the configuration or metadata!\\\"));\nreturn;\n}\nctx.executeAsync(() -> {\ntry {\nproducer.send(new ProducerRecord<>(topic, key, msg.getData().toString()), (recordMetadata, exception) -> {\nif (exception!= null) {\nlog().error(\\\"[{}] Failed to publish message: {}\\\", ctx.getSelfId(), exception.getMessage(), exception);\nctx.notifyError(new ThingsboardKafkaClientError(exception));\n}\n});\n} catch (Exception e) {\nlog().error(\\\"[{}] Failed to publish message: {}\\\", ctx.getSelfId(), e.getMessage(), e);\nctx.notifyError(new ThingsboardKafkaClientError(e));\n}\n});\n}", "CodeBleu_score": 36.31380560136163}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\tif (initError!= null) {\nlog().error(\\\"[{}] Failed to initialize: {}\\\", ctx.getSelfId(), initError.getMessage());\nctx.notifyError(initError);\nreturn;\n}\nString topic = config.getTopic();\nString key = config.getKey();\nif (topic == null || topic.isEmpty()) {\ntopic = msg.getMetadata().get(TOPIC);\n}\nif (key == null || key.isEmpty()) {\nkey = msg.getMetadata().get(PARTITION);\n}\nif (topic == null || topic.isEmpty()) {\nlog().error(\\\"[{}] Topic is not defined!\\\", ctx.getSelfId());\nreturn;\n}\nif (key == null || key.isEmpty()) {\nlog().error(\\\"[{}] Key is not defined!\\\", ctx.getSelfId());\nreturn;\n}\nctx.async()\n.doInBackground(() -> {\ntry {\nProducerRecord<String, String> record = new ProducerRecord<>(topic, key, msg.getData().toString());\nif (addMetadataKeyValuesAsKafkaHeaders) {\nmsg.getMetadata().forEach((k, v) -> {\nif (!k.startsWith(TB_MSG_MD_PREFIX)) {\nreturn;\n}\nrecord.headers().add(k.substring(TB_MSG_MD_PREFIX.length()), v.getBytes(toBytesCharset));\n});\n}\nproducer.send(record).get();\n} catch (Exception e) {\nlog().error(\\\"[{}] Failed to publish message to Kafka: {}\\\", ctx.getSelfId(), e.getMessage());\nctx.notifyError(e);\n}\n})\n.onFailure(e -> log().error(\\\"[{}] Failed to publish message to Kafka: {}\\\", ctx.getSelfId(), e.getMessage()))\n.onSuccess(r -> log().debug(\\\"[{}] Successfully published message to Kafka\\\", ctx.getSelfId()))\n.done();\n}", "CodeBleu_score": 35.183888496639604}, {"code": "public void onMsg(TbContext ctx, TbMsg msg){\n\t", "CodeBleu_score": 2.454125543921295}]}
{"instance_id": "35f835e8c387", "generation_result": [{"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tif (addMetadataKeyValuesAsKafkaHeaders) {\nMap<String, String> headers = new HashMap<>();\nmsg.getMetadata().forEach((k, v) -> headers.put(TB_MSG_MD_PREFIX + k, v.toString()));\nProducerRecord<String, String> record = new ProducerRecord<>(topic, key, msg.getData().toString(), headers);\nproducer.send(record, (recordMetadata, exception) -> {\nif (exception!= null) {\nlog.debug(\\\"[{}] Failed to send message to Kafka topic: {}\\\", ctx.getMsgId(), exception.getMessage());\n}\n});\n} else {\nproducer.send(new ProducerRecord<>(topic, key, msg.getData().toString()), (recordMetadata, exception) -> {\nif (exception!= null) {\nlog.debug(\\\"[{}] Failed to send message to Kafka topic: {}\\\", ctx.getMsgId(), exception.getMessage());\n}\n});\n}\n}", "CodeBleu_score": 35.85598599110301}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\t\tif (addMetadataKeyValuesAsKafkaHeaders) {\n\t\tMap<String, String> headers = new HashMap<>();\n\t\tmsg.getMetadata().forEach((k, v) -> headers.put(TB_MSG_MD_PREFIX + k, v));\n\t\ttry {\n\t\t\tproducer.send(new ProducerRecord<>(topic, key, msg.getData().toString(), headers), (recordMetadata, exception) -> {\n\t\t\t\tif (exception!= null) {\n\t\t\t\t\tlog.debug(\\\"[{}] Failed to send message to Kafka topic [{}]: {}\\\", ctx.getMsgId(), topic, exception.getMessage());\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (Exception e) {\n\t\t\tlog.debug(\\\"[{}] Failed to send message to Kafka topic [{}]: {}\\\", ctx.getMsgId(), topic, e.getMessage());\n\t\t}\n\t} else {\n\t\ttry {\n\t\t\tproducer.send(new ProducerRecord<>(topic, key, msg.getData().toString()), (recordMetadata, exception) -> {\n\t\t\t\tif (exception!= null) {\n\t\t\t\t\tlog.debug(\\\"[{}] Failed to send message to Kafka topic [{}]: {}\\\", ctx.getMsgId(), topic, exception.getMessage());\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (Exception e) {\n\t\t\tlog.debug(\\\"[{}] Failed to send message to Kafka topic [{}]: {}\\\", ctx.getMsgId(), topic, e.getMessage());\n\t\t}\n\t}\n}", "CodeBleu_score": 35.21357521333467}, {"code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key){\n\tif (addMetadataKeyValuesAsKafkaHeaders) {\nMap<String, String> headers = new HashMap<>();\nmsg.getMetadata().forEach((k, v) -> headers.put(TB_MSG_MD_PREFIX + k, v));\nProducerRecord<String, String> record = new ProducerRecord<>(topic, key, msg.getData().toString(), headers);\nproducer.send(record, (recordMetadata, exception) -> {\nif (exception!= null) {\nlog.debug(\\\"[{}] Failed to send message to Kafka topic: {}\\\", ctx.getMsgId(), exception.getMessage());\n}\n});\n} else {\nProducerRecord<String, String> record = new ProducerRecord<>(topic, key, msg.getData().toString());\nproducer.send(record, (recordMetadata, exception) -> {\nif (exception!= null) {\nlog.debug(\\\"[{}] Failed to send message to Kafka topic: {}\\\", ctx.getMsgId(), exception.getMessage());\n}\n});\n}\n}", "CodeBleu_score": 33.21244812701223}]}
{"instance_id": "111a11189434", "generation_result": [{"code": "public void destroy(){\n\tif (producer!= null) {\ntry {\nproducer.close();\n} catch (Exception e) {\nlog.error(\\\"[{}] Failed to close Kafka producer: {}\\\", getSelfId(), e.getMessage());\n}\n}\n}", "CodeBleu_score": 42.71130215650264}, {"code": "public void destroy(){\n\tif (producer!= null) {\ntry {\nproducer.close();\n} catch (Exception e) {\nlog.error(\\\"[{}] Failed to close Kafka producer: {}\\\", getSelfId(), e.getMessage());\n}\n}\n}", "CodeBleu_score": 42.71130215650264}, {"code": "public void destroy(){\n\tif (producer!= null) {\ntry {\nproducer.close();\n} catch (Exception e) {\nlog.error(\"Failed to close Kafka rule node producer: {}\", e.getMessage());\n}\n}\n}", "CodeBleu_score": 47.376972875283684}]}
{"instance_id": "f5d2487100a9", "generation_result": [{"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\tTbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\nrequestBuilder.settings(kafkaSettings);\nrequestBuilder.clientId(\"monolith-rule-engine-messages-\" + serviceInfoProvider.getServiceId());\nrequestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getRuleEngineTopic()));\nrequestBuilder.admin(ruleEngineAdmin);\nreturn requestBuilder.build();\n}", "CodeBleu_score": 87.46803951204654}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\t\tTbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n\trequestBuilder.settings(kafkaSettings);\n\trequestBuilder.clientId(\"monolith-rule-engine-messages-\" + serviceInfoProvider.getServiceId());\n\trequestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getRuleEngineTopic()));\n\trequestBuilder.admin(ruleEngineAdmin);\n\treturn requestBuilder.build();\n}", "CodeBleu_score": 87.4179755450318}, {"code": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer(){\n\t\tTbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n\trequestBuilder.settings(kafkaSettings);\n\trequestBuilder.clientId(\"monolith-rule-engine-messages-\" + serviceInfoProvider.getServiceId());\n\trequestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getRuleEngineTopic()));\n\trequestBuilder.admin(ruleEngineAdmin);\n\treturn requestBuilder.build();\n}", "CodeBleu_score": 87.4179755450318}]}
{"instance_id": "886264f5ca27", "generation_result": [{"code": "private void startLogScheduling(){\n\t", "CodeBleu_score": 0.6449045691040067}, {"code": "private void startLogScheduling(){\n\t\tDuration timeout = Duration.ofMillis(kafkaSettings.getKafkaResponseTimeout());\n\tstatsPrintScheduler.scheduleWithFixedDelay(() -> {\n\n\t\tif (!statsConfig.getEnabled()) {\n\n\t\t\treturn;\n\n\t\t}\n\n\t\tfor (String monitoredGroup : monitoredGroups) {\n\n\t\t\ttry {\n\n\t\t\t\tMap<TopicPartition, Long> offsets = consumer.listOffsets(consumer.listTopics().entrySet(), timeout);\n\n\t\t\t\tMap<TopicPartition, Long> endOffsets = consumer.endOffsets(offsets.keySet(), timeout);\n\n\t\t\t\tList<TopicPartition> laggingPartitions = new ArrayList<>();\n\n\t\t\t\tfor (Map.Entry<TopicPartition, Long> offset : offsets.entrySet()) {\n\n\t\t\t\t\tif (offset.getValue() < endOffsets.get(offset.getKey())) {\n\n\t\t\t\t\t\tlaggingPartitions.add(offset.getKey());\n\n\t\t\t\t\t}\n\n\t\t\t\t}\n\n\t\t\t\tif (!laggingPartitions.isEmpty()) {\n\n\t\t\t\t\tlog.warn(\\\"Kafka consumer group [{}] has lagging partitions: {}\\\", monitoredGroup, laggingPartitions);\n\n\t\t\t\t}\n\n\t\t\t} catch (Exception e) {\n\n\t\t\t\tlog.warn(\\\"Failed to retrieve Kafka consumer group offsets for group [{}]. Error: {}\\\", monitoredGroup, e.getMessage(), e);\n\n\t\t\t}\n\n\t\t}\n\n\t}, statsConfig.getPrintInterval(), statsConfig.getPrintInterval(), TimeUnit.MILLISECONDS);\n\n}", "CodeBleu_score": 36.748636505701676}, {"code": "private void startLogScheduling(){\n\t\t// Calculate timeout duration based on Kafka response timeout configuration\n\tDuration timeout = Duration.ofMillis(kafkaSettings.getKafkaResponseTimeout());\n\n\t// Schedule a task with a fixed delay\n\tstatsPrintScheduler.scheduleWithFixedDelay(() -> {\n\n\t\t// Check if statistics printing is required\n\t\tif (!statsConfig.getEnabled()) {\n\n\t\t\treturn;\n\n\t\t}\n\n\n\t\t// For each monitored consumer group\n\t\tfor (String group : monitoredGroups) {\n\n\t\t\ttry {\n\n\t\t\t\t// Retrieve and compare group offsets and end offsets\n\t\t\t\tMap<TopicPartition, Long> groupOffsets = consumer.committed(timeout);\n\n\t\t\t\tMap<TopicPartition, Long> endOffsets = consumer.endOffsets(groupOffsets.keySet(), timeout);\n\n\n\t\t\t\t// Identify topics with lag\n\t\t\t\tList<TopicPartition> laggingTopics = new ArrayList<>();\n\n\t\t\t\tfor (TopicPartition topicPartition : groupOffsets.keySet()) {\n\n\t\t\t\t\tif (groupOffsets.get(topicPartition) < endOffsets.get(topicPartition)) {\n\n\t\t\t\t\t\tlaggingTopics.add(topicPartition);\n\n\t\t\t\t\t}\n\n\t\t\t\t}\n\n\n\t\t\t\t// Log details for lagging topics\n\t\t\t\tif (!laggingTopics.isEmpty()) {\n\n\t\t\t\t\tlog.info(\\\"Kafka consumer group [{}] has lagging topics: {}\\\", group, laggingTopics);\n\n\t\t\t\t}\n\n\t\t\t} catch (Exception e) {\n\n\t\t\t\tlog.warn(\\\"Error retrieving Kafka consumer group [{}] statistics. Error: {}\\\", group, e.getMessage(), e);\n\n\t\t\t}\n\n\t\t}\n\n\t}, statsConfig.getPeriod(), statsConfig.getPeriod(), TimeUnit.SECONDS);\n\n}", "CodeBleu_score": 33.475426403053476}]}
{"instance_id": "81553181cdc8", "generation_result": [{"code": "protected Properties toAdminProps(){\n\tProperties props = toProps();\nprops.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\nprops.put(AdminClientConfig.RETRIES_CONFIG, retries);\nreturn props;\n}", "CodeBleu_score": 96.75968336616603}, {"code": "protected Properties toAdminProps(){\n\tProperties props = toProps();\nprops.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\nprops.put(AdminClientConfig.RETRIES_CONFIG, retries);\nreturn props;\n}", "CodeBleu_score": 96.75968336616603}, {"code": "protected Properties toAdminProps(){\n\tProperties props = toProps();\nprops.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\nprops.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\nreturn props;\n}", "CodeBleu_score": 86.15854245442624}]}
{"instance_id": "3c8bf3ff31b7", "generation_result": [{"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tring scopedName = getScopedName(metricName, group);\n\tMetricEvent event = new MetricEvent(metric, scopedName, getTags(metricName, group));\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, event);\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, event);\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, event);\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, event);\n\t} else {\n\t\tLOG.warn(\\\"Unknown metric type: \\\" + metric.getClass().getName());\n\t}\n}", "CodeBleu_score": 65.28007383837291}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\tring scopedName = getScopedName(metricName, group);\n\tMap<String, String> tags = getTags(metricName, group);\n\tMetricEvent event = new MetricEvent(scopedName, tags);\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, event);\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, event);\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, event);\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, event);\n\t} else {\n\t\tLOG.warn(\\\"Unsupported metric type: {}\\\", metric.getClass());\n\t}\n}", "CodeBleu_score": 66.92869939310694}, {"code": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group){\n\ttricEvent event = new MetricEvent();\n\tevent.setScope(getScope(metric, metricName, group));\n\tevent.setTags(getTags(metric, metricName, group));\n\tif (metric instanceof Counter) {\n\t\tcounters.put((Counter) metric, event);\n\t} else if (metric instanceof Gauge) {\n\t\tgauges.put((Gauge<?>) metric, event);\n\t} else if (metric instanceof Histogram) {\n\t\thistograms.put((Histogram) metric, event);\n\t} else if (metric instanceof Meter) {\n\t\tmeters.put((Meter) metric, event);\n\t} else {\n\t\tLOG.warn(\\\"Unknown metric type {}\\\", metric.getClass().getName());\n\t}\n}", "CodeBleu_score": 61.95156297214987}]}
{"instance_id": "77c6addc9b99", "generation_result": [{"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tObject value = gauge.getValue();\n\tif (value == null) {\n\t\treturn metricEvent;\n\t}\n\tif (metricEvent.getFields()!= null && metricEvent.getFields().size() > 0) {\n\t\tmetricEvent.getFields().put(field, convertValue(value));\n\t} else {\n\t\tMap<String, Object> fields = new HashMap<>();\n\t\tfields.put(field, convertValue(value));\n\t\tmetricEvent.setFields(fields);\n\t}\n\treturn metricEvent;\n}", "CodeBleu_score": 43.467952721839914}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tObject value = gauge.getValue();\n\tif (value instanceof Number) {\n\t\tmetricEvent.addField(field, (Number) value);\n\t} else {\n\t\tmetricEvent.addField(field, value.toString());\n\t}\n\treturn metricEvent;\n}", "CodeBleu_score": 31.502928155497443}, {"code": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge){\n\t\tObject value = gauge.getValue();\n\tif (metricEvent.getFields()!= null) {\n\t\tmetricEvent.addField(field, value);\n\t} else {\n\t\tMap<String, Object> fields = new HashMap<>();\n\t\tfields.put(field, value);\n\t\tmetricEvent.setFields(fields);\n\t}\n\treturn metricEvent;\n}", "CodeBleu_score": 37.560522720612624}]}
{"instance_id": "a48971ffb3e9", "generation_result": [{"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t\tString[] groups = metricEvent.getName().split(\\\"_\\\");\n\tif (groups.length < 3) {\n\t\treturn null;\n\t}\n\tString topic = groups[2];\n\tString partition = groups[3];\n\tString consumerGroup = groups[4];\n\tString metricName = metricEvent.getName();\n\tMap<String, Object> fields = metricEvent.getFields();\n\tMap<String, Object> kafkaLagTimes = (Map<String, Object>) fields.get(\\\"kafkaLagTimes\\\");\n\tif (kafkaLagTimes == null) {\n\t\tkafkaLagTimes = new HashMap<>();\n\t}\n\tObject value = gauge.getValue();\n\tif (value instanceof Map) {\n\t\tMap<String, Object> lagMap = (Map<String, Object>) value;\n\t\tString currentDataTimestampOffsets = (String) lagMap.get(\\\"currentDataTimestampOffsets\\\");\n\t\tString committedOffsets = (String) lagMap.get(\\\"committedOffsets\\\");\n\t\tif (currentDataTimestampOffsets!= null && committedOffsets!= null) {\n\t\t\tString[] currentDataTimestampOffsetsArray = currentDataTimestampOffsets.split(\\\",\\\");\n\t\t\tString[] committedOffsetsArray = committedOffsets.split(\\\",\\\");\n\t\t\tif (currentDataTimestampOffsetsArray.length!= committedOffsetsArray.length) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\tfor (int i = 0; i < currentDataTimestampOffsetsArray.length; i++) {\n\t\t\t\tString currentDataTimestampOffset = currentDataTimestampOffsetsArray[i];\n\t\t\t\tString committedOffset = committedOffsetsArray[i];\n\t\t\t\tString key = topic + \\\"_\\\" + partition + \\\"_\\\" + consumerGroup + \\\"_\\\" + currentDataTimestampOffset;\n\t\t\t\tif (!kafkaLagTimes.containsKey(key)) {\n\t\t\t\t\tkafkaLagTimes.put(key, timestamp);\n\t\t\t\t}\n\t\t\t}\n\t\t\tfields.put(\\\"kafkaLagTimes\\\", kafkaLagTimes);\n\t\t\tmetricEvent.setFields(fields);\n\t\t\tmetricEvent.setTimestamp(timestamp);\n\t\t\treturn metricEvent;\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 33.141063015707026}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t", "CodeBleu_score": 2.8929956351891004}, {"code": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge){\n\t\tString name = metricEvent.getName();\n\tString[] groups = name.split(\\\"_\\\");\n\tString topic = groups[groups.length - 1];\n\tString groupId = groups[groups.length - 2];\n\tString[] offsetAndTimestamp = gauge.getValue().toString().split(\\\"\\\\|\\\\|\\\");\n\tString[] currentOffsets = offsetAndTimestamp[0].split(\\\"\\\\|\\\");\n\tString[] committedOffsets = offsetAndTimestamp[1].split(\\\"\\\\|\\\");\n\tString[] dataTimestamps = offsetAndTimestamp[2].split(\\\"\\\\|\\\");\n\tMap<String, Object> fields = metricEvent.getFields();\n\tif (fields == null) {\n\t\tfields = new HashMap<>();\n\t\tmetricEvent.setFields(fields);\n\t}\n\tif (kafkaLagTimes.containsKey(groupId + \\\"_\\\" + topic)) {\n\t\tMap<String, Object> kafkaLagTime = kafkaLagTimes.get(groupId + \\\"_\\\" + topic);\n\t\tif (kafkaLagTime.get(\\\"currentOffsets\\\").equals(currentOffsets[0]) && kafkaLagTime.get(\\\"committedOffsets\\\").equals(committedOffsets[0]) && kafkaLagTime.get(\\\"dataTimestamps\\\").equals(dataTimestamps[0])) {\n\t\t\treturn null;\n\t\t}\n\t}\n\tkafkaLagTimes.put(groupId + \\\"_\\\" + topic, Map.of(\\\"currentOffsets\\\", currentOffsets[0], \\\"committedOffsets\\\", committedOffsets[0], \\\"dataTimestamps\\\", dataTimestamps[0]));\n\tfields.put(\\\"currentOffsets\\\", currentOffsets[0]);\n\tfields.put(\\\"committedOffsets\\\", committedOffsets[0]);\n\tfields.put(\\\"dataTimestamps\\\", dataTimestamps[0]);\n\tmetricEvent.setTimestamp(timestamp);\n\treturn metricEvent;\n}", "CodeBleu_score": 36.201499435147255}]}
{"instance_id": "c2080c299bb0", "generation_result": [{"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\t// Retrieve global job parameters from the StreamExecutionEnvironment\nParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n\n// Build Kafka properties\nProperties props = KafkaConfigUtil.buildKafkaProps(parameter);\n\n// Instantiate a FlinkKafkaConsumer for MetricEvent\nFlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, new MetricSchema(), props);\n\n// If a specific start time is provided, set the consumer to start from the specific offsets based on the given time\nif (time!= 0) {\n// Get the KafkaConsumer from the FlinkKafkaConsumer\nKafkaConsumer<String, String> kafkaConsumer = consumer.getKafkaConsumer();\n\n// Get the partition information for the topic\nList<PartitionInfo> partitionInfos = kafkaConsumer.partitionsFor(topic);\n\n// Create a map to store the partition offsets\nMap<TopicPartition, Long> partitionOffsets = new HashMap<>();\n\n// Iterate over the partition information and get the offsets for each partition\nfor (PartitionInfo partitionInfo : partitionInfos) {\nTopicPartition topicPartition = new TopicPartition(topic, partitionInfo.partition());\n\n// Get the offset and timestamp for the partition based on the given time\nOffsetAndTimestamp offsetAndTimestamp = kafkaConsumer.offsetsForTimes(Map.of(topicPartition, time))[0];\n\n// If the offset is not null, add it to the map\nif (offsetAndTimestamp!= null) {\npartitionOffsets.put(topicPartition, offsetAndTimestamp.offset());\n}\n}\n\n// If the partition offsets are not empty, set the consumer to start from the specified offsets\nif (!partitionOffsets.isEmpty()) {\nconsumer.setStartFromSpecificOffsets(partitionOffsets);\n}\n}\n\n// Add the consumer as a source to the StreamExecutionEnvironment\nreturn env.addSource(consumer);\n}", "CodeBleu_score": 59.41219273050585}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\t", "CodeBleu_score": 8.44308697353304}, {"code": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException{\n\t\tParameterTool parameter = (ParameterTool) env.getConfig().getGlobalJobParameters();\n\tProperties props = buildKafkaProps(parameter);\n\tFlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(topic, MetricSchema.METRIC_EVENT_SCHEMA, props);\n\tif (time > 0) {\n\t\tconsumer.setStartFromTimestamp(time, new KafkaConsumer<>(props));\n\t}\n\treturn env.addSource(consumer);\n}", "CodeBleu_score": 55.40186573161214}]}
{"instance_id": "5e828afbe8e4", "generation_result": [{"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\tprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"buildOffsetByTime\" + time);\n\tKafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n\tString topic = parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC);\n\tList<PartitionInfo> partitions = consumer.partitionsFor(topic);\n\tMap<TopicPartition, Long> partitionOffset = new HashMap<>();\n\tfor (PartitionInfo partitionInfo : partitions) {\n\t\tTopicPartition topicPartition = new TopicPartition(topic, partitionInfo.partition());\n\t\tpartitionOffset.put(topicPartition, time);\n\t}\n\tMap<TopicPartition, OffsetAndTimestamp> offsets = consumer.offsetsForTimes(partitionOffset);\n\tMap<KafkaTopicPartition, Long> partitionOffsetMap = new HashMap<>();\n\tfor (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsets.entrySet()) {\n\t\tpartitionOffsetMap.put(new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), entry.getValue().offset());\n\t}\n\tconsumer.close();\n\treturn partitionOffsetMap;\n}", "CodeBleu_score": 54.21962879336609}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\ttring groupId = parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + \"_time_\" + time;\nprops.put(\\\"group.id\\\", groupId);\nKafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\nString topic = parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC);\nList<PartitionInfo> partitionInfos = consumer.partitionsFor(topic);\nMap<TopicPartition, Long> timestampsToSearch = new HashMap<>();\nfor (PartitionInfo partitionInfo : partitionInfos) {\n    timestampsToSearch.put(new TopicPartition(topic, partitionInfo.partition()), time);\n}\nMap<TopicPartition, OffsetAndTimestamp> offsets = consumer.offsetsForTimes(timestampsToSearch);\nMap<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\noffsets.forEach((topicPartition, offsetAndTimestamp) -> {\n    partitionOffset.put(new KafkaTopicPartition(topicPartition.topic(), topicPartition.partition()), offsetAndTimestamp.offset());\n});\nconsumer.close();\nreturn partitionOffset;\n}", "CodeBleu_score": 56.55225847438202}, {"code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time){\n\ttring groupId = parameterTool.get(PropertiesConstants.KAFKA_GROUP_ID, DEFAULT_KAFKA_GROUP_ID) + time;\nprops.put(\\\"group.id\\\", groupId);\nKafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\nString topic = parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC);\nList<PartitionInfo> partitionInfos = consumer.partitionsFor(topic);\nMap<TopicPartition, Long> partitionOffset = new HashMap<>();\nfor (PartitionInfo partitionInfo : partitionInfos) {\nTopicPartition topicPartition = new TopicPartition(topic, partitionInfo.partition());\nconsumer.assign(List.of(topicPartition));\nOffsetAndTimestamp offsetAndTimestamp = consumer.offsetsForTimes(Map.of(topicPartition, time))[topicPartition];\npartitionOffset.put(topicPartition, offsetAndTimestamp.offset());\n}\nconsumer.close();\nreturn partitionOffset.entrySet().stream().collect(Collectors.toMap(entry -> new KafkaTopicPartition(entry.getKey().topic(), entry.getKey().partition()), Map.Entry::getValue));\n}", "CodeBleu_score": 49.06456589270126}]}
{"instance_id": "28da89f56f67", "generation_result": [{"code": "public static void writeToKafka() throws InterruptedException{\n\t\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", broker_list);\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\nKafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\nMetricEvent metricEvent = new MetricEvent();\nmetricEvent.setTimestamp(System.currentTimeMillis());\nmetricEvent.setName(\"flink-metric-event\");\nmetricEvent.setTags(new HashMap<String, String>() {{\nput(\"host\", \"localhost\");\nput(\"topic\", \"metric\");\n}});\nmetricEvent.setFields(new HashMap<String, Object>() {{\nput(\"value\", 1);\n}});\n\nProducerRecord<String, String> record = new ProducerRecord<>(topic, GsonUtil.toJson(metricEvent));\nproducer.send(record);\n\nproducer.flush();\nproducer.close();\n}", "CodeBleu_score": 49.20841277153338}, {"code": "public static void writeToKafka() throws InterruptedException{\n\t// Set up Kafka producer properties\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", broker_list);\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n// Create a KafkaProducer instance\nKafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\n// Create a MetricEvent and populate it with data\nMetricEvent metricEvent = new MetricEvent();\nmetricEvent.setTimestamp(System.currentTimeMillis());\nmetricEvent.setName(\"cpu\");\nMap<String, String> tags = new HashMap<>();\ntags.put(\"host\", \"localhost\");\nmetricEvent.setTags(tags);\nMap<String, Object> fields = new HashMap<>();\nfields.put(\"value\", 100);\nmetricEvent.setFields(fields);\n\n// Serialize the MetricEvent and send it to the specified topic\nString serializedMetricEvent = GsonUtil.toJson(metricEvent);\nProducerRecord<String, String> record = new ProducerRecord<>(topic, serializedMetricEvent);\nproducer.send(record);\n\n// Print the sent data and flush the producer to ensure all data is sent\nSystem.out.println(\"Sent data: \" + serializedMetricEvent);\nproducer.flush();\nproducer.close();\n}", "CodeBleu_score": 58.896991113859784}, {"code": "public static void writeToKafka() throws InterruptedException{\n\t\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", broker_list);\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\nKafkaProducer<String, String> producer = new KafkaProducer<>(props);\n\nMetricEvent metricEvent = new MetricEvent();\nmetricEvent.setTimestamp(System.currentTimeMillis());\nmetricEvent.setName(\"metric\");\nMap<String, String> tags = new HashMap<>();\ntags.put(\"host\", \"127.0.0.1\");\nmetricEvent.setTags(tags);\nMap<String, Object> fields = new HashMap<>();\nfields.put(\"value\", 100);\nmetricEvent.setFields(fields);\n\nProducerRecord<String, String> record = new ProducerRecord<>(topic, null, GsonUtil.toJson(metricEvent));\nproducer.send(record);\n\nSystem.out.println(record.value());\nproducer.flush();\nThread.sleep(1000);\nproducer.close();\n}", "CodeBleu_score": 58.6373277638838}]}
{"instance_id": "14bffe09c25a", "generation_result": [{"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\t", "CodeBleu_score": 4.214743313165087}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\tg msg = event.getMessage().getFormattedMessage();\n\nString level = event.getLevel().toString();\n\nString loggerName = event.getLoggerName();\n\nif (level.equalsIgnoreCase(this.level) &&!loggerName.contains(\"xxx\")) {\n\nLogEvent logEvent = new LogEvent();\n\nlogEvent.setSource(source);\n\nlogEvent.setTimestamp(event.getTimeMillis());\n\nlogEvent.setLevel(level);\n\nlogEvent.setLoggerName(loggerName);\n\nlogEvent.setThreadName(event.getThreadName());\n\nlogEvent.setMsg(msg);\n\nlogEvent.setAppId(appId);\n\nlogEvent.setContainerId(containerId);\n\nlogEvent.setContainerType(containerType);\n\nlogEvent.setTaskName(taskName);\n\nlogEvent.setTaskId(taskId);\n\nlogEvent.setNodeIp(nodeIp);\n\nlogEvent.setTraceId(UUID.randomUUID().toString());\n\ntry {\n\nString json = JacksonUtil.writeValueAsString(logEvent);\n\nproducer.send(new ProducerRecord<>(topic, json));\n\n} catch (Exception e) {\n\nlog.warn(\\\"Failed to send log event to Kafka topic: {}\\\", e.getMessage());\n\n}\n\n}\n\n}", "CodeBleu_score": 38.36407875389739}, {"code": "public void append(org.apache.logging.log4j.core.LogEvent event){\n\t log = null;\ntry {\nlog = new String(getLayout().toByteArray(event));\n} catch (Exception e) {\nlog = event.getMessage().getFormattedMessage();\n}\n\nif (log == null) {\nreturn;\n}\n\nif (level.equals(event.getLevel().toString())) {\nLogEvent logEvent = new LogEvent();\nlogEvent.setSource(source);\nlogEvent.setAppId(appId);\nlogEvent.setContainerId(containerId);\nlogEvent.setContainerType(containerType);\nlogEvent.setTaskName(taskName);\nlogEvent.setTaskId(taskId);\nlogEvent.setLog(log);\nlogEvent.setTimestamp(event.getTimeMillis());\nlogEvent.setThreadName(event.getThreadName());\nlogEvent.setLoggerName(event.getLoggerName());\nlogEvent.setLevel(event.getLevel().toString());\nlogEvent.setMarker(event.getMarker());\nlogEvent.setNodeIp(nodeIp);\n\ntry {\nproducer.send(new ProducerRecord<>(topic, null, null, JacksonUtil.toJson(logEvent)));\n} catch (Exception e) {\nlog.warn(\"send log to kafka error\", e);\n}\n}\n}", "CodeBleu_score": 34.352434089270126}]}
{"instance_id": "a9e0786fb5c1", "generation_result": [{"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif validMethod == method {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\\\"unknown topic suffix method: %s\\\", method)\n}", "CodeBleu_score": 72.97867783274157}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, m := range ValidTopicSuffixMethods {\n\t\tif m == method {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\\\"unknown topic suffix method: %s\\\", method)\n}", "CodeBleu_score": 66.05148795672542}, {"code": "func ValidateTopicSuffixMethod(method string) error{\n\tfor _, validMethod := range ValidTopicSuffixMethods {\n\t\tif method == validMethod {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn fmt.Errorf(\\\"unknown topic suffix method: %q\\\", method)\n}", "CodeBleu_score": 77.59677383921586}]}
{"instance_id": "9b88d720d48a", "generation_result": [{"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tutingTag!= \\\"\\\" {\n\t\tif key, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn key, nil\n\t\t}\n\t}\n\tif k.RoutingKey == \\\"random\\\" {\n\t\tid, err := uuid.NewV4()\n\t\tif err!= nil {\n\t\t\treturn \\\"\\\", err\n\t\t}\n\t\treturn id.String(), nil\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 70.09832209931713}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tutingTag!= \\\"\\\" {\n\t\tif tag, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn tag, nil\n\t\t}\n\t}\n\n\tif k.RoutingKey == \\\"random\\\" {\n\t\tid, err := uuid.NewV4()\n\t\tif err!= nil {\n\t\t\treturn \\\"\\\", err\n\t\t}\n\t\treturn id.String(), nil\n\t}\n\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 66.87235116754222}, {"code": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n\tutingTag!= \\\"\\\" {\n\t\tif v, ok := metric.GetTag(k.RoutingTag); ok {\n\t\t\treturn v, nil\n\t\t}\n\t}\n\tif k.RoutingKey == \\\"random\\\" {\n\t\treturn uuid.NewString()\n\t}\n\treturn k.RoutingKey, nil\n}", "CodeBleu_score": 50.21307168115546}]}
{"instance_id": "dc35e80e7f8d", "generation_result": [{"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\txps = make([]regexp.Regexp, len(k.TopicRegexps))\n\tfor i, reStr := range k.TopicRegexps {\n\t\tre, err := regexp.Compile(reStr)\n\t\tif err!= nil {\n\t\t\treturn fmt.Errorf(\\\"invalid topic regexp: %w\\\", err)\n\t\t}\n\t\tk.regexps[i] = *re\n\t}\n\treturn nil\n}", "CodeBleu_score": 48.97201928167279}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\txps = make([]regexp.Regexp, 0, len(k.TopicRegexps))\n\tfor _, re := range k.TopicRegexps {\n\t\tr, err := regexp.Compile(re)\n\\tif err!= nil {\n\\t\\treturn fmt.Errorf(\\\"invalid topic regexp %q: %w\\\", re, err)\n\\t}\n\\tk.regexps = append(k.regexps, *r)\n}\n\nreturn nil\n}", "CodeBleu_score": 62.85704696466081}, {"code": "func (k *KafkaConsumer) compileTopicRegexps() error{\n\txps = make([]regexp.Regexp, 0, len(k.TopicRegexps))\n\tfor _, r := range k.TopicRegexps {\n\t\tre, err := regexp.Compile(r)\n\t\tif err!= nil {\n\t\t\treturn fmt.Errorf(\\\"failed to compile regexp %q: %w\\\", r, err)\n\t\t}\n\t\tk.regexps = append(k.regexps, *re)\n\t}\n\treturn nil\n}", "CodeBleu_score": 73.28156731720466}]}
{"instance_id": "daf09ba144b2", "generation_result": [{"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\t\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tmsg, ok := h.undelivered[track.ID()]\n\tif!ok {\n\t\th.log.Errorf(\\\"received delivery for unknown message %v\\\", track.ID())\n\t\treturn\n\t}\n\n\tif!track.Delivered() {\n\t\treturn\n\t}\n\n\tmsg.session.MarkMessage(msg.message, \\\"\\\")\n\tdelete(h.undelivered, track.ID())\n\th.sem.release()\n}", "CodeBleu_score": 65.87741905268318}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\t\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tmsg, ok := h.undelivered[track.ID()]\n\tif!ok {\n\t\th.log.Debugf(\\\"dropping delivery for unknown message %v\\\", track.ID())\n\t\treturn\n\t}\n\n\tif!track.Delivered() {\n\t\th.log.Debugf(\\\"dropping undelivered message %v\\\", track.ID())\n\t\treturn\n\t}\n\n\tmsg.session.MarkMessage(msg.message, \\\"\\\")\n\tdelete(h.undelivered, track.ID())\n\th.sem <- empty{}\n}", "CodeBleu_score": 62.72896852799862}, {"code": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo){\n\t\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tmsg, ok := h.undelivered[track.ID()]\n\tif!ok {\n\t\th.log.Errorf(\\\"message delivery not found: %s\\\", track.ID())\n\t\treturn\n\t}\n\n\tif!track.Delivered() {\n\t\th.log.Errorf(\\\"message delivery failed: %s\\\", track.ID())\n\t\treturn\n\t}\n\n\tmsg.session.MarkMessage(msg.message, \\\"\\\")\n\tdelete(h.undelivered, track.ID())\n\th.sem <- empty{}\n}", "CodeBleu_score": 63.9210422132674}]}
{"instance_id": "8ecf2a1efead", "generation_result": [{"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase msg, ok := <-claim.Messages():\n\t\t\tif!ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\terr := h.Reserve(ctx)\n\t\t\tif err!= nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\terr = h.Handle(session, msg)\n\t\t\tif err!= nil {\n\t\t\t\th.log.Errorf(\\\"Error handling message: %s\\\", err)\n\t\t\t}\n\n\t\t\th.release()\n\t\t}\n\t}\n}", "CodeBleu_score": 60.97588339756621}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase msg, ok := <-claim.Messages():\n\t\t\tif!ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\terr := h.Reserve(ctx)\n\t\t\tif err!= nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = h.Handle(session, msg)\n\t\t\tif err!= nil {\n\t\t\t\th.log.Errorf(\\\"Failed to handle message: %v\\\", err)\n\t\t\t}\n\t\t\th.release()\n\t\t}\n\t}\n}", "CodeBleu_score": 60.92635688473973}, {"code": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error{\n\t\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase msg, ok := <-claim.Messages():\n\t\t\tif!ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\terr := h.Reserve(ctx)\n\t\t\tif err!= nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = h.Handle(session, msg)\n\t\t\tif err!= nil {\n\t\t\t\th.log.Errorf(\\\"Error handling message: %v\\\", err)\n\t\t\t}\n\t\t\th.release()\n\t\t}\n\t}\n}", "CodeBleu_score": 60.97588339756621}]}
{"instance_id": "eba9dafa99ab", "generation_result": [{"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\ncheckCondition(!logkafka_id.equals(\".\"), LogkafkaIdIsDot)\ncheckCondition(!logkafka_id.equals(\"..\"), LogkafkaIdIsDotDot)\ncheckCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\nrgx.findFirstIn(logkafka_id) match {\ncase Some(t) =>\ncheckCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\ncase None =>\ncheckCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n}\n}", "CodeBleu_score": 83.52495753170572}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\ncheckCondition(!logkafka_id.equals(\".\"), LogkafkaIdIsDot)\ncheckCondition(!logkafka_id.equals(\"..\"), LogkafkaIdIsDotDot)\ncheckCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\nrgx.findFirstIn(logkafka_id) match {\ncase Some(t) =>\ncheckCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\ncase None =>\ncheckCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n}\n}", "CodeBleu_score": 83.52495753170572}, {"code": "def validateLogkafkaId(logkafka_id: String){\n\tcheckCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\ncheckCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\ncheckCondition(!logkafka_id.equals(\".\"), LogkafkaIdIsDot)\ncheckCondition(!logkafka_id.equals(\"..\"), LogkafkaIdIsDotDot)\nrgx.findFirstIn(logkafka_id) match {\ncase Some(t) =>\ncheckCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\ncase None =>\ncheckCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n}\n}", "CodeBleu_score": 81.91507491070745}]}
{"instance_id": "45d5aefd3f6c", "generation_result": [{"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 5.120481927710843}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 5.120481927710843}, {"code": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 5.120481927710843}]}
{"instance_id": "a535ce861222", "generation_result": [{"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 8.47457627118644}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 8.47457627118644}, {"code": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] ={\n\t", "CodeBleu_score": 8.47457627118644}]}
{"instance_id": "ad37d3914acc", "generation_result": [{"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\t", "CodeBleu_score": 8.064516129032258}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\t", "CodeBleu_score": 8.064516129032258}, {"code": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] ={\n\t", "CodeBleu_score": 8.064516129032258}]}
{"instance_id": "3ecdb2c1534f", "generation_result": [{"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\timplicit val ec = apiExecutionContext\nwithKafkaManagerActor(\nKMClusterCommandRequest(\nclusterName,\nCMRunPreferredLeaderElectionWithAllTopics\n)\n) { result: Future[CMCommandResult] =>\nresult.map(cmr => toDisjunction(cmr.result))\n}\n}", "CodeBleu_score": 46.40732581455866}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\timplicit val ec = apiExecutionContext\nfor {\ntopics <- getTopics(clusterName)\nresult <- topics.fold(Future.successful(-\\/(ApiError.fromThrowable(new Exception(s\\\"Failed to get topics for cluster : $clusterName\\\")))))(runPreferredLeaderElection(clusterName, _))\n} yield result\n}", "CodeBleu_score": 53.025123810602956}, {"code": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) ={\n\timplicit val ec = apiExecutionContext\nwithKafkaManagerActor(KMClusterCommandRequest(clusterName, CMGetTopics)) { result: Future[CMCommandResult] =>\nresult.flatMap { cmr =>\ntoDisjunction(cmr.result).fold(\nerr => Future.successful(-\\/(err)),\ntopics => runPreferredLeaderElection(clusterName, topics.toSet)\n)\n}\n}\n}", "CodeBleu_score": 56.02541774907228}]}
{"instance_id": "5955ade118b9", "generation_result": [{"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\timplicit val ec = apiExecutionContext\nwithKafkaManagerActor(KMClusterCommandRequest(clusterName, CMManualPartitionAssignments(assignments))) { result: Future[CMCommandResult] =>\nresult.map { cmr =>\ncmr.result.fold(\n{ e =>\n-\\/(e)\n},\n{ _ =>\n\\/-(())\n}\n)\n}\n}\n}", "CodeBleu_score": 29.971875484804254}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\timplicit val ec = apiExecutionContext\nwithKafkaManagerActor(KMClusterCommandRequest(clusterName, CMManualPartitionAssignment(assignments))) { result: Future[CMCommandResult] =>\nresult.map { cmr =>\ncmr.result.fold(\nerrors => -\\/(errors.map(ApiError.from).toList),\n_ => \\/-(())\n)\n}\n}\n}", "CodeBleu_score": 31.588024620249914}, {"code": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = {\n\timplicit val ec = apiExecutionContext\nwithKafkaManagerActor(\nKMClusterCommandRequest(\nclusterName,\nCMManualPartitionAssignment(assignments)\n)\n) { result: Future[CMCommandResult] =>\nresult.map(cmr => toDisjunction(cmr.result))\n}\n}", "CodeBleu_score": 20.439911791965308}]}
{"instance_id": "26fbc2ba1d28", "generation_result": [{"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\t\tDescribeClusterResult describeClusterResult = admin.describeCluster();\n\tCollection<Node> nodes = describeClusterResult.nodes().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n\tif (nodes.isEmpty()) {\n\t\tthrow new ConnectException(\"No nodes available to describe cluster\");\n\t}\n\tNode node = nodes.iterator().next();\n\tDescribeConfigsResult describeConfigsResult = admin.describeConfigs(Collections.singleton(new ConfigResource(ConfigResource.Type.BROKER, node.id())));\n\tMap<ConfigResource, Config> configs = describeConfigsResult.all().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n\tif (configs.isEmpty()) {\n\t\tthrow new ConnectException(\"No configurations received for broker\");\n\t}\n\treturn configs.values().iterator().next();\n}", "CodeBleu_score": 65.19011283379427}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\t\tDescribeClusterResult describeClusterResult = admin.describeCluster();\n\tCollection<Node> nodes = describeClusterResult.nodes().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n\tif (nodes.isEmpty()) {\n\t\tthrow new ConnectException(\"No nodes available\");\n\t}\n\tNode node = nodes.iterator().next();\n\treturn admin.describeConfigs(Collections.singleton(new ConfigResource(ConfigResource.Type.BROKER, node.id()))).all().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS).get(new ConfigResource(ConfigResource.Type.BROKER, node.id()));\n}", "CodeBleu_score": 46.8638355198523}, {"code": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception{\n\t\tDescribeClusterResult describeClusterResult = admin.describeCluster();\n\tCollection<Node> nodes = describeClusterResult.nodes().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n\tif (nodes.isEmpty()) {\n\t\tthrow new ConnectException(String.format(\"No nodes found in Kafka cluster at %s\", producerConfig.getString(BOOTSTRAP_SERVERS)));\n\t}\n\tDescribeConfigsResult describeConfigsResult = admin.describeConfigs(Collections.singleton(new ConfigResource(ConfigResource.Type.BROKER, nodes.iterator().next().id())));\n\tMap<ConfigResource, Config> configs = describeConfigsResult.all().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n\tif (configs.isEmpty()) {\n\t\tthrow new ConnectException(String.format(\"No configuration found for Kafka broker at %s\", producerConfig.getString(BOOTSTRAP_SERVERS)));\n\t}\n\treturn configs.values().iterator().next();\n}", "CodeBleu_score": 59.14654394146179}]}
{"instance_id": "adf40b68c93e", "generation_result": [{"code": "private static Validator forKafka(final Validator validator){\n\treturn (config, name) -> {\nif (config.getString(SCHEMA_HISTORY_CONFIG).equals(KAFKA_SCHEMA_HISTORY_CONFIG)) {\nreturn validator.validate(config, name);\n}\nreturn 0;\n};\n}", "CodeBleu_score": 20.73354282921127}, {"code": "private static Validator forKafka(final Validator validator){\n\treturn (config, name) -> {\nfinal String schemaHistory = config.getString(SCHEMA_HISTORY_CONFIG);\nif (KafkaSchemaHistory.class.getName().equals(schemaHistory)) {\nreturn validator.ensureValid(config, name);\n}\nreturn 0;\n};\n}", "CodeBleu_score": 40.70377497569366}, {"code": "private static Validator forKafka(final Validator validator){\n\t\treturn config -> {\n\t\tif (config.getString(SCHEMA_HISTORY_CONFIG_NAME).equals(KAFKA_SCHEMA_HISTORY_CONFIG_VALUE)) {\n\t\t\treturn validator.apply(config);\n\t\t}\n\t\treturn 0;\n\t};\n}", "CodeBleu_score": 14.253176348486768}]}
{"instance_id": "07ff1ca41c93", "generation_result": [{"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tn kafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n\tboolean topicCreationEnabled = kafkaConnectVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0;\n\tif (config.containsKey(\"topic.creation.enable\")) {\n\t\ttopicCreationEnabled = (boolean) config.get(\"topic.creation.enable\");\n\t}\n\treturn topicCreationEnabled;\n}", "CodeBleu_score": 46.78807841252093}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tn kafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n\tif (kafkaConnectVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) >= 0) {\n\t\tObject topicCreationEnabled = config.getOrDefault(\"topic.creation.enable\", true);\n\t\treturn (Boolean) topicCreationEnabled;\n\t}\n\treturn false;\n}", "CodeBleu_score": 42.33095697529645}, {"code": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config){\n\tn topicCreationEnabled = true;\n\tVersion kafkaConnectVersion = null;\n\ttry {\n\t\tkafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n\t} catch (Exception e) {\n\t\tthrow new DebeziumException(\"Unable to parse Kafka Connect version\", e);\n\t}\n\tif (kafkaConnectVersion.compareTo(TOPIC_CREATION_KAFKA_VERSION) < 0) {\n\t\ttopicCreationEnabled = false;\n\t} else {\n\t\ttopicCreationEnabled = (Boolean) config.getOrDefault(\"topic.creation.enable\", true);\n\t}\n\treturn topicCreationEnabled;\n}", "CodeBleu_score": 45.43844829989857}]}
{"instance_id": "e2cc420e1e63", "generation_result": [{"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\tif (config.get(CLIENT_ID_CONFIG) == null) {\nthrow new DebeziumException(\"Client ID must be set for KafkaOffsetBackingStore\");\n}\n\nMap<String, Object> adminProps = new HashMap<>();\nadminProps.put(CLIENT_ID_CONFIG, config.get(CLIENT_ID_CONFIG));\nStream.of(DistributedConfig.BOOTSTRAP_SERVERS_CONFIG, DistributedConfig.REPLICATION_FACTOR_CONFIG, DistributedConfig.OFFSET_STORAGE_TOPIC_REPLICATION_FACTOR_CONFIG).forEach(key -> {\nif (config.get(key) == null) {\nthrow new DebeziumException(\"Configuration option \" + key + \" must be set for KafkaOffsetBackingStore\");\n}\nadminProps.put(key, config.get(key));\n});\n\nSharedTopicAdmin sharedTopicAdmin = new SharedTopicAdmin(adminProps);\nreturn new KafkaOffsetBackingStore(sharedTopicAdmin, config.get(CLIENT_ID_CONFIG), converterForOffsetStore());\n}", "CodeBleu_score": 46.31327178229759}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\t\n    if (config.get(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG) == null) {\n        throw new DebeziumException(\\\"The \\\" + DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG + \\\" configuration must be set.\\\");\n    }\n    if (config.get(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG) == null) {\n        throw new DebeziumException(\\\"The \\\" + DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG + \\\" configuration must be set.\\\");\n    }\n    if (config.get(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG) == null) {\n        throw new DebeziumException(\\\"The \\\" + DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG + \\\" configuration must be set.\\\");\n    }\n\n    final Map<String, String> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, \\\"debezium-admin\\\");\n\n    final SharedTopicAdmin admin = new SharedTopicAdmin(adminProps);\n    return new KafkaOffsetBackingStore(admin, CLIENT_ID_CONFIG, converterForOffsetStore());\n}", "CodeBleu_score": 25.748300265411117}, {"code": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config){\n\t\n        String clientId = config.get(DistributedConfig.CONNECT_CLIENT_ID_CONFIG);\n        if (clientId == null) {\n            throw new DebeziumException(\\\"Missing required configuration option: \\\" + DistributedConfig.CONNECT_CLIENT_ID_CONFIG);\n        }\n\n        Map<String, Object> adminProps = new HashMap<>(config);\n        adminProps.put(CLIENT_ID_CONFIG, clientId + \\\"-admin\\\");\n\n        return new KafkaOffsetBackingStore(new SharedTopicAdmin(adminProps), clientId, converterForOffsetStore());\n    }", "CodeBleu_score": 28.944315477790063}]}
{"instance_id": "4ecb2279231e", "generation_result": [{"code": "private T readKey(JsonNode node) throws IOException{\n\t(node.isObject()) {\n\t\tJsonNode payload = node.get(PAYLOAD_FIELD);\n\t\tif (payload!= null) {\n\t\t\tnode = payload;\n\t\t}\n\t}\n\tIterator<String> fieldNames = node.fieldNames();\n\tif (fieldNames.hasNext()) {\n\t\tString fieldName = fieldNames.next();\n\t\tif (!fieldNames.hasNext()) {\n\t\t\treturn reader.readValue(node.get(fieldName));\n\t\t}\n\t}\n\treturn reader.readValue(node);\n}", "CodeBleu_score": 46.08134905292923}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tnode.isObject()) {\nJsonNode payload = node.get(PAYLOAD_FIELD);\nif (payload!= null) {\nnode = payload;\n}\n}\nIterator<String> fieldNames = node.fieldNames();\nif (fieldNames.hasNext()) {\nString fieldName = fieldNames.next();\nif (!fieldNames.hasNext()) {\nreturn reader.readValue(node.get(fieldName));\n}\n}\nreturn reader.readValue(node);\n}", "CodeBleu_score": 44.599563846033426}, {"code": "private T readKey(JsonNode node) throws IOException{\n\tnode.isObject()) {\nJsonNode payload = node.get(PAYLOAD_FIELD);\n\n// Schema + payload format\nif (payload!= null) {\nnode = payload;\n}\n}\n\nIterator<String> fieldNames = node.fieldNames();\nif (fieldNames.hasNext()) {\nString fieldName = fieldNames.next();\nif (!fieldNames.hasNext()) {\nreturn reader.readValue(node.get(fieldName));\n}\n}\n\nreturn reader.readValue(node);\n}", "CodeBleu_score": 44.599563846033426}]}
{"instance_id": "b8ba1bfc7b5e", "generation_result": [{"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\nStringBuilder sb = new StringBuilder();\nfor (char c : topicName.toCharArray()) {\nif (isValidTopicNameChar(c)) {\nsb.append(c);\n} else {\nsb.append(REPLACEMENT_CHAR);\n}\n}\nString sanitized = sb.toString();\nif (!topicName.equals(sanitized)) {\nLOGGER.warn(\\\"Sanitized topic name for {} from {} to {}\\\", id, topicName, sanitized);\n}\nreturn sanitized;\n}", "CodeBleu_score": 36.527347218130394}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\nStringBuilder sanitizedTopicName = new StringBuilder();\nfor (char c : topicName.toCharArray()) {\nif (c == '-' || c == '.' || c == '_' || (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || (c >= '0' && c <= '9')) {\nsanitizedTopicName.append(c);\n} else {\nLOGGER.warn(\\\"Invalid character '{}' in topic name '{}', replacing with '_'\\\", c, topicName);\nsanitizedTopicName.append(REPLACEMENT_CHAR);\n}\n}\nreturn sanitizedTopicName.toString();\n}", "CodeBleu_score": 34.56768610405439}, {"code": "public String topicNameFor(I id, String prefix, String delimiter){\n\tString topicName = delegate.topicNameFor(id, prefix, delimiter);\nStringBuilder sb = new StringBuilder(topicName);\nfor (int i = 0; i < sb.length(); i++) {\nchar c = sb.charAt(i);\nif (!TopicNamingStrategy.isTopicNameValid(c)) {\nsb.setCharAt(i, REPLACEMENT_CHAR);\n}\n}\nif (sb.toString().equals(topicName)) {\nreturn topicName;\n} else {\nLOGGER.warn(\\\"Sanitized topic name for {}: {} -> {}\\\", id, topicName, sb.toString());\nreturn sb.toString();\n}\n}", "CodeBleu_score": 41.641568965654116}]}
{"instance_id": "9cf0d74c098d", "generation_result": [{"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\t", "CodeBleu_score": 10.29977892531036}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\t", "CodeBleu_score": 10.29977892531036}, {"code": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n\tthis.connectorConfig = connectorConfig;\nthis.queue = queue;\nthis.producerThrowable = new AtomicReference<>();\nthis.maxRetries = connectorConfig.getRetries();\nif (replacedErrorHandler!= null) {\n    this.retries = replacedErrorHandler.retries;\n}\n}", "CodeBleu_score": 62.75634900759207}]}
{"instance_id": "8ab4bf3a4dd8", "generation_result": [{"code": "public void validate(Configuration configuration, Field.Set fields){\n\tnfiguration.validateAndRecord();\n\tfor (Map.Entry<String, ConfigValue> entry : configuration.validationResults().entrySet()) {\n\t\tif (entry.getValue().errorMessages()!= null &&!entry.getValue().errorMessages().isEmpty()) {\n\t\t\tthrow new ConfigException(entry.getKey(), entry.getValue().errorMessages().get(0));\n\t\t}\n\t}\n}", "CodeBleu_score": 41.836781223991196}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\t(configuration == null) {\nthrow new NullPointerException(\"configuration must not be null\");\n}\nif (fields == null) {\nthrow new NullPointerException(\"fields must not be null\");\n}\nfor (Field field : fields) {\nConfigValue configValue = configuration.getConfigValue(field);\nif (configValue == null) {\nthrow new NullPointerException(\"configValue must not be null\");\n}\nif (configValue.errorMessages().size() > 0) {\nthrow new ConfigException(configValue.errorMessages().get(0));\n}\n}\n}", "CodeBleu_score": 31.04442645830231}, {"code": "public void validate(Configuration configuration, Field.Set fields){\n\t (configuration == null) {\n\t\tthrow new NullPointerException(\"configuration must not be null\");\n\t}\n\tif (fields == null) {\n\t\tthrow new NullPointerException(\"fields must not be null\");\n\t}\n\tfor (Field field : fields) {\n\t\tConfigValue configValue = configuration.getConfigValue(field.name());\n\t\tif (configValue == null) {\n\t\t\tthrow new NullPointerException(\"configValue must not be null\");\n\t\t}\n\t\tif (configValue.errorMessages()!= null &&!configValue.errorMessages().isEmpty()) {\n\t\t\tthrow new ConfigException(field.name(), configValue.value(), configValue.errorMessages().iterator().next());\n\t\t}\n\t}\n}", "CodeBleu_score": 35.5018050623279}]}
{"instance_id": "860405dc9abb", "generation_result": [{"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t\nif (oldSchema.isPrimitive()) {\nreturn oldSchema;\n}\n\nSchemaBuilder schemaBuilder = SchemaBuilder.struct().name(oldSchema.name());\n\nfor (org.apache.kafka.connect.data.Field field : oldSchema.fields()) {\nif (isContainedIn(field.name(), nestedFields)) {\nschemaBuilder.field(field.name(), buildNewSchema(field.name(), field.schema(), headerToProcess, nestedFields, ++level));\n} else {\nschemaBuilder.field(field.name(), field.schema());\n}\n}\n\nfor (int i = 0; i < headers.size(); i++) {\n\nHeader currentHeader = headerToProcess.get(headers.get(i));\n\nif (currentHeader!= null) {\nOptional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\nfieldNameToAdd.ifPresent(s -> schemaBuilder.field(s, currentHeader.schema()));\n}\n}\n\nreturn schemaBuilder.build();\n}", "CodeBleu_score": 54.06793421824803}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t\tSchemaBuilder builder = SchemaBuilder.struct();\n\tfor (org.apache.kafka.connect.data.Field field : oldSchema.fields()) {\n\t\tif (oldSchema.field(field.name())!= null) {\n\t\t\tif (isContainedIn(field.name(), nestedFields)) {\n\t\t\t\tSchema nestedFieldSchema = requireStruct(oldSchema.field(field.name()).schema(), \"Nested field\").schema();\n\t\t\t\tbuilder.field(field.name(), buildNewSchema(field.name(), nestedFieldSchema, headerToProcess, nestedFields, ++level));\n\t\t\t} else {\n\t\t\t\tbuilder.field(field.name(), oldSchema.field(field.name()).schema());\n\t\t\t}\n\t\t}\n\t}\n\tfor (int i = 0; i < headers.size(); i++) {\n\t\tHeader currentHeader = headerToProcess.get(headers.get(i));\n\t\tif (currentHeader!= null) {\n\t\t\tOptional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n\t\t\tfieldNameToAdd.ifPresent(s -> {\n\t\t\t\tbuilder.field(s, currentHeader.schema());\n\t\t\t\tLOGGER.debug(\"Added field {} with schema {} to schema {}\", s, currentHeader.schema(), builder.schema());\n\t\t\t});\n\t\t}\n\t}\n\treturn builder.schema();\n}", "CodeBleu_score": 50.84815798662377}, {"code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level){\n\t\nif (oldSchema.isPrimitive()) {\n\nLOGGER.trace(\\\"Schema is primitive, returning old schema\\\");\n\nreturn oldSchema;\n\n}\n\nSchemaBuilder builder = SchemaBuilder.struct().name(oldSchema.name()).version(oldSchema.version()).doc(oldSchema.doc());\n\nfor (org.apache.kafka.connect.data.Field field : oldSchema.fields()) {\n\nif (isContainedIn(field.name(), nestedFields)) {\n\nLOGGER.trace(\\\"Nested field found: {}\\\", field.name());\n\nbuilder.field(field.name(), buildNewSchema(field.name(), field.schema(), headerToProcess, nestedFields, ++level));\n\n} else {\n\nLOGGER.trace(\\\"Field not nested: {}\\\", field.name());\n\nbuilder.field(field.name(), field.schema());\n\n}\n\n}\n\nfor (int i = 0; i < headers.size(); i++) {\n\nHeader currentHeader = headerToProcess.get(headers.get(i));\n\nif (currentHeader!= null) {\n\nOptional<String> fieldNameToAdd = getFieldName(fields.get(i), fieldName, level);\n\nfieldNameToAdd.ifPresent(s -> {\n\nLOGGER.trace(\\\"Adding field {} to schema\\\", s);\n\nbuilder.field(s, Schema.STRING_SCHEMA);\n\n});\n\n}\n\n}\n\nreturn builder.build();\n\n}", "CodeBleu_score": 53.2273627795699}]}
{"instance_id": "681a0415df83", "generation_result": [{"code": "public static SnapshotRecord fromSource(Struct source){\n\t\tString snapshotString = source.getString(SNAPSHOT_KEY);\n\tif (snapshotString!= null) {\n\t\tsnapshotString = snapshotString.toUpperCase();\n\t\tif (snapshotString.equals(TRUE.toString())) {\n\t\t\treturn TRUE;\n\t\t} else if (snapshotString.equals(FIRST.toString())) {\n\t\t\treturn FIRST;\n\t\t} else if (snapshotString.equals(FIRST_IN_DATA_COLLECTION.toString())) {\n\t\t\treturn FIRST_IN_DATA_COLLECTION;\n\t\t} else if (snapshotString.equals(LAST_IN_DATA_COLLECTION.toString())) {\n\t\t\treturn LAST_IN_DATA_COLLECTION;\n\t\t} else if (snapshotString.equals(LAST.toString())) {\n\t\t\treturn LAST;\n\t\t} else if (snapshotString.equals(FALSE.toString())) {\n\t\t\treturn FALSE;\n\t\t} else if (snapshotString.equals(INCREMENTAL.toString())) {\n\t\t\treturn INCREMENTAL;\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 34.090708789430906}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\tif (source == null) {\nreturn null;\n}\nif (!source.schema().field(\"SNAPSHOT_KEY\").schema().isEnum()) {\nreturn null;\n}\nString snapshotString = source.getString(\"SNAPSHOT_KEY\");\nif (snapshotString == null) {\nreturn null;\n}\nswitch (snapshotString.toUpperCase()) {\ncase \"TRUE\":\nreturn TRUE;\ncase \"FIRST\":\nreturn FIRST;\ncase \"FIRST_IN_DATA_COLLECTION\":\nreturn FIRST_IN_DATA_COLLECTION;\ncase \"LAST_IN_DATA_COLLECTION\":\nreturn LAST_IN_DATA_COLLECTION;\ncase \"LAST\":\nreturn LAST;\ncase \"FALSE\":\nreturn FALSE;\ncase \"INCREMENTAL\":\nreturn INCREMENTAL;\ndefault:\nreturn null;\n}\n}", "CodeBleu_score": 33.82283535392034}, {"code": "public static SnapshotRecord fromSource(Struct source){\n\t\nif (source == null ||!source.schema().isStruct() ||!source.schema().field(\"SNAPSHOT_KEY\").hasLogicalType()) {\nreturn null;\n}\n\nString snapshotString = source.getString(\"SNAPSHOT_KEY\");\nif (snapshotString == null) {\nreturn null;\n}\n\nswitch (snapshotString.toUpperCase()) {\ncase \"TRUE\":\nreturn TRUE;\ncase \"FIRST\":\nreturn FIRST;\ncase \"FIRST_IN_DATA_COLLECTION\":\nreturn FIRST_IN_DATA_COLLECTION;\ncase \"LAST_IN_DATA_COLLECTION\":\nreturn LAST_IN_DATA_COLLECTION;\ncase \"LAST\":\nreturn LAST;\ncase \"FALSE\":\nreturn FALSE;\ncase \"INCREMENTAL\":\nreturn INCREMENTAL;\ndefault:\nreturn null;\n}\n}", "CodeBleu_score": 35.069380653779405}]}
{"instance_id": "85ef1162e38a", "generation_result": [{"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\t\tString[] versionParts = version.split(\"\\\\.\");\n\tif (versionParts.length > 2) {\n\t\tversionParts = new String[] {versionParts[0], versionParts[1]};\n\t}\n\tString[] versionPartsWithBuild = versionParts[1].split(\"-\");\n\treturn Optional.of(Float.parseFloat(versionParts[0] + \".\" + versionPartsWithBuild[0]));\n}", "CodeBleu_score": 46.51493318809372}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\t\treturn Optional.of(version).map(v -> v.split(\"\\\\.\")).map(v -> v.length > 2? v[0] + \".\" + v[1] : v[0]).map(v -> v.split(\"-\")).map(v -> Float.parseFloat(v[0])).map(Optional::of).orElse(Optional.empty());\n}", "CodeBleu_score": 31.68513601634997}, {"code": "public static Optional<Float> parse(String version) throws NumberFormatException{\n\t\tif (version == null) {\n\t\treturn Optional.empty();\n\t}\n\tString[] versionParts = version.split(\"\\\\.\");\n\tif (versionParts.length > 2) {\n\t\tversionParts = Arrays.copyOfRange(versionParts, 0, 2);\n\t}\n\tString[] versionPartsWithPatch = versionParts[1].split(\"-\");\n\tFloat versionFloat = Float.parseFloat(versionParts[0] + \".\" + versionPartsWithPatch[0]);\n\treturn Optional.of(versionFloat);\n}", "CodeBleu_score": 47.973672332741124}]}
{"instance_id": "64346d69eba0", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tontext context = AccessContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.CREATE)\n   .operationName(\"createConnector\")\n   .build();\n\nreturn validateAccess(context)\n   .then(connector.flatMap(connectorDTO -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, connectorDTO)))\n   .thenReturn(ResponseEntity.ok().build())\n   .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 67.1363038463317}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\tvar context = AccessContext.builder()\n       .cluster(clusterName)\n       .connect(connectName)\n       .connectActions(ConnectAction.CREATE)\n       .operationName(\\\"createConnector\\\")\n       .build();\n\n    return validateAccess(context)\n       .then(connector.flatMap(dto -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, dto)))\n       .thenReturn(ResponseEntity.ok().build())\n       .doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 68.12334157368869}, {"code": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange){\n\ttext = AccessContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.CREATE)\n   .operationName(\"createConnector\")\n   .build();\n\nreturn validateAccess(context)\n   .then(connector.flatMap(c -> kafkaConnectService.createConnector(getCluster(clusterName), connectName, c)))\n   .thenReturn(ResponseEntity.ok())\n   .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 65.5859047436223}]}
{"instance_id": "4c446fd64db6", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n   .connector(connectorName)\n   .operationName(\"deleteConnector\")\n   .build();\n\nreturn validateAccess(context).then(\n    kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n       .map(ResponseEntity::ok)\n).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 58.58479133936899}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n   .connector(connectorName)\n   .operationName(\"deleteConnector\")\n   .build();\n\nreturn validateAccess(context).then(\n    kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n       .then(Mono.just(ResponseEntity.ok().build()))\n).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 52.35295918552496}, {"code": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange){\n\tContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.VIEW, ConnectAction.DELETE)\n   .connector(connectorName)\n   .operationName(\"deleteConnector\")\n   .build();\n\nreturn validateAccess(context).then(\n    kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n       .then(Mono.just(ResponseEntity.noContent().build()))\n).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 52.43721541230245}]}
{"instance_id": "d34a49f3ca1b", "generation_result": [{"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t\nvar context = AccessContext.builder()\n   .cluster(clusterName)\n   .connect(connectName)\n   .connectActions(ConnectAction.VIEW, ConnectAction.EDIT)\n   .operationName(\"updateConnectorState\")\n   .operationParams(Map.of(CONNECTOR_NAME, connectorName))\n   .build();\n\nreturn validateAccess(context).then(\n    kafkaConnectService.updateConnectorState(getCluster(clusterName), connectName, connectorName, action)\n       .map(ResponseEntity::ok)\n).doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 55.681362127274845}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t\n    var context = AccessContext.builder()\n       .cluster(clusterName)\n       .connect(connectName)\n       .connectActions(ConnectAction.VIEW, ConnectAction.EDIT)\n       .operationName(\\\"updateConnectorState\\\")\n       .operationParams(Map.of(CONNECTOR_NAME, connectorName))\n       .build();\n\n    var connectAction = action.getAction();\n    return validateAccess(context)\n       .then(kafkaConnectService.updateConnectorState(getCluster(clusterName), connectName, connectorName, connectAction))\n       .map(ResponseEntity::ok)\n       .doOnEach(sig -> audit(context, sig));\n  }", "CodeBleu_score": 54.15827334180774}, {"code": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n\t\nvar context = AccessContext.builder()\n       .cluster(clusterName)\n       .connect(connectName)\n       .connectActions(ConnectAction.VIEW, ConnectAction.EDIT)\n       .operationName(\"updateConnectorState\")\n       .operationParams(Map.of(CONNECTOR_NAME, connectorName))\n       .build();\n\nreturn validateAccess(context).then(\n        kafkaConnectService.updateConnectorState(getCluster(clusterName), connectName, connectorName, action)\n           .map(ResponseEntity::ok))\n   .doOnEach(sig -> audit(context, sig));\n}", "CodeBleu_score": 55.0870582845699}]}
{"instance_id": "199cfe41b374", "generation_result": [{"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\t\t@Override\n\tpublic Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig(String clusterName, String connectName, String pluginName, Mono<Map<String, Object>> requestBody, ServerWebExchange exchange) {\n\t\tvar context = AccessContext.builder()\n\t\t\t\t.cluster(clusterName)\n\t\t\t\t.connect(connectName)\n\t\t\t\t.connectActions(ConnectAction.VIEW)\n\t\t\t\t.operationName(\"validateConnectorPluginConfig\")\n\t\t\t\t.build();\n\n\t\treturn validateAccess(context).then(\n\t\t\t\tkafkaConnectService.validateConnectorPluginConfig(getCluster(clusterName), connectName, pluginName, requestBody)\n\t\t\t\t\t\t.map(ResponseEntity::ok)\n\t\t).doOnEach(sig -> audit(context, sig));\n\t}\n}", "CodeBleu_score": 62.83551273794053}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\t\treturn kafkaConnectService.validateConnectorPluginConfig(getCluster(clusterName), connectName, pluginName, requestBody)\n\t\t.map(ResponseEntity::ok);\n}", "CodeBleu_score": 37.0605374333078}, {"code": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig{\n\t", "CodeBleu_score": 7.1690436822820445}]}
{"instance_id": "88380ae300c4", "generation_result": [{"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\t\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n\t\t\t.initializers(context -> context.getBeanFactory().registerSingleton(DynamicConfigOperations.BEAN_NAME, new DynamicConfigOperations()))\n\t\t\t.run(args);\n}", "CodeBleu_score": 40.296558692186025}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\t\treturn new SpringApplicationBuilder(KafkaUiApplication.class)\n\t\t\t.initializers(context -> context.getBeanFactory().registerSingleton(\"dynamicConfigOperations\", new DynamicConfigOperations()))\n\t\t\t.run(args);\n}", "CodeBleu_score": 40.78093525630116}, {"code": "public static ConfigurableApplicationContext startApplication(String[] args){\n\t\treturn new SpringApplicationBuilder(KafkaUiApplication.class).initializers(new DynamicConfigOperations()).run(args);\n}", "CodeBleu_score": 31.3959941740324}]}
{"instance_id": "3f022876f737", "generation_result": [{"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\t == null) {\n\t\treturn FullConnectorInfoDTO::matchesSearchTerm;\n\t}\n\n\treturn connector -> connector.matchesSearchTerm(search);\n}", "CodeBleu_score": 23.30030173343674}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\tch == null\n? (dto) -> true\n: (dto) -> Stream.of(dto.getConnector().getName(), dto.getConnector().getConnectorClass(), dto.getConnector().getType())\n.anyMatch(s -> StringUtils.containsIgnoreCase(s, search));\n}", "CodeBleu_score": 40.93839330534599}, {"code": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search){\n\trch == null?\n\t\tconnector -> true :\n\t\tconnector -> Stream.of(connector.getConnector().getName(), connector.getConnector().getConnectorClass(), connector.getConnector().getType(), connector.getConnector().getConnectorClass())\n\t\t\t.anyMatch(str -> StringUtils.containsIgnoreCase(str, search));\n}", "CodeBleu_score": 39.70990924789197}]}
{"instance_id": "9754aa1ec96c", "generation_result": [{"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\t", "CodeBleu_score": 11.217587828108096}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\t   if (prefix == null) {\n      prefix = \\\"\\\";\n    }\n\n    Map<String, Object> flattened = new HashMap<>();\n\n    if (propertiesMap!= null) {\n      for (Map.Entry<String, Object> entry : propertiesMap.entrySet()) {\n        String key = entry.getKey();\n        Object value = entry.getValue();\n        String newKey = prefix + key;\n\n        if (value instanceof Map) {\n          flattened.putAll(flattenClusterProperties(newKey + \\\".\\\", (Map<String, Object>) value));\n        } else {\n          flattened.put(newKey, value);\n        }\n      }\n    }\n\n    return flattened;\n  }", "CodeBleu_score": 34.43993469156996}, {"code": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap){\n\t", "CodeBleu_score": 11.217587828108096}]}
{"instance_id": "99266677c846", "generation_result": [{"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tef configDef = new ConfigDef();\n configDef.define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_KEYSTORE_PASSWORD_DOC);\n configDef.define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_KEY_PASSWORD_DOC);\n configDef.define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_TRUSTSTORE_PASSWORD_DOC);\n configDef.define(SaslConfigs.SASL_JAAS_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_JAAS_DOC);\n configDef.define(SaslConfigs.SASL_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_PASSWORD_DOC);\n return configDef.names().stream()\n.filter(name -> configDef.type(name) == ConfigDef.Type.PASSWORD)\n.collect(Collectors.toSet());\n }", "CodeBleu_score": 50.893194455729464}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\t", "CodeBleu_score": 2.9334771606863974}, {"code": " private static Set<String> kafkaConfigKeysToSanitize(){\n\tef configDef = new ConfigDef();\n configDef.addImportingConfig(SslConfigs.config());\n configDef.addImportingConfig(SaslConfigs.config());\n return configDef.configKeys().stream()\n.filter(key -> key.type() == ConfigDef.Type.PASSWORD)\n.map(ConfigDef.ConfigKey::name)\n.collect(Collectors.toSet());\n }", "CodeBleu_score": 59.406598616720885}]}
{"instance_id": "deebb6fcb0a8", "generation_result": [{"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\tn adminClient.aclView(clusterDescription)\n.flatMap(aclView -> {\nif (!aclView.isAclViewEnabled()) {\nreturn Mono.empty();\n}\nSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\nif (authorizedOperations.contains(AclOperation.ALL) || authorizedOperations.contains(AclOperation.ALTER)) {\nreturn Mono.just(ClusterFeature.KAFKA_ACL_EDIT);\n}\nreturn Mono.empty();\n});\n}", "CodeBleu_score": 37.546058624557546}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\trn adminClient.isAclViewEnabled() && (clusterDescription.getAuthorizedOperations().contains(AclOperation.ALL) || clusterDescription.getAuthorizedOperations().contains(AclOperation.ALTER))? Mono.just(ClusterFeature.KAFKA_ACL_EDIT) : Mono.empty();\n}", "CodeBleu_score": 34.34895874638122}, {"code": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription){\n\trn aclView(adminClient).flatMap(feature -> {\n\t\tSet<AclOperation> authorizedOperations = clusterDescription.getAuthorizedOperations();\n\t\tif (authorizedOperations.contains(AclOperation.ALL) || authorizedOperations.contains(AclOperation.ALTER)) {\n\t\t\treturn Mono.just(ClusterFeature.KAFKA_ACL_EDIT);\n\t\t}\n\t\treturn Mono.empty();\n\t});\n}", "CodeBleu_score": 38.28080485295653}]}
{"instance_id": "a5d8cc77955f", "generation_result": [{"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\tTreeMap<TopicPartition, FromToOffset> nextRange = new TreeMap<>(Comparator.comparing(TopicPartition::partition));\nMap<TopicPartition, Long> offsetsToReadFrom = prevRange.isEmpty()? seekOperations.getOffsetsToReadFrom() : prevRange.values().stream()\n.collect(Collectors.toMap(FromToOffset::getTopicPartition, FromToOffset::getToOffset));\nint messagesToPollPerPartition = messagesPerPage / offsetsToReadFrom.size();\nfor (Map.Entry<TopicPartition, Long> entry : offsetsToReadFrom.entrySet()) {\nTopicPartition topicPartition = entry.getKey();\nlong startOffset = Math.max(0, entry.getValue() - messagesToPollPerPartition);\nnextRange.put(topicPartition, new FromToOffset(topicPartition, startOffset, entry.getValue()));\n}\nreturn nextRange;\n}", "CodeBleu_score": 42.0876023539689}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\t", "CodeBleu_score": 3.709413387662834}, {"code": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations){\n\t\n/**\n * The nextPollingRange function calculates the next range of offsets to poll for each topic partition.\nIt initializes a map of offsets to read from based on the previous range or the offsets from seek operations if the previous range is empty.\nIt calculates the number of messages to poll per partition.\nFor each topic partition, it determines the start offset and adjusts it based on the number of messages to poll, ensuring it does not go below the beginning offset.\nThe function returns a map of topic partitions to their respective polling ranges.\n */\nprotected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations) {\n    TreeMap<TopicPartition, Long> fromOffsets = new TreeMap<>(Comparator.comparing(TopicPartition::partition));\n    if (prevRange!= null &&!prevRange.isEmpty()) {\n\n      fromOffsets.putAll(prevRange.values().stream().collect(Collectors.toMap(FromToOffset::getTopicPartition, FromToOffset::getToOffset)));\n    } else {\n\n      fromOffsets.putAll(seekOperations.getSeekOperations().stream().collect(Collectors.toMap(SeekOperation::getTopicPartition, SeekOperation::getOffset)));\n    }\n    long messagesPerPartition = messagesPerPage / fromOffsets.size();\n    TreeMap<TopicPartition, FromToOffset> pollingRange = new TreeMap<>();\n    for (Map.Entry<TopicPartition, Long> entry : fromOffsets.entrySet()) {\n\n      long fromOffset = entry.getValue();\n      if (fromOffset - messagesPerPartition > 0) {\n\n        fromOffset -= messagesPerPartition;\n      }\n      pollingRange.put(entry.getKey(), new FromToOffset(entry.getKey(), fromOffset, entry.getValue()));\n    }\n    return pollingRange;\n  }\n}", "CodeBleu_score": 38.412391628669255}]}
{"instance_id": "c4c657caeba5", "generation_result": [{"code": "public void doLaunch(final MessageInput input){\n\t", "CodeBleu_score": 2.3542208163979508}, {"code": "public void doLaunch(final MessageInput input){\n\t", "CodeBleu_score": 2.3542208163979508}, {"code": "public void doLaunch(final MessageInput input){\n\t\tfinal boolean legacyMode = configuration.getBoolean(CK_LEGACY);\n\tif (legacyMode) {\n\t\tif (Strings.isNullOrEmpty(configuration.getString(CK_ZOOKEEPER))) {\n\t\t\tthrow new ConfigurationException(\"Legacy mode requires a ZooKeeper connection string.\");\n\t\t}\n\t} else {\n\t\tif (Strings.isNullOrEmpty(configuration.getString(CK_BOOTSTRAP))) {\n\t\t\tthrow new ConfigurationException(\"Bootstrap servers must be configured.\");\n\t\t}\n\t}\n\n\tserverStatus.waitForRunning();\n\tserverEventBus.register(this);\n\n\tif (legacyMode) {\n\t\tlaunchLegacyInput(input);\n\t} else {\n\t\tlaunchConsumerInput(input);\n\t}\n\n\tscheduler.scheduleAtFixedRate(new Runnable() {\n\t\t@Override\n\t\tpublic void run() {\n\t\t\tlastSecBytesRead.set(lastSecBytesReadTmp.get());\n\t\t\tlastSecBytesReadTmp.set(0);\n\t\t}\n\t}, 1, 1, TimeUnit.SECONDS);\n}", "CodeBleu_score": 43.97557292412665}]}
{"instance_id": "dffe85dbb47b", "generation_result": [{"code": "private void insertCustomProperties(Properties props){\n\t\ttry {\n\t\tfinal Properties customProps = new Properties();\n\t\tcustomProps.load(new StringReader(configuration.getString(CK_CUSTOM_PROPERTIES)));\n\t\tprops.putAll(customProps);\n\t} catch (IOException e) {\n\t\tLOG.error(\"Error loading custom properties: \" + e.getMessage());\n\t}\n}", "CodeBleu_score": 38.76528822841562}, {"code": "private void insertCustomProperties(Properties props){\n\t\ttry {\n\t\tfinal Properties customProps = new Properties();\n\t\tfinal String customPropsStr = configuration.getString(CK_CUSTOM_PROPERTIES);\n\t\tif (customPropsStr!= null) {\n\t\t\tcustomProps.load(new StringReader(customPropsStr));\n\t\t}\n\t\tprops.putAll(customProps);\n\t} catch (IOException e) {\n\t\tLOG.error(\"Error loading custom properties\", e);\n\t}\n}", "CodeBleu_score": 48.828631072261345}, {"code": "private void insertCustomProperties(Properties props){\n\t\ttry {\n\t\tfinal Properties customProps = new Properties();\n\t\tcustomProps.load(new StringReader(configuration.getString(CK_CUSTOM_PROPERTIES)));\n\t\tprops.putAll(customProps);\n\t} catch (IOException e) {\n\t\tLOG.error(\"Failed to load custom properties: \" + e.getMessage());\n\t}\n}", "CodeBleu_score": 41.674981957626024}]}
{"instance_id": "ea146a7bc30c", "generation_result": [{"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tcts.requireNonNull(entries, \"entries must not be null\");\n\nList<LocalKafkaJournal.Entry> journalEntries = entries.stream()\n.filter(Objects::nonNull)\n.map(e -> new LocalKafkaJournal.Entry(e.getPayload(), e.getTimestamp(), e.getFields()))\n.collect(Collectors.toList());\n\nlong totalBytes = journalEntries.stream()\n.mapToLong(e -> e.getMessage().length + e.getFields().length)\n.sum();\n\ntry {\n    writeRetryer.call(() -> kafkaJournal.write(journalEntries));\n} catch (ExecutionException | RetryException e) {\n    throw new MessageQueueException(\\\"Unable to write to journal\\\", e);\n}\n\nmetrics.writtenMessages().inc(journalEntries.size());\nmetrics.writtenBytes().inc(totalBytes);\n}", "CodeBleu_score": 52.10104964772218}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tilter out null entries\nList<RawMessageEvent> nonNullEntries = entries.stream().filter(Objects::nonNull).collect(Collectors.toList());\n\n// Map each event to a journal entry\nList<LocalKafkaJournal.Entry> journalEntries = nonNullEntries.stream()\n.map(entry -> new LocalKafkaJournal.Entry(entry.getMessageId(), entry.getPayload()))\n.collect(Collectors.toList());\n\n// Calculate the total message bytes\nAtomicLong totalMessageBytes = new AtomicLong(0);\n\njournalEntries.forEach(entry -> totalMessageBytes.addAndGet(entry.getPayload().length));\n\n// Attempt to write the entries to the journal\ntry {\n\nwriteRetryer.call(() -> {\n\nkafkaJournal.write(journalEntries);\n\nreturn null;\n\n});\n\n} catch (RetryException | ExecutionException e) {\n\nthrow new MessageQueueException(\\\"Unable to write to journal\\\", e);\n\n}\n\n\n// Update metrics\nmetrics.writtenMessages().mark(nonNullEntries.size());\n\nmetrics.writtenBytes().mark(totalMessageBytes.get());\n\n}", "CodeBleu_score": 51.77699085004354}, {"code": "public void write(List<RawMessageEvent> entries) throws MessageQueueException{\n\tilter out null entries\nList<RawMessageEvent> nonNullEntries = entries.stream()\n.filter(Objects::nonNull)\n.collect(Collectors.toList());\n\n// Map each event to a journal entry\nList<Journal.Entry> journalEntries = nonNullEntries.stream()\n.map(event -> new Journal.Entry(event.getTimestamp(), event.getMessage()))\n.collect(Collectors.toList());\n\n// Calculate the total message bytes\nAtomicLong totalMessageBytes = new AtomicLong(0);\n\nfor (RawMessageEvent entry : nonNullEntries) {\n\ntotalMessageBytes.addAndGet(entry.getMessage().length);\n\n}\n\n\n// Attempt to write the entries to the journal\ntry {\n\nwriteRetryer.call(() -> {\n\nkafkaJournal.write(journalEntries);\n\nreturn null;\n\n});\n\n} catch (RetryException | ExecutionException e) {\n\nthrow new MessageQueueException(\\\"Failed to write to journal\\\", e);\n\n}\n\n\n// Update the written messages and bytes metrics\nmetrics.writtenMessages().mark(nonNullEntries.size());\n\nmetrics.writtenBytes().mark(totalMessageBytes.get());\n\n}", "CodeBleu_score": 51.35966901366519}]}
{"instance_id": "944e77614042", "generation_result": [{"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxValidMessageQueueId = messages.stream()\n.map(Message::getQueueId)\n.filter(this::isValidMessageQueueId)\n.max(Long::compareTo);\n\nmaxValidMessageQueueId.ifPresent(kafkaJournal::acknowledge);\n\nmetrics.updateAcknowledgedMessages(messages.size());\n}", "CodeBleu_score": 43.29417559856016}, {"code": "public void acknowledge(List<Message> messages){\n\tOptional<Long> maxValidId = messages.stream()\n.map(message -> (Long) message.getId())\n.filter(id -> id!= null)\n.max(Long::compare);\n\nmaxValidId.ifPresent(kafkaJournal::acknowledge);\n\nupdateMetrics(messages.size());\n}", "CodeBleu_score": 39.053970098520885}, {"code": "public void acknowledge(List<Message> messages){\n\tif (messages.isEmpty()) {\n    return;\n}\n\nOptional<Long> maxValidMessageQueueId = messages.stream()\n   .map(Message::getMessageQueueId)\n   .filter(Optional::isPresent)\n   .map(Optional::get)\n   .max(Long::compareTo);\n\nmaxValidMessageQueueId.ifPresent(kafkaJournal::acknowledge);\n\nmetrics.updateAcknowledged(messages.size());\n}", "CodeBleu_score": 47.95384916421576}]}
{"instance_id": "981280879490", "generation_result": [{"code": "private void registerLegacyMetrics(){\n\tfinal List<String> filteredMetrics = this.metricRegistry.getGauges().keySet().stream()\n.filter(metricName -> metricName.startsWith(LocalKafkaJournal.class.getName()))\n.collect(Collectors.toList());\n\nfor (final String metricName : filteredMetrics) {\nfinal String legacyMetricName = metricName.replace(LocalKafkaJournal.class.getName(), \"org.graylog2.journal.kafka\");\ntry {\nthis.metricRegistry.register(legacyMetricName, this.metricRegistry.getGauges().get(metricName));\n} catch (final IllegalArgumentException e) {\nLOG.warn(\"Unable to register legacy metric {}: {}\", legacyMetricName, e.getMessage());\n}\n}\n}", "CodeBleu_score": 46.88184244152992}, {"code": "private void registerLegacyMetrics(){\n\tfinal List<String> metricsToRegister = metricRegistry.getNames().stream()\n.filter(name -> name.startsWith(LocalKafkaJournal.class.getName()))\n.collect(Collectors.toList());\n\nfor (final String metricName : metricsToRegister) {\nfinal String legacyMetricName = metricName.replace(LocalKafkaJournal.class.getName(), \"org.graylog2.journal\");\ntry {\nmetricRegistry.register(legacyMetricName, metricRegistry.getGauges().get(metricName));\n} catch (IllegalArgumentException e) {\nLOG.warn(\"Unable to register legacy metric {}. Original metric name: {}\", legacyMetricName, metricName);\n}\n}\n}", "CodeBleu_score": 44.36946650588111}, {"code": "private void registerLegacyMetrics(){\n\t\tfinal Set<String> filteredMetrics = this.metricRegistry.getNames().stream()\n\t\t\t.filter(name -> name.startsWith(LocalKafkaJournal.class.getName()))\n\t\t\t.collect(Collectors.toSet());\n\n\tfor (final String metricName : filteredMetrics) {\n\t\tfinal String legacyMetricName = metricName.replace(LocalKafkaJournal.class.getName() + \".\", \"\");\n\t\ttry {\n\t\t\tthis.metricRegistry.register(legacyMetricName, this.metricRegistry.getMetrics().get(metricName));\n\t\t} catch (IllegalArgumentException e) {\n\t\t\tLOG.warn(\"Failed to register legacy metric {}: {}\", legacyMetricName, e.getMessage());\n\t\t}\n\t}\n}", "CodeBleu_score": 43.587585236002944}]}
{"instance_id": "3ca34bfd2d64", "generation_result": [{"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\t\tfinal Timer timer;\n\n\ttry {\n\n\t\ttimer = metricRegistry.register(metricName, new HdrTimer());\n\n\t} catch (IllegalArgumentException e) {\n\n\t\ttimer = metricRegistry.getTimers().get(metricName);\n\n\t}\n\n\treturn timer;\n\n}", "CodeBleu_score": 40.19822456677842}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\t\ttry {\n\t\treturn metricRegistry.register(metricName, new HdrTimer());\n\t} catch (IllegalArgumentException e) {\n\t\treturn metricRegistry.getTimers().get(metricName);\n\t}\n}", "CodeBleu_score": 20.613787288301083}, {"code": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName){\n\t\tfinal Timer timer = metricRegistry.timer(metricName);\n\ttry {\n\t\tmetricRegistry.register(metricName, timer);\n\t} catch (IllegalArgumentException e) {\n\t\treturn metricRegistry.timer(metricName);\n\t}\n\treturn timer;\n}", "CodeBleu_score": 28.140690578405408}]}
{"instance_id": "e1d07a1fa986", "generation_result": [{"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t\tmetricRegistry.register(name, new Gauge<Long>() {\n\t\t@Override\n\t\tpublic Long getValue() {\n\t\t\tfinal long logEndOffset = kafkaLog.logEndOffset();\n\t\t\tfinal long committedOffset = LocalKafkaJournal.this.committedOffset.get();\n\t\t\tif (logEndOffset == 0) {\n\t\t\t\treturn 0L;\n\t\t\t} else if (committedOffset == DEFAULT_COMMITTED_OFFSET) {\n\t\t\t\treturn logEndOffset - kafkaLog.logStartOffset();\n\t\t\t} else {\n\t\t\t\treturn logEndOffset - committedOffset;\n\t\t\t}\n\t\t}\n\t});\n}", "CodeBleu_score": 41.03609243127718}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t\ttry {\n\t\tmetricRegistry.register(name, (Gauge<Long>) () -> {\n\t\t\tif (kafkaLog.size() == 0) {\n\t\t\t\treturn 0L;\n\t\t\t} else if (committedOffset.get() == DEFAULT_COMMITTED_OFFSET) {\n\t\t\t\treturn kafkaLog.logEndOffset() - kafkaLog.logStartOffset();\n\t\t\t} else {\n\t\t\t\treturn kafkaLog.logEndOffset() - committedOffset.get();\n\t\t\t}\n\t\t});\n\t} catch (IllegalArgumentException e) {\n\t\t// Ignore the exception.\n\t}\n}", "CodeBleu_score": 53.166357303006905}, {"code": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name){\n\t\ttry {\n\t\tmetricRegistry.register(name, (Gauge<Long>) () -> {\n\t\t\tif (kafkaLog.size() == 0) {\n\t\t\t\treturn 0L;\n\t\t\t}\n\t\t\tif (committedOffset.get() == 0) {\n\t\t\t\treturn kafkaLog.logEndOffset() - kafkaLog.logStartOffset();\n\t\t\t}\n\t\t\treturn kafkaLog.logEndOffset() - committedOffset.get();\n\t\t});\n\t} catch (IllegalArgumentException e) {\n\t\t// Ignore exception.\n\t}\n}", "CodeBleu_score": 54.28698570093273}]}
{"instance_id": "8b30a387069f", "generation_result": [{"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tif (messages.isEmpty()) {\nLOG.debug(\\\"No messages to flush\\\");\nreturn getLogEndOffset();\n}\nfinal ByteBufferMessageSet messageSet = new ByteBufferMessageSet(messages);\nfinal long messageSetSize = messageSet.sizeInBytes();\nLOG.debug(\\\"Flushing {} bytes message set with {} messages\\\", messageSetSize, messages.size());\nfinal long lastWriteOffset = append(messageSet);\nLOG.debug(\\\"Wrote message set with {} messages to offset {}\\\", messages.size(), lastWriteOffset);\nwrittenMessages.mark(messages.size());\nreturn lastWriteOffset;\n}", "CodeBleu_score": 41.910598031318614}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\t\tif (messages.size() == 0) {\n\t\tLOG.debug(\"Empty message set, skipping write\");\n\t\treturn 0;\n\t}\n\n\tfinal ByteBufferMessageSet messageSet = ByteBufferMessageSet.create(messages);\n\tfinal int messageSetSize = messageSet.sizeInBytes();\n\n\tif (LOG.isTraceEnabled()) {\n\t\tLOG.trace(\"Writing {} bytes message set with {} messages\", messageSetSize, messages.size());\n\t}\n\n\tfinal long lastWriteOffset = append(messageSet);\n\n\tif (LOG.isTraceEnabled()) {\n\t\tLOG.trace(\"Wrote {} bytes message set with {} messages to offset {}\", messageSetSize, messages.size(), lastWriteOffset);\n\t}\n\n\twrittenMessages.mark(messages.size());\n\n\treturn lastWriteOffset;\n}", "CodeBleu_score": 46.12372771611454}, {"code": "private long flushMessages(List<Message> messages, long payloadSize){\n\tif (messages.isEmpty()) {\nLOG.debug(\\\"Flushing empty message set\\\");\nreturn -1L;\n}\nfinal ByteBufferMessageSet messageSet = new ByteBufferMessageSet(messages);\nfinal int messageSetSize = messageSet.sizeInBytes();\nLOG.debug(\\\"Flushing {} bytes message set with {} messages\\\", messageSetSize, messages.size());\nfinal long lastOffset = append(messageSet);\nLOG.debug(\\\"Wrote message set with {} messages to offset {} with size {} bytes\\\", messages.size(), lastOffset, messageSetSize);\nwrittenMessages.mark(messages.size());\nwritePayloadSize.mark(payloadSize);\nreturn lastOffset;\n}", "CodeBleu_score": 43.24023171851822}]}
{"instance_id": "ee338b61e75d", "generation_result": [{"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n\t\tList<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n\t\tif (adminClientList == null) {\n\t\t\treturn;\n\t\t}\n\n\t\tLOGGER.info(\\\"close kafka admin-client start, clusterPhyId={}\\\", clusterPhyId);\n\n        try {\n            for (AdminClient adminClient : adminClientList) {\n                adminClient.close();\n            }\n\n            LOGGER.info(\\\"close kafka admin-client success, clusterPhyId={}\\\", clusterPhyId);\n        } catch (Exception e) {\n            LOGGER.error(\\\"close kafka admin-client failed, clusterPhyId={}\\\", clusterPhyId, e);\n        }\n\n        KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n    }\n}", "CodeBleu_score": 47.796748833668964}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\tist<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\nif (adminClientList == null) {\nreturn;\n}\n\nLOGGER.info(\\\"start to close kafka admin client, clusterPhyId={}\\\", clusterPhyId);\n\nfor (AdminClient adminClient : adminClientList) {\ntry {\nadminClient.close();\n} catch (Exception e) {\nLOGGER.error(\\\"close kafka admin client failed, clusterPhyId={}\\\", clusterPhyId, e);\n}\n}\n\nLOGGER.info(\\\"close kafka admin client success, clusterPhyId={}\\\", clusterPhyId);\n\nKAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n}", "CodeBleu_score": 40.40694461839405}, {"code": "private void closeKafkaAdminClient(Long clusterPhyId) {\n\t// Lock the client map to ensure thread safety\n\tsynchronized (KAFKA_ADMIN_CLIENT_MAP) {\n\t\t// Retrieve the list of AdminClient instances for the specified cluster ID\n\t\tList<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.get(clusterPhyId);\n\t\tif (adminClientList == null) {\n\t\t\treturn;\n\t\t}\n\n\t\t// Log the start of the closing process\n\t\tLOGGER.info(\\\"Closing Kafka AdminClient instances for clusterPhyId: {}\\\", clusterPhyId);\n\n\t\ttry {\n\t\t\t// Close all AdminClient instances in the list\n\t\t\tfor (AdminClient adminClient : adminClientList) {\n\t\t\t\tadminClient.close();\n\t\t\t}\n\n\t\t\t// Log a success message if all clients are closed successfully\n\t\t\tLOGGER.info(\\\"Successfully closed all Kafka AdminClient instances for clusterPhyId: {}\\\", clusterPhyId);\n\n\t\t} catch (Exception e) {\n\t\t\t// Log an error message if any closure fails\n\t\t\tLOGGER.error(\\\"Failed to close Kafka AdminClient instances for clusterPhyId: {}\\\", clusterPhyId, e);\n\n\t\t} finally {\n\t\t\t// Release the lock\n\t\t\tKAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n\t\t}\n\t}\n}", "CodeBleu_score": 43.15026414568468}]}
{"instance_id": "6f0b935b971c", "generation_result": [{"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\t (adminClientList == null) {\nreturn true;\n}\n\nboolean success = true;\nfor (AdminClient adminClient : adminClientList) {\ntry {\nadminClient.close(Duration.ofSeconds(30));\n} catch (Exception e) {\nLOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\nsuccess = false;\n}\n}\n\nreturn success;\n}", "CodeBleu_score": 75.98014085369005}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tf (adminClientList == null) {\n\t\treturn true;\n\t}\n\n\tboolean success = true;\n\tfor (AdminClient adminClient : adminClientList) {\n\t\ttry {\n\t\t\tadminClient.close(Duration.ofSeconds(30));\n\t\t} catch (Exception e) {\n\t\t\tLOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n\t\t\tsuccess = false;\n\t\t}\n\t}\n\n\treturn success;\n}", "CodeBleu_score": 75.78147353499286}, {"code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList){\n\tf (adminClientList == null) {\n\t\treturn true;\n\t}\n\n\tboolean success = true;\n\tfor (AdminClient adminClient : adminClientList) {\n\t\ttry {\n\t\t\tadminClient.close(Duration.ofSeconds(30));\n\t\t} catch (Exception e) {\n\t\t\tLOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n\t\t\tsuccess = false;\n\t\t}\n\t}\n\n\treturn success;\n}", "CodeBleu_score": 75.78147353499286}]}
{"instance_id": "b5475aa94c1a", "generation_result": [{"code": "public void stop() {\n\t\tif (app!= null) {\n\t\tapp.stop();\n\t\tfor (int i = 0; i < 5; i++) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t\tif (app.isActive()) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 50.77630415937987}, {"code": "public void stop() {\n\t\tif (app!= null) {\n\t\tapp.stop();\n\t\tfor (int i = 0; i < 5; i++) {\n\t\t\ttry {\n\t\t\t\tapp.awaitTermination();\n\t\t\t\tbreak;\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t}\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 46.14073622607117}, {"code": "public void stop() {\n\t\tif (app!= null) {\n\t\tapp.stop();\n\t\tapp.close();\n\t\tapp = null;\n\t}\n}", "CodeBleu_score": 15.276024826039498}]}
{"instance_id": "5da405de9e3d", "generation_result": [{"code": "static EurekaServerRunner startEurekaServer(Class config){\n\t\tEurekaServerRunner serverRunner = new EurekaServerRunner(config);\nserverRunner.enableTls(true);\nserverRunner.setKeystore(caCert, KEY_STORE_PASSWORD);\nserverRunner.setTruststore(caCert, KEY_STORE_PASSWORD);\nserverRunner.start();\nreturn serverRunner;\n}", "CodeBleu_score": 54.00614281180995}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\t\tEurekaServerRunner runner = new EurekaServerRunner(config);\nrunner.enableSsl();\nrunner.setKeyStore(caCert, KEY_STORE_PASSWORD, KEY_PASSWORD);\nrunner.setTrustStore(caCert, KEY_STORE_PASSWORD);\nrunner.start();\nreturn runner;\n}", "CodeBleu_score": 53.441930738699725}, {"code": "static EurekaServerRunner startEurekaServer(Class config){\n\t\tEurekaServerRunner runner = new EurekaServerRunner(config);\n\n\trunner.enableTls();\n\trunner.setKeyStore(caCert, KEY_STORE_PASSWORD);\n\trunner.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\trunner.start();\n\n\treturn runner;\n}", "CodeBleu_score": 52.33135017995084}]}
{"instance_id": "19cbf70751b2", "generation_result": [{"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\t\tEurekaClientRunner service = new EurekaClientRunner(server, config);\n\tservice.enableTls();\n\tservice.start(\\\"testservice\\\");\n\treturn service;\n}", "CodeBleu_score": 57.50966441847311}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\t\tEurekaClientRunner client = new EurekaClientRunner(config, server);\n\tclient.enableTls();\n\tclient.start(\\\"testservice\\\");\n\treturn client;\n}", "CodeBleu_score": 46.448074647676144}, {"code": "static EurekaClientRunner startService(EurekaServerRunner server, Class config){\n\t\tEurekaClientRunner runner = new EurekaClientRunner(config, server);\n\trunner.enableTls();\n\trunner.start(\"testservice\");\n\treturn runner;\n}", "CodeBleu_score": 47.63292953399495}]}
{"instance_id": "d6ccff5c0536", "generation_result": [{"code": "static void createCertificates() throws Exception{\n\t", "CodeBleu_score": 1.403125619965593}, {"code": "static void createCertificates() throws Exception{\n\t\tFile caDir = new File(\"ca\");\n\tcaDir.mkdir();\n\n\tFile serverDir = new File(\"server\");\n\tserverDir.mkdir();\n\n\tFile clientDir = new File(\"client\");\n\tclientDir.mkdir();\n\n\tFile wrongClientDir = new File(\"wrongClient\");\n\twrongClientDir.mkdir();\n\n\tFile wrongCaDir = new File(\"wrongCa\");\n\twrongCaDir.mkdir();\n\n\tKeyTool caKeyTool = new KeyTool(caDir);\n\tcaKeyTool.createCa();\n\tcaCert = saveCert(caKeyTool, caDir, \"ca\");\n\n\tKeyTool serverKeyTool = new KeyTool(serverDir);\n\tserverKeyTool.createServer(\"localhost\", caKeyTool);\n\tserverCert = saveCert(serverKeyTool, serverDir, \"server\");\n\n\tKeyTool clientKeyTool = new KeyTool(clientDir);\n\tclientKeyTool.createClient(\"localhost\", caKeyTool);\n\tclientCert = saveCert(clientKeyTool, clientDir, \"client\");\n\n\tKeyTool wrongClientKeyTool = new KeyTool(wrongClientDir);\n\twrongClientKeyTool.createClient(\"localhost\", caKeyTool);\n\twrongClientCert = saveCert(wrongClientKeyTool, wrongClientDir, \"wrongClient\");\n\n\tKeyTool wrongCaKeyTool = new KeyTool(wrongCaDir);\n\twrongCaKeyTool.createCa();\n\twrongCaCert = saveCert(wrongCaKeyTool, wrongCaDir, \"wrongCa\");\n}", "CodeBleu_score": 28.068264328461794}, {"code": "static void createCertificates() throws Exception{\n\t", "CodeBleu_score": 1.403125619965593}]}
{"instance_id": "d2eeaa6a6bbe", "generation_result": [{"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\t\tlong timeLimit = System.currentTimeMillis() + seconds * 1000;\n\twhile (true) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t\tif (System.currentTimeMillis() > timeLimit) {\n\t\t\tthrow new RuntimeException(\"Timeout\");\n\t\t}\n\t}\n}", "CodeBleu_score": 42.55424576173855}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\t\tlong timeLimit = System.currentTimeMillis() + (seconds * 1000);\n\twhile (!assertion.getAsBoolean()) {\n\t\tif (System.currentTimeMillis() > timeLimit) {\n\t\t\tthrow new RuntimeException(\\\"Assertion failed within \\\" + seconds + \\\" seconds\\\");\n\t\t}\n\t}\n}", "CodeBleu_score": 37.02416945353536}, {"code": "private void assertInSeconds(BooleanSupplier assertion, int seconds){\n\t\tlong startTime = System.currentTimeMillis();\n\tlong timeLimit = startTime + (seconds * 1000);\n\twhile (true) {\n\t\tif (assertion.getAsBoolean()) {\n\t\t\treturn;\n\t\t}\n\t\tif (System.currentTimeMillis() > timeLimit) {\n\t\t\tthrow new RuntimeException(\\\"Assertion timed out\\\");\n\t\t}\n\t}\n}", "CodeBleu_score": 47.43416860343856}]}
{"instance_id": "6bb261b0e4e1", "generation_result": [{"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\t\treturn createCert(keyPair.getPublic(), keyPair.getPrivate(), ca, ca);\n}", "CodeBleu_score": 15.23846686933878}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\t\treturn createCert(keyPair.getPublic(), keyPair.getPrivate(), ca, ca);\n}", "CodeBleu_score": 15.23846686933878}, {"code": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception{\n\t\treturn createCert(keyPair.getPublic(), keyPair.getPrivate(), ca, ca);\n}", "CodeBleu_score": 15.23846686933878}]}
{"instance_id": "e4f9c32a7c9e", "generation_result": [{"code": "EurekaHttpClient getEurekaHttpClient(){\n\tif (eurekaHttpClient.get() == null) {\n\t\ttry {\n\t\t\teurekaHttpClient.set(((EurekaTransport) eurekaTransportField.get(this)).registrationClient);\n\t\t}\n\t\tcatch (IllegalAccessException e) {\n\t\t\tlog.error(\"Unable to get EurekaHttpClient instance\", e);\n\t\t}\n\t}\n\treturn eurekaHttpClient.get();\n}", "CodeBleu_score": 28.212367335289766}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient client = eurekaHttpClient.get();\n\tif (client == null) {\n\t\ttry {\n\t\t\tclient = (EurekaHttpClient) eurekaTransportField.get(this).registrationClient;\n\t\t\teurekaHttpClient.set(client);\n\t\t}\n\t\tcatch (IllegalAccessException ex) {\n\t\t\tlog.error(\"Cannot access the registrationClient field\", ex);\n\t\t}\n\t}\n\treturn client;\n}", "CodeBleu_score": 38.08390490022627}, {"code": "EurekaHttpClient getEurekaHttpClient(){\n\tEurekaHttpClient client = eurekaHttpClient.get();\n\tif (client == null) {\n\t\tsynchronized (eurekaHttpClient) {\n\t\t\tclient = eurekaHttpClient.get();\n\t\t\tif (client == null) {\n\t\t\t\ttry {\n\t\t\t\t\tclient = (EurekaHttpClient) eurekaTransportField.get(this);\n\t\t\t\t}\n\t\t\t\tcatch (IllegalAccessException e) {\n\t\t\t\t\tlog.error(\"Cannot access eurekaTransport field\", e);\n\t\t\t\t}\n\t\t\t\teurekaHttpClient.set(client);\n\t\t\t}\n\t\t}\n\t}\n\treturn client;\n}", "CodeBleu_score": 42.5028111178257}]}
{"instance_id": "5c404f04e34c", "generation_result": [{"code": "public List<String> getServices(){\n\t\tApplications applications = eurekaClient.getApplications();\n\tif (applications == null || applications.getRegisteredApplications() == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications.getRegisteredApplications()) {\n\t\tif (application.getInstances()!= null &&!application.getInstances().isEmpty()) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 57.21612830503828}, {"code": "public List<String> getServices(){\n\t\tApplications applications = this.eurekaClient.getApplications();\n\tif (applications == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications.getRegisteredApplications()) {\n\t\tif (application.getInstances().size() > 0) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 67.47620505415725}, {"code": "public List<String> getServices(){\n\t\tApplications applications = this.eurekaClient.getApplications();\n\tif (applications == null) {\n\t\treturn Collections.emptyList();\n\t}\n\tList<String> services = new ArrayList<>();\n\tfor (Application application : applications.getRegisteredApplications()) {\n\t\tif (application.getInstances()!= null &&!application.getInstances().isEmpty()) {\n\t\t\tservices.add(application.getName().toLowerCase());\n\t\t}\n\t}\n\treturn services;\n}", "CodeBleu_score": 69.91131097462812}]}
{"instance_id": "a21251337ade", "generation_result": [{"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\t\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\tif (entry.getValue() instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor compositeHealthContributor = (DiscoveryCompositeHealthContributor) entry.getValue();\n\t\t\tfor (NamedContributor<Health> indicator : compositeHealthContributor.getHealthIndicators().values()) {\n\t\t\t\tif (!(indicator instanceof EurekaHealthIndicator)) {\n\t\t\t\t\thealthContributors.put(entry.getKey(), indicator);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\thealthContributors.put(entry.getKey(), entry.getValue());\n\t\t}\n\t}\n}", "CodeBleu_score": 45.562753210679695}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\t\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\tif (entry.getValue() instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor compositeHealthContributor = (DiscoveryCompositeHealthContributor) entry.getValue();\n\t\t\tfor (Map.Entry<String, HealthIndicator> indicator : compositeHealthContributor.getHealthIndicators().entrySet()) {\n\t\t\t\tif (!(indicator.getValue() instanceof EurekaHealthIndicator)) {\n\t\t\t\t\thealthContributors.put(indicator.getKey(), indicator.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\thealthContributors.put(entry.getKey(), entry.getValue());\n\t\t}\n\t}\n}", "CodeBleu_score": 42.861481141736135}, {"code": "void populateHealthContributors(Map<String, HealthContributor> healthContributors){\n\t\tfor (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n\t\tif (entry.getValue() instanceof DiscoveryCompositeHealthContributor) {\n\t\t\tDiscoveryCompositeHealthContributor composite = (DiscoveryCompositeHealthContributor) entry.getValue();\n\t\t\tfor (Map.Entry<String, HealthIndicator> indicator : composite.getHealthIndicators().entrySet()) {\n\t\t\t\tif (!(indicator.getValue() instanceof EurekaHealthIndicator)) {\n\t\t\t\t\tthis.healthContributors.put(indicator.getKey(), indicator.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tthis.healthContributors.put(entry.getKey(), entry.getValue());\n\t\t}\n\t}\n}", "CodeBleu_score": 48.44605290247936}]}
{"instance_id": "998fee829929", "generation_result": [{"code": "public String hello(){\n\t\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n\tif (instances.isEmpty()) {\n\t\tthrow new RuntimeException(\"No instances found for provider-application\");\n\t}\n\tURI uri = instances.get(0).getUri();\n\treturn restTemplate.postForObject(uri + \"/echo\", \"Hello\", String.class);\n}", "CodeBleu_score": 39.08017782542381}, {"code": "public String hello(){\n\t\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\n\tif (instances.isEmpty()) {\n\t\tthrow new RuntimeException(\"No instances found for provider-application\");\n\t}\n\tURI uri = instances.get(0).getUri();\n\treturn restTemplate.postForObject(uri + \"/echo\", \"Hello, world!\", String.class);\n}", "CodeBleu_score": 39.26773468101234}, {"code": "public String hello(){\n\tList<ServiceInstance> instances = discoveryClient.getInstances(\"provider-application\");\nif (instances.isEmpty()) {\nthrow new RuntimeException(\"No instances found for provider-application\");\n}\nServiceInstance instance = instances.get(0);\nURI uri = instance.getUri();\nreturn restTemplate.postForObject(uri + \"/echo\", \"Hello World\", String.class);\n}", "CodeBleu_score": 46.445851348067585}]}
{"instance_id": "31166ae7d3dc", "generation_result": [{"code": "public String world(){\n\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\nif (serviceInstance!= null) {\nURI uri = serviceInstance.getUri();\nString response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\nreturn response;\n}\nthrow new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 93.79349141034233}, {"code": "public String world(){\n\t\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n\tif (serviceInstance!= null) {\n\t\tURI uri = serviceInstance.getUri();\n\t\tString response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n\t\treturn response;\n\t}\n\n\tthrow new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 93.85293074518553}, {"code": "public String world(){\n\t\tServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n\tif (serviceInstance!= null) {\n\t\tURI uri = serviceInstance.getUri();\n\t\tString response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n\t\treturn response;\n\t}\n\n\tthrow new RuntimeException(\"No service instance for provider-application found\");\n}", "CodeBleu_score": 93.85293074518553}]}
{"instance_id": "d7714f9cf0e9", "generation_result": [{"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\t\tlogger.info(\"Registering service: {}, ip: {}, port: {}\", serviceName, ip, port);\n\tSet<Server> servers = serverMap.computeIfAbsent(serviceName, k -> Collections.synchronizedSet(new HashSet<>()));\n\tservers.add(new Server(ip, port));\n\treturn true;\n}", "CodeBleu_score": 64.65815169766401}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\t\tlogger.info(\"register service: \" + serviceName + \", ip: \" + ip + \", port: \" + port);\n\tif (!serverMap.containsKey(serviceName)) {\n\t\tserverMap.put(serviceName, Collections.synchronizedSet(new HashSet<>()));\n\t}\n\tserverMap.get(serviceName).add(new Server(ip, port));\n\treturn true;\n}", "CodeBleu_score": 52.75071511798325}, {"code": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\tlogger.info(\"register service: serviceName={}, ip={}, port={}\", serviceName, ip, port);\nSet<Server> servers = serverMap.computeIfAbsent(serviceName, k -> Collections.synchronizedSet(new HashSet<>()));\nservers.add(new Server(ip, port));\nreturn true;\n}", "CodeBleu_score": 58.109407444427255}]}
{"instance_id": "45f91ed1c560", "generation_result": [{"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\t\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n\tServer server = new Server(ip, port);\n\tSet<Server> serverSet = serverMap.get(serviceName);\n\tif (serverSet!= null) {\n\t\tserverSet.remove(server);\n\t\tif (serverSet.isEmpty()) {\n\t\t\tserverMap.remove(serviceName);\n\t\t}\n\t}\n\treturn true;\n}", "CodeBleu_score": 73.87285985279514}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\t\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n\tif (serverMap.containsKey(serviceName)) {\n\t\tServer server = new Server(ip, port);\n\t\tserverMap.get(serviceName).remove(server);\n\t}\n\treturn true;\n}", "CodeBleu_score": 62.89242249738668}, {"code": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port){\n\t\tlogger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n\tif (serverMap.containsKey(serviceName)) {\n\t\tServer server = new Server(ip, port);\n\t\tserverMap.get(serviceName).remove(server);\n\t\treturn true;\n\t}\n\treturn false;\n}", "CodeBleu_score": 62.84700614859836}]}
{"instance_id": "f671d9564e40", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getCanonicalName());\nif (attributes!= null && attributes.containsKey(\"defaultConfiguration\")) {\nString name = \"default.\" + attributes.get(\"defaultConfiguration\");\nBeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class);\nbuilder.addConstructorArgValue(name);\nregistry.registerBeanDefinition(name, builder.getBeanDefinition());\n}\n}", "CodeBleu_score": 53.91697753978993}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\tMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getName());\nif (attributes!= null && attributes.containsKey(\"defaultConfiguration\")) {\nString name = \"default.\" + attributes.get(\"defaultConfiguration\").getClass().getName();\nBeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(ClientConfiguration.class);\nbuilder.addConstructorArgValue(attributes.get(\"defaultConfiguration\"));\nregistry.registerBeanDefinition(name, builder.getBeanDefinition());\n}\n}", "CodeBleu_score": 54.00333575174319}, {"code": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry){\n\t// \u83b7\u53d6\u6ce8\u89e3\u7684\u5c5e\u6027\nMap<String, Object> attributes = metadata.getAnnotationAttributes(RibbonClients.class.getCanonicalName());\n// \u5982\u679c\u5c5e\u6027\u4e2d\u5305\u542bdefaultConfiguration\u952e\uff0c\u5219\u751f\u6210\u540d\u79f0\u5e76\u4f7f\u7528\u68c0\u7d22\u5230\u7684\u914d\u7f6e\u5411BeanDefinitionRegistry\u6ce8\u518c\u5ba2\u6237\u7aef\u914d\u7f6e\u3002\nif (attributes!= null && attributes.containsKey(\"defaultConfiguration\")) {\nString name = \"default.\" + attributes.get(\"defaultConfiguration\");\nBeanDefinitionBuilder.genericBeanDefinition(RibbonClientConfiguration.class).addConstructorArgValue(name).setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE).setScope(BeanDefinition.SCOPE_SINGLETON).setRole(BeanDefinition.ROLE_INFRASTRUCTURE);\nregistry.registerBeanDefinition(name, beanDefinition.getBeanDefinition());\n}\n}", "CodeBleu_score": 48.89003352436278}]}
{"instance_id": "dbc92d8f146f", "generation_result": [{"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\t", "CodeBleu_score": 9.97081270039238}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\t", "CodeBleu_score": 9.97081270039238}, {"code": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n\t", "CodeBleu_score": 9.97081270039238}]}
{"instance_id": "362eadbd3c5e", "generation_result": [{"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n\t\tString scheme = original.getScheme();\n\t\tString host = server.getHost();\n\t\tint port = server.getPort();\n\t\tString path = original.getRawPath();\n\t\tString query = original.getRawQuery();\n\t\tString fragment = original.getRawFragment();\n\t\tif (scheme == null) {\n\t\t\tscheme = \"http\";\n\t\t}\n\t\tif (path == null) {\n\t\t\tpath = \"/\";\n\t\t}\n\t\tif (query!= null) {\n\t\t\tquery = \"?\" + query;\n\t\t}\n\t\tif (fragment!= null) {\n\t\t\tfragment = \"#\" + fragment;\n\t\t}\n\t\treturn new URI(scheme, null, host, port, path, query, fragment);\n\t}\n\tcatch (URISyntaxException ex) {\n\t\tthrow new RuntimeException(ex);\n\t}\n}", "CodeBleu_score": 33.60737387721204}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\ttry {\n\t\tString scheme = original.getScheme();\n\t\tString host = server.getHost();\n\t\tint port = server.getPort();\n\t\tString path = original.getRawPath();\n\t\tString query = original.getRawQuery();\n\t\tif (StrUtil.isNotBlank(query)) {\n\t\t\treturn new URI(scheme, null, host, port, path, query, null);\n\t\t}\n\t\telse {\n\t\t\treturn new URI(scheme, null, host, port, path, null, null);\n\t\t}\n\t}\n\tcatch (URISyntaxException e) {\n\t\tthrow new RuntimeException(e);\n\t}\n}", "CodeBleu_score": 38.75482779018699}, {"code": "public URI reconstructURI(ServiceInstance server, URI original) {\n\tString host = server.getHost();\n\tint port = server.getPort();\n\tString scheme = server.getScheme();\n\tString path = original.getRawPath();\n\tString query = original.getRawQuery();\n\tString userInfo = original.getRawUserInfo();\n\tString fragment = original.getRawFragment();\n\ttry {\n\t\treturn new URI(scheme, userInfo, host, port, path, query, fragment);\n\t}\n\tcatch (URISyntaxException e) {\n\t\tthrow new RuntimeException(e);\n\t}\n}", "CodeBleu_score": 36.669562479299486}]}
{"instance_id": "6801d2b2e4c8", "generation_result": [{"code": "private List<TutuServer> getServer(){\n\t// Construct the parameters\nMap<String, Object> params = new HashMap<>();\nparams.put(\"serviceName\", discoveryProperties.getServiceName());\n\n// Send a GET request to the specified server address concatenated with \"/list\"\nString response = HttpUtil.get(discoveryProperties.getServerAddress() + \"/list\", params);\n\n// Parse the JSON array into a list of TutuServer objects\nList<TutuServer> servers = JSON.parseArray(response, TutuServer.class);\n\n// Return the list of TutuServer instances\nreturn servers;\n}", "CodeBleu_score": 36.8544936518502}, {"code": "private List<TutuServer> getServer(){\n\tString serviceName = discoveryProperties.getServiceName();\nString serverAddress = discoveryProperties.getServerAddress();\nMap<String, Object> params = new HashMap<>();\nparams.put(\"serviceName\", serviceName);\nString url = serverAddress + \"/list\";\nString response = HttpUtil.get(url, params);\nList<TutuServer> servers = JSON.parseArray(response, TutuServer.class);\nreturn servers;\n}", "CodeBleu_score": 36.79919348331367}, {"code": "private List<TutuServer> getServer(){\n\tString serviceName = getDiscoveryProperties().getServiceName();\nString serverAddress = getDiscoveryProperties().getServerAddress();\nString url = serverAddress + \"/list\";\nMap<String, Object> params = new HashMap<>();\nparams.put(\"serviceName\", serviceName);\nString response = HttpUtil.get(url, params);\nList<TutuServer> serverList = JSON.parseArray(response, TutuServer.class);\nreturn serverList;\n}", "CodeBleu_score": 36.88731205423533}]}
{"instance_id": "ab2fb77484fd", "generation_result": [{"code": "public Route getMatchingRoute(String path){\n\t\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n\t\tString pattern = entry.getKey();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString serviceId = entry.getValue().getServiceId();\n\t\t\tif (serviceId!= null) {\n\t\t\t\tString targetPath = path.replaceFirst(pattern, \"\");\n\t\t\t\treturn new Route(serviceId, targetPath);\n\t\t\t}\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 58.848451000028}, {"code": "public Route getMatchingRoute(String path){\n\t\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n\t\tString pattern = entry.getKey();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString targetPath = path.substring(pattern.length());\n\t\t\treturn new Route(entry.getValue().getServiceId(), targetPath);\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 56.12000356061683}, {"code": "public Route getMatchingRoute(String path){\n\t\tfor (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n\t\tString pattern = entry.getKey();\n\t\tif (pathMatcher.match(pattern, path)) {\n\t\t\tString serviceId = entry.getValue().getServiceId();\n\t\t\tString extractedPath = path.substring(pattern.length());\n\t\t\treturn new Route(extractedPath, serviceId);\n\t\t}\n\t}\n\treturn null;\n}", "CodeBleu_score": 58.21202916281756}]}
{"instance_id": "822071925513", "generation_result": [{"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\t// \u83b7\u53d6FeignClient\u6ce8\u89e3\u7684\u7c7b\nSet<String> basePackages = getBasePackages(importingClassMetadata);\n// \u83b7\u53d6FeignClient\u6ce8\u89e3\u7684\u7c7b\nSet<String> clientClasses = ClassUtil.scanPackageByAnnotation(FeignClient.class, basePackages);\nfor (String clientClass : clientClasses) {\n// \u6ce8\u518cFeignClient\u6ce8\u89e3\u7684\u7c7b\nregisterFeignClient(registry, clientClass);\n}\n}", "CodeBleu_score": 12.957383885481944}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\t// \u83b7\u53d6\u5305\u540d\nString packageName = ClassUtils.getPackageName(importingClassMetadata.getClassName());\n// \u626b\u63cf\u5305\u540d\u4e0b\u6240\u6709\u7c7b\nSet<Class<?>> classes = ClassUtil.scanPackageByAnnotation(packageName, FeignClient.class);\nfor (Class<?> clazz : classes) {\n// \u521b\u5efaBeanDefinition\nGenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n// \u8bbe\u7f6eBeanDefinition\u7684\u7c7b\u540d\nbeanDefinition.setBeanClassName(FeignClientFactoryBean.class.getName());\n// \u8bbe\u7f6eBeanDefinition\u7684\u5c5e\u6027\nbeanDefinition.getPropertyValues().add(\"contextId\", clazz.getName());\nbeanDefinition.getPropertyValues().add(\"type\", clazz);\n// \u6ce8\u518cBeanDefinition\nregistry.registerBeanDefinition(clazz.getName(), beanDefinition);\n}\n}", "CodeBleu_score": 59.46753358370456}, {"code": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry){\n\t", "CodeBleu_score": 2.4248718776194633}]}
{"instance_id": "3d678e3bf3c8", "generation_result": [{"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}, {"code": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties){\n\treturn new TutuDiscoveryClient(tutuDiscoveryProperties);\n}", "CodeBleu_score": 72.51924881462398}]}
