{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.cached_ldsr_model is not None:\n        return self.cached_ldsr_model\n    else:\n        model = instantiate_from_config(self.config)\n        model.load_state_dict(safetensors.torch.load_file(self.config.model_path))\n        model.eval()\n        if half_attention:\n            model.half_attention()\n        if self.config.memory_format == \"channels_last\":\n            model.to(memory_format=torch.channels_last)\n        self.cached_ldsr_model = model\n        return model\n\n", "CodeBleu_score": 18.72145162091772}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if not self.cached_ldsr_model or not ismap(self.cached_ldsr_model):\n        model_config = OmegaConf.load(self.model_config_path)\n        model_config.model.params.attention_config.use_cache = True\n        model_config.model.params.attention_config.rescale_output_factor = 1.0\n        model_config.model.params.attention_config.rescale_output = True\n        model_config.model.params.use_memory_efficient_attention = True\n        model_config.model.params.attention_config.use_flash_attention = True\n        model_config.model.params.attention_config.use_flash_attention_v2 = True\n        model_config.model.params.attention_config.use_flash_attention_v3 = True\n        model_config.model.params.attention_config.use_flash_attention_v4 = True\n        model_config.model.params.attention_config.use_flash_attention_v5 = True\n        model_config.model.params.attention_config.use_flash_attention_v6 = True\n        model_config.model.params.attention_config.use_flash_attention_v7 = True\n        model_config.model.params.attention_config.use_flash_attention_v8 = True\n        model_config.model.params.attention_config.use_flash_attention_v9 = True\n        model_config.model.params.attention_config.use_flash_attention_v10 = True\n        model_config.model.params.attention_config.use_flash_attention_v11 = True\n        model_config.model.params.attention_config.use_flash_attention_v12 = True\n        model_config.model.params.attention_config.use_flash_attention_v13 = True\n        model_config.model.params.attention_config.use_flash_attention_v14 = True\n        model_config.model.params.attention_config.use_flash_attention_v15 = True\n        model", "CodeBleu_score": 19.31741671171088}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    global cached_ldsr_model\n    if cached_ldsr_model is not None:\n        return {\"model\": cached_ldsr_model}\n    config = OmegaConf.load(self.config_path)\n    if self.model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load_file(self.model_path)\n    else:\n        model = torch.load(self.model_path, map_location=\"cpu\")\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(model.state_dict() if isinstance(model, torch.nn.Module) else model)\n    if half_attention:\n        model = sd_hijack.convert_attention_to_fp16(model)\n    model = model.eval()\n    if self.use_cache:\n        cached_ldsr_model = model\n    return {\"model\": model}\n\n", "CodeBleu_score": 37.619475541883965}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model, cond, shape)\n    print(f\"eta: {eta}, steps: {steps}\")\n    samples, values = sampler.sample(steps, batch_size=shape[0], shape=shape, eta=eta, callback=callback,\n                                     normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                                     temperature=temperature, score_corrector=score_corrector,\n                                     corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples, values\n\n", "CodeBleu_score": 48.61280516350192}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler. It initializes the sampler with the provided model, extracts the batch size from the shape parameter, and prints the eta and steps values. The function then calls the sample method of the DDIMSampler instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t. Finally, it returns the generated samples and intermediate values.\n\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    samples, intermediates = sampler.sample(steps, batch_size, shape, cond, eta=eta, callback=callback,\n                                            normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                            quantize_x0=quantize_x0, temperature=temperature,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                            x_t=x_t\n                                            )\n    return samples, intermediates\n\n", "CodeBleu_score": 55.70810791469577}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    samples, values = sampler.sample(S=steps, batch_size=batch_size, shape=shape, cond=cond, eta=eta,\n                                     callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                     quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector,\n                                     corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples, values\n\n", "CodeBleu_score": 55.41261218771809}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.model_ema.store(self)\n        if context is not None:\n            print(f\\\"{self.__class__.__name__}: Using EMA weights for {context}.\\\")\n        yield\n        self.model_ema.restore(self)\n    else:\n        yield\n\n    def on_train_start(self):\n        if self.use_ema:\n            self.model_ema.init(self)\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=None):\n        if ignore_keys is None:\n            ignore_keys = []\n        ignore_keys = set(ignore_keys)\n        print(f\\\"Loading {ckpt_path}.\\\")\n        if version.parse(torch.__version__) >= version.parse(\\\"1.12.0\\\"):\n            sd = torch.load(ckpt_path, map_location=\\\"cpu\\\")\n        else:\n            sd = torch.load(ckpt_path, map_location=\\\"cpu\\\", pickle_module=pickle)\n        sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n        self.load_state_dict(sd, strict=False)\n\n    def forward(self, x):\n        if self.batch_resize_range is not None:\n            h, w = x.shape[-2:]\n            resize_factor = torch.randint(self.batch_resize_range[0], self.batch_resize_range[1], (1,)).item()\n            x = F.interpolate(x, (h // resize_factor, w // resize_factor), mode=\\\"bilinear\\\")\n        z, _, _ = self.encoder(x)\n        z = self.quant_conv(z)\n        z_q, _, _ = self.quantize(z)\n        x_recon = self.decoder(self.post_quant_conv(z_q))\n        return x_recon\n\n    def training_step(self, batch, batch_idx):\n        x = batch[self.image_key]\n        x_recon = self(x)\n        loss = self.loss(x,", "CodeBleu_score": 36.88826906438211}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is not None:\n        print(f\"Switching to EMA weights for {context}.\")\n    self.model_ema.store(self)\n    yield\n    if context is not None:\n        print(f\"Switching back to training weights for {context}.\")\n    self.model_ema.restore(self)\n\n", "CodeBleu_score": 46.662767341127584}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            print(f\\\"{context}: Using EMA weights.\\\")\n        self.model_ema.store()\n        self.model_ema.copy_to()\n        yield\n        if context is not None:\n            print(f\\\"{context}: Restored weights.\\\")\n        self.model_ema.restore()\n    else:\n        yield\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=None):\n        if ignore_keys is None:\n            ignore_keys = []\n        if ckpt_path is not None:\n            print(f\\\"{self.__class__.__name__}: Loading {ckpt_path}.\\\")\n            ckpt = torch.load(ckpt_path, map_location=\\\"cpu\\\")\n            if version.parse(torch.__version__) >= version.parse(\\\"2.0.0\\\"):\n                ckpt = {k.replace(\\\"module.\\\", \\\"\\\"): v for k, v in ckpt[\\\"state_dict\\\"].items()}\n            self.load_state_dict(ckpt, strict=False)\n            for key in ignore_keys:\n                del self.state_dict()[key]\n            print(f\\\"{self.__class__.__name__}: Loaded {ckpt_path}.\\\")\n\n    def get_batch_resize_factor(self, batch_size):\n        if self.batch_resize_range is None:\n            return 1\n        else:\n            return np.random.uniform(*self.batch_resize_range)\n\n    def get_batch_resize_factor_image(self, image):\n        if self.batch_resize_range is None:\n            return image\n        else:\n            return F.interpolate(image, scale_factor=self.get_batch_resize_factor(image.shape[0]), mode=\\\"bilinear\\\", align_corners=False)\n\n    def forward(self, image, **kwargs):\n        image = self.get_batch_resize_factor_image(image)\n        z = self.encoder(image)\n        z = self.quant_conv(z)\n        loss, quant, perplexity, _ =", "CodeBleu_score": 37.58879067586856}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    ignore_keys = ignore_keys or []\n    state_dict = torch.load(path)\n    if ignore_keys:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys, unexpected_keys = self.named_missing_keys_and_unexpected_keys(state_dict)\n    if missing_keys:\n        print(f\\\"{self.__class__.__name__}: Missing keys: {missing_keys}\\\")\n    if unexpected_keys:\n        print(f\\\"{self.__class__.__name__}: Unexpected keys: {unexpected_keys}\\\")\n\n", "CodeBleu_score": 24.615848990604654}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            sd.pop(k, None)\n    self.load_state_dict(sd, strict=False)\n    missing_keys, unexpected_keys = [], []\n    for name, param in self.named_parameters():\n        if param.requires_grad and param.grad is not None:\n            if param.grad.is_sparse:\n                raise RuntimeError(\n                    \\\"Sparse gradients are not supported yet!\\\"\n                )\n        if name not in sd:\n            missing_keys.append(name)\n        elif sd[name].shape!= param.shape:\n            unexpected_keys.append(name)\n    if missing_keys or unexpected_keys:\n        print(f\\\"{self.__class__.__name__}: Missing keys: {missing_keys}\\\")\n        print(f\\\"{self.__class__.__name__}: Unexpected keys: {unexpected_keys}\\\")\n\n\n", "CodeBleu_score": 30.577916306410422}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    print(f\"Loading {path}\")\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            state_dict.pop(key, None)\n    self.load_state_dict(state_dict, strict=False)\n    print(f\\\"Loaded {path}\\\")\n    missing_keys, unexpected_keys = self.named_buffers()\n    if missing_keys:\n        print(f\\\"Missing keys: {missing_keys}\\\")\n    if unexpected_keys:\n        print(f\\\"Unexpected keys: {unexpected_keys}\\\")\n\n    def step(self, batch, batch_idx):\n        loss = self.loss(self, batch)\n        self.log(\\\"loss\\\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        if self.scheduler_config is None:\n            return torch.optim.AdamW(self.parameters(), lr=1e-4)\n        else:\n            opt_config = self.scheduler_config[\\\"opt_config\\\"]\n            scheduler_config = self.scheduler_config[\\\"scheduler_config\\\"]\n            opt = torch.optim.AdamW(self.parameters(), **opt_config)\n            if scheduler_config[\\\"name\\\"] == \\\"CosineAnnealingWarmRestarts\\\":\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, **scheduler_config[\\\"config\\\"])\n            elif scheduler_config[\\\"name\\\"] == \\\"CosineAnnealingLR\\\":\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, **scheduler_config[\\\"config\\\"])\n            elif scheduler_config[\\\"name\\\"] == \\\"LambdaLR\\\":\n                scheduler = LambdaLR(opt, **scheduler_config[\\\"config\\\"])\n            else:\n                raise ValueError(f\\\"Unknown scheduler: {scheduler_config}\\\")\n            return [opt], [scheduler]\n\n    def forward(self, x, **kwargs):\n        z, _, _ = self.encoder(x)\n        z = self.quantize(self.quant_conv(z))\n        return self.decoder(self", "CodeBleu_score": 29.622001449794382}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    with self.ema_scope(\\\"Forward\\\"):\n        quant, emb_loss, info = self.encode(input)\n        dec = self.decode(quant)\n        diff = self.loss(input, dec)\n        if return_pred_indices:\n            return dec, diff, info[\\\"pred_indices\\\"]\n        else:\n            return dec, diff\n\n    def training_step(self, batch, batch_idx):\n        if self.batch_resize_range is not None:\n            batch = self.batch_resize(batch)\n        dec, diff = self.forward(batch)\n        self.log(\\\"train_loss\\\", diff)\n        return diff\n\n    def validation_step(self, batch, batch_idx):\n        if self.batch_resize_range is not None:\n            batch = self.batch_resize(batch)\n        dec, diff = self.forward(batch)\n        self.log(\\\"val_loss\\\", diff)\n        return diff\n\n    def test_step(self, batch, batch_idx):\n        if self.batch_resize_range is not None:\n            batch = self.batch_resize(batch)\n        dec, diff = self.forward(batch)\n        self.log(\\\"test_loss\\\", diff)\n        return diff\n\n    def batch_resize(self, batch):\n        new_size = np.random.randint(*self.batch_resize_range)\n        batch = F.interpolate(batch, new_size, mode=\\\"bilinear\\\")\n        return batch\n\n    def configure_optimizers(self):\n        if self.scheduler_config is not None:\n            if version.parse(pl.__version__) < version.parse(\\\"1.4.0\\\"):\n                raise NotImplementedError(\\\"Scheduler config is not supported in PyTorch Lightning < 1.4.0\\\")\n            if self.scheduler_config[\\\"name\\\"] == \\\"cosine_warmup\\\":\n                scheduler_config = self.scheduler_config\n                scheduler_config[\\\"warmup_end_lr\\\"] = 0.0\n                scheduler_config[\\\"warmup_start_lr\\\"] = 0.0\n                scheduler_config[\\\"warmup_steps\\\"] = 0\n                scheduler_", "CodeBleu_score": 37.85422127650788}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    with self.ema_scope(\\\"Forward\\\"):\n        x = input[self.image_key]\n        if self.batch_resize_range is not None:\n            x = resize_batch(x, self.batch_resize_range)\n        x = x.to(self.device)\n        quant, emb_loss, info = self.encode(x)\n        if return_pred_indices:\n            pred_indices = info[\\\"indices\\\"]\n        dec = self.decode(quant)\n        diff = self.loss(x, dec)\n        if return_pred_indices:\n            return dec, diff, pred_indices\n        else:\n            return dec, diff\n\n    def training_step(self, batch, batch_idx):\n        dec, diff = self(batch)\n        if self.monitor is not None:\n            self.monitor(batch, dec)\n        self.log(\\\"train_loss\\\", diff, on_epoch=True, on_step=False)\n        return diff\n\n    def validation_step(self, batch, batch_idx):\n        dec, diff = self(batch)\n        if self.monitor is not None:\n            self.monitor(batch, dec)\n        self.log(\\\"val_loss\\\", diff, on_epoch=True, on_step=False)\n        return diff\n\n    def configure_optimizers(self):\n        opt_g = torch.optim.Adam(self.parameters(), lr=self.lr_g_factor * self.scheduler_config[\\\"lr_g\\\"], betas=(0.9, 0.999), weight_decay=self.scheduler_config[\\\"weight_decay\\\"])\n        opt_q = torch.optim.Adam(self.quantize.parameters(), lr=self.scheduler_config[\\\"lr_q\\\"], betas=(0.9, 0.999), weight_decay=self.scheduler_config[\\\"weight_decay\\\"])\n        sched_g = LambdaLR(opt_g, self.scheduler_config[\\\"lr_g_schedule\\\"])\n        sched_q = LambdaLR(opt_q, self.scheduler_config[\\\"lr_q_schedule\\\"])\n        return [opt_g, opt_", "CodeBleu_score": 38.02160727803518}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.batch_resize_range is not None:\n        # resize the batch to a random size in the specified range\n        # this is to help with training stability\n        # see https://github.com/CompVis/latent-diffusion/issues/23\n        batch_size = input.shape[0]\n        resize_factor = np.random.uniform(*self.batch_resize_range)\n        input = F.interpolate(input, scale_factor=resize_factor, mode=\\\"bilinear\\\", align_corners=False)\n        input = input[:batch_size]\n    quant, emb_loss, info = self.encode(input)\n    pred = self.decode(quant)\n    diff = self.loss(pred, input)\n    if return_pred_indices:\n        return pred, diff, quant, info\n    else:\n        return pred, diff\n\n", "CodeBleu_score": 33.76487961656125}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if input.ndim == 3:\n        input = input.unsqueeze(0)\n    input = input.permute(0, 3, 1, 2)\n    input = input.to(self.device).contiguous().float()\n    if self.batch_resize_range is not None:\n        input = F.interpolate(input, scale_factor=self.batch_resize_range[self.global_step % len(self.batch_resize_range)], mode=\\\"bicubic\\\")\n    input = input.detach()\n    return input\n\n\n", "CodeBleu_score": 32.10382648397995}, {"code": "def get_input(self, batch, k)\n\t\n    if k == \"image\":\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x.unsqueeze(0)\n        x = x.permute(0, 3, 1, 2)\n        x = x.to(torch.float32)\n        if self.batch_resize_range is not None:\n            cur_step = self.trainer.global_step\n            resize_range = self.batch_resize_range\n            resize_steps = resize_range[1] - resize_range[0]\n            resize_factor = (cur_step - resize_range[0]) / resize_steps\n            resize_factor = np.clip(resize_factor, 0.0, 1.0)\n            resize_factor = 1.0 - resize_factor\n            resize_factor = 1.0 - resize_factor ** 2\n            resize_factor = resize_range[2] * resize_factor\n            resize_factor = int(resize_factor)\n            if resize_factor > 0:\n                x = F.interpolate(x, scale_factor=resize_factor, mode=\\\"bicubic\\\")\n                x = x.permute(0, 2, 3, 1)\n                x = x.contiguous()\n                x = x.permute(0, 3, 1, 2)\n        x = x.detach()\n        return x\n    else:\n        raise ValueError(f\\\"Unknown key {k}\\\")\n\n\n", "CodeBleu_score": 48.62942650289597}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(torch.float32)\n    x = x.contiguous()\n    if self.batch_resize_range is not None:\n        if self.global_step is None:\n            raise ValueError(\\\"Must set global_step before using batch resizing\\\")\n        scale = self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * self.global_step / self.max_steps\n        x = F.interpolate(x, scale_factor=scale, mode=\\\"bicubic\\\", align_corners=False)\n    x = x.detach()\n    return x\n\n    def training_step(self, batch, batch_idx):\n        x = self.get_input(batch, self.image_key)\n        x_enc, diff, info = self.encode(x)\n        loss = self.loss(x, x_enc, diff)\n        self.log(\\\"train_loss\\\", loss)\n        if self.monitor is not None:\n            self.monitor.update(loss.item())\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = self.get_input(batch, self.image_key)\n        x_enc, diff, info = self.encode(x)\n        loss = self.loss(x, x_enc, diff)\n        self.log(\\\"val_loss\\\", loss)\n        if self.monitor is not None:\n            self.monitor.update(loss.item())\n        return loss\n\n    def configure_optimizers(self):\n        g_params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n        g_params += list(self.quant_conv.parameters()) + list(self.post_quant_conv.parameters())\n        g_params += list(self.quantize.parameters())\n        if hasattr(self, \\\"colorize\\\"):\n", "CodeBleu_score": 44.715595121233}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    x = self.get_input(batch, self.image_key)\n\n    if optimizer_idx == 0:\n        # autoencoding loss\n        pred, diff, ind = self.forward(x, return_pred_indices=True)\n        loss = self.loss(pred, x, diff)\n        if self.monitor is not None:\n            self.log(\\\"loss\\\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n            self.monitor.update(loss.item())\n        return loss\n    elif optimizer_idx == 1:\n        # discriminator loss\n        with self.ema_scope(\\\"discriminator loss\\\"): # use ema for discriminator loss\n            pred, diff, ind = self.forward(x, return_pred_indices=True)\n            loss = self.loss(pred, x, diff)\n        return loss\n    else:\n        raise NotImplementedError\n\n    def training_step_end(self, outputs):\n        \"\"\"This function is called after the training step is completed, and it is used to aggregate the outputs from different optimizers.", "CodeBleu_score": 35.99627672347204}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    if self.ddim_autoencoder_loss_enabled:\n        # DDIM Autoencoder Loss\n        x = self.get_input(batch, self.image_key)\n        x_ddim = self.get_input(batch, \\\"image_ddim\\\")\n        if self.batch_resize_range is not None:\n            x_ddim = F.interpolate(x_ddim, size=x.shape[2], mode=\\\"bicubic\\\")\n        x_ddim = x_ddim.detach()\n\n        if self.ddim_autoencoder_loss_weight == 0:\n            # this is a hack to make the training stable\n            # when the loss weight is set to 0\n            loss = torch.tensor(0.0)\n        else:\n            with self.ema_scope(\\\"DDIM Autoencoder Loss\\\"):  # TODO: only for encoder\n                with torch.no_grad():\n                    _, diff, ind = self.encode(x_ddim)\n                pred, diff_loss, _ = self.encode(x)\n                loss = self.ddim_autoencoder_loss_weight * diff_loss\n\n        if self.monitor is not None:\n            self.monitor.update(loss=loss.item(), n_samples=x.shape[0])\n        self.log(\\\"train_loss\\\", loss, on_step=True, on_epoch=False, prog_bar=True)\n        return loss\n\n    if optimizer_idx == 0:\n        # Autoencoder Loss\n        x = self.get_input(batch, self.image_key)\n        pred, diff_loss, _ = self.encode(x)\n        loss = self.loss(pred, x)\n\n        if self.monitor is not None:\n            self.monitor.update(loss=loss.item(), n_samples=x.shape[0])\n        self.log(\\\"train_loss\\\", loss, on_step=True, on_epoch=False, prog_bar=True)\n        return loss\n    else:\n        # Discriminator Loss\n        x = self.get_input(batch, self.image_key)\n        pred, diff", "CodeBleu_score": 37.98000278937008}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    input = self.get_input(batch, self.image_key)\n    if self.colorize is not None:\n        input = torch.cat([input, self.colorize], dim=1)\n    pred, diff, pred_indices = self.forward(input, return_pred_indices=True)\n    if optimizer_idx == 0:\n        loss = self.loss(pred, input, diff)\n        self.log(\\\"loss\\\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        if self.monitor is not None:\n            self.monitor(pred, input)\n        return loss\n    elif optimizer_idx == 1:\n        loss = self.loss(pred, input, diff)\n        self.log(\\\"loss_disc\\\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        if self.monitor is not None:\n            self.monitor(pred, input)\n        return loss\n\n    def configure_optimizers(self):\n        if self.scheduler_config is None:\n            return torch.optim.Adam(self.parameters(), lr=1e-4)\n        else:\n            lr_g = self.scheduler_config[\\\"lr_g\\\"]\n            lr_d = self.scheduler_config[\\\"lr_d\\\"]\n            print(f\\\"{self.__class__.__name__}: Using learning rates {lr_g} and {lr_d}.\\\")\n            opt_g = torch.optim.Adam(self.parameters(), lr=lr_g)\n            opt_d = torch.optim.Adam(self.parameters(), lr=lr_d)\n            if self.scheduler_config[\\\"sched_g\\\"] is not None:\n                sched_g = self.get_lr_scheduler(self.scheduler_config[\\\"sched_g\\\"])\n                opt_g = self.configure_optimizers_with_scheduler(opt_g, sched_g)\n            if self.scheduler_config[\\\"sched_d\\\"] is not None:\n                sched_d = self.get_lr_scheduler(self.scheduler_config[\\\"sched_d\\\"])\n                opt_d = self.configure_optimizers_with_scheduler(opt_", "CodeBleu_score": 37.97103103857752}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if plot_ema and self.use_ema:\n        self.model_ema(self)\n    if only_inputs:\n        x = batch[self.image_key]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize!= x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n            x = x.detach()\n        x = x.cpu().numpy()\n        if self.num_channels == 1:\n            x = x[..., 0]\n        if x.shape[-1] == 1:\n            x = np.tile(x, (1, 1, 3))\n        x = x.transpose(0, 2, 3, 1)\n        self.logger.experiment.log(x, step=self.global_step, **kwargs)\n        return\n\n    x = batch[self.image_key]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom", "CodeBleu_score": 27.270978696340247}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if only_inputs:\n        x = self.get_input(batch, self.image_key)\n        if plot_ema:\n            with self.ema_scope():\n                xrec = self(x)[0]\n        else:\n            xrec = self(x)[0]\n        x = x.detach()\n        xrec = xrec.detach()\n        x = x.permute(0, 2, 3, 1)\n        xrec = xrec.permute(0, 2, 3, 1)\n        if x.shape[3] == 1:\n            x = x[..., 0]\n            xrec = xrec[..., 0]\n        if x.shape[3] == 3:\n            x = x.cpu().numpy()\n            xrec = xrec.cpu().numpy()\n        else:\n            x = x.cpu().numpy()\n            xrec = xrec.cpu().numpy()\n            x = colorize(x)\n            xrec = colorize(xrec)\n        self.logger.experiment.log({\n            \\\"train/inputs\\\": [wandb.Image(x[0], caption=\\\"input\\\"),\n                              wandb.Image(x[1], caption=\\\"input\\\"),\n                              wandb.Image(x[2], caption=\\\"input\\\")],\n            \\\"train/inputs_rec\\\": [wandb.Image(xrec[0], caption=\\\"reconstruction\\\"),\n                                   wandb.Image(xrec[1], caption=\\\"reconstruction\\\"),\n                                   wandb.Image(xrec[2], caption=\\\"reconstruction\\\")],\n        })\n        return\n\n    x = self.get_input(batch, self.image_key)\n    if plot_ema:\n        with self.ema_scope():\n            xrec = self(x)[0]\n    else:\n        xrec = self(x)[0]\n    x = x.detach()\n    xrec = xrec.detach()\n    x = x.permute(0, 2, 3, 1)\n    xrec = xrec.permute(0, 2, 3, 1)\n    if x", "CodeBleu_score": 32.98173839289194}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if only_inputs:\n        return\n    x = self.get_input(batch, self.image_key)\n    if x.shape[1] == 1:\n        x = x.repeat(1, 3, 1, 1)\n    x = x.detach().cpu()\n    if plot_ema:\n        with self.ema_scope():\n            xrec, _, _ = self(x)\n    else:\n        xrec, _, _ = self(x)\n    xrec = xrec.detach().cpu()\n    x = vutils.make_grid(x, nrow=8, normalize=True, range=(-1, 1))\n    xrec = vutils.make_grid(xrec, nrow=8, normalize=True, range=(-1, 1))\n    if plot_ema:\n        self.logger.experiment.add_image(\\\"val_ema/inputs_and_reconstructions\\\",\n                                         torch.cat([x, xrec]), self.global_step, dataformats=\\\"HWC\\\")\n    else:\n        self.logger.experiment.add_image(\\\"val/inputs_and_reconstructions\\\",\n                                         torch.cat([x, xrec]), self.global_step, dataformats=\\\"HWC\\\")\n\n\n", "CodeBleu_score": 37.68792962564411}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} not supported.')\n    return loss\n\n", "CodeBleu_score": 51.78710464761873}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 51.900800949365134}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = F.l1_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 51.73297631677356}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.has_time_embed:\n        t = kwargs.get('t', None)\n        if t is None:\n            t = torch.randint(0, self.num_time_embed, (x.shape[0],))\n        x = torch.cat((x, self.time_embed(t)), dim=1)\n\n    if self.has_cond_embed:\n        c = self.cond_embed(c)\n        x = torch.cat((x, c), dim=1)\n\n    if self.has_learned_cond_embed:\n        c = self.learned_cond_embed(c)\n        x = torch.cat((x, c), dim=1)\n\n    return self.model(x, *args, **kwargs)\n\n", "CodeBleu_score": 40.84250267592958}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training and self.conditioning_type == \"learned\":\n        if self.conditioning_scale == \"full\":\n            c = self.conditioning_net(x, c)\n        elif self.conditioning_scale == \"partial\":\n            c = self.conditioning_net(x)\n    if self.training and self.timestep_embedding_type == \"learned\":\n        t = self.timestep_embedding(torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device))\n    else:\n        t = torch.linspace(0, 1, self.num_timesteps, device=x.device)\n    if self.training:\n        t = t.repeat(x.shape[0], 1, 1, 1, 1)\n    else:\n        t = t.repeat(x.shape[0], 1, 1, 1, 1)\n    x = torch.cat([x, t], dim=1)\n    if self.conditioning_type == \"concat\":\n        x = torch.cat([x, c], dim=1)\n    elif self.conditioning_type == \"mlp\":\n        x = torch.cat([x, self.conditioning_net(c)], dim=1)\n    elif self.conditioning_type == \"learned\":\n        x = torch.cat([x, c], dim=1)\n    if self.training:\n        loss = self.loss(x)\n    else:\n        loss = self.loss(x)\n    return loss\n\n", "CodeBleu_score": 39.66758790286998}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training and self.random_time_step:\n        time_step = torch.randint(0, self.num_time_steps, (x.shape[0],), device=x.device)\n    else:\n        time_step = self.num_time_steps - 1\n\n    if self.conditioning_type is not None:\n        c = self.conditioning(c)\n\n    if self.conditioning_type == \\\"2d\\\":\n        x = torch.cat((x, c), dim=1)\n\n    elif self.conditioning_type == \\\"3d\\\":\n        x = torch.cat((x, c.unsqueeze(2)), dim=1)\n\n    elif self.conditioning_type == \\\"4d\\\":\n        x = torch.cat((x, c.unsqueeze(2).unsqueeze(3)), dim=1)\n\n    x = self.model(x, time_step, *args, **kwargs)\n\n    if self.loss_type == \\\"l1\\\":\n        loss = F.l1_loss(x, c)\n\n    elif self.loss_type == \\\"mse\\\":\n        loss = F.mse_loss(x, c)\n\n    elif self.loss_type == \\\"huber\\\":\n        loss = F.smooth_l1_loss(x, c)\n\n    elif self.loss_type == \\\"ssim\\\":\n        loss = 1 - ssim(x, c, data_range=1.0)\n\n    elif self.loss_type == \\\"psnr\\\":\n        loss = 1 - psnr(x, c, data_range=1.0)\n\n    elif self.loss_type == \\\"lpips\\\":\n        loss = 1 - lpips(x, c, normalize=True)\n\n    elif self.loss_type == \\\"vgg\\\":\n        loss = 1 - vgg(x, c, normalize=True)\n\n    elif self.loss_type == \\\"perceptual\\\":\n        loss = 1 - perceptual(x, c, normalize=True)\n\n    elif self.loss_type ==", "CodeBleu_score": 39.64221477605279}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t    if self.remap is not None:\n        indices = self.unmap_to_all(indices)\n    return rearrange(self.embedding(indices), \"b h w c -> b c h w\").reshape(shape)\n", "CodeBleu_score": 25.909372806804036}, {"code": "def get_codebook_entry(self, indices, shape)\n\t    if self.remap is not None:\n        indices = self.unmap_to_all(indices)\n    indices = indices.reshape(-1)\n    entry = self.embedding(indices)\n    entry = entry.reshape(shape)\n    return entry\n", "CodeBleu_score": 41.90515837839375}, {"code": "def get_codebook_entry(self, indices, shape)\n\t    indices = indices.reshape(-1)\n    if self.remap is not None:\n        indices = self.unmap_to_all(indices)\n    return self.embedding(indices).view(shape).permute(0, 3, 1, 2).contiguous()\n\n", "CodeBleu_score": 33.5490702110247}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        factor = dimension\n    m = 1\n    n = dimension\n    for i in range(2, factor + 1):\n        if dimension % i == 0:\n            m = i\n            n = dimension // i\n            if m > n:\n                m, n = n, m\n            if m <= n:\n                break\n    return m, n\n\n", "CodeBleu_score": 47.9665549798165}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor > 0:\n        factor = -1\n    if factor < 0:\n        factor = -factor\n    m = int(np.sqrt(dimension))\n    while m > 0:\n        if dimension % m == 0:\n            n = dimension // m\n            if n >= m:\n                return m, n\n        m -= 1\n    return 1, dimension\n\n", "CodeBleu_score": 37.338771599376514}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"Dimension {dimension} cannot be divided by factor {factor}.\")\n    else:\n        factor = 1\n        while factor ** 2 <= dimension:\n            if dimension % factor == 0:\n                m, n = factor, dimension // factor\n                if m <= n:\n                    return m, n\n            factor += 1\n        raise ValueError(f\"Dimension {dimension} cannot be factorized.\")\n\n", "CodeBleu_score": 37.47576195085843}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if self.sd_module is None:\n        return None\n\n    weight = updown.get_weight()\n\n    if weight is None:\n        return None\n\n    if self.sd_module.weight.shape!= weight.shape:\n        return None\n\n    if self.sd_module.weight.dtype!= weight.dtype:\n        return None\n\n    if self.sd_module.weight.device!= weight.device:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self.sd_module.weight.shape!= orig_weight.shape:\n        return None\n\n    if self.sd_module.weight.device!= orig_weight.device:\n        return None\n\n    if self.sd_module.weight.dtype!= orig_weight.dtype:\n        return None\n\n    if self", "CodeBleu_score": 13.27668971931988}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if updown == 'up':\n        if self.dora_scale is not None:\n            return self.dora_scale * orig_weight\n        else:\n            return orig_weight\n    else:\n        if self.dora_scale is not None:\n            return orig_weight / self.dora_scale\n        else:\n            return orig_weight\n\n", "CodeBleu_score": 12.140106226218851}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if not hasattr(self, \"orig_weight\"):\n        self.orig_weight = orig_weight\n    if not hasattr(self, \"updown\"):\n        self.updown = updown\n    if not hasattr(self, \"scale\"):\n        self.scale = 1.0\n    if not hasattr(self, \"dora_scale\"):\n        self.dora_scale = None\n    if not hasattr(self, \"dora_norm_dims\"):\n        self.dora_norm_dims = 0\n    if not hasattr(self, \"dora_norm_eps\"):\n        self.dora_norm_eps = 1e-05\n    if not hasattr(self, \"dora_norm_affine\"):\n        self.dora_norm_affine = True\n    if not hasattr(self, \"dora_norm_track_running_stats\"):\n        self.dora_norm_track_running_stats = True\n    if not hasattr(self, \"dora_norm_inplace\"):\n        self.dora_norm_inplace = False\n    if not hasattr(self, \"dora_norm_momentum\"):\n        self.dora_norm_momentum = 0.1\n    if not hasattr(self, \"dora_norm_running_mean\"):\n        self.dora_norm_running_mean = None\n    if not hasattr(self, \"dora_norm_running_var\"):\n        self.dora_norm_running_var = None\n    if not hasattr(self, \"dora_norm_num_batches_tracked\"):\n        self.dora_norm_num_batches_tracked = None\n    if not hasattr(self, \"dora_norm_weight\"):\n        self.dora_norm_weight = None\n    if not hasattr(self, \"dora_norm_bias\"):\n        self.dora_norm_bias = None\n    if not hasattr(self, \"dora_norm_weight_orig\"):\n        self.dora_norm_weight_orig = None\n    if not hasattr(self, \"dora_norm_bias_orig\"):\n        self.dora_norm_bias_orig = None\n    if not hasattr(self, \"dora_norm_weight_adjusted\"):\n        self.dora_norm_weight_adjusted = None\n    if not hasattr(self, \"dora_norm_bias_adjusted\"):\n        self.d", "CodeBleu_score": 12.736514304816183}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n\n    if self.bias is not None:\n        updown = updown + self.bias.to(device=updown.device, dtype=updown.dtype)\n\n    updown = updown.reshape(output_shape)\n\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n\n    if self.scale is not None:\n        updown = updown * self.scale\n    elif self.alpha is not None:\n        updown = updown * self.alpha / self.dim\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 67.33350840418204}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n\n    updown = updown + self.bias\n\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 50.25761658001689}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown + self.bias\n    updown = updown.reshape(output_shape)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 49.676008022947556}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.ex_bias is not None:\n        bias = self.ex_bias.to(orig_weight.device)\n    else:\n        bias = None\n\n    return self._calc_updown(orig_weight, self.weight, bias)\n\n", "CodeBleu_score": 42.5712609090418}, {"code": "def calc_updown(self, orig_weight)\n\t\n    # Transfer the current weight tensor to the device of the original weight tensor\n    if self.weight.device!= orig_weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n\n    if self.ex_bias is not None:\n        if self.ex_bias.device!= orig_weight.device:\n            self.ex_bias = self.ex_bias.to(orig_weight.device)\n\n    # Call the function to finalize the updated weights\n    return self._calc_updown(orig_weight)\n\n", "CodeBleu_score": 47.50903940977603}, {"code": "def calc_updown(self, orig_weight)\n\t\n    return self.calc_updown_device(orig_weight, self.weight.to(orig_weight.device), self.ex_bias.to(orig_weight.device) if self.ex_bias is not None else None)\n\n", "CodeBleu_score": 23.890879960408675}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(shared.networks) > shared.network_limit:\n        logging.debug(f\\\"Purging {len(shared.networks) - shared.network_limit} networks from memory\\\")\n\n        for network in list(shared.networks.values())[:len(shared.networks) - shared.network_limit]:\n            network.unload()\n\n        gc.collect()\n\n\n\n\n", "CodeBleu_score": 29.141833588483756}, {"code": "def purge_networks_from_memory()\n\t\n    if len(shared.networks) > shared.config.max_networks_in_memory:\n        to_remove = sorted(shared.networks.values(), key=lambda x: x.mtime, reverse=True)[shared.config.max_networks_in_memory:]\n        for net in to_remove:\n            logging.info(f\\\"Purging network {net.name} from memory\\\")\n            shared.networks.pop(net.name)\n        gc.collect()\n\n", "CodeBleu_score": 27.21332236960121}, {"code": "def purge_networks_from_memory()\n\t\n    if len(shared.networks) > shared.config.max_networks_in_memory:\n        networks_to_remove = []\n        for name, network in shared.networks.items():\n            if network.mtime < time.time() - shared.config.max_age_of_unused_networks_in_memory:\n                networks_to_remove.append(name)\n\n        for name in networks_to_remove:\n            shared.networks.pop(name)\n\n        gc.collect()\n\n", "CodeBleu_score": 32.03591097346295}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight = self.weight_backup\n\n    if self.bias_backup is not None:\n        self.bias = self.bias_backup\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight_backup is not None:\n            self.in_proj_weight = self.in_proj_weight_backup\n\n        if self.in_proj_bias_backup is not None:\n            self.in_proj_bias = self.in_proj_bias_backup\n\n    self.weight_backup = None\n    self.bias_backup = None\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight_backup = None\n        self.in_proj_bias_backup = None\n\n\n\n", "CodeBleu_score": 37.81994123468283}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup') and self.weight_backup is not None:\n        self.weight = self.weight_backup\n        self.weight_backup = None\n\n    if hasattr(self, 'bias_backup') and self.bias_backup is not None:\n        self.bias = self.bias_backup\n        self.bias_backup = None\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'in_proj_weight_backup') and self.in_proj_weight_backup is not None:\n            self.in_proj_weight = self.in_proj_weight_backup\n            self.in_proj_weight_backup = None\n\n        if hasattr(self, 'in_proj_bias_backup') and self.in_proj_bias_backup is not None:\n            self.in_proj_bias = self.in_proj_bias_backup\n            self.in_proj_bias_backup = None\n\n\n\n", "CodeBleu_score": 38.49304772300463}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight.data = self.weight_backup\n        self.weight_backup = None\n\n    if self.bias_backup is not None:\n        self.bias.data = self.bias_backup\n        self.bias_backup = None\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight_backup is not None:\n            self.in_proj_weight.data = self.in_proj_weight_backup\n            self.in_proj_weight_backup = None\n\n        if self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup\n            self.in_proj_bias_backup = None\n\n\n\n", "CodeBleu_score": 33.6315480225195}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not loaded_networks:\n        return original_forward(input)\n\n    input = input.to(devices.current)\n\n    network_restore_weights_from_backup(org_module)\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad():\n                    output = module.apply_forward(output, org_module.network_layer_name)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n            continue\n\n        module_q = net.modules.get(org_module.network_layer_name + \"_q_proj\", None)\n        module_k = net.modules.get(org_module.network_layer_name + \"_k_proj\", None)\n        module_v = net.modules.get(org_module.network_layer_name + \"_v_proj\", None)\n        module_out = net.modules.get(org_module.network_layer_name + \"_out_proj\", None)\n\n        if isinstance(org_module, torch.nn.MultiheadAttention) and module_q and module_k and module_v and module_out:\n            try:\n                with torch.no_grad():\n                    # Send \"real\" orig_weight into MHA's lora module\n                    qw, kw, vw = org_module.in_proj_weight.chunk(3, 0)\n                    updown_q, _ = module_q.calc_updown(qw)\n                    updown_k, _ = module_k.calc_updown(kw)\n                    updown_v, _ = module_v.calc_updown(vw)\n                    del qw, kw, vw\n                    updown_qkv = torch.vstack([updown_q, updown_k, updown_v])\n                    updown_out, ex_bias = module_out.calc_updown(org_module.out_proj.weight)\n\n                    org_module", "CodeBleu_score": 32.695474206169735}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n\n    input = input.to(devices.get_device(input))\n\n    if getattr(org_module, \"network_current_names\", None) is None:\n        network_restore_weights_from_backup(org_module)\n        org_module.network_current_names = ()\n\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad():\n                    output = module(output)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n            continue\n\n        module_q = net.modules.get(org_module.network_layer_name + \"_q_proj\", None)\n        module_k = net.modules.get(org_module.network_layer_name + \"_k_proj\", None)\n        module_v = net.modules.get(org_module.network_layer_name + \"_v_proj\", None)\n        module_out = net.modules.get(org_module.network_layer_name + \"_out_proj\", None)\n\n        if isinstance(org_module, torch.nn.MultiheadAttention) and module_q and module_k and module_v and module_out:\n            try:\n                with torch.no_grad():\n                    # Send \"real\" orig_weight into MHA's lora module\n                    qw, kw, vw = org_module.in_proj_weight.chunk(3, 0)\n                    updown_q, _ = module_q(qw)\n                    updown_k, _ = module_k(kw)\n                    updown_v, _ = module_v(vw)\n                    del qw, kw, vw\n                    updown_qkv = torch.vstack([updown_q, updown_k, updown_v])\n                    updown_out, ex_bias = module_out(org_module.out_proj", "CodeBleu_score": 33.99972819319113}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n\n    input = input.to(devices.get_torch_device())\n\n    network_restore_weights_from_backup(org_module)\n    network_reset_weights(org_module)\n\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                output = module.forward(output)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n            continue\n\n        if isinstance(org_module, torch.nn.MultiheadAttention):\n            module_q = net.modules.get(org_module.network_layer_name + \"_q_proj\", None)\n            module_k = net.modules.get(org_module.network_layer_name + \"_k_proj\", None)\n            module_v = net.modules.get(org_module.network_layer_name + \"_v_proj\", None)\n            module_out = net.modules.get(org_module.network_layer_name + \"_out_proj\", None)\n\n            if module_q and module_k and module_v and module_out:\n                try:\n                    output = module_q.forward(output[0])\n                    output = module_k.forward(output[1])\n                    output = module_v.forward(output[2])\n                    output = module_out.forward(output)\n                except RuntimeError as e:\n                    logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                    extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n                continue\n\n        if module is None:\n            continue\n\n        logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: couldn't find supported operation\")\n        extra_network_lora.errors[net.", "CodeBleu_score": 38.27555581157876}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t  trainables = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 58.39896110930864}, {"code": "def trainables(self)\n\t  return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.32156763555954}, {"code": "def trainables(self)\n\t  return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.32156763555954}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.loaded_hypernetworks:\n            hypernetworks.append(shared.loaded_hypernetworks[name])\n        else:\n            hypernetwork = load_hypernetwork(name)\n            if hypernetwork:\n                hypernetwork.set_multiplier(multiplier)\n                hypernetworks.append(hypernetwork)\n            else:\n                errors.report(f\\\"Error loading hypernetwork {name}\\\")\n\n    shared.loaded_hypernetworks.clear()\n    for hypernetwork in hypernetworks:\n        shared.loaded_hypernetworks[hypernetwork.name] = hypernetwork\n\n    return hypernetworks\n\n\n\n\n", "CodeBleu_score": 36.77480331040602}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    # Check for already loaded hypernetworks\n    loaded_hypernetworks = [hypernetwork for hypernetwork in shared.hypernetworks if hypernetwork.name in names]\n\n    # Clear the current list\n    shared.hypernetworks = []\n\n    # Load or retrieve each hypernetwork by name, setting its multiplier if provided\n    for name in names:\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            if multipliers is not None and name in multipliers:\n                hypernetwork.set_multiplier(multipliers[name])\n            shared.hypernetworks.append(hypernetwork)\n\n    # Append each loaded hypernetwork to the shared list\n    for hypernetwork in loaded_hypernetworks:\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 38.732387599942854}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if names is None:\n        return\n\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    if len(names)!= len(multipliers):\n        raise ValueError(f\"Expected {len(names)} multipliers but got {len(multipliers)}\")\n\n    shared.hypernetworks = {}\n    shared.loaded_hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        hypernetwork.set_multiplier(multiplier)\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n    if shared.opts.print_hypernet_extra:\n        print(f\\\"Loaded {len(shared.loaded_hypernetworks)} hypernetworks:\\\")\n        for hypernetwork in shared.loaded_hypernetworks:\n            print(f\\\"  {hypernetwork.name} (multiplier={hypernetwork.multiplier})\\\")\n\n", "CodeBleu_score": 43.291710367003}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n", "CodeBleu_score": 75.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n", "CodeBleu_score": 68.4402416707376}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n", "CodeBleu_score": 68.4402416707376}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    query = self.query(x)\n    key = self.key(context)\n    value = self.value(context)\n\n    query = query.view(x.shape[0], x.shape[1], self.num_heads, -1).transpose(1, 2)\n    key = key.view(context.shape[0], context.shape[1], self.num_heads, -1).transpose(1, 2)\n    value = value.view(context.shape[0], context.shape[1], self.num_heads, -1).transpose(1, 2)\n\n    context_k, context_v = apply_hypernetworks(shared.loaded_hypernetworks, context, layer=self)\n    context_k = context_k.view(context.shape[0], context.shape[1], self.num_heads, -1).transpose(1, 2)\n    context_v = context_v.view(context.shape[0], context.shape[1], self.num_heads, -1).transpose(1, 2)\n\n    query = query / math.sqrt(self.head_dim)\n\n    if self.mask_value is not None:\n        mask = mask.view(mask.shape[0], 1, 1, mask.shape[1])\n        mask = mask.expand(mask.shape[0], mask.shape[1], query.shape[2], mask.shape[3])\n        mask = mask.transpose(2, 3)\n        mask = mask.contiguous().view(mask.shape[0], mask.shape[1], -1)\n\n    scores = torch.matmul(query, key.transpose(-2, -1))\n\n    if self.mask_value is not None:\n        scores = scores.masked_fill(mask == 0, self.mask_value)\n\n    scores = self.softmax(scores)\n\n    scores = self.dropout(scores)\n\n    output = torch.matmul(scores, context_v)\n    output = output.transpose(1, 2).contiguous().view(x.shape[0], x.shape[1], -1)\n\n    return self.out(output)\n\n", "CodeBleu_score": 34.55280747566502}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        raise ValueError(\\\"CrossAttention requires context.\\\")\n\n    if self.hypernetwork_k is not None:\n        context, _ = apply_single_hypernetwork(self.hypernetwork_k, context, context)\n\n    if self.hypernetwork_v is not None:\n        _, context = apply_single_hypernetwork(self.hypernetwork_v, context, context)\n\n    if self.hypernetwork_q is not None:\n        x, _ = apply_single_hypernetwork(self.hypernetwork_q, x, x)\n\n    if self.hypernetwork_c is not None:\n        context, _ = apply_single_hypernetwork(self.hypernetwork_c, context, context)\n\n    if self.hypernetwork_out is not None:\n        x, _ = apply_single_hypernetwork(self.hypernetwork_out, x, x)\n\n    x = self.to_q(x)\n    context = self.to_k(context)\n    context = self.to_v(context)\n\n    if self.use_projection:\n        x = self.to_out(x)\n\n    if self.scale_attn:\n        x = x * self.scale\n\n    x = x.reshape(x.shape[0], x.shape[1], self.num_heads, self.head_dim)\n    context = context.reshape(context.shape[0], context.shape[1], self.num_heads, self.head_dim)\n\n    x = x.permute(0, 2, 1, 3)\n    context = context.permute(0, 2, 1, 3)\n\n    x = torch.matmul(x, context.transpose(-2, -1))\n\n    if mask is not None:\n        x = x.masked_fill(mask == 0, float(\\\"-inf\\\"))\n\n    x = F.softmax(x, dim=-1)\n\n    x = torch.matmul(x, context)\n\n    x = x.permute(0, 2, 1, 3)\n    x =", "CodeBleu_score": 34.82679365869193}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    x = self.to_q(x)\n    context = self.to_k(context)\n    context = self.to_v(context)\n\n    if self.hypernetwork_context:\n        context = apply_hypernetworks(self.hypernetwork_context, context, self)\n\n    x = self.reshape(x)\n    context = self.reshape(context)\n\n    if mask is not None:\n        mask = self.reshape(mask)\n\n    x = self.attention(x, context, mask=mask, **kwargs)\n\n    x = self.to_out(self.reshape_out(x))\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 27.654023121488898}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Back up the original attributes\n    original_attributes = dict(hypernetwork.checkpoint_attributes)\n\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.checkpoint_attributes = dict(\n        checkpoint=checkpoint,\n        name=hypernetwork_name,\n        filename=filename,\n    )\n\n    # Attempt to save the hypernetwork\n    try:\n        torch.save(hypernetwork, filename)\n    except Exception:\n        # If an error occurs, restore the original attributes and re-raise the exception\n        hypernetwork.checkpoint_attributes = original_attributes\n        raise\n\n    # Restore the original attributes\n    hypernetwork.checkpoint_attributes = original_attributes\n\n", "CodeBleu_score": 26.16523968697692}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_optimizer_name = hypernetwork.optimizer_name\n    old_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    hypernetwork.optimizer_name = checkpoint.optimizer_name\n    hypernetwork.optimizer_state_dict = checkpoint.optimizer_state_dict\n    try:\n        torch.save(hypernetwork, filename)\n    except Exception:\n        hypernetwork.optimizer_name = old_optimizer_name\n        hypernetwork.optimizer_state_dict = old_optimizer_state_dict\n        raise\n    hypernetwork.optimizer_name = old_optimizer_name\n    hypernetwork.optimizer_state_dict = old_optimizer_state_dict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.608519508085525}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_checkpoint = hypernetwork.checkpoint\n    original_name = hypernetwork.name\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.name = hypernetwork_name\n    try:\n        torch.save(hypernetwork, filename)\n    except Exception:\n        hypernetwork.checkpoint = original_checkpoint\n        hypernetwork.name = original_name\n        raise\n    hypernetwork.checkpoint = original_checkpoint\n    hypernetwork.name = original_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 31.1208405619254}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and self.model_ema is not None:\n        if context is None:\n            context = self.model_ema.store_context()\n        self.model_ema.copy_to(self.model)\n        yield\n        self.model_ema.restore_context(context)\n    else:\n        yield\n\n\n    @torch.no_grad()\n    def init_from_ckpt(self, ckpt_path, ignore_keys=None, only_model=False):\n        \"\"\"Loads weights from a checkpoint into the model. If the checkpoint contains EMA weights, they are loaded into the EMA model.", "CodeBleu_score": 39.12992943402539}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and self.model_ema is not None:\n        self.model_ema.store(self.model, context=context)\n        yield\n        self.model_ema.restore(self.model, context=context)\n    else:\n        yield\n\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=[], only_model=False):\n        \"\"\"Initializes the model from a checkpoint.\n        Args:\n            ckpt_path (str): path to the checkpoint to initialize from\n            ignore_keys (list): list of keys to ignore when loading the checkpoint\n            only_model (bool): if True, only loads the model weights, ignoring the scheduler\n        \"\"\"\n        print(f\\\"Initializing from {ckpt_path}...\\\")\n        ckpt = torch.load(ckpt_path, map_location=\\\"cpu\\\")\n        if only_model:\n            self.model.load_state_dict(ckpt[\\\"model\\\"], strict=False)\n        else:\n            self.load_state_dict(ckpt, ignore_keys=ignore_keys)\n\n        if self.use_ema:\n            print(f\\\"Initializing EMA from {ckpt_path}...\\\")\n            self.model_ema.load_ema_state_dict(ckpt[\\\"model\\\"])\n\n        if self.use_scheduler:\n            print(f\\\"Initializing scheduler from {ckpt_path}...\\\")\n            self.scheduler.load_state_dict(ckpt[\\\"scheduler\\\"])\n\n    def load_ema_from_ckpt(self, ckpt_path):\n        \"\"\"Loads EMA weights from a checkpoint.\n        Args:\n            ckpt_path (str): path to the checkpoint to load EMA weights from\n        \"\"\"\n        if self.use_ema:\n            print(f\\\"Initializing EMA from {ckpt_path}...\\\")\n            ckpt = torch.load(ckpt_path, map_location=\\\"cpu\\\")\n            self.model_ema.load_ema_state_dict(ckpt[\\\"model\\\"])\n\n    def save(self, path):\n        \"\"\"Saves the model to a checkpoint.\n        Args:\n           ", "CodeBleu_score": 33.22235207857493}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and self.model_ema is not None:\n        if context is None:\n            context = self.model_ema.store_ctx()\n        else:\n            context = self.model_ema.store_ctx(context)\n        self.model_ema.copy_to(self.model)\n        yield\n        self.model_ema.restore(context)\n    else:\n        yield\n\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=[], only_model=False):\n        \"\"\"Loads weights from a checkpoint file.", "CodeBleu_score": 40.95819160541051}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.p_model(x, t)\n    if self.parameterization == \"eps\":\n        model_mean, eps_std = model_output.chunk(2, dim=-3)\n        # model_mean = model_mean[..., None, None]\n        # eps_std = eps_std[..., None, None]\n        if clip_denoised:\n            model_mean = model_mean.clamp(-1, 1)\n        posterior_log_variance = 2 * (eps_std - math.log(10000))\n        posterior_mean = self.predict_start_from_noise(x, t, eps_std * torch.randn_like(x))\n        posterior_variance = torch.exp(posterior_log_variance)\n        return model_mean, posterior_mean, posterior_variance, posterior_log_variance\n    elif self.parameterization == \"x0\":\n        model_mean = model_output\n        if clip_denoised:\n            model_mean = model_mean.clamp(-1, 1)\n        posterior_log_variance = extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n        posterior_mean = self.predict_start_from_noise(x, t, torch.randn_like(x))\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x.shape)\n        return model_mean, posterior_mean, posterior_variance, posterior_log_variance\n    else:\n        raise NotImplementedError(\"parameterization not supported\")\n\n", "CodeBleu_score": 38.508874601825916}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.p_sample(x, t, clip_denoised)\n    if self.parameterization == \"eps\":\n        model_mean, eps_std = model_output.chunk(2, dim=-3)\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n    elif self.parameterization == \"x0\":\n        model_mean = model_output\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n    else:\n        raise NotImplementedError(\"parameterization not supported\")\n    if clip_denoised:\n        model_mean = model_mean.clamp(-1.0, 1.0)\n    return model_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 40.55095679504529}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.parameterization == \"eps\":\n        if clip_denoised:\n            denoised = x - extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * model_output\n            denoised = denoised.clamp(-1.0, 1.0)\n        else:\n            denoised = x\n        model_mean = denoised - extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * model_output\n    elif self.parameterization == \"x0\":\n        if clip_denoised:\n            denoised = extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \\\n                       extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape) * model_output\n            denoised = denoised.clamp(-1.0, 1.0)\n        else:\n            denoised = x\n        model_mean = extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * (denoised + model_output)\n    else:\n        raise NotImplementedError(\"mu not supported\")\n\n    posterior_variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x.shape)\n    posterior_log_variance_clipped = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x.shape)\n    return model_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 41.24641932116647}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 51.689269393251315}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = F.l1_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 51.54614923546187}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unsupported loss type {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 51.87369456328916}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    b = x_start.shape[0]\n    t = torch.full((b,), self.num_timesteps, device=x_start.device, dtype=x_start.dtype)\n    mean, var = self.predict_start_from_t(x_start, t)\n    kl = 0.5 * (mean ** 2 + var - torch.log(var) - 1)\n    return torch.mean(kl) / np.log(2)\n\n", "CodeBleu_score": 25.803123657341594}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps, device=x_start.device)\n    mean, logvar = self.predict(x_start, t)\n    kl = -0.5 * (1 + logvar - mean ** 2 - logvar.exp())\n    return torch.mean(kl) / np.log(2)\n\n\n", "CodeBleu_score": 26.502980581649787}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full_like(x_start[..., :1], self.num_timesteps - 1)\n    mean, log_var = self.q_posterior(x_start, t)\n    kl_div = 0.5 * (log_var.exp() + mean ** 2 - log_var - 1)\n    return kl_div.mean(dim=tuple(range(1, kl_div.ndim))).log().mean()\n\n\n", "CodeBleu_score": 24.518981029878432}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    global caches\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        else:\n            cache = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n            caches[subsection] = cache\n            return cache\n\n", "CodeBleu_score": 55.99685500634596}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n\n    global caches\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        else:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n            return caches[subsection]\n\n", "CodeBleu_score": 55.63883243072944}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    global cache_lock, cache_dir, caches\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, disk_min_file_size=256 * 1024)\n    return caches[subsection]\n\n", "CodeBleu_score": 53.25020259192299}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 68.97054294273212}, {"code": "def cache(subsection)\n\t\n    global caches\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 64.52829652585012}, {"code": "def cache(subsection)\n\t\n\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n\n\n", "CodeBleu_score": 68.97054294273212}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache_obj.get(title)\n\n    if cached_data and cached_data[0] >= mtime:\n        return cached_data[1]\n\n    data = func()\n    cache_obj[title] = (mtime, data)\n    dump_cache()\n\n    return data\n\n\n\n", "CodeBleu_score": 11.945624803916276}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime = cache_obj[title + \\\"_mtime\\\"]\n        if cached_mtime > mtime:\n            return cache_obj[title]\n\n    data = func()\n    cache_obj[title] = data\n    cache_obj[title + \\\"_mtime\\\"] = mtime\n    dump_cache()\n\n    return data\n\n", "CodeBleu_score": 21.866930584066207}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n\n    mtime = os.path.getmtime(filename)\n\n    cached_data = cache_obj.get(title)\n    if cached_data is not None:\n        cached_mtime = cached_data.get(\\\"mtime\\\")\n        if cached_mtime is not None and mtime <= cached_mtime:\n            return cached_data.get(\\\"data\\\")\n\n    data = func()\n    cache_obj[title] = {\\\"mtime\\\": mtime, \\\"data\\\": data}\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 29.526404904621174}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        for line in traceback.format_exc().splitlines():\n            print(f\"    {line}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 51.79982645686492}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 52.94417170818152}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    if exc_info:\n        record_exception()\n    for line in textwrap.dedent(message).splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for line in textwrap.indent(traceback.format_exc(), \"    \").splitlines():\n            print(line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 53.28969834442232}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 76.87779512483417}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    try:\n        _, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        _, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    except ValueError:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 61.09815347708933}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 76.87779512483417}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n\n", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n\n\n", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n\n\n", "CodeBleu_score": 64.07063310277653}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"`increase_only` and `decrease_only` are mutually exclusive\")\n    if increase_only:\n        if target_dBFS > _dBFS(wav):\n            return normalize_volume(wav, target_dBFS)\n        else:\n            return wav\n    if decrease_only:\n        if target_dBFS < _dBFS(wav):\n            return normalize_volume(wav, target_dBFS)\n        else:\n            return wav\n    change_in_dBFS = target_dBFS - _dBFS(wav)\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 37.54356909506554}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"`increase_only` and `decrease_only` are mutually exclusive\")\n\n    if increase_only:\n        if target_dBFS > _compute_rms_dbfs(wav):\n            return normalize_volume(wav, target_dBFS)\n        else:\n            return wav\n\n    if decrease_only:\n        if target_dBFS < _compute_rms_dbfs(wav):\n            return normalize_volume(wav, target_dBFS)\n        else:\n            return wav\n\n    change_in_dbfs = target_dBFS - _compute_rms_dbfs(wav)\n    return wav * (10 ** (change_in_dbfs / 20))\n\n", "CodeBleu_score": 36.86718635790296}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"`increase_only` and `decrease_only` are mutually exclusive\")\n\n    db_change = target_dBFS - rms_db(wav, None)\n    if db_change < 0 and increase_only or db_change > 0 and decrease_only:\n        return wav\n    else:\n        return wav * (10 ** (db_change / 20))\n\n", "CodeBleu_score": 46.4362186892471}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model = SpeakerEncoder(**encoder_kwargs)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=_device))\n    _model.to(_device)\n    _model.eval()\n    print(\"Loaded model from {}\".format(weights_fpath))\n    print(\"Trained from step: {}\".format(\n        _model.get_step()\n    ))\n\n", "CodeBleu_score": 51.796501231719304}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    _device = torch.device(device)\n    _model = SpeakerEncoder(device=_device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=_device))\n    _model.eval()\n    print(\"Loaded model from {}\".format(weights_fpath))\n    print(\"Trained until step {}\".format(get_step_from_fname(weights_fpath)))\n\n", "CodeBleu_score": 24.80441579023073}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _device = device\n    _model = SpeakerEncoder(**encoder_kwargs)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=_device))\n    _model.to(_device)\n    _model.eval()\n    print(\"Loaded model from {}\".format(weights_fpath))\n    print(\"Model's state_dict:\")\n    for param_tensor in _model.state_dict():\n        print(param_tensor, \"\\t\", _model.state_dict()[param_tensor].size())\n\n", "CodeBleu_score": 51.853006326495944}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if not is_loaded():\n        raise Exception(\"Model is not loaded. Please call load_model() before calling this function.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch)\n        embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 53.85307317319822}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not is_loaded():\n        raise RuntimeError(\"Model not loaded, call load_model() first.\")\n    # Convert to torch tensor and transfer to device\n    frames_batch = torch.from_numpy(frames_batch)\n    frames_batch = frames_batch.to(_device)\n    # Perform forward pass\n    embeddings = _model.forward(frames_batch)\n    # Detach from graph, move to cpu and convert back to numpy\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 48.76429862472706}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not is_loaded():\n        raise Exception(\"Model not loaded. Call load_model() first.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.embed_utterance(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 59.915452902923604}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1, \\\n        \"min_pad_coverage must be between 0 and 1 (not including 0)\"\n    assert 0 < overlap < 1, \"overlap must be between 0 and 1 (not including 0 or 1)\"\n    n_frames = partial_utterance_n_frames\n    frame_step = int(n_frames * (1 - overlap))\n    n_slices = int(np.ceil(n_samples / frame_step))\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_slices):\n        start = i * frame_step\n        end = start + n_frames\n        wav_slices.append(wav[start:end])\n        mel_slices.append(mels[start:end])\n    coverage = (n_samples - n_frames) / n_samples\n    if coverage < min_pad_coverage and n_slices > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.97208134653131}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1, f'Overlap must be between 0 and 1, got {overlap}'\n    assert 0 <= min_pad_coverage <= 1, f'Minimum pad coverage must be between 0 and 1, got {min_pad_coverage}'\n    assert partial_utterance_n_frames > 0, f'Partial utterance n frames must be greater than 0, got {partial_utterance_n_frames}'\n\n    # Number of frames and frame step\n    n_frames = partial_utterance_n_frames\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for start in range(0, n_samples, frame_step):\n        end = start + n_frames\n        wav_slices.append(slice(start, end))\n        mel_slices.append(slice(start // hop_length, end // hop_length))\n\n    # Evaluate the need for extra padding\n    if len(wav_slices) > 1:\n        last_wav_slice = wav_slices[-1]\n        last_mel_slice = mel_slices[-1]\n        coverage = (last_wav_slice.stop - last_wav_slice.start) / n_frames\n        if coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 38.275911653317564}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \\\"Overlap must be in (0.0, 1.0)\\\"\n    assert 0.0 < min_pad_coverage < 1.0, \\\"Minimum padding coverage must be in (0.0, 1.0)\\\"\n    n_frames = partial_utterance_n_frames\n    frame_step = int(partial_utterance_n_frames * (1.0 - overlap))\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_samples - partial_utterance_n_frames + frame_step, frame_step):\n        wav_slices.append(slice(i, i + partial_utterance_n_frames))\n        mel_slices.append(slice(i // hop_length, i // hop_length + n_frames))\n    last_wav_slice_coverage = (n_samples - wav_slices[-1].start) / partial_utterance_n_frames\n    if last_wav_slice_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 34.66808870177543}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    ax.imshow(embed.reshape(shape), cmap=\\\"jet\\\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([], [])\n    ax.set_yticks([], [])\n    ax.set_title(title)\n    plt.colorbar()\n\n", "CodeBleu_score": 37.59339711847202}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n    embed = embed.reshape(shape)\n\n    ax.imshow(embed, cmap=plt.cm.RdBu, vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar()\n\n", "CodeBleu_score": 51.269765527798015}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n        embed = embed.reshape((shape, shape))\n    ax.imshow(embed, cmap=plt.get_cmap(\\\"Greys\\\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar()\n    return ax\n\n\n", "CodeBleu_score": 47.113088193436816}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Number of speakers in the batch\n    n_speakers = embeds.shape[0]\n\n    # Number of utterances per speaker\n    n_utterances = embeds.shape[1]\n\n    # Compute centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-5)\n    centroids_excl = torch.mean(torch.cat([embeds[:, :i, :], embeds[:, i + 1:, :]], dim=1), dim=1)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-5)\n\n    # Initialize similarity matrix\n    sim_matrix = torch.zeros((n_speakers, n_utterances, n_utterances)).to(self.loss_device)\n    mask = torch.ones_like(sim_matrix)\n    mask = mask.triu(diagonal=1).to(self.loss_device)\n\n    # Compute similarity scores\n    for i in range(n_speakers):\n        sim_matrix[i] = torch.mm(embeds[i], centroids_incl[i].unsqueeze(1))\n        sim_matrix[i] += torch.mm(embeds[i], centroids_excl[i].unsqueeze(1))\n\n    # Scale and bias\n    sim_matrix = self.similarity_weight * sim_matrix + self.similarity_bias\n\n    # Mask self-similarity\n    sim_matrix = sim_matrix * mask\n\n    return sim_matrix\n\n\n\n", "CodeBleu_score": 32.396257904678215}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    centroids_excl = torch.zeros_like(centroids_incl)\n    for i in range(n_speakers):\n        centroids_excl[i] = torch.cat((embeds[:i], embeds[i + 1:])).mean(dim=0, keepdim=True)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Compute similarity matrix\n    sim_matrix = torch.zeros((n_speakers, n_utterances, n_speakers, n_utterances), device=self.loss_device)\n    mask = torch.ones_like(sim_matrix)\n    for i in range(n_speakers):\n        for j in range(n_utterances):\n            sim_matrix[i, j, i, j] = -1\n            mask[i, j, i, j] = 0\n\n    for i in range(n_speakers):\n        for j in range(n_utterances):\n            for k in range(n_speakers):\n                for l in range(n_utterances):\n                    sim_matrix[i, j, k, l] = torch.matmul(embeds[i, j], centroids_incl[k])\n                    sim_matrix[i, j, k, l] += torch.matmul(embeds[i, j], centroids_excl[k])\n                    sim_matrix[i, j, k, l] *= 0.5\n\n    # Scale and bias the similarity matrix\n    sim_matrix = self.similarity_weight * sim_matrix + self.similarity_bias\n\n    return sim_matrix, mask\n\n", "CodeBleu_score": 43.16061328218604}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-5)\n\n    centroids_excl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_excl = centroids_excl - embeds / (n_utterances - 1)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n    centroids_excl = torch.squeeze(centroids_excl)\n\n    sim_matrix = torch.zeros((n_speakers, n_utterances, n_utterances)).to(self.loss_device)\n    mask = torch.ones((n_speakers, n_utterances, n_utterances)).to(self.loss_device)\n\n    for i in range(n_speakers):\n        sim_matrix[i] = torch.matmul(embeds[i], centroids_incl[i].unsqueeze(1).transpose(0, 1))\n        sim_matrix[i] = torch.matmul(embeds[i], centroids_excl[i].unsqueeze(1).transpose(0, 1))\n        mask[i] = 1 - torch.eye(n_utterances).to(self.loss_device)\n\n    sim_matrix = sim_matrix * mask\n    sim_matrix = self.similarity_weight * sim_matrix + self.similarity_bias\n\n    return sim_matrix\n\n", "CodeBleu_score": 38.09060864917525}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape similarity matrix\n    sim_matrix = sim_matrix.view(speakers_per_batch, utterances_per_speaker * (utterances_per_speaker - 1))\n\n    # Generate labels\n    labels = torch.zeros(speakers_per_batch, utterances_per_speaker * (utterances_per_speaker - 1)).to(self.loss_device)\n    for i in range(speakers_per_batch):\n        labels[i, i * (utterances_per_speaker - 1):(i + 1) * (utterances_per_speaker - 1)] = 1\n\n    # Compute loss\n    loss = self.loss_fn(sim_matrix, labels.long())\n\n    # Compute EER\n    # Compute ROC curve\n    sim_matrix_np = sim_matrix.detach().cpu().numpy()\n    labels_np = labels.detach().cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels_np, sim_matrix_np)\n\n    # Compute EER\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 52.09447853981}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    labels = torch.arange(speakers_per_batch).repeat_interleave(utterances_per_speaker)\n    loss = self.loss_fn(sim_matrix, labels)\n    loss = loss.mean()\n    self.do_gradient_ops()\n    return loss\n\n", "CodeBleu_score": 26.061488645656983}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.view(speakers_per_batch, utterances_per_speaker, -1)\n    # Generate labels\n    labels = torch.arange(speakers_per_batch, device=self.loss_device)\n    labels = labels.view(speakers_per_batch, 1).expand(-1, utterances_per_speaker)\n    labels = labels.contiguous().view(-1)\n    # Calculate loss\n    loss = self.loss_fn(sim_matrix, labels)\n    # Compute EER\n    sim_matrix = sim_matrix.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels, sim_matrix)\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n\n", "CodeBleu_score": 51.322593326915275}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Get the parameters\n    params = self.get_params()\n\n    # Format the parameters\n    self.implementation_string = \\\"\\\"\n    for param_name, param_value in params.items():\n        self.implementation_string += \\\"<br>%s: %s\\\" % (param_name, param_value)\n\n    # Display the parameters\n    self.implementation_win = self.vis.text(self.implementation_string, win=self.implementation_win, opts={\n        \\\"title\\\": \\\"Parameters\\\"\n    })\n\n", "CodeBleu_score": 15.098461288144222}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Get the model and data parameters\n    params = self.get_params()\n\n    # Create the string\n    string = \"\"\n    for name, value in params.items():\n        string += name + \": \" + str(value) + \"\\", "CodeBleu_score": 10.060636150204955}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    self.implementation_string += \"Model parameters:", "CodeBleu_score": 4.317374172318357}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    if out_fpath is not None:\n        out_fpath = os.path.join(out_fpath, \"projections\")\n        os.makedirs(out_fpath, exist_ok=True)\n        out_fpath = os.path.join(out_fpath, \"step_%06d.png\" % step)\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = np.concatenate(embeds, axis=0)\n    # Assign colors based on speaker identity\n    colors = np.concatenate([np.full(u, i) for i, u in enumerate(utterances_per_speaker)])\n    colors = colormap[colors]\n\n    # UMAP reduces the dimensionality of the embeddings\n    umap_embeds = umap.UMAP(n_neighbors=10, min_dist=0.01, metric=\\\"cosine\\\").fit_transform(embeds)\n\n    # Plot the embeddings and display or save to a file\n    plt.figure(figsize=(10, 10))\n    plt.scatter(umap_embeds[:, 0], umap_embeds[:, 1], s=1, c=colors)\n    plt.title(\\\"UMAP projections at step %d\\\" % step)\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath)\n        plt.close()\n\n\n\n", "CodeBleu_score": 46.54416695583717}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    if out_fpath is None:\n        out_fpath = os.path.join(self.vis.get_env_path(self.env_name), \"projections.png\")\n    if len(embeds) > max_speakers * 2:\n        embeds = embeds[:max_speakers * 2]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    n_speakers = len(utterances_per_speaker)\n    embeds = embeds.reshape(n_speakers * 2, -1)\n    n_utterances = np.sum(utterances_per_speaker)\n    colors = np.zeros((n_utterances, 3))\n    for i in range(n_speakers):\n        colors[i * 2:(i + 1) * 2] = colormap[i]\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.01)\n    projections = reducer.fit_transform(embeds)\n    self.projection_win = self.vis.scatter(\n        projections,\n        win=self.projection_win,\n        update=\\\"replace\\\" if self.projection_win else None,\n        opts=dict(\n            title=\\\"Projections (step %d)\\\" % step,\n            xlabel=\\\"UMAP 1\\\",\n            ylabel=\\\"UMAP 2\\\",\n            markersize=3,\n            markercolor=colors,\n        )\n    )\n    if out_fpath is not None:\n        self.vis.save([self.env_name], out_fpath)\n\n\n", "CodeBleu_score": 41.26544916730165}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    if len(embeds) == 0:\n        return\n    if out_fpath is None:\n        out_fpath = self.env_name + \"_step_%d.png\" % step\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    flattened_embeds = np.concatenate(embeds, axis=0)\n    speaker_ids = np.concatenate([np.full(utterances_per_speaker[i], i, dtype=np.int)\n                                  for i in range(len(utterances_per_speaker))], axis=0)\n    colors = np.array([colormap[i] for i in speaker_ids])\n    umap_embeds = umap.UMAP(n_neighbors=10, min_dist=0.01, metric=\"cosine\").fit_transform(flattened_embeds)\n    self.projection_win = self.vis.scatter(\n        umap_embeds,\n        win=self.projection_win,\n        update=\\\"append\\\" if self.projection_win else None,\n        opts=dict(\n            xtickmin=0,\n            xtickmax=1,\n            ytickmin=0,\n            ytickmax=1,\n            markersymbol=\\\"dot\\\",\n            markercolor=colors,\n            title=\\\"Embedding projections (step %d)\\\"\n        )\n    )\n    if out_fpath is not None:\n        self.vis.save([self.projection_win], out_fpath)\n\n", "CodeBleu_score": 39.47219171632358}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    B, T, C = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding = speaker_embedding.unsqueeze(1).expand(B, T, C)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n\n", "CodeBleu_score": 9.627686560723959}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    B, T, C = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = speaker_embedding.shape[-1]\n    speaker_embedding_index = 1\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(T, 1)\n    # Reshape the speaker embedding to match the shape of the input tensor\n    speaker_embedding = speaker_embedding.reshape(B, T, speaker_embedding_dim)\n    # Transpose the speaker embedding to match the shape of the input tensor\n    speaker_embedding = speaker_embedding.transpose(speaker_embedding_index, 0)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 18.45760486878545}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, num_features = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = speaker_embedding.size(1)\n    speaker_embedding_index = 1\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(seq_len, 1)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.view(seq_len, batch_size, speaker_embedding_dim)\n    speaker_embedding = speaker_embedding.transpose(speaker_embedding_index, 0)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n", "CodeBleu_score": 32.0270744436765}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size = encoder_seq_proj.size(0)\n    seq_len = encoder_seq_proj.size(1)\n    self.cumulative = torch.zeros((batch_size, seq_len), dtype=torch.float, device=device)\n    self.attention = torch.zeros((batch_size, seq_len), dtype=torch.float, device=device)\n\n", "CodeBleu_score": 27.958939220729146}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(batch_size, seq_len, device=encoder_seq_proj.device)\n    self.attention = torch.zeros(batch_size, seq_len, device=encoder_seq_proj.device)\n\n", "CodeBleu_score": 29.340844328187288}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size = encoder_seq_proj.size(0)\n    seq_len = encoder_seq_proj.size(1)\n    self.cumulative = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 24.44058941306469}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n    if optimizer is not None and \"optim_state\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optim_state\"])\n    if \"step\" in checkpoint:\n        self.step = self.step.data.new_tensor(checkpoint[\"step\"])\n    if \"stop_threshold\" in checkpoint:\n        self.stop_threshold = self.stop_threshold.data.new_tensor(checkpoint[\"stop_threshold\"])\n    if \"r\" in checkpoint:\n        self.r = checkpoint[\"r\"]\n\n", "CodeBleu_score": 47.64721238854428}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device\n    sd = torch.load(path, map_location=device)\n    self.load_state_dict(sd[\"model\"])\n    if optimizer is not None and \"optim\" in sd:\n        optimizer.load_state_dict(sd[\"optim\"])\n", "CodeBleu_score": 32.80052372066637}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    self.load_state_dict(torch.load(path, map_location=device))\n    if optimizer is not None and 'optimizer' in self.state_dict():\n        optimizer.load_state_dict(self.state_dict()['optimizer'])\n\n", "CodeBleu_score": 33.51983318105782}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    checkpoint = {\n        \"model_state\": self.state_dict(),\n    }\n\n    if optimizer is not None:\n        checkpoint[\"optimizer_state\"] = optimizer.state_dict()\n\n    torch.save(checkpoint, str(path))\n\n", "CodeBleu_score": 60.74184692599244}, {"code": "def save(self, path, optimizer=None)\n\t\n    state = {\n        \"model_state\": self.state_dict(),\n        \"step\": self.get_step(),\n    }\n    if optimizer is not None:\n        state[\"optimizer_state\"] = optimizer.state_dict()\n    torch.save(state, str(path))\n\n", "CodeBleu_score": 63.43267350948503}, {"code": "def save(self, path, optimizer=None)\n\t\n    checkpoint = {\n        \"model_state\": self.state_dict(),\n        \"step\": self.get_step(),\n    }\n    if optimizer is not None:\n        checkpoint[\"optimizer_state\"] = optimizer.state_dict()\n    torch.save(checkpoint, str(path))\n\n", "CodeBleu_score": 63.25172645684791}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 80.26350733165178}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 80.26350733165178}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text) > 0:\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token:\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 77.33534545743069}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet in curly braces:\n            if len(s) > 1 and s[0] == \\\"@\\\":\n                s = \\\"{\\\" + s[1:] + \\\"}\\\"\n            result.append(s)\n    return \\\" \\\".join(result)\n\n", "CodeBleu_score": 47.414325651719444}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{\" + s[1:] + \"}\"\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 68.48842255193142}, {"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet in curly braces:\n            if len(s) > 1 and s[0] == \\\"@\\\":\n                s = \\\"{\\\" + s[1:] + \\\"}\\\"\n            result.append(s)\n    return \\\" \\\".join(result)\n\n", "CodeBleu_score": 47.414325651719444}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    global encoder_model\n    if encoder_model is None:\n        encoder_model = Encoder(encoder_model_fpath)\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(encoder_model, wav)\n        np.save(fpath.replace(\".wav\", \".npy\"), embed, allow_pickle=False)\n\n", "CodeBleu_score": 37.025651537313934}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    global encoder, encoder_loaded\n    if not encoder_loaded:\n        encoder = Encoder(encoder_model_fpath)\n        encoder_loaded = True\n\n    wav_fpath = Path(fpaths[0])\n    wav = encoder.preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 48.63605204328417}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    global encoder\n    if encoder is None:\n        encoder = Encoder(encoder_model_fpath)\n    embeds = []\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(wav)\n        embeds.append(embed)\n    return embeds\n\n", "CodeBleu_score": 41.06873203818752}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify the existence of the required directories and files\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(\"Synthesizer root directory not found.\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\"Encoder model file not found.\")\n\n    # Read metadata and prepare file paths for audio and embeddings\n    metadata = read_metadata(synthesizer_root)\n    audio_fpaths = [synthesizer_root.joinpath(\"audio\", fpath) for fpath, _, _ in metadata]\n    embed_fpaths = [synthesizer_root.joinpath(\"embed\", fpath) for _, _, fpath, *_ in metadata]\n\n    # Create the multiprocessing pool and process the audio files in parallel\n    with Pool(n_processes) as pool:\n        with tqdm(total=len(metadata)) as pbar:\n            for _ in pool.imap_unordered(embed_utterance, zip(audio_fpaths, embed_fpaths)):\n                pbar.update()\n\n", "CodeBleu_score": 34.24927769498625}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check that the synthesizer root exists and contains the required directories\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(\"Synthesizer root directory not found: %s\" % synthesizer_root)\n    if not synthesizer_root.joinpath(\"audio\").exists():\n        raise FileNotFoundError(\"Audio directory not found: %s\" % synthesizer_root.joinpath(\"audio\"))\n    if not synthesizer_root.joinpath(\"embeds\").exists():\n        raise FileNotFoundError(\"Embeds directory not found: %s\" % synthesizer_root.joinpath(\"embeds\"))\n\n    # Read the metadata\n    metadata = read_metadata(synthesizer_root.joinpath(\"metadata.csv\"))\n\n    # Prepare the file paths\n    wav_fpaths = [synthesizer_root.joinpath(\"audio\", wav_fpath) for wav_fpath, _, _, _, _, _ in metadata]\n    embed_fpaths = [synthesizer_root.joinpath(\"embeds\", embed_fpath) for _, _, embed_fpath, _, _, _ in metadata]\n    fpaths = list(zip(wav_fpaths, embed_fpaths))\n\n    # Create the multiprocessing pool and process the audio files\n    with Pool(n_processes) as pool:\n        for _ in tqdm(pool.imap_unordered(partial(embed_utterance, encoder_model_fpath=encoder_model_fpath), fpaths), total=len(fpaths)):\n            pass\n\n", "CodeBleu_score": 38.39971510385406}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify the existence of required directories and files\n    assert synthesizer_root.exists(), f\"The synthesizer root directory '{synthesizer_root}' does not exist.\"\n    assert encoder_model_fpath.exists(), f\"The encoder model file '{encoder_model_fpath}' does not exist.\"\n\n    # Read metadata and prepare file paths for audio and embeddings\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n    assert metadata_fpath.exists(), f\"The metadata file '{metadata_fpath}' does not exist.\"\n    metadata = pd.read_csv(metadata_fpath, sep=\"|\")\n    wav_fpaths = [synthesizer_root.joinpath(\"audio\", f\"audio-{basename}.npy\") for basename in metadata[\"basenames\"]]\n    embed_fpaths = [synthesizer_root.joinpath(\"embeds\", f\"embed-{basename}.npy\") for basename in metadata[\"basenames\"]]\n\n    # Create the multiprocessing pool and process the audio files in parallel\n    with Pool(n_processes) as pool:\n        for i, _ in tqdm(enumerate(pool.imap_unordered(partial(embed_utterance, encoder_model_fpath=encoder_model_fpath), zip(wav_fpaths, embed_fpaths)), total=len(wav_fpaths)), desc=\"Embedding\"):\n            pass\n\n", "CodeBleu_score": 42.45259893297673}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Plot and save attention\n    attention_plot_fpath = plot_dir.joinpath(\\\"Attention_{:06d}_{:02d}.png\\\".format(step, sample_num))\n    plot_attention(attention, attention_plot_fpath, hparams.tts_attention_plot_freq)\n\n    # Plot and save mel spectrogram\n    mel_prediction_fpath = mel_output_dir.joinpath(\\\"Mel_{:06d}_{:02d}.png\\\".format(step, sample_num))\n    plot_spectrogram(mel_prediction, mel_prediction_fpath)\n\n    # Reconstruct waveform\n    waveform_fpath = wav_dir.joinpath(\\\"Waveform_{:06d}_{:02d}.wav\\\".format(step, sample_num))\n    wav = spectrogram_to_waveform(mel_prediction, hparams)\n    save_wav(wav, waveform_fpath)\n\n    # Plot and save predicted and target mel spectrograms\n    plot_fpath = plot_dir.joinpath(\\\"Eval_{:06d}_{:02d}.png\\\".format(step, sample_num))\n    plot_spectrogram(mel_prediction, plot_fpath, title=\\\"Predicted Mel Spectrogram | Step: {} | Loss: {:0.4f}\\\".format(step, loss))\n    plot_spectrogram(target_spectrogram, plot_fpath, title=\\\"Target Mel Spectrogram | Step: {} | Loss: {:0.4f}\\\".format(step, loss), overlay=mel_prediction)\n\n    # Print input sequence\n    print(\\\"| Step: {} | Input: {}\\\".format(step, input_seq))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 38.39565150013812}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_fpath = plot_dir.joinpath(f\\\"attention_{step:06d}_{sample_num:02d}.png\\\")\n    plot_attention(attention, attention_fpath)\n\n    # Save predicted mel spectrogram\n    mel_prediction_fpath = mel_output_dir.joinpath(f\\\"mel_{step:06d}_{sample_num:02d}.png\\\")\n    plot_spectrogram(mel_prediction, mel_prediction_fpath)\n\n    # Save reconstructed waveform\n    wav_fpath = wav_dir.joinpath(f\\\"{step:06d}_{sample_num:02d}.wav\\\")\n    audio = spectrogram2wav(mel_prediction)\n    audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n    save_wav(audio_denoised, wav_fpath, sr=hparams.sampling_rate)\n\n    # Save plot comparing predicted and target mel spectrograms\n    plot_fpath = plot_dir.joinpath(f\\\"{step:06d}_{sample_num:02d}.png\\\")\n    title = f\\\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n    Step: {step:06d}\n    Loss: {loss.item():.4f}\\\"\n    plot_spectrogram_prediction(target_spectrogram, mel_prediction, plot_fpath, title)\n\n    # Print input sequence\n    print(f\\\"| Step: {step:06d} | Sample: {sample_num:02d} | Text: {input_seq}\\\")\n\n\n\n", "CodeBleu_score": 37.068971270770604}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Create plot directory if it doesn't exist\n    plot_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create mel output directory if it doesn't exist\n    mel_output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create wav directory if it doesn't exist\n    wav_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save attention plot\n    attention_plot_fpath = plot_dir.joinpath(f\\\"attention_{step:06d}_{sample_num:02d}.png\\\")\n    plot_attention(attention, attention_plot_fpath)\n\n    # Save predicted mel spectrogram\n    mel_prediction_fpath = mel_output_dir.joinpath(f\\\"mel_prediction_{step:06d}_{sample_num:02d}.png\\\")\n    plot_spectrogram(mel_prediction, mel_prediction_fpath)\n\n    # Save reconstructed waveform\n    wav_fpath = wav_dir.joinpath(f\\\"wav_{step:06d}_{sample_num:02d}.wav\\\")\n    wav = spectrogram2wav(mel_prediction.T, hparams)\n    wavfile.write(wav_fpath, hparams.sample_rate, wav)\n\n    # Plot predicted and target mel spectrogram with loss\n    plot_fpath = plot_dir.joinpath(f\\\"mel_prediction_{step:06d}_{sample_num:02d}.png\\\")\n    plot_spectrogram(mel_prediction, plot_fpath, title=f\\\"{hparams.tts_model_type} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Step {step:06d} | Loss {loss:#.4}\\\")\n\n    # Print input sequence\n    print(f\\\"| Step: {step:06d} | Sample: {sample_num:02d} | Input: {input_seq}\\\")\n\n\n\n\n", "CodeBleu_score": 32.37722857216638}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    print('Number of trainable parameters: %.3fM' % num_params)\n\n", "CodeBleu_score": 19.481678424326635}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    print('Number of trainable parameters: %.3fM' % num_params)\n\n", "CodeBleu_score": 19.481678424326635}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params /= 1000000.0\n    print('Number of parameters: %.3fM' % num_params)\n", "CodeBleu_score": 17.510164500149695}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad <= 0:\n        return x\n    if side == 'both':\n        pad_left = pad_right = pad\n    elif side == 'before':\n        pad_left = pad\n        pad_right = 0\n    elif side == 'after':\n        pad_left = 0\n        pad_right = pad\n    else:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\"')\n    n_feats = x.size(1)\n    padded_size = x.size(0) + pad_left + pad_right\n    if torch.cuda.is_available():\n        padded = torch.zeros(padded_size, n_feats).cuda()\n    else:\n        padded = torch.zeros(padded_size, n_feats).cpu()\n    padded[pad_left:pad_left + x.size(0), :] = x\n    return padded\n\n", "CodeBleu_score": 37.22553924233113}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        pad_left = pad_right = pad\n    elif side == 'left':\n        pad_left = pad\n        pad_right = 0\n    elif side == 'right':\n        pad_left = 0\n        pad_right = pad\n    else:\n        raise ValueError('Side should be either \"before\", \"after\" or \"both\".')\n    return F.pad(x, (pad_left, pad_right), 'constant', 0)\n\n\n    def pad_tensor(self, x, pad, side='both'):\n        \"\"\"The function pads a given tensor along the time dimension based on the specified padding and side.\n        It first determines the new total size of the tensor after padding.\n        Then, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n        The original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\n        Finally, the padded tensor is returned.\"\"\"\n        if side == 'both':\n            pad_left = pad_right = pad\n        elif side == 'left':\n            pad_left = pad\n            pad_right = 0\n        elif side == 'right':\n            pad_left = 0\n            pad_right = pad\n        else:\n            raise ValueError('Side should be either \"before\", \"after\" or \"both\".')\n        return F.pad(x, (pad_left, pad_right), 'constant', 0)\n\n\n    def pad_tensor(self, x, pad, side='both'):\n        \"\"\"The function pads a given tensor along the time dimension based on the specified padding and side.\n        It first determines the new total size of the tensor after padding.\n        Then, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n        The original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\n        Finally, the padded tensor is returned.\"\"\"\n        if side == 'both':\n            pad_left = pad_right = pad\n        elif side == 'left':\n            pad_left = pad\n            pad_right = ", "CodeBleu_score": 16.663120435362604}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        pad_left = pad_right = pad\n    elif side == 'left':\n        pad_left = pad\n        pad_right = 0\n    elif side == 'right':\n        pad_left = 0\n        pad_right = pad\n    else:\n        raise ValueError('Side must be one of \"before\", \"after\", or \"both\".')\n    return F.pad(x, (pad_left, pad_right))\n\n", "CodeBleu_score": 21.23854896103555}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.imshow(attn, cmap='viridis', aspect='auto')\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 47.940956921800925}, {"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + ['<eos>'] + ['<pad>'] + ['<unk>'] + ['<mask>'] + ['<sep>'] + ['<cls>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad>'] + ['<pad", "CodeBleu_score": 31.736373532833085}, {"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n    plt.figure(figsize=(15, 15))\n    plt.imshow(attn, interpolation='nearest', cmap='gray')\n    plt.savefig(path + '.png')\n    plt.close()\n\n", "CodeBleu_score": 46.993771846548505}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t   import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x-axis', color='red', fontsize=16)\n    ax.set_ylabel('y-axis', color='red', fontsize=16)\n    ax.tick_params(axis='x', colors='red', labelsize=16)\n    ax.tick_params(axis='y', colors='red', labelsize=16)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 35.42344299381206}, {"code": "def plot(array)\n\t   import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('X', color='#000000', fontsize=16)\n    ax.set_ylabel('Y', color='#000000', fontsize=16)\n    ax.tick_params(axis='x', colors='#000000', labelsize=16)\n    ax.tick_params(axis='y', colors='#000000', labelsize=16)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 34.963176819366296}, {"code": "def plot(array)\n\t   fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('Time', color='white', fontsize=12)\n    ax.set_ylabel('Frequency', color='white', fontsize=12)\n    ax.tick_params(axis='x', colors='white', labelsize=10)\n    ax.tick_params(axis='y', colors='white', labelsize=10)\n    plt.imshow(array, interpolation='nearest', aspect='auto')\n    plt.show()\n\n", "CodeBleu_score": 27.86902583561953}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\\\"image_name\\\": image_name, \\\"prediction\\\": prediction, \\\"confidence\\\": confidence}\n    with open(csv_path, \\\"a\\\") as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        if not os.path.exists(csv_path):\n            writer.writeheader()\n        writer.writerow(data)\n\n        # Print the results\n        print(f\\\"Image: {image_name}\\\")\n        print(f\\\"Prediction: {prediction}\\\")\n        print(f\\\"Confidence: {confidence}\\\")\n\n        # Save results (image with detections)\n        if save_img or view_img:\n            save_path = str(save_dir / Path(path).name)\n            print(f\\\"Results saved to {colorstr('bold', save_path)}\\\")\n            if dataset.mode == \\\"image\\\":\n                im0 = im0s[0]\n            elif dataset.mode == \\\"video\\\":\n                im0 = im0s\n            if len(pred) and save_img:\n                plot_images(im0, pred, save_path, names, classes, imgsz)\n            elif view_img:\n                plot_images(im0, pred, None, names, classes, imgsz)\n\n        # Print time (inference + NMS)\n        print(f'Done. ({t3.elapsed:.3f}s)", "CodeBleu_score": 40.99383976671379}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \\\"image_name\\\": image_name,\n        \\\"prediction\\\": prediction,\n        \\\"confidence\\\": confidence,\n    }\n\n    with open(csv_path, \\\"a\\\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if not os.path.exists(csv_path):\n            writer.writeheader()\n        writer.writerow(data)\n\n\n    # Process detections\n    for i, det in enumerate(pred):\n        if webcam:\n            p, s, im0 = Path(dataset.path[i]), f\\\"{s:.3f}\\\", im0s[i].copy()\n        else:\n            p, s, im0 = Path(path), \\\"\\\", im0s\n\n        save_path = str(save_dir / p.name)\n        txt_path = str(save_dir / \\\"labels\\\" / p.stem) + (\\\"_\\\" + dataset.mode if dataset.mode else \\\"\\\") + \\\".txt\\\"\n        s += \\\": \\\" + \\\", \\\".join(f + \\\"{0:.2f}\\\" for f in filter(det, names)) if det is not None else \\\"\\\"\n\n        # Print results\n        print(f\\\"{s}\\\")\n\n        # Write results\n        if save_txt or save_img or save_crop:\n            with open(txt_path, \\\"w\\\") as f:\n                if det is not None and len(det):\n                    # Rescale boxes from img_size to im0 size\n                    scale_coords(imgsz, det[:, :4], im0.shape).round()\n\n                    # Write results\n                    for *xyxy, conf, cls in reversed(det):\n                        if save_conf:\n                            conf = round(conf.item(), 3)\n                        if save_crop:\n                            save_one_box(xyxy, im0, file=save_dir / \\\"crops\\\" / names[int(cls)], bgr=True)\n                        line = (cls, *xyxy, conf) if save_conf else (cls, *xyxy)\n                        f.write(('%g'* len(line)).", "CodeBleu_score": 45.73757711872939}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(csv_path, mode=\"a\") as csv_file:\n        writer = csv.writer(csv_file)\n        if csv_file.tell() == 0:\n            writer.writerow([\"Image\", \"Prediction\", \"Confidence\"])\n        writer.writerow([image_name, prediction, confidence])\n\n", "CodeBleu_score": 36.9892702104169}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    if optimize:\n        model = optimize_for_mobile(model, [im])\n        file = file.replace(\\\".torchscript\\\", \\\"_optimized.torchscript\\\")\n    torch.jit.save(model, file)\n    yaml_save(model.yaml, file.replace(\\\".torchscript\\\", \\\".yaml\\\"))\n    return file, None\n\n", "CodeBleu_score": 27.662071480559714}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    if optimize:\n        model = optimize_for_mobile(model)\n        file = file.replace(\\\".torchscript\\\", \\\"_mobile.torchscript\\\")\n    traced_script = torch.jit.trace(model, im)\n    traced_script.save(file)\n    torch.save(model.state_dict(), file.replace(\\\".torchscript\\\", \\\"_model.pt\\\"))\n    torch.save(model.yaml, file.replace(\\\".torchscript\\\", \\\"_model.yaml\\\"))\n    return file, None\n\n", "CodeBleu_score": 35.04349818107888}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    if optimize:\n        # Export model for mobile use\n        LOGGER.info(f\"{prefix} optimizing model for mobile use...\")\n        with torch.no_grad():\n            model.eval()\n            model.fuse()\n            model.export(file)\n            model.model = optimize_for_mobile(model.model)\n            torch.jit.save(model.model, file)\n    else:\n        # Export standard TorchScript model\n        with torch.no_grad():\n            model.eval()\n            model.fuse()\n            model.model = torch.jit.trace(model, im)\n            torch.jit.save(model.model, file)\n    # Include extra files with model details\n    yaml_save(model.yaml, file.with_suffix(\\\".yaml\\\"))\n    return file, None\n\n", "CodeBleu_score": 38.86539035842432}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    import coremltools as ct\n\n    LOGGER.info(f\"", "CodeBleu_score": 10.562769710465622}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    from coremltools.converters.mil import register_torch_op\n\n    LOGGER.info(f\"", "CodeBleu_score": 8.978377987840132}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    import coremltools as ct\n\n    LOGGER.info(\n        f\"", "CodeBleu_score": 10.564057366078048}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0][0], input_ranges[0][1]))\n        input_ranges.pop(0)\n    return individual\n\n", "CodeBleu_score": 43.353064187374066}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        return self.log_alpha_array.to(t.device).bilinear(t, 1 - t)\n    else:\n        if self.schedule == 'linear':\n            return self.beta_0 * t + self.beta_1 * (1 - t)\n        else:\n            return self.cosine_log_alpha_0 + (math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.)) - self.cosine_log_alpha_0) * torch.cos(t * math.pi / self.cosine_t_max) / torch.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.)\n\n", "CodeBleu_score": 44.0737237455044}, {"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        return self._interpolate_log_alpha_array(t)\n    elif self.schedule == 'linear':\n        return -0.5 * (self.beta_0 * t + self.beta_1 * (1. - t))\n    elif self.schedule == 'cosine':\n        t = t * self.cosine_t_max\n        return self.cosine_log_alpha_0 - t * (1. + self.cosine_s) / math.pi\n\n\n\n", "CodeBleu_score": 35.40052419566575}, {"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        return torch.nn.functional.interpolate(self.log_alpha_array, t, mode='linear', align_corners=False)\n    elif self.schedule == 'linear':\n        return torch.log(1. - (1. - self.beta_0) * t / self.T)\n    else:\n        t = torch.clamp(t, 0., self.cosine_t_max)\n        return self.cosine_log_alpha_0 + torch.log(torch.cos(t / (1. + self.cosine_s) * math.pi / 2.))\n\n", "CodeBleu_score": 40.21605896412531}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        beta = lamb + 0.5 * torch.log(1. - torch.exp(2. * lamb))\n        beta = torch.clamp(beta, min=self.beta_0, max=self.beta_1)\n        t = torch.sqrt((beta - self.beta_0) / (self.beta_1 - self.beta_0))\n        return t\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        log_alpha_t = lamb + self.cosine_log_alpha_0\n        s = torch.acos(torch.exp(log_alpha_t)) * (1. + self.cosine_s) / math.pi - self.cosine_s\n        t = s / (1. + self.cosine_s) * 2. * math.pi / math.pi\n        return t\n\n", "CodeBleu_score": 45.69113244398741}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        beta_0 = self.beta_0\n        beta_1 = self.beta_1\n        temp = (beta_0 * beta_1 * lamb + beta_0 ** 2) / (beta_0 * beta_1 - beta_0 ** 2)\n        t = (beta_1 * temp - beta_0) / (beta_1 - beta_0)\n        return t\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        log_alpha_t = lamb + self.cosine_log_alpha_0\n        return log_alpha_fn.inverse(log_alpha_t)\n\n", "CodeBleu_score": 32.23163303292344}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        # For the linear schedule, we have\n        #   log(alpha_t) = -0.25 * t^2 * (beta_1 - beta_0) - 0.5 * t * beta_0\n        #   log(sigma_t) = -0.25 * t^2 * (beta_1 - beta_0) - 0.5 * t * beta_1\n        #   log(alpha_t) - log(sigma_t) = -0.5 * t * beta_0\n        #   log(alpha_t) - log(sigma_t) = lambda_t\n        #   log(alpha_t) = lambda_t + 0.5 * t * beta_0\n        #   log(alpha_t) = lambda_t + 0.5 * t * beta_0\n        #   -0.25 * t^2 * (beta_1 - beta_0) - 0.5 * t * beta_0 = lambda_t\n        #   t^2 * (beta_1 - beta_0) / 4. + t * beta_0 = -2 * lambda_t\n        #   t^2 * (beta_1 - beta_0) / 4. + t * beta_0 + 2 * lambda_t = 0\n        #   t = (-beta_0 + sqrt(beta_0^2 - 4 * (beta_1 - beta_0) * lambda_t)) / (2 * (beta_1 - beta_0))\n        #   t = (beta_0 - sqrt(beta_0^2 - 4 * (beta_1 - beta_0) * lambda_t)) / (2 * (beta_1 - beta_0))\n        #   t = (beta_0 - sqrt(beta_0^2 - 4 * (beta_1 - beta_0) * lambda_t)) / (2 * (beta_1 - beta_0))\n        return (self.beta_0 - torch.sqrt(self.", "CodeBleu_score": 12.778524990122717}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    t_input = t_input.expand(x.shape[0], -1)\n    if model_type == \"noise\":\n        noise = model(x, t_input, **model_kwargs)\n        if noise_schedule.schedule == \"discrete\":\n            noise = noise * noise_schedule.beta_1\n        return noise\n    elif model_type == \"x_start\":\n        x_start = model(x, t_input, **model_kwargs)\n        if noise_schedule.schedule == \"discrete\":\n            x_start = x_start / noise_schedule.alpha_1\n        return x_start\n    elif model_type == \"v\":\n        v = model(x, t_input, **model_kwargs)\n        if noise_schedule.schedule == \"discrete\":\n            v = v * noise_schedule.beta_1\n        return v\n    elif model_type == \"score\":\n        score = model(x, t_input, **model_kwargs)\n        if noise_schedule.schedule == \"discrete\":\n            score = score * noise_schedule.beta_1\n        return -noise_schedule.sigma_t * score\n    else:\n        raise ValueError(f\"Unsupported model type {model_type}\")\n\n", "CodeBleu_score": 29.59464170585249}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == \"noise\":\n        return noise_schedule.noise_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"x_start\":\n        return noise_schedule.x_start_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"v\":\n        return noise_schedule.v_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"score\":\n        return noise_schedule.score_pred(model, x, t_input, **model_kwargs)\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n\n", "CodeBleu_score": 20.45330610506299}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == \"noise\":\n        noise_pred = model(x, t_input, **model_kwargs)\n    elif model_type == \"x_start\":\n        x_start_pred = model(x, t_input, **model_kwargs)\n        noise_pred = -noise_schedule.sigma(t_input) * x_start_pred / noise_schedule.beta(t_input)\n    elif model_type == \"v\":\n        v_pred = model(x, t_input, **model_kwargs)\n        noise_pred = -noise_schedule.sigma(t_input) * v_pred / noise_schedule.beta(t_input)\n    elif model_type == \"score\":\n        score_pred = model(x, t_input, **model_kwargs)\n        noise_pred = -noise_schedule.sigma(t_input) * score_pred\n    else:\n        raise ValueError(f\"Invalid model_type: {model_type}\")\n\n    if guidance_type == \"uncond\":\n        noise_pred = noise_pred\n    elif guidance_type == \"classifier\":\n        if cond is None:\n            raise ValueError(\"cond is required for classifier guidance.\")\n        logits = classifier_fn(x, t_input, cond, **classifier_kwargs)\n        noise_pred = noise_pred + guidance_scale * logits\n    elif guidance_type == \"classifier-free\":\n        if cond is None:\n            raise ValueError(\"cond is required for classifier-free guidance.\")\n        if cond == unconditional_condition:\n            noise_pred = noise_pred\n        else:\n            logits = classifier_fn(x, t_input, cond, **classifier_kwargs)\n            noise_pred = noise_pred + guidance_scale * logits\n    else:\n        raise ValueError(f\"Invalid guidance_type: {guidance_type}\")\n\n    return noise_pred\n\n", "CodeBleu_score": 31.521394644345847}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0_abs = x0.abs()\n    x0_abs_flattened = x0_abs.flatten(1)\n    if t is not None:\n        t_input = self.noise_schedule.get_model_input_time(t)\n        quantile = self.noise_schedule.marginal_quantile(t_input)\n    else:\n        quantile = 0.99\n    threshold = x0_abs_flattened.kthvalue(int(quantile * x0_abs_flattened.shape[1]), dim=1).values.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n    scaling_factor = threshold / x0_abs\n    scaling_factor = scaling_factor.clamp(0, 1)\n    x0 = x0 * scaling_factor\n    x0 = x0.clamp(-self.max_val, self.max_val)\n    return x0\n\n", "CodeBleu_score": 31.890618359215704}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if self.thresholding:\n        if t is None:\n            t = torch.ones(x0.shape[0], device=x0.device)\n        q = self.noise_schedule.quantile(t)\n        x0_abs = torch.abs(x0)\n        x0_abs = x0_abs.reshape(x0_abs.shape[0], -1)\n        x0_abs = torch.quantile(x0_abs, q, dim=1)\n        x0_abs = x0_abs.reshape(x0_abs.shape[0], *([1] * (len(x0.shape) - 1)))\n        scale = x0_abs / (self.max_val * q)\n        x0 = torch.clamp(x0, -scale, scale)\n    return x0 * self.max_val\n\n", "CodeBleu_score": 41.00446249403119}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.noise_schedule.total_T\n    x0_abs = x0.abs()\n    x0_abs_flat = x0_abs.reshape((-1, 1))\n    x0_abs_flat_sorted = x0_abs_flat.sort(0, descending=True)[0]\n    q = x0_abs_flat_sorted[int(x0_abs_flat.shape[0] * (1 - self.thresholding))]\n    q = q.expand(x0_abs_flat.shape)\n    scale = 1 / (1 + q)\n    x0_scaled = x0 * scale\n    x0_scaled = torch.clamp(x0_scaled, -self.max_val, self.max_val)\n    return x0_scaled\n\n", "CodeBleu_score": 36.350440512044756}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.thresholding:\\\n        x = self.dynamic_thresholding_fn(x)\\\n    return x\n\n", "CodeBleu_score": 8.59907525782218}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.thresholding:\\\n        x0 = self.dynamic_thresholding_fn(x, t)\\\n    else:\\\n        x0 = x\\\n    return x0\n\n", "CodeBleu_score": 9.260172556132623}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.thresholding:\\\n        x = self.dynamic_thresholding_fn(x, t)\\\n    x = self.model(x, t)\\\n    return x\n\n", "CodeBleu_score": 10.20805325294974}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    x = input / sigma_in\n    eps = model(x, **kwargs)\n    return eps * sigma_in\n\n", "CodeBleu_score": 17.650590939518025}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    scaled_input = input / sigma\n    prediction = model(scaled_input, **kwargs)\n    return prediction * sigma + input\n\n", "CodeBleu_score": 21.176452810796608}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(1, -1, 1, 1)\n    scaled_input = input / sigma\n    prediction = model(scaled_input, **kwargs)\n    return prediction * sigma\n\n", "CodeBleu_score": 23.652883033269323}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # Calculate alpha and its logarithm.\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    # Compute the gradient of log_alpha with respect to taus.\n    grad_log_alpha = torch.autograd.grad(log_alpha, taus, torch.ones_like(log_alpha), retain_graph=True)[0]\n    # Calculate the integrand using the gradient and alpha values.\n    integrand = alpha * grad_log_alpha\n    return integrand\n\n", "CodeBleu_score": 20.569342375303673}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().detach().requires_grad_()\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    integrand = log_alpha * taus.grad\n    return integrand\n\n", "CodeBleu_score": 19.733115360464605}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().detach().requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha), retain_graph=True)\n    grad_log_alpha = taus.grad\n    integrand = grad_log_alpha * alpha\n    return integrand\n\n", "CodeBleu_score": 24.084540438561085}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = noise_sampler or default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_up, sigma_down) * s_noise\n            x = x + eps * (sigma_up ** 2 - sigma_down ** 2) ** 0.5\n        denoised = model(x, sigmas[i], **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i], 'denoised': denoised})\n        dt = sigmas[i + 1] - sigmas[i]\n        # Euler method\n        x = x + d * dt\n    return x\n\n", "CodeBleu_score": 56.521527134421426}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        denoised = model(x, sigmas[i], **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i], 'denoised': denoised})\n        dt = sigmas[i + 1] - sigmas[i]\n        # Euler method\n        x = x + d * dt + noise_sampler(sigmas[i], sigma_down) * s_noise\n        x = x + noise_sampler(sigma_down, sigma_down + sigma_up) * s_noise\n    return x\n\n", "CodeBleu_score": 56.006029273255095}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        denoised = model(x, sigma_down * s_in, **extra_args)\n        d = to_d(x, sigma_down, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_down': sigma_down, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_down\n        # Euler method\n        x = x + d * dt\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_up, sigmas[i + 1]) * s_noise\n            x = x + eps * sigma_up\n    return x\n\n", "CodeBleu_score": 62.67972331780046}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(f'Order {order} is too high for step index {i}.')\n    def product_term(k):\n        if k == j:\n            return 1\n        return (t[i] - t[k]) / (t[j] - t[k])\n    return integrate.quad(lambda x: product(product_term(k) for k in range(i + 1) if k!= j), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 36.35078488763691}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(f'The order {order} is too high for the step index {i}.')\n    def product_term(k):\n        if k == j:\n            return 1\n        return (t[i] - t[k]) / (t[i] - t[j])\n    return integrate.quad(lambda k: product_term(k), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 30.13819820098823}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i + 1:\n        raise ValueError(f'Order {order} is too high for step index {i}.')\n    def product_term(k):\n        if k == j:\n            return 1\n        else:\n            return (t[i] - t[k]) / (t[j] - t[k])\n    return integrate.quad(lambda x: product(product_term(k) for k in range(i + 1) if k!= j), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 34.50469417524527}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] > 0:\n            x = (x - denoised) / sigmas[i]\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1]\n        else:\n            x = denoised\n    return x\n\n", "CodeBleu_score": 73.23477242055823}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] > 0:\n            x = x - (sigmas[i] / sigmas[i + 1]) * (x - denoised) + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1]\n        else:\n            x = denoised\n    return x\n\n", "CodeBleu_score": 73.42133801454145}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = x - (sigmas[i] ** 2.0) * denoised\n        if sigmas[i + 1]!= 0:\n            x += sigmas[i + 1] ** 2.0 * noise_sampler(sigmas[i], sigmas[i + 1])\n    return x\n\n", "CodeBleu_score": 72.86646022175663}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t    if x.ndim < target_dims:\n        return x.expand(*x.shape, *((1,) * (target_dims - x.ndim)))\n    else:\n        raise ValueError(f'The number of dimensions of the tensor is already {x.ndim}, which is greater than the target dimensions {target_dims}.')\n\n", "CodeBleu_score": 22.513520147463108}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(\n            f\"The number of dimensions ({len(x.shape)}) is greater than or equal to the target dimensions ({target_dims})\"\n        )\n    return x.expand(*x.shape, *((1,) * (target_dims - len(x.shape)))).detach() if x.device.type =='mps' else x.expand(\n        *x.shape, *((1,) * (target_dims - len(x.shape)))\n    )\n\n", "CodeBleu_score": 30.27296813118827}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(\n            f\"The number of dimensions of the input tensor ({len(x.shape)}) is greater than or equal to the target dimensions ({target_dims}).\"\n        )\n    expand_dims = target_dims - len(x.shape)\n    if x.device.type == \"mps\":\n        return x.expand(*x.shape, *((1,) * expand_dims)).detach().clone()\n    else:\n        return x.expand(*x.shape, *((1,) * expand_dims))\n\n", "CodeBleu_score": 29.8092091677164}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        assert hashlib.sha256(path.read_bytes()).hexdigest() == digest\n    return path\n\n", "CodeBleu_score": 29.521823864930603}, {"code": "def download_file(path, url, digest=None)\n\t    if not Path(path).exists():\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        with open(path, 'rb') as f:\n            actual_digest = hashlib.sha256(f.read()).hexdigest()\n        if digest!= actual_digest:\n            raise ValueError(f'The downloaded file at {path} has an incorrect SHA-256 hash. Expected {digest}, got {actual_digest}')\n    return path\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 48.48984009737246}, {"code": "def download_file(path, url, digest=None)\n\t    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest()!= digest:\n            raise ValueError(f'The downloaded file at {path} has an incorrect SHA-256 hash.')\n    return path\n\n", "CodeBleu_score": 35.13963406463142}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t   if mode:\n        model.train()\n    else:\n        model.eval()\n    yield\n    for module in model.modules():\n        module.train(module._original_training_mode)\n\n", "CodeBleu_score": 29.58905989810794}, {"code": "def train_mode(model, mode=True)\n\t   if not isinstance(model, nn.Module):\n        raise TypeError(f'Expected model to be a nn.Module but got {type(model)}')\n    original_modes = {module: module.training for module in model.modules()}\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, training_mode in original_modes.items():\n            module.train(training_mode)\n\n", "CodeBleu_score": 35.73724617706638}, {"code": "def train_mode(model, mode=True)\n\t   if not isinstance(model, nn.Module):\n        raise TypeError(f'model must be a nn.Module, but got {type(model)}')\n    original_modes = {module: module.training for module in model.modules()}\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in original_modes.items():\n            module.train(mode)\n\n", "CodeBleu_score": 34.84867831674755}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t   if decay >= 1.0:\n        warnings.warn('decay is set to 1.0, EMA model will be set to the current model', UserWarning)\n        averaged_model.load_state_dict(model.state_dict())\n        return\n    for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.data.mul_(decay).add_(buffer.data, alpha=1 - decay)\n\n", "CodeBleu_score": 37.56177840299909}, {"code": "def ema_update(model, averaged_model, decay)\n\t   # check if both models have matching parameters and buffers\n    if not all(param.shape == averaged_param.shape for param, averaged_param in zip(model.parameters(), averaged_model.parameters())):\n        raise ValueError(\"The current model and the averaged model have mismatched parameters.\")\n    if not all(buffer.shape == averaged_buffer.shape for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers())):\n        raise ValueError(\"The current model and the averaged model have mismatched buffers.\")\n    # update the averaged model's parameters and buffers\n    for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 36.43448451360696}, {"code": "def ema_update(model, averaged_model, decay)\n\t   for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.mul_(decay).add_(buffer.data, alpha=1 - decay)\n\n", "CodeBleu_score": 30.93813386404357}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.sigmoid((min_value - loc) / scale)\n    cdf_max = torch.sigmoid((max_value - loc) / scale)\n    uniform_samples = torch.empty(shape, device=device, dtype=dtype).uniform_(cdf_min, cdf_max)\n    return loc + scale * torch.log(uniform_samples / (1 - uniform_samples))\n\n", "CodeBleu_score": 36.023133924536225}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype) * (log_logistic_cdf(max_value, loc, scale) - log_logistic_cdf(min_value, loc, scale)) + log_logistic_cdf(min_value, loc, scale)\n    return log_logistic_icdf(u, loc, scale)\n\n", "CodeBleu_score": 34.59682811970842}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid(logit((min_value - loc) / scale))\n    max_cdf = torch.sigmoid(logit((max_value - loc) / scale))\n    return scale * torch.sigmoid(torch.randn(shape, device=device, dtype=dtype) * (logit(max_cdf) - logit(min_cdf)) + logit(min_cdf)) + loc\n\n", "CodeBleu_score": 32.313778179849514}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(f\"Unsupported activation: {activation}\")\n    if antialias:\n        act = Activation1d(act)\n    return act\n\n", "CodeBleu_score": 72.40297290323515}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation {activation}\")\n\n    if antialias:\n        act = Activation1d(act)\n\n    return act\n\n", "CodeBleu_score": 75.42793690925954}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(f\"Unsupported activation type: {activation}\")\n    if antialias:\n        return Activation1d(act)\n    else:\n        return act\n\n", "CodeBleu_score": 64.9637876392338}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.float()\n    if freqs.dtype == torch.float16:\n        freqs = freqs.float()\n\n    t_shape = t.shape\n    t = t.reshape(-1, 2)\n    freqs = freqs.reshape(-1, 2)\n\n    t = torch.stack((t[:, 0] * freqs[:, 0], t[:, 1] * freqs[:, 1]), dim = -1)\n    t = torch.stack((t[:, 0] - t[:, 1], t[:, 0] + t[:, 1]), dim = -1)\n\n    t = t * scale\n    t = t.reshape(t_shape)\n\n    return t\n", "CodeBleu_score": 30.04748944957661}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.to(torch.float32)\n\n    if freqs.dtype == torch.float16:\n        freqs = freqs.to(torch.float32)\n\n    t = torch.cat((t, t), dim = -1)\n    freqs = torch.cat((freqs, freqs), dim = -1)\n\n    t = t * scale\n    t = t + freqs\n\n    t1, t2 = t.unbind(dim = -2)\n    return torch.cat((t1, t2), dim = -1)\n\n", "CodeBleu_score": 24.819381460007744}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype in (torch.float16, torch.bfloat16):\n        t = t.to(torch.float32)\n\n    t = rearrange(t, '... (j d) ->... j d', j = 2)\n    freqs = rearrange(freqs, '... d ->... d 1')\n    t = t + freqs * scale\n\n    return torch.cat((t, rotate_half(t)), dim = -1)\n\n", "CodeBleu_score": 19.27289770278027}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if dim == -1:\n        dim = len(idx.shape) - 1\n    return torch.index_select(self.codebook, dim, idx.flatten()).view(idx.shape + self.codebook.shape[1:])\n\n", "CodeBleu_score": 33.022639399977294}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).permute(0, 2, 1) if dim!= -1 else self.codebook(idx)\n\n", "CodeBleu_score": 31.1690367113263}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embedding = self.codebook(idx)\n    if dim!= -1:\n        embedding = embedding.transpose(dim, -1)\n    return embedding\n\n", "CodeBleu_score": 29.38348469466636}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    z = self.in_block(x)\n    z = self.down_blocks(z)\n    z_e = z\n    if quantize:\n        z_q, (vq_loss, commit_loss), indices = self.vquantizer(z_e)\n        return z_q, (vq_loss, commit_loss), z_e, indices\n    else:\n        return z_e\n\n", "CodeBleu_score": 33.25604721187691}, {"code": "def encode(self, x, quantize=False)\n\t\n    z = self.in_block(x)\n    z = self.down_blocks(z)\n    if quantize:\n        z_q, (vq_loss, commit_loss), indices = self.vquantizer(z)\n        return z_q, (vq_loss, commit_loss), indices\n    else:\n        return z\n\n", "CodeBleu_score": 32.35245990652388}, {"code": "def encode(self, x, quantize=False)\n\t\n    z = self.in_block(x)\n    z = self.down_blocks(z)\n    z_e_x = z\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.vquantizer(z_e_x)\n        return z_q_x, z_e_x, (vq_loss, commit_loss), indices\n    else:\n        return z_e_x\n\n", "CodeBleu_score": 32.7942262268593}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = logpy.info\n        context(f\\\"Applying EMA weights\\\")\n        self.model_ema.store(self)\n        try:\n            yield\n        finally:\n            self.model_ema.restore(self)\n            context(f\\\"Restored original weights\\\")\n\n    else:\n        yield\n\n    def encode(self, batch: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, Any]]:\n        raise NotImplementedError()\n\n    def decode(self, z: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, Any]]:\n        raise NotImplementedError()\n\n    def decode_and_encode(self, batch: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, Any]]:\n        raise NotImplementedError()\n\n    def forward(self, batch: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, Any]]:\n        raise NotImplementedError()\n\n    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> Dict[str, Any]:\n        raise NotImplementedError()\n\n    def training_step_end(self, batch: Dict[str, Any], batch_idx: int, outputs: Dict[str, Any]) -> Dict[str, Any]:\n        return outputs\n\n    def training_epoch_end(self, outputs: List[Dict[str, Any]]) -> Dict[str, Any]:\n        return {\\\"log\\\": {k: sum([o[k] for o in outputs]) / len(outputs) for k in outputs[0][\\\"log\\\"]}}\n\n    def validation_step(self, batch: Dict[str, Any], batch_idx: int) -> Dict[str, Any]:\n        raise NotImplementedError()\n\n    def validation_step_end(self, batch: Dict[str, Any], batch_idx: int, outputs: Dict[str, Any]) -> Dict[str, Any]:\n        return outputs\n\n    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> Dict[str, Any]:\\", "CodeBleu_score": 27.015436909051395}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    self.model_ema.store(self)\n    if context is not None:\n        logpy.info(f\\\"Switching to EMA weights for {context}.\\\")\n    yield\n    self.model_ema.restore(self)\n    if context is not None:\n        logpy.info(f\\\"Switched back to original weights for {context}.\\\")\n\n", "CodeBleu_score": 51.794366921811076}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = {}\n        context[\\\"ema_scope\\\"] = True\n        self.model_ema.store(self)\n        self.load_state_dict(self.model_ema.state_dict(), strict=False)\n        if \\\"ema_scope\\\" not in context:\n            logpy.info(\\\"Switching to EMA weights.\\\")\n        yield\n        self.load_state_dict(self.model_ema.stored_state_dict, strict=False)\n        if \\\"ema_scope\\\" not in context:\n            logpy.info(\\\"Switching back to non-EMA weights.\\\")\n    else:\n        yield\n\n", "CodeBleu_score": 34.997944311182586}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    if not self.pre_only:\n        x = attn + gate_msa * x\n        x = self.norm2(x)\n        if not self.scale_mod_only:\n            x = self.mlp(modulate(x, shift_mlp, scale_mlp)) * gate_mlp\n            return x\n        else:\n            x = self.mlp(x) * gate_mlp\n            return x\n    else:\n        x = attn + x\n        x = self.norm2(x)\n        return x\n\n", "CodeBleu_score": 36.711490213241674}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    if not self.pre_only:\n        if not self.scale_mod_only:\n            x = x + attn * gate_msa\n            x = self.norm2(x)\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x) * gate_mlp\n            return x\n        else:\n            x = x + attn * gate_msa\n            x = self.norm2(x)\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x)\n            return x\n    else:\n        if not self.scale_mod_only:\n            x = x + attn * gate_msa\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x) * gate_mlp\n            return x\n        else:\n            x = x + attn * gate_msa\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x)\n            return x\n\n", "CodeBleu_score": 36.776778138627634}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    if not self.pre_only:\n        x = x + attn * gate_msa\n        if not self.scale_mod_only:\n            x = self.norm2(x)\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x) * gate_mlp\n        else:\n            x = self.norm2(x)\n            x = modulate(x, shift_mlp, scale_mlp)\n            x = self.mlp(x)\n        return x\n    else:\n        x = x + attn * gate_msa\n        x = self.norm2(x)\n        x = modulate(x, shift_mlp, scale_mlp)\n        x = self.mlp(x) * gate_mlp\n        return x\n\n", "CodeBleu_score": 38.223718334840605}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    attn_context, attn_x = attn.split([qkv_context.shape[1], qkv_x.shape[1]], dim=1)\n    if context_block.pre_only:\n        context = context + context_block.attn.post_attention(attn_context)\n    else:\n        context = context + context_block.post_attention(attn_context, *intermediates_context)\n    if x_block.pre_only:\n        x = x + x_block.attn.post_attention(attn_x)\n    else:\n        x = x + x_block.post_attention(attn_x, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 32.96605609960317}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv, context_intermediates = context_block.pre_attention(context, c)\n    x_qkv, x_intermediates = x_block.pre_attention(x, c)\n    qkv = torch.cat((context_qkv, x_qkv), dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context_attn, x_attn = attn.chunk(2, dim=1)\n    if context_block.pre_only:\n        context = context + context_block.attn.post_attention(context_attn)\n        x = x + x_block.attn.post_attention(x_attn)\n    else:\n        context = context_block.post_attention(\n            context_attn, *context_intermediates\n        )\n        x = x_block.post_attention(x_attn, *x_intermediates)\n    return context, x\n\n", "CodeBleu_score": 41.05900019930688}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    attn_context, attn_x = attn.chunk(2, dim=1)\n    if context_block.pre_only:\n        context = context_block.post_attention(attn_context, *intermediates_context)\n        x = x_block.post_attention(attn_x, *intermediates_x)\n    else:\n        context = context_block.post_attention(attn_context, *intermediates_context)\n        x = x_block.post_attention(attn_x, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 31.793632002191526}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert timesteps.dim() == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, device=timesteps.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 70.42313245535323}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert embedding_dim % 2 == 0, \"embedding_dim must be divisible by 2\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 69.1797229539812}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    if timesteps.ndim!= 1:\n        raise ValueError(f\"Timesteps input must be one-dimensional, got {timesteps.ndim} dimensions\")\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), \"constant\", 0.0)\n    return emb\n\n", "CodeBleu_score": 69.7102668721607}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b, c, h, w = q.shape\n    q = q.reshape(b, c, h * w)\n    q = q.permute(0, 2, 1)  # b,hw,c\n    k = k.reshape(b, c, h * w)  # b,c,hw\n    v = v.reshape(b, c, h * w)\n\n    try:\n        r1 = torch.bmm(q, k)\n        scale = (int(q.shape[-1]) ** (-0.5))\n        r1 = r1 * scale\n        r1 = torch.nn.functional.softmax(r1, dim=2)\n        r1 = torch.bmm(v, r1)\n        h_ = r1.reshape(b, c, h, w)\n        del r1\n    except model_management.OOM_EXCEPTION:\n        h_ = slice_attention(q, k, v)\n    return h_\n\n", "CodeBleu_score": 26.304699887440297}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    try:\n        r1 = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, dropout_p=0.0, is_causal=False)\n    except model_management.OOM_EXCEPTION as e:\n        r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n\n", "CodeBleu_score": 27.673646918316138}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    try:\n        r1 = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=False)\n    except model_management.OOM_EXCEPTION as e:\n        r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n", "CodeBleu_score": 28.979764408802044}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        control_value = control.pop(name)\n        try:\n            h = h + control_value\n        except Exception as e:\n            logger.warning(f\"Control {name} could not be applied to h: {e}\")\n    return h\n\n", "CodeBleu_score": 43.30131510985322}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        control_value = control.pop(name)\n        try:\n            h += control_value\n        except Exception:\n            print(f\"Warning: control {name} could not be applied.\")\n    return h\n\n", "CodeBleu_score": 36.26465218747257}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        control_value = control.pop(name)\n        try:\n            h += control_value\n        except Exception as e:\n            warnings.warn(f\\\"Failed to add {name} control to {h}. {e}\\\")\n    return h\n\n", "CodeBleu_score": 44.23477560407635}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t  one_hot = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    one_hot.scatter_(1, indices, 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.view(shape).permute(0, 3, 1, 2).contiguous()\n    return quantized\n\n", "CodeBleu_score": 43.75105940591556}, {"code": "def get_codebook_entry(self, indices, shape)\n\t  one_hot = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    one_hot.scatter_(1, indices, 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.view(shape).permute(0, 3, 1, 2).contiguous()\n    return quantized\n\n", "CodeBleu_score": 43.75105940591556}, {"code": "def get_codebook_entry(self, indices, shape)\n\t  # create one-hot encoding of indices\n    one_hot = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    one_hot.scatter_(1, indices, 1)\n    # get quantized latent vectors\n    z_q = torch.matmul(one_hot, self.embedding.weight)\n    # reshape and permute to match original input shape\n    if shape is not None:\n        z_q = z_q.view(shape).permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 48.30610982839276}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noises = []\n    for i in range(self.num_layers):\n        resolution = 2**((i + 5) // 2)\n        shape = [1, 1, resolution, resolution]\n        noises.append(torch.randn(*shape, device=self.device))\n        if i % 2 == 0:\n            noises.append(torch.randn(*shape, device=self.device))\n    return noises\n\n", "CodeBleu_score": 28.736682514644734}, {"code": "def make_noise(self)\n\t\n    noises = []\n    for i in range(self.num_layers):\n        resolution = 2 ** ((i + 5) // 2)\n        shape = [1, 1, resolution, resolution]\n        noises.append(torch.randn(*shape).to(self.device))\n        if i % 2 == 0:\n            noises.append(torch.randn(*shape).to(self.device))\n    return noises\n", "CodeBleu_score": 27.313687724504625}, {"code": "def make_noise(self)\n\t\n    noises = []\n    for i in range(self.num_latent):\n        size = 4 * 2**i\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n    return noises\n\n", "CodeBleu_score": 24.628668270416433}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n    latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 62.10884582781706}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=x.device)\n    latent = self.style_mlp(latent_in)\n    latent_mean = latent.mean(0, keepdim=True)\n    return latent_mean\n\n", "CodeBleu_score": 38.81134920234642}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n    latent = self.get_latent(latent_in)\n    latent = latent.mean(dim=0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 51.7152603866117}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t    img = img.permute(1, 2, 0)  # HWC\n    img = img.numpy()\n    # shift = np.random.randint(-shift, shift, size=3)\n    # img = img + shift\n    # img = np.clip(img, 0, 1)\n    # img = img.transpose(2, 0, 1)  # CHW\n    # img = torch.from_numpy(img)\n    # return img\n    # img = img.permute(1, 2, 0)  # HWC\n    # img = img.numpy()\n    img = np.clip(img, 0, 1)\n    img = img.transpose(2, 0, 1)  # CHW\n    img = torch.from_numpy(img)\n    return img\n\n", "CodeBleu_score": 43.759448285404915}, {"code": "def color_jitter(img, shift)\n\t    if shift > 0.0:\n        out = np.clip(img + np.random.uniform(-shift, shift, size=img.shape), 0.0, 1.0)\n    else:\n        out = img\n    return out\n\n", "CodeBleu_score": 38.64822203159485}, {"code": "def color_jitter(img, shift)\n\t    r = np.random.randint(-shift, shift)\n    g = np.random.randint(-shift, shift)\n    b = np.random.randint(-shift, shift)\n    img = img + np.array([r, g, b])\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 53.0505988019395}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.81975192363866}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)  # LQ images\n    if 'gt' in data.keys():\n        self.gt = data['gt'].to(self.device)  # GT images\n    else:\n        self.gt = None\n    if 'left_eye' in data.keys():\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data.keys():\n        self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data.keys():\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 45.64865637360815}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)  # low-quality images\n    if data.get('gt', None) is not None:\n        self.gt = data['gt'].to(self.device)  # ground-truth images\n    else:\n        self.gt = None\n\n    if data.get('facial_component_locations', None) is not None:\n        self.left_eye_locations = data['facial_component_locations']['left_eye']\n        self.right_eye_locations = data['facial_component_locations']['right_eye']\n        self.mouth_locations = data['facial_component_locations']['mouth']\n\n    # get the current iteration number\n    self.iteration = data['iteration']\n\n", "CodeBleu_score": 44.091034801612885}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for _ in range(self.opt['network_g']['num_scales'] - 1):\n        gt_downscaled = F.interpolate(img_pyramid[-1], scale_factor=0.5, mode='bilinear', align_corners=True)\n        img_pyramid.append(gt_downscaled)\n    return img_pyramid\n\n\n\n", "CodeBleu_score": 19.848293001049537}, {"code": "def construct_img_pyramid(self)\n\t\n    pyramid = []\n    pyramid.append(self.gt)\n    for _ in range(self.opt['train']['n_pyramid'] - 1):\n        gt_downscaled = F.interpolate(\n            pyramid[-1], scale_factor=0.5, mode='bilinear', align_corners=False)\n        pyramid.append(gt_downscaled)\n    return pyramid\n\n\n", "CodeBleu_score": 20.753218243268243}, {"code": "def construct_img_pyramid(self)\n\t\n    pyramid = [self.gt]\n    for _ in range(self.opt['train']['pyramid_levels'] - 1):\n        pyramid.insert(0, F.interpolate(pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid\n\n", "CodeBleu_score": 25.695109422523938}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['dist']:\n        torch.distributed.barrier()\n    if self.opt['rank'] == 0:\n        if hasattr(self, 'net_g_ema'):\n            torch.save(self.net_g_ema.state_dict(),\n                       osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_G_{epoch}_{current_iter}.pth'))\n        torch.save(self.net_g.state_dict(),\n                   osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_G_{epoch}_{current_iter}.pth'))\n        if self.opt['is_train']:\n            torch.save(self.net_d.state_dict(),\n                       osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_D_{epoch}_{current_iter}.pth'))\n            if self.use_facial_disc:\n                torch.save(self.net_d_left_eye.state_dict(),\n                           osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_D_left_eye_{epoch}_{current_iter}.pth'))\n                torch.save(self.net_d_right_eye.state_dict(),\n                           osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_D_right_eye_{epoch}_{current_iter}.pth'))\n                torch.save(self.net_d_mouth.state_dict(),\n                           osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_D_mouth_{epoch}_{current_iter}.pth'))\n            torch.save(self.state_dict(),\n                       osp.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_state_dict_{epoch}_{current_iter}.pth'))\n\n", "CodeBleu_score": 29.79970784521343}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['rank'] == 0:\n        if hasattr(self, 'net_g_ema'):\n            state_dict = self.net_g_ema.state_dict()\n            torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_g_ema_{epoch + 1}.pth'))\n        state_dict = self.net_g.state_dict()\n        torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_g_{epoch + 1}.pth'))\n        if self.use_disc:\n            state_dict = self.net_d.state_dict()\n            torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_d_{epoch + 1}.pth'))\n        if self.use_facial_disc:\n            state_dict = self.net_d_left_eye.state_dict()\n            torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_d_left_eye_{epoch + 1}.pth'))\n            state_dict = self.net_d_right_eye.state_dict()\n            torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_d_right_eye_{epoch + 1}.pth'))\n            state_dict = self.net_d_mouth.state_dict()\n            torch.save(state_dict, osp.join(self.opt['path']['models'], f'net_d_mouth_{epoch + 1}.pth'))\n\n        state_dict = {\n            'epoch': epoch,\n            'iter': current_iter,\n           'state_dict_g': self.net_g.state_dict(),\n           'state_dict_d': self.net_d.state_dict()\n        }\n        if hasattr(self, 'net_g_ema'):\n            state_dict['state_dict_g_ema'] = self.net_g_ema.state_dict()\n        if self.use_facial_disc", "CodeBleu_score": 31.12604971965417}, {"code": "def save(self, epoch, current_iter)\n\t\n    if hasattr(self, 'net_g_ema') and not self.opt['train']['no_val_ema']:\n        net_g_ema_state_dict = self.net_g_ema.state_dict()\n    else:\n        net_g_ema_state_dict = None\n\n    if hasattr(self, 'net_d_left_eye') and self.use_facial_disc:\n        net_d_left_eye_state_dict = self.net_d_left_eye.state_dict()\n    else:\n        net_d_left_eye_state_dict = None\n\n    if hasattr(self, 'net_d_right_eye') and self.use_facial_disc:\n        net_d_right_eye_state_dict = self.net_d_right_eye.state_dict()\n    else:\n        net_d_right_eye_state_dict = None\n\n    if hasattr(self, 'net_d_mouth') and self.use_facial_disc:\n        net_d_mouth_state_dict = self.net_d_mouth.state_dict()\n    else:\n        net_d_mouth_state_dict = None\n\n    if hasattr(self, 'optimizer_g'):\n        optimizer_g_state_dict = self.optimizer_g.state_dict()\n    else:\n        optimizer_g_state_dict = None\n\n    if hasattr(self, 'optimizer_d'):\n        optimizer_d_state_dict = self.optimizer_d.state_dict()\n    else:\n        optimizer_d_state_dict = None\n\n    if hasattr(self, 'optimizer_d_left_eye') and self.use_facial_disc:\n        optimizer_d_left_eye_state_dict = self.optimizer_d_left_eye.state_dict()\n    else:\n        optimizer_d_left_eye_state_dict = None\n\n    if hasattr(self, 'optimizer_d_right_eye') and self.use_facial_disc:\n        optimizer_d_right_eye_state_dict = self", "CodeBleu_score": 28.242865844857327}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 48.45688172366009}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 48.45688172366009}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.88650947584345}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n\n    sampled_exps = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in sampled_exps]\n    return sampled_exps\n\n", "CodeBleu_score": 67.24631333365694}, {"code": "def next_batch(self, sample_size=1)\n\t    if len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n\n    sampled_exps = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in sampled_exps]\n\n    return sampled_exps\n\n", "CodeBleu_score": 55.205617481634306}, {"code": "def next_batch(self, sample_size=1)\n\t    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n\n        batch = random.sample(self.exps, sample_size)\n        self.exps = [exp for exp in self.exps if exp not in batch]\n        return batch\n\n\n    def get_best_exp(self):\n        return max(self.exps, key=lambda exp: exp.metric)\n\n\n    def get_best_config(self):\n        return self.get_best_exp().config\n\n\n    def get_best_metric(self):\n        return self.get_best_exp().metric\n\n\n    def get_best_model_path(self):\n        return self.get_best_exp().model_path\n\n\n    def get_best_model_state_dict_path(self):\n        return self.get_best_exp().model_state_dict_path\n\n\n    def get_best_optimizer_state_dict_path(self):\n        return self.get_best_exp().optimizer_state_dict_path\n\n\n    def get_best_lr_scheduler_state_dict_path(self):\n        return self.get_best_exp().lr_scheduler_state_dict_path\n\n\n    def get_best_exp_id(self):\n        return self.get_best_exp().exp_id\n\n\n    def get_best_exp_dir(self):\n        return self.get_best_exp().exp_dir\n\n\n    def get_best_exp_log_dir(self):\n        return self.get_best_exp().exp_log_dir\n\n\n    def get_best_exp_config_path(self):\n        return self.get_best_exp().exp_config_path\n\n\n    def get_best_exp_log_path(self):\n        return self.get_best_exp().exp_log_path\n\n\n    def get_best_exp_metrics_path(self):\n        return self", "CodeBleu_score": 50.57334240913023}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\tll_configs = self.all_configs\n    all_configs = [flatten_dict(e) for e in all_configs]\n    all_configs = np.array([list(e.values()) for e in all_configs])\n\n    # filter out non-numerical values\n    all_configs = all_configs[:, [i for i in range(all_configs.shape[1]) if isinstance(all_configs[0][i], numbers.Number)]]\n\n    # predict the performance\n    estimates = self.cost_model.predict(all_configs)\n\n    # sort the configurations based on the estimated performance\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        estimates = np.argsort(estimates)\n    else:\n        estimates = np.argsort(-estimates)\n\n    return estimates\n\n", "CodeBleu_score": 28.02479952782474}, {"code": "def find_estimated_top_configs(self)\n\tll_configs = self.all_configs\n    all_configs_flatten = []\n    for config in all_configs:\n        config_flatten = []\n        for key in config:\n            if isinstance(config[key], numbers.Number):\n                config_flatten.append(config[key])\n        all_configs_flatten.append(config_flatten)\n    all_configs_flatten = np.array(all_configs_flatten)\n\n    # Predict the performance of all configurations\n    predicted_perf = self.cost_model.predict(all_configs_flatten)\n\n    # Sort the configurations based on the metric\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        sorted_indices = np.argsort(predicted_perf)\n    else:\n        sorted_indices = np.argsort(predicted_perf)[::-1]\n\n    return sorted_indices\n\n", "CodeBleu_score": 36.83753093611859}, {"code": "def find_estimated_top_configs(self)\n\tll_configs = self.all_configs\n    all_configs_flattened = [flatten_config(c) for c in all_configs]\n    all_configs_flattened = np.array(all_configs_flattened)\n    all_configs_flattened = all_configs_flattened[:, np.where(self.dims)[0]]\n    all_configs_flattened = all_configs_flattened.astype(np.float32)\n\n    # Predict estimates\n    estimates = self.cost_model.predict(all_configs_flattened)\n\n    # Sort\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        top_configs_idx = np.argsort(estimates)\n    else:\n        top_configs_idx = np.argsort(-estimates)\n\n    return top_configs_idx\n\n", "CodeBleu_score": 28.059651904426726}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.50541011711259}, {"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.50541011711259}, {"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.50541011711259}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        logger.info(f\\\"Running the optimal command found by autotuning: {self.optimal_cmd}\\\")\n        subprocess.Popen(self.optimal_cmd, shell=True).wait()\n        logger.info(f\\\"Autotuning completed successfully.\\\")\n    else:\n        logger.info(f\\\"Autotuning did not find an optimal configuration.\\\")\n\n\n", "CodeBleu_score": 35.53120006843192}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(f\\\"Running the optimal DeepSpeed command found by autotuning: {self.optimal_cmd}\\\")\n        p = subprocess.Popen(self.optimal_cmd)\n        p.wait()\n        logger.info(f\\\"Autotuning finished successfully. You can find the optimal DeepSpeed configuration at {self.results_dir}/ds_config_optimal.json\\\")\n    else:\n        logger.info(f\\\"Autotuning finished but no optimal configuration was found. You can find the results at {self.results_dir}/tuning_results.json\\\")\n\n    \n", "CodeBleu_score": 51.41580711973145}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is None:\n        logger.info(\n            \"Autotuning did not find an optimal configuration. Skipping the command after tuning.\")\n        return\n    logger.info(\"Running the command after tuning:\")\n    logger.info(\" \".join(self.optimal_cmd))\n    p = subprocess.Popen(self.optimal_cmd)\n    p.wait()\n    logger.info(\"The command after tuning has finished.\")\n\n", "CodeBleu_score": 51.72976947749039}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t  exp_id = exp[\"exp_id\"]\n    script = exp[\"script\"]\n    args = exp[\"args\"]\n    result_dir = exp[\"result_dir\"]\n\n    if self.args.master_port is not None:\n        args.append(f\\\"--master_port={self.args.master_port}\\\")\n    if self.args.master_addr is not None:\n        args.append(f\\\"--master_addr={self.args.master_addr}\\\")\n    if self.args.master_port is not None:\n        args.append(f\\\"--master_port={self.args.master_port}\\\")\n    if self.args.master_addr is not None:\n        args.append(f\\\"--master_addr={self.args.master_addr}\\\")\n\n    if self.arg_mappings is not None:\n        for k, v in self.arg_mappings.items():\n            if k in args:\n                idx = args.index(k)\n                args[idx] = v\n\n    if reservations is not None:\n        args.append(f\\\"--reservations={reservations}\\\")\n\n    if not os.path.exists(result_dir):\n        os.makedirs(result_dir)\n\n    t = threading.Thread(target=self.run_exp, args=(exp_id, script, args))\n    t.start()\n    self.running_experiments[exp_id] = (t, time.time())\n\n", "CodeBleu_score": 36.031062016308454}, {"code": "def run_job(self, exp: dict, reservations)\n\t  master_port = dist.get_master_port()\n    result_dir = exp[\\\"result_dir\\\"]\n    os.makedirs(result_dir, exist_ok=True)\n\n    # update user args with arg mappings\n    for k, v in self.arg_mappings.items():\n        if k in exp[\\\"user_args\\\"]:\n            exp[\\\"user_args\\\"][k] = v\n\n    # start experiment in a new thread\n    t = threading.Thread(target=self.run_experiment, args=(exp, master_port, result_dir, reservations))\n    t.start()\n    self.running_experiments[exp[\\\"exp_id\\\"]] = (exp, time.time())\n\n", "CodeBleu_score": 20.493307111970545}, {"code": "def run_job(self, exp: dict, reservations)\n\t  exp_id = exp[\"exp_id\"]\n    exp_name = exp[\"name\"]\n    exp_path = os.path.join(self.exps_dir, exp_name)\n    script = exp[\"script\"]\n    args = exp[\"args\"]\n    master_port = exp[\"master_port\"]\n    result_dir = exp[\"result_dir\"]\n\n    # update user arguments based on argument mappings\n    if self.arg_mappings:\n        for k, v in self.arg_mappings.items():\n            if k in args:\n                args[k] = v\n\n    # set the master port and result directory\n    args[\\\"--master_port\\\"] = master_port\n    args[\\\"--result_dir\\\"] = result_dir\n\n    # create a new thread to run the experiment\n    t = threading.Thread(target=self.run_experiment, args=(exp_id, exp_path, script, args, reservations))\n    t.start()\n\n    # store the running experiment details with a timestamp\n    self.running_experiments[exp_id] = (t, time.time())\n\n", "CodeBleu_score": 31.45387278115839}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n\n    if dist.is_initialized():\n        if not dist.get_rank() == 0:\n            return\n        if ranks is not None and dist.get_rank() not in ranks:\n            return\n\n    if path is None:\n        path = os.path.join(self.args.results_dir, \"results.json\")\n\n    if not os.path.exists(path):\n        with open(path, \"w\") as f:\n            f.write(\"[]\")\n\n    with open(path, \"r\") as f:\n        data = json.load(f)\n\n    if ranks is not None:\n        message[\"rank\"] = dist.get_rank()\n\n    data.append(message)\n\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n", "CodeBleu_score": 32.85007320289354}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n\n    if dist.is_initialized():\n        if ranks is None:\n            ranks = [dist.get_rank()]\n        if dist.get_rank() in ranks:\n            message = f\\\"rank={dist.get_rank()} {message}\\\"\n    else:\n        if ranks is None:\n            ranks = [0]\n\n    if dist.get_rank() in ranks:\n        if path is None:\n            path = self.args.log_file\n        with open(path, 'a') as f:\n            f.write(json.dumps(message) + '\\", "CodeBleu_score": 33.08134795805104}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.deepspeed_config.get(LOGGING, False) and dist.is_initialized():\n        if ranks is None:\n            ranks = [dist.get_rank()]\n        else:\n            ranks = [dist.get_rank() for _ in range(len(ranks))]\n\n        message = {\\\"rank\\\": ranks, \\\"message\\\": message}\n        if path is None:\n            path = os.path.join(self.output_dir, \\\"results.json\\\")\n        with open(path, \\\"a+\\\") as f:\n            f.write(json.dumps(message))\n            f.write(\\\"\\", "CodeBleu_score": 31.544205710467114}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_keys[layer_index].startswith(LAYER_PREFIX):\n        raise ValueError(f'layer_index {layer_index} is not a layer index')\n\n    layer_files = self._get_files_with_prefix(self.layer_keys[layer_index])\n    if len(layer_files) == 0:\n        raise ValueError(f'no files found for layer {layer_index}')\n\n    layer_files.sort()\n    return self._partition_files(layer_files, self.tp_degree)\n\n", "CodeBleu_score": 32.721389603803644}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.has_layer_files(layer_index):\n        return {}\n\n    assert layer_index >= 0 and layer_index < len(self.layer_keys)\n    layer_prefix = self.layer_keys[layer_index]\n    layer_files = self._get_layer_files(layer_prefix)\n    layer_files = self._filter_files_by_degree(layer_files, self.tp_degree)\n    tp_to_layer_map = self._partition_files_by_degree(layer_files, self.tp_degree)\n    return tp_to_layer_map\n\n", "CodeBleu_score": 40.98269066294547}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if layer_index >= len(self.layer_keys):\n        raise ValueError(f'layer_index {layer_index} is out of range')\n\n    layer_prefix = self.layer_keys[layer_index]\n    layer_files = [fname for fname in self.mp_rank_files if fname.startswith(layer_prefix)]\n    if len(layer_files) == 0:\n        raise ValueError(f'layer {layer_prefix} does not exist')\n\n    tp_to_layer_files = defaultdict(list)\n    for fname in layer_files:\n        tp_index = self._get_tp_index_from_file_name(fname)\n        tp_to_layer_files[tp_index].append(fname)\n\n    return tp_to_layer_files\n\n", "CodeBleu_score": 33.354071855361006}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    system_info = {\n        \"OS\": platform.system(),\n        \"OS Version\": platform.version(),\n        \"Architecture\": platform.machine(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n    }\n    return system_info\n\n", "CodeBleu_score": 53.96435924511904}, {"code": "def system_info()\n\t\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 83.71932669032101}, {"code": "def system_info()\n\t\n    info = {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture()[0],\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n    return info\n\n", "CodeBleu_score": 59.195681050056294}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    cuda_available = torch.cuda.is_available()\n    if cuda_available:\n        cuda_version = torch.version.cuda\n        device_count = torch.cuda.device_count()\n        device_names = [torch.cuda.get_device_name(i) for i in range(device_count)]\n        return {\"cuda_available\": cuda_available, \"cuda_version\": cuda_version, \"device_count\": device_count, \"device_names\": device_names}\n    else:\n        return {\"cuda_available\": cuda_available}\n\n", "CodeBleu_score": 43.48244963551694}, {"code": "def cuda_info()\n\t\n    return {\n        \"cuda_available\": torch.cuda.is_available(),\n        \"cuda_version\": torch.version.cuda,\n        \"gpu_devices\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n    }\n\n", "CodeBleu_score": 69.87940987537189}, {"code": "def cuda_info()\n\t\n    if not torch.cuda.is_available():\n        return {\"cuda_available\": False}\n    cuda_info = {\"cuda_available\": True}\n    cuda_info[\"cuda_version\"] = torch.version.cuda\n    cuda_info[\"device_count\"] = torch.cuda.device_count()\n    cuda_info[\"devices\"] = []\n    for device_id in range(cuda_info[\"device_count\"]):\n        cuda_info[\"devices\"].append(torch.cuda.get_device_name(device_id))\n    return cuda_info\n\n", "CodeBleu_score": 45.57139379124025}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Retrieve the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\", {})\n\n    # Initialize the accuracy dictionary for each class\n    accuracy_dict = {}\n\n    # Iterate over each item in the dataset\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item and class_name_key in encoder_manager.config:\n            # Retrieve the true class name\n            true_class_name = item[class_name_key]\n\n            # Predict the class label\n            predicted_class_name = encoder_manager.predict_class_label(embedding)\n\n            # If the true class name is in the class ID to name mapping, use the mapped name\n            if true_class_name in class_id_to_name:\n                true_class_name = class_id_to_name[true_class_name]\n\n            # If the predicted class name is in the class ID to name mapping, use the mapped name\n            if predicted_class_name in class_id_to_name:\n                predicted_class_name = class_id_to_name[predicted_class_name]\n\n            # Record the accuracy for the true class name\n            accuracy_dict[true_class_name] = accuracy_dict.get(true_class_name, 0) + (\n                1 if true_class_name == predicted_class_name else 0\n            )\n\n    # Calculate the accuracy for each class\n    for class_name, count in accuracy_dict.items():\n        accuracy = count / len(dataset_items) * 100\n        print(f\"Accuracy for class {class_name}: {accuracy:.2f}%\")\n\n    # Calculate the average accuracy across all classes\n    total_count = sum(accuracy_dict.values())\n    average_accuracy = total_count / len(dataset_items) / len(accuracy_dict) * 100\n    print(f\"Average accuracy:", "CodeBleu_score": 33.35706017846902}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\", None)\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\", None)\n\n    # Initialize the accuracy dictionary\n    accuracy_dict = {}\n\n    # Loop over the dataset items\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item.get(\"audio_file\", None)\n\n        # Compute the embedding\n        embedding = encoder_manager.get_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key and class_id_to_name_mapping:\n            # Retrieve the class name from the item\n            class_name = item.get(class_name_key, None)\n\n            # Predict the class label\n            predicted_label = encoder_manager.predict_class_label(embedding)\n\n            # Convert the predicted label to class name\n            predicted_class_name = class_id_to_name_mapping.get(predicted_label, None)\n\n            # Update the accuracy for the class\n            if class_name not in accuracy_dict:\n                accuracy_dict[class_name] = {\"correct\": 0, \"total\": 0}\n\n            accuracy_dict[class_name][\"total\"] += 1\n\n            if predicted_class_name == class_name:\n                accuracy_dict[class_name][\"correct\"] += 1\n\n    # Print the accuracy for each class\n    for class_name, accuracy in accuracy_dict.items():\n        accuracy_percent = accuracy[\"correct\"] / accuracy[\"total\"] * 100\n        print(f\"Accuracy for class {class_name}: {accuracy_percent:.2f}%\")\n\n    # Calculate and print the average accuracy across all classes\n    total_correct = sum(accuracy[\"correct\"] for accuracy in accuracy_dict.values())\n    total_total = sum(accuracy[\"total\"] for accuracy in accuracy_dict.values())\n    average_accuracy = total_correct /", "CodeBleu_score": 32.783246931692986}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_name_to_id = encoder_manager.config.get(\"class_name_to_id\", None)\n    accuracies = {}\n    total_accuracy = 0\n    total_count = 0\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.get_embedding(audio_file)\n        true_class_name = item[class_name_key]\n        if class_name_to_id is not None and true_class_name in class_name_to_id:\n            true_class_id = class_name_to_id[true_class_name]\n        else:\n            true_class_id = true_class_name\n        predicted_class_id = encoder_manager.predict_class(embedding)\n        accuracies.setdefault(true_class_id, {\"correct\": 0, \"total\": 0})\n        accuracies[true_class_id][\"total\"] += 1\n        if predicted_class_id == true_class_id:\n            accuracies[true_class_id][\"correct\"] += 1\n        total_count += 1\n        if predicted_class_id == true_class_id:\n            total_accuracy += 1\n    for class_id, accuracy in accuracies.items():\n        accuracy[\"accuracy\"] = accuracy[\"correct\"] / accuracy[\"total\"]\n        print(f\"Accuracy for class {class_id}: {accuracy['accuracy']}\")\n    print(f\"Average accuracy: {total_accuracy / total_count}\")\n\n", "CodeBleu_score": 30.63835232651766}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_path = output_path.replace(\".wav\", \".mp3\")\n    output_path = pathlib.Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not args.force:\n        return output_path, False\n    else:\n        vad_model, vad_utils = get_vad_model_and_utils(args.vad_model_path, args.vad_device)\n        audio, _ = vad_utils.load_audio(audio_path)\n        audio = vad_utils.preprocess(audio)\n        speech_detected = remove_silence(\n            audio,\n            vad_model,\n            vad_utils,\n            output_path,\n            args.vad_threshold,\n            args.vad_min_length,\n            args.vad_device,\n        )\n        return output_path, speech_detected\n\n", "CodeBleu_score": 46.55429260812062}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_path = output_path.replace(\".wav\", \".mp3\")\n    output_path = pathlib.Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.is_file() and not args.force:\n        return output_path, False\n    if not args.no_vad:\n        audio, sr = torchaudio.load(str(audio_path))\n        audio = remove_silence(audio, sr, vad_model, vad_utils, args.vad_threshold)\n        if audio.shape[1] == 0:\n            return output_path, False\n    torchaudio.save(str(output_path), audio, sr)\n    return output_path, True\n\n", "CodeBleu_score": 46.491971121332156}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    if os.path.isfile(output_path) and not args.force:\n        return output_path, False\n    pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    if args.vad_model_path is not None:\n        audio, sr = remove_silence(\n            audio_path,\n            vad_model_path=args.vad_model_path,\n            vad_utils_path=args.vad_utils_path,\n            vad_threshold=args.vad_threshold,\n            vad_frame_length=args.vad_frame_length,\n            vad_frame_shift=args.vad_frame_shift,\n            vad_padding=args.vad_padding,\n            vad_min_duration=args.vad_min_duration,\n            vad_max_duration=args.vad_max_duration,\n            vad_sample_rate=args.vad_sample_rate,\n            vad_device=args.vad_device,\n        )\n    else:\n        audio, sr = torchaudio.load(audio_path)\n    torchaudio.save(output_path, audio, sr)\n    return output_path, True\n\n", "CodeBleu_score": 47.03707238182715}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    file_list = glob.glob(os.path.join(output_dir, \\\"**/*\\\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        for _ in tqdm(p.imap_unordered(resample_file, zip(file_list, [output_sr] * len(file_list))), total=len(file_list)):\n            pass\n    print(\\\"Resampling complete.\\\")\n\n", "CodeBleu_score": 35.59296824395551}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n    output_dir = input_dir\n", "CodeBleu_score": 3.9716944085416968}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if not output_dir:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    filenames = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    func_args = [(filename, output_sr) for filename in filenames]\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, func_args), total=len(func_args)))\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 35.729547475347445}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   model.eval()\n    eval_loss = 0.0\n    eval_steps = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            # Reshape inputs and labels to group samples of each class\n            inputs = inputs.reshape(-1, inputs.size(-1))\n            labels = labels.reshape(-1)\n\n            # Dispatch data to the GPU if available\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Update evaluation loss and steps\n            eval_loss += loss.item()\n            eval_steps += 1\n\n            # Log evaluation statistics\n            if global_step % c.log_interval == 0:\n                print(\n                    f\" > Evaluation Step: {global_step} | Loss: {eval_loss / eval_steps:.4f}\"\n                )\n\n            # Generate and log UMAP plot of embeddings\n            if global_step % c.log_interval == 0:\n                plot_embeddings(\n                    model,\n                    inputs,\n                    labels,\n                    global_step,\n                    os.path.join(c.experiment_dir, \"eval\"),\n                )\n\n    return eval_loss / eval_steps\n\n", "CodeBleu_score": 34.36442945844948}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   model.eval()\n    total_loss = 0.0\n    num_batches = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            if i % 10 == 0:\n                print(f\"Eval Batch {i}/{len(data_loader)}\")\n            num_batches += 1\n            inputs, labels = batch\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            inputs = inputs.reshape(c.num_classes_in_batch, c.num_utter_per_class, -1)\n            labels = labels.reshape(c.num_classes_in_batch, c.num_utter_per_class)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            if i == 0 and global_step % c.log_eval_step == 0:\n                plot_embeddings(outputs, labels, global_step, c.eval_plot_path, c.num_classes_in_batch)\n    model.train()\n    avg_loss = total_loss / num_batches\n    print(f\"Eval Loss: {avg_loss}\")\n    return avg_loss\n\n", "CodeBleu_score": 36.20701932338819}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   model.eval()\n    avg_loss = 0.0\n    avg_loss_per_class = {}\n    avg_loss_per_class_count = {}\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            # reshape inputs and labels\n            inputs = inputs.reshape(-1, inputs.size(-1))\n            labels = labels.reshape(-1)\n            # dispatch data to GPU if available\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            # compute loss\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            # compute average loss\n            avg_loss += loss.item()\n            # compute average loss per class\n            for label in labels.unique():\n                if label not in avg_loss_per_class:\n                    avg_loss_per_class[label] = 0.0\n                    avg_loss_per_class_count[label] = 0\n                avg_loss_per_class[label] += loss[labels == label].sum().item()\n                avg_loss_per_class_count[label] += (labels == label).sum().item()\n            # generate UMAP plot for last batch\n            if i == len(data_loader) - 1:\n                plot_embeddings(outputs.cpu(), labels.cpu(), global_step, map_classid_to_classname, c.experiment_dir)\n    # compute average loss and log evaluation statistics\n    avg_loss /= len(data_loader)\n    for label in avg_loss_per_class:\n        avg_loss_per_class[label] /= avg_loss_per_class_count[label]\n    print(f\\\"Eval Loss: {avg_loss:.4f}\\\")\n    print(f\\\"Eval Loss per Class: {avg_loss_per_class}\\\")\n    return avg_loss\n\n", "CodeBleu_score": 35.01683931631145}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    try:\n        if model_name == \\\"xtts\\\":\n            config_class_name = f\\\"{model_name.capitalize()}Config\\\"\n            config_class = find_module(f\\\"TTS.config.{model_name}_configs\\\", config_class_name)\n        else:\n            config_class_name = f\\\"{model_name.capitalize()}Config\\\"\n            config_class = find_module(f\\\"TTS.config.{model_name}_configs\\\", config_class_name)\n        return config_class\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\\\"Configuration class {config_class_name} not found.\\\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.57817043767987}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"-\", \"_\") + \"_config\"\n    try:\n        if model_name.startswith(\"xtts\"):\n            config_class = find_module(f\"TTS.config.xtts_config.{config_class_name}\")\n        else:\n            config_class = find_module(f\"TTS.config.{config_class_name}\")\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"Could not find config class {config_class_name}\")\n    return config_class\n\n", "CodeBleu_score": 26.228429614772264}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = f\"{model_name.replace('-', '_').replace(' ', '_').title()}Config\"\n    if model_name == \"xtts\":\n        try:\n            from TTS.config.xtts_configs import XttsConfig\n\n            return XttsConfig\n        except ModuleNotFoundError:\n            pass\n    for path in [\n        \"TTS.config.configs\",\n        \"TTS.config.configs.fastspeech2\",\n        \"TTS.config.configs.tacotron2\",\n        \"TTS.config.configs.transformer\",\n        \"TTS.config.configs.fastpitch\",\n    ]:\n        try:\n            module = find_module(path, config_class_name)\n            return module\n        except ModuleNotFoundError:\n            pass\n    raise ModuleNotFoundError(f\"No config class found for model {model_name}\")\n\n", "CodeBleu_score": 36.89304194368458}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown config type: {config_path}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 47.82251777220982}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\".yml\") or config_path.endswith(\".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise ValueError(f\" [!] Unknown config file type: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 48.198478279861746}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = os.path.expanduser(config_path)\n    config_path = os.path.abspath(config_path)\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\\\" [!] {config_path} not found.\\\")\n\n    # read config file\n    config_dict = {}\n    _, ext = os.path.splitext(config_path)\n    if ext == \\\".yml\\\" or ext == \\\".yaml\\\":\n        with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif ext == \\\".json\\\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\" [!] Unknown config extension {ext}.\\\")\n\n    # get model name\n    model_name = _process_model_name(config_dict)\n\n    # register config\n    config_class = register_config(model_name)\n\n    # create config\n    config = config_class()\n    config.update_from_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 47.95172298730891}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and contains not in filename:\n                continue\n            if validExts is None or filename.endswith(validExts):\n                # construct the full path to the file and yield it\n                yield os.path.join(rootDir, filename)\n\n", "CodeBleu_score": 50.09195512367267}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and is not in the filename, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an audio file\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the file and yield it\n                path = os.path.join(rootDir, filename)\n                yield path\n\n", "CodeBleu_score": 78.21007681717917}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and is not in the filename, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an audio file\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the file and yield it\n                path = os.path.join(rootDir, filename)\n                yield path\n\n", "CodeBleu_score": 78.21007681717917}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide paths to the XTTS checkpoint, config, and vocab.\"\n    config = XttsConfig.load_config(xtts_config)\n    model = Xtts(config)\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded.\")\n    return \"XTTS model loaded.\"\n\n", "CodeBleu_score": 46.47354377529527}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the paths to the model checkpoint, config, and vocab.\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    if torch.cuda.is_available():\n        model = model.cuda()\n    model.eval()\n    vocab = torch.load(xtts_vocab)\n    print(\"XTTS model loaded successfully!\")\n    return model, vocab\n\n", "CodeBleu_score": 47.019419029456955}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide paths for checkpoint, config, and vocab.\"\n    global XTTS_MODEL\n    config = XttsConfig.from_json(xtts_config)\n    XTTS_MODEL = Xtts(config)\n    XTTS_MODEL.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    XTTS_MODEL.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n    print(\"Model loaded successfully!\")\n    return \"Model loaded successfully!\"\n\n", "CodeBleu_score": 52.1855683208481}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_stft:\n        hop_length = self.audio_config[\\\"hop_length\\\"]\n        num_frames = (num_frames * hop_length) // self.audio_config[\\\"win_length\\\"]\n\n    assert x.dim() == 2\n    assert x.size(1) == self.audio_config[\\\"sample_rate\\\"]\n\n    if x.size(0) < num_frames:\n        num_frames = x.size(0)\n\n    num_offsets = num_frames - num_eval + 1\n    offsets = torch.arange(0, num_offsets, dtype=torch.long)\n    if self.use_torch_stft:\n        offsets = offsets * hop_length\n\n    x_frames = torch.stack([x[offset : offset + num_frames] for offset in offsets])\n    x_frames = self.get_torch_mel_spectrogram_class(self.audio_config)(x_frames)\n\n    embeddings = self.inference(x_frames, l2_norm)\n\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n\n    return embeddings\n\n", "CodeBleu_score": 39.65460484109528}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.hop_length is not None and self.hop_length!= 1:\n        num_frames = int(np.ceil(num_frames * self.hop_length))\n\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n\n    num_offset = num_frames // num_eval\n    offset = np.arange(0, num_frames, num_offset)\n    offset[-1] = num_frames - num_offset\n\n    x = x[:, offset]\n    x = x.reshape(x.shape[0] * x.shape[1], x.shape[2])\n    x = self.inference(x, l2_norm)\n\n    if return_mean:\n        x = x.reshape(x.shape[0] // num_eval, num_eval, -1)\n        x = torch.mean(x, dim=1)\n\n    return x\n\n", "CodeBleu_score": 36.37828795879982}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_stft:\n        num_frames = num_frames * self.audio_config[\"hop_length\"]\n    num_frames = min(num_frames, x.shape[1])\n    num_eval = min(num_eval, num_frames)\n    offsets = np.linspace(0, num_frames - num_eval, num_eval).astype(np.int32)\n    embeddings = []\n    for offset in offsets:\n        frames = x[:, offset : offset + num_eval]\n        frames = frames.unsqueeze(0)\n        emb = self.inference(frames, l2_norm)\n        embeddings.append(emb)\n    embeddings = torch.cat(embeddings, dim=0)\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n    return embeddings\n\n", "CodeBleu_score": 49.89126870516608}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 87.8906400059689}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 87.8906400059689}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 87.8906400059689}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n    # If the number of classes is greater than 10, limit the number of classes to 10\n    if num_classes_in_batch.shape[1] > 10:\n        # Calculate the indices of the top 10 classes with the most utterances\n        top_10_classes = np.argsort(num_utterances_per_class)[-10:]\n        # Create a new array with the top 10 classes\n        num_classes_in_batch = num_classes_in_batch[:, top_10_classes]\n        # Create a new array with the embeddings of the top 10 classes\n        embeddings = embeddings[np.where(np.isin(num_classes_in_batch, 1))]\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n    # Calculate the number of classes\n    num_classes = num_classes_in_batch.shape[1]\n    # Create a new array with the embeddings of the top 10 classes\n    embeddings = embeddings[np.where(np.isin(num_classes_in_batch, 1))]\n    # Create a new array with the labels of the top 10 classes\n    labels = np.argmax(num_classes_in_batch, axis=1)\n    # Create a new array with the colors for the top 10 classes\n    colors = colormap[labels]\n    # Use UMAP to reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(random_state=42)\n    embeddings_reduced = reducer.fit_transform(embeddings)\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(\n        embeddings_reduced[:, 0],\n        embeddings_reduced[:, 1],\n        c=colors,\n        cmap=matplotlib.colors.ListedColormap(colormap),\n    )\n    # Add a legend with the class names\n    legend_labels = [\n        f\"Class {i}: {num_utterances_per_class[i]} utterances\"\n        for i in", "CodeBleu_score": 25.910676951290117}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(num_classes_in_batch)\n    # If there are more than 10 classes, limit the number of classes to 10\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n    # Calculate the number of classes\n    num_classes = len(num_utterances_per_class)\n    # Calculate the number of utterances\n    num_utterances = np.sum(num_utterances_per_class)\n    # Initialize the figure\n    fig = plt.figure(figsize=(10, 10))\n    # Initialize UMAP with n_neighbors=10, min_dist=0.1, and metric=\"cosine\"\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n    # Transform the embeddings using UMAP\n    embeddings_reduced = reducer.fit_transform(embeddings)\n    # Create a scatter plot of the reduced embeddings\n    plt.scatter(\n        embeddings_reduced[:, 0],\n        embeddings_reduced[:, 1],\n        c=num_classes_in_batch,\n        cmap=plt.cm.get_cmap(\"tab10\", num_classes),\n    )\n    # Add a colorbar to the plot\n    plt.colorbar(ticks=range(num_classes))\n    # Set the title of the plot\n    plt.title(\"UMAP projection\")\n    # Set the aspect ratio of the plot to be equal\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    # Save the plot as a figure\n    fig.savefig(\"umap.png\")\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 39.92732300790084}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_classes = len(np.unique(embeddings.labels))\n    # Limit the number of classes to 10 if necessary\n    if num_classes > 10:\n        num_classes = 10\n    # Calculate the number of utterances per class\n    num_per_class = np.bincount(embeddings.labels)\n    # Create a UMAP object\n    reducer = umap.UMAP(n_neighbors=10, n_components=2, metric=\"cosine\")\n    # Transform the embeddings\n    embedding = reducer.fit_transform(embeddings.embeddings)\n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(\n        embedding[:, 0],\n        embedding[:, 1],\n        c=colormap[embeddings.labels % num_classes],\n        s=num_per_class[embeddings.labels] * 10,\n    )\n    # Set the title and adjust for equal aspect ratio\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n    # Save the plot as \"umap\"\n    fig.savefig(\"umap.png\")\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 37.55396323482575}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_mat = []\n    for spkr in range(dvecs.size(0)):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, 0)\n        for utt in range(dvecs.size(1)):\n            sim = F.cosine_similarity(dvecs[spkr, utt], new_centroids, dim=1)\n            sim = torch.clamp(sim, min=1e-6)\n            sim_mat.append(sim)\n            new_centroids = self.calc_new_centroids(dvecs, new_centroids, spkr, utt + 1)\n    return torch.stack(sim_mat, dim=1)\n\n", "CodeBleu_score": 32.25399353221275}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_mat = []\n    for spkr in range(dvecs.size(0)):\n        for utt in range(dvecs.size(1)):\n            centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = F.cosine_similarity(dvecs[spkr, utt], centroids, dim=1)\n            sim = torch.clamp(sim, min=1e-06)\n            sim_mat.append(sim)\n    return torch.stack(sim_mat)\n\n", "CodeBleu_score": 28.013982901343336}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = F.cosine_similarity(dvecs[spkr, utt], centroids)\n            sim = torch.clamp(sim, min=1e-6)\n            cos_sim.append(sim)\n    return torch.stack(cos_sim)\n\n", "CodeBleu_score": 27.797892206556725}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # pylint: disable=E1102\n    losses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            # get the cosine similarities for the speaker\n            cs_row = cos_sim_matrix[spkr_idx, utt_idx]\n            # calculate the loss for the utterance\n            loss = -torch.log(\n                torch.exp(self.w * cs_row[spkr_idx])\n                / torch.sum(torch.exp(self.w * cs_row + self.b))\n            )\n            losses.append(loss)\n    return torch.stack(losses)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n        \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 28.544982363809346}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        for utt_idx, utterance in enumerate(speaker):\n            # compute the loss for the positive utterance\n            pos_loss = torch.nn.functional.log_softmax(\n                cos_sim_matrix[spkr_idx, utt_idx, :], dim=0\n            )[spkr_idx]\n            # compute the loss for the negative utterances\n            neg_loss = torch.mean(\n                torch.nn.functional.log_softmax(\n                    cos_sim_matrix[spkr_idx, utt_idx, :], dim=0\n                )[:spkr_idx]\n                + torch.nn.functional.log_softmax(\n                    cos_sim_matrix[spkr_idx, utt_idx, :], dim=0\n                )[spkr_idx + 1 :]\n            )\n            loss.append(pos_loss - neg_loss)\n    return torch.stack(loss)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n        \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 25.422700934612198}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for spkr_idx, speaker in enumerate(cos_sim_matrix):\n        # cos_sim_matrix is (N,M,N)\n        # speaker is (M,N)\n        # speaker[utt_idx] is (N)\n        for utt_idx, utterance in enumerate(speaker):\n            # utterance is (N)\n            loss = F.log_softmax(-utterance, dim=0)\n            loss = loss[spkr_idx]\n            losses.append(loss)\n    return torch.stack(losses)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n        \\\"\\\"\\\"\n        The function calculates the embedding loss using contrastive loss.\n        It iterates over the embeddings, computing the contrastive loss for each embedding.\n        The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n        \\\"\\\"\\\"\n        losses = []\n        for spkr_idx, speaker in enumerate(cos_sim_matrix):\n            # cos_sim_matrix is (N,M,N)\n            # speaker is (M,N)\n            # speaker[utt_idx] is (N)\n            for utt_idx, utterance in enumerate(speaker):\n                # utterance is (N)\n                # utterance[spkr_idx] is the cosine similarity of the reference utterance\n                loss = torch.log(1 + torch.sum(torch.exp(-utterance + utterance[spkr_idx])))\n                losses.append(loss)\n        return torch.stack(losses)\n\n    def forward(self, dvecs):\n        \\\"\\\"\\\"\n        The function calculates the Generalized End-to-End loss.\n        It first calculates the cosine similarity matrix for the input embeddings.\n        It then computes the embedding loss using the specified loss method.\n        The function returns the mean of the embedding losses.\n        \\\"\\\"\\\"\n        centroids = torch.mean(dvecs, 1)\n        cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n        embed_loss = self.embed_", "CodeBleu_score": 27.39718185324378}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n            # vector based cosine similarity for speed\n            cs_row.append(\n                torch.clamp(\n                    torch.mm(\n                        utterance.unsqueeze(1).transpose(0, 1),\n                        new_centroids.transpose(0, 1),\n                    )\n                    / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)),\n                    1e-6,\n                )\n            )\n            L_row = torch.cat(L_row, dim=0)\n            cos_sim_matrix.append(cs_row)\n        return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 36.98334691758873}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids = cos_sim_matrix[j, i]\n            centroids = torch.cat((centroids[:j], centroids[j + 1 :]))\n            L_row.append(\n                -torch.log(torch.sigmoid(centroids[torch.argmax(centroids)] - centroids))\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 57.38043153407245}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids = cos_sim_matrix[j, i]\n            centroids = torch.cat((centroids[:j], centroids[j + 1 :]))\n            L_row.append(F.binary_cross_entropy_with_logits(centroids, torch.ones_like(centroids)))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 55.214946502002896}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if x.shape[1] < 2:\n        raise RuntimeError(\"GE2ELoss: There must be at least 2 utterances per speaker\")\n    centroids = torch.mean(x, 1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    cos_sim_matrix = (cos_sim_matrix * self.w) + self.b\n    embed_loss = self.embed_loss(x, cos_sim_matrix)\n    return torch.mean(embed_loss)\n\n", "CodeBleu_score": 60.94069833825153}, {"code": "def forward(self, x, _label=None)\n\t\n    if _label is not None:\n        x = x.contiguous().view(-1, x.size(-1))\n        _label = _label.contiguous().view(-1)\n        x = x[_label.nonzero().squeeze(1)]\n    x = x.contiguous().view(self.num_spks, self.num_utts, -1)\n    assert x.size(0) >= 2, \"At least 2 speakers required for GE2ELoss\"\n    assert x.size(1) >= 2, \"At least 2 utterances per speaker required for GE2ELoss\"\n    centroids = torch.mean(x, 1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift\n    cos_sim_matrix = self.w * (cos_sim_matrix - self.b)\n    # compute the embedding loss\n    L = self.embed_loss(x, cos_sim_matrix)\n    # average the loss across all the embeddings\n    return torch.mean(L)\n\n", "CodeBleu_score": 55.588778547766495}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.shape[1] < 2:\n        raise ValueError(\"The number of utterances per speaker must be at least two.\")\n    centroids = torch.mean(x, dim=1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    L = self.embed_loss(x, cos_sim_matrix)\n    L = L * (self.w + self.b)\n    return torch.mean(L)\n\n", "CodeBleu_score": 49.9150029574287}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for root, _, files in os.walk(extra_voice_dir):\n            if root == extra_voice_dir:\n                continue\n            if len(files) == 0:\n                continue\n            if len(glob(os.path.join(root, \"*.npz\"))) == 0:\n                if len(glob(os.path.join(root, \"*.wav\"))) == 0:\n                    if len(glob(os.path.join(root, \"*.mp3\"))) == 0:\n                        continue\n            name = os.path.basename(root)\n            voices[name] = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[name].append(os.path.join(root, file))\n                elif file.endswith(\".wav\"):\n                    voices[name].append(os.path.join(root, file))\n                elif file.endswith(\".mp3\"):\n                    voices[name].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 24.8882070265242}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for dirpath, _, filenames in os.walk(extra_voice_dir):\n            if filenames:\n                npz_files = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".npz\")]\n                if not npz_files:\n                    wav_files = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".wav\")]\n                    mp3_files = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".mp3\")]\n                    voices[os.path.basename(dirpath)] = wav_files + mp3_files\n                else:\n                    voices[os.path.basename(dirpath)] = npz_files\n    return voices\n\n", "CodeBleu_score": 28.914507366633984}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = defaultdict(list)\n    for voice_dir in [VOICE_DIR] + extra_voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            npz_files = glob(os.path.join(root, \\\"*.npz\\\"))\n            if not npz_files:\n                wav_files = glob(os.path.join(root, \\\"*.wav\\\"))\n                mp3_files = glob(os.path.join(root, \\\"*.mp3\\\"))\n                files = wav_files + mp3_files\n            for f in files:\n                voices[os.path.basename(root)].append(f)\n    return voices\n\n\n\n\n", "CodeBleu_score": 25.555073066042144}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   voices = get_voices(extra_voice_dirs)\n    if voice == \\\"random\\\":\n        return None, None, None\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice}.\\\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\\\"Voice path {voice_path} does not exist.\\\")\n    if voice_path.endswith(\\\".npz\\\"):\n        return load_npz(voice_path)\n    else:\n        # generate npz file\n        npz_file = voice_path.replace(\\\".wav\\\", \\\".npz\\\").replace(\\\".mp3\\\", \\\".npz\\\")\n        if not os.path.exists(npz_file):\n            # generate npz file\n            audio, _ = librosa.load(voice_path, sr=model.sample_rate)\n            audio = torch.from_numpy(audio).unsqueeze(0)\n            with inference_mode(model):\n                with torch.no_grad():\n                    with clear_cuda_cache():\n                        semantic, coarse, fine = model.encode(audio)\n            np.savez(npz_file, semantic_prompt=semantic.numpy(), coarse_prompt=coarse.numpy(), fine_prompt=fine.numpy())\n        return load_npz(npz_file)\n\n", "CodeBleu_score": 33.70691076458726}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   if voice == \"random\":\n        return None, None, None, None, None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found in {voices.keys()}\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voice paths found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice path {voice_path} not found\")\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    x_history = convert_audio(voice_path, model.sampling_rate)\n    semantic, coarse, fine = model.get_prompt(x_history)\n    np.savez(voice_path, semantic_prompt=semantic, coarse_prompt=coarse, fine_prompt=fine)\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 51.91209709979867}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   if voice == \"random\":\n        return None, None, None, None, None, None, None, None, None, None, None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} not found in {voices.keys()}\")\n    if len(voices[voice]) > 1:\n        raise ValueError(f\"Multiple voices found for {voice} in {voices[voice]}\")\n    voice_file = voices[voice][0]\n    if voice_file.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_file)\n    else:\n        # generate npz file\n        npz_file = voice_file.replace(\".wav\", \".npz\").replace(\".mp3\", \".npz\")\n        if not os.path.exists(npz_file):\n            convert_audio(voice_file, npz_file)\n        semantic, coarse, fine = load_npz(npz_file)\n    # load voice\n    voice_data = np.load(voice_file)\n    return voice_data, semantic, coarse, fine\n\n\n\n\n", "CodeBleu_score": 44.44232725246378}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length, _ = pos_score.shape\n    zero_tensor = pos_score.new_zeros((batch_size, num_heads, length, 1))\n    pos_score = torch.cat((zero_tensor, pos_score), dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, length + 1, length)\n    pos_score = pos_score[:, :, 1:, :]\n    return pos_score\n\n", "CodeBleu_score": 29.628189739149168}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length, dim = pos_score.shape\n    zeros = pos_score.new_zeros((batch_size, num_heads, seq_length, 1))\n    pos_score = torch.cat((zeros, pos_score), dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, seq_length + 1, seq_length)\n    pos_score = pos_score[:, :, 1:, :]\n    return pos_score\n\n", "CodeBleu_score": 30.628526316819343}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length, _ = pos_score.shape\n    zero_tensor = pos_score.new_zeros((batch_size, num_heads, 1, seq_length))\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, seq_length + 1, seq_length)\n    pos_score = pos_score[:, :, :seq_length, 1:].reshape_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 34.65603225921356}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t   if self.use_linear:\n        context = self.dense(context)\n\n    if self.lstm_type!= \\\"\\\":\n        context = context.transpose(1, 2)\n        context = self.bilstm(context)[0]\n        context = context.transpose(1, 2)\n\n    context = self.dropout(context)\n\n    context_list = []\n    for i in range(context.size(0)):\n        context_list.append(context[i, : lens[i]])\n\n    context = torch.nn.utils.rnn.pad_sequence(context_list, batch_first=True)\n\n    return context\n\n", "CodeBleu_score": 38.395154510926275}, {"code": "def run_padded_sequence(self, context, lens)\n\t   batch_size = context.size(0)\n    max_len = context.size(1)\n    context_lens = lens.long()\n    context_padded = torch.zeros(batch_size, max_len, self.out_dim).to(context.device)\n    for i in range(batch_size):\n        context_padded[i, : context_lens[i], :] = self.forward(context[i, : context_lens[i], :])\n    return context_padded\n\n", "CodeBleu_score": 22.14652505511579}, {"code": "def run_padded_sequence(self, context, lens)\n\t   # pylint: disable=too-many-locals, too-many-branches, too-many-statements\n    # get the max length of the context", "CodeBleu_score": 1.459942084942085}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   sorted_lens, indices = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, indices)\n    sorted_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context = fn(sorted_context)\n    sorted_context = nn.utils.rnn.pad_packed_sequence(sorted_context, batch_first=True)[0]\n    _, unsorted_indices = torch.sort(indices, descending=False)\n    sorted_context = sorted_context.index_select(0, unsorted_indices)\n    return sorted_context\n\n", "CodeBleu_score": 19.852987823770455}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   # sort by length\n    lens, indices = lens.sort(0, descending=True)\n    context = context[indices]\n    context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n    context = fn(context)\n    context = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 22.286036192021253}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   sorted_lens, sorted_idx = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_idx]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    packed_context = fn(packed_context)\n    unpacked_context, _ = nn.utils.rnn.pad_packed_sequence(packed_context, batch_first=True)\n    context = unpacked_context[torch.argsort(sorted_idx)]\n    return context\n\n", "CodeBleu_score": 20.24011145000198}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre[1].remove_weight_norm()\n    for block in self.conv_blocks:\n        block[1].remove_weight_norm()\n\n", "CodeBleu_score": 34.287881114546934}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    remove_weight_norm_from_layer(self.convt_pre)\n    for block in self.conv_blocks:\n        remove_weight_norm_from_layer(block)\n\n", "CodeBleu_score": 34.13688208856986}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for layer in self.convt_pre.modules():\n        if isinstance(layer, torch.nn.utils.parametrizations.weight_norm):\n            torch.nn.utils.remove_weight_norm(layer)\n    for conv_block in self.conv_blocks:\n        for layer in conv_block.modules():\n            if isinstance(layer, torch.nn.utils.parametrizations.weight_norm):\n                torch.nn.utils.remove_weight_norm(layer)\n\n", "CodeBleu_score": 41.57331397737566}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        torch_name = 'gluon_se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        torch_name = 'gluon_senet_' + mx_name[6:]\n    elif mx_name.startswith('inceptionv3'):\n        torch_name = 'gluon_inceptionv3'\n    else:\n        torch_name = 'gluon_' + mx_name\n    return torch_name\n\n", "CodeBleu_score": 44.4669585171149}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'gluon_se_')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'gluon_se_')\n    elif mx_name == 'inceptionv3':\n        mx_name = 'gluon_inceptionv3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 41.92585370000689}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'gluon_' + mx_name.replace('se_','se')\n    elif mx_name.startswith('senet_'):\n        mx_name = 'gluon_' + mx_name.replace('senet_','se')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = 'gluon_inceptionv3'\n    else:\n        mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 44.018070792598486}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t  num_samples_per_worker = int(math.ceil(self.num_samples * self.repeats / self.global_num_workers))\n    if self.is_training and self.dist_num_replicas > 1:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.dist_num_replicas))\n    if self.is_training and self.batch_size > 1:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size)) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 42.40171606234155}, {"code": "def _num_samples_per_worker(self)\n\t  # If training or multiple replicas are used, round up the number of samples.\n    if self.is_training or self.dist_num_replicas > 1:\n        num_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    else:\n        num_samples_per_worker = self.num_samples // self.global_num_workers\n\n    # If a batch size is specified during training, adjust the number of samples to be a multiple of the batch size.\n    if self.is_training and self.batch_size > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 52.219322958028194}, {"code": "def _num_samples_per_worker(self)\n\t  if self.is_training and self.repeats > 1:\n        # We need to round up the number of samples to ensure that each worker sees the same number of samples per epoch\n        # This is necessary when using multiple repeats and training, since the dataset is shuffled at the beginning of each repeat\n        # If we don't round up, some workers might see fewer samples per epoch than others, which can cause issues with the learning rate scheduler\n        num_samples_per_worker = math.ceil(self.num_samples * self.repeats / self.global_num_workers)\n    else:\n        num_samples_per_worker = self.num_samples / self.global_num_workers\n\n    if self.is_training and self.batch_size > 1:\n        # We need to adjust the number of samples to be a multiple of the batch size\n        # This is necessary when using a batch size greater than 1 during training\n        # If we don't adjust, some batches might be smaller than the batch size, which can cause issues with the model\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 49.411013974444}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    paths = []\n    targets = []\n    for root, _, files in os.walk(folder):\n        for fname in files:\n            is_valid = False\n            if any(fname.endswith(ext) for ext in types):\n                is_valid = True\n            if not is_valid:\n                continue\n            path = os.path.join(root, fname)\n            if leaf_name_only:\n                cls = os.path.basename(root)\n            else:\n                cls = root\n            if cls not in class_to_idx:\n                class_to_idx[cls] = len(class_to_idx)\n            paths.append(path)\n            targets.append(class_to_idx[cls])\n    if sort:\n        paths, targets = zip(*sorted(zip(paths, targets), key=lambda p: natural_key(p[0])))\n    return paths, targets, class_to_idx\n\n", "CodeBleu_score": 37.62694126799471}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    if leaf_name_only:\n        folder_names = set(\n            [os.path.basename(os.path.normpath(folder))]\n        )\n    else:\n        folder_names = set(\n            [os.path.basename(os.path.normpath(d)) for d in folder]\n        )\n    if sort:\n        folder_names = sorted(folder_names, key=natural_key)\n    for folder_name in folder_names:\n        folder_path = os.path.join(folder, folder_name)\n        if os.path.isdir(folder_path):\n            images, targets = find_images_and_targets(\n                folder_path,\n                types=types,\n                class_to_idx=class_to_idx,\n                leaf_name_only=leaf_name_only,\n                sort=sort\n            )\n            if len(images) > 0:\n                if folder_name not in class_to_idx:\n                    class_to_idx[folder_name] = len(class_to_idx)\n                for image, target in zip(images, targets):\n                    yield image, class_to_idx[folder_name]\n        else:\n            if os.path.splitext(folder_name)[1].lower() in types:\n                if folder_name not in class_to_idx:\n                    class_to_idx[folder_name] = len(class_to_idx)\n                yield folder_path, class_to_idx[folder_name]\n\n", "CodeBleu_score": 33.32569405209023}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    if leaf_name_only:\n        folder = os.path.basename(folder)\n    for root, _, files in os.walk(folder):\n        for f in files:\n            if not f.lower().endswith(types):\n                continue\n            if leaf_name_only:\n                label = os.path.basename(root)\n            else:\n                label = os.path.relpath(root, folder)\n            if label not in class_to_idx:\n                class_to_idx[label] = len(class_to_idx)\n            yield os.path.join(root, f), class_to_idx[label]\n    if sort:\n        return sorted(list(find_images_and_targets(folder, types, class_to_idx, leaf_name_only, False)), key=lambda x: natural_key(x[0]))\n    else:\n        return list(find_images_and_targets(folder, types, class_to_idx, leaf_name_only, False))\n\n", "CodeBleu_score": 36.19668245129934}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is None:\n        return random.randrange(2**32)\n    else:\n        return worker_info.seed % 2**32\n\n", "CodeBleu_score": 39.14194176739405}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = dist.get_rank()\n    return seed\n\n", "CodeBleu_score": 54.72990307095612}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = os.environ.get('WDS_RANK', 0)\n    return seed\n\n", "CodeBleu_score": 51.86718186951163}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        return ImageOps.solarize(img, thresh).point(lambda p: p + add)\n    return img\n\n", "CodeBleu_score": 14.640262306537554}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.convert(\"RGB\", lut=lut)\n    return img\n\n", "CodeBleu_score": 37.93984863751023}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.point(lut)\n    if img.mode == \"RGB\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.point(lut + lut + lut)\n    return img\n\n", "CodeBleu_score": 39.30229723213229}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = []\n    transforms_probs = []\n    for transform_name, transform_prob in transforms.items():\n        transforms_names.append(transform_name)\n        transforms_probs.append(transform_prob)\n    transforms_probs = np.array(transforms_probs)\n    transforms_probs = transforms_probs / transforms_probs.sum()\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 32.59781141016644}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / transform_probs.sum()\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 18.384340765051128}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.698957173702425}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = np.zeros_like(img)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed += w * img_aug\n    img_mixed = np.clip(img_mixed, 0, 255).astype(np.uint8)\n    img_mixed = Image.fromarray(img_mixed)\n    img = Image.blend(img, img_mixed, m)\n    return img\n\n", "CodeBleu_score": 55.98898282654141}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_aug = np.zeros_like(img_orig)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        for op in ops:\n            img_aug = op(img_aug)\n        img_aug = np.clip(img_orig + img_aug * w, 0, 255).astype(np.uint8)\n    return Image.blend(img_orig, Image.fromarray(img_aug), m)\n\n", "CodeBleu_score": 47.86780444846394}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mix = np.zeros_like(img)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mix += w * img_aug\n    img_mix = np.clip(img_mix, 0, 255).astype(np.uint8)\n    img = Image.blend(Image.fromarray(img_mix), img_orig, m)\n    return img\n\n", "CodeBleu_score": 53.912484019195986}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tf not isinstance(x, (tuple, list)):\n        x = tuple(x for _ in range(n))\n    elif len(x)!= n:\n        if len(x) == 1:\n            x = tuple(x[0] for _ in range(n))\n        else:\n            _logger.warning(f\"Length of x {len(x)} does not match n {n}, repeating mean instead\")\n            x = tuple(sum(x) / len(x) for _ in range(n))\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 41.27990062898871}, {"code": "def adapt_to_chs(x, n)\n\tf not isinstance(x, (tuple, list)):\n        return tuple(repeat(x, n))\n    if len(x)!= n:\n        _logger.warning(f\"Length of x ({len(x)}) does not match n ({n}), repeating mean of x\")\n        x = tuple(repeat(torch.mean(torch.stack(x)), n))\n    assert len(x) == n, f\"Length of x ({len(x)}) does not match n ({n})\"\n    return x\n\n", "CodeBleu_score": 44.384597426696516}, {"code": "def adapt_to_chs(x, n)\n\tf not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x)!= n:\n        _logger.warning(f\\\"Expected x to have length {n}, but got length {len(x)}. Replacing with {n} repetitions of mean of x\\\")\n        x = tuple(repeat(sum(x) / len(x), n))\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 49.06835450054214}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\trker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    assert worker_info.id == worker_id\n    seed = worker_info.seed\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError('worker_seeding must be a callable or \"all\"')\n\n", "CodeBleu_score": 43.74199852209811}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\trker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n    elif callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n    else:\n        raise ValueError(f'worker_seeding must be a callable or \"all\", got {worker_seeding}')\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n", "CodeBleu_score": 43.578526152523736}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\trker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"This dataset was designed to be used with torch.utils.data.DataLoader, and should not be used by another DataLoader.\")\n    worker_id = worker_info.id\n    if worker_id >= worker_info.num_workers:\n        raise RuntimeError(f\"worker_id {worker_id} should be in range 0 <= worker_id < num_workers {worker_info.num_workers}\")\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_id)\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed)\n        else:\n            raise ValueError(f\"worker_seeding must be a callable or 'all', got {worker_seeding}\")\n\n", "CodeBleu_score": 46.59205994759332}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    original_shape = tf.shape(image)\n    height, width = original_shape[0], original_shape[1]\n    image_size_const = tf.constant(image_size, dtype=tf.int32)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop_image(image_bytes, image_size),\n        lambda: _decode_and_random_crop(image_bytes, image_size, resize_method),\n    )\n    return image\n\n", "CodeBleu_score": 18.19892914382721}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    image_depth = shape[2]\n    crop_size = tf.cast(tf.minimum(image_height, image_width), dtype=tf.int32)\n    offset_height = ((image_height - crop_size) + 1) // 2\n    offset_width = ((image_width - crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, crop_size, crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 46.20099841398527}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    image_depth = shape[2]\n    image_channels = 3\n\n    padded_center_crop_size = tf.cast(image_size / (image_size + CROP_PADDING), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=image_channels)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n", "CodeBleu_score": 67.4292342201018}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 39.31474823660988}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 39.31474823660988}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 39.31474823660988}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   # TODO(huxu) support multiple eval dataloaders\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\n            \\\"Please specify an eval_dataloader or set one in the task config.\\\")\n    self.evaluator.eval_dataloader = eval_dataloader\n    self.evaluator.predictor.model = model\n    self.evaluator.predictor.output_file = output_file\n    self.evaluator.predictor.pred_dir = self.get_pred_dir(model.get_checkpoint_dir())\n    return self.evaluator()\n\n", "CodeBleu_score": 44.59783097424627}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\\\"no eval_dataloader found.\\\")\n    if output_file is None:\n        output_file = \\\"merged\\\"\n    output_file = os.path.join(self.pred_dir, output_file)\n    if os.path.exists(output_file + \\\".npy\\\"):\n        return\n    with torch.no_grad():\n        model.eval()\n        for i, batch in enumerate(eval_dataloader):\n            if isinstance(batch, dict):\n                batch = batch[\\\"input\\\"]\n            batch = batch.to(self.device)\n            outputs = model(batch)\n            np.save(output_file + \\\"_\\\" + str(i) + \\\".npy\\\", outputs.cpu().numpy())\n    outputs = np.concatenate(\n        [np.load(output_file + \\\"_\\\" + str(i) + \\\".npy\\\")\n            for i in range(len(eval_dataloader))])\n    np.save(output_file + \\\".npy\\\", outputs)\n\n\n\n", "CodeBleu_score": 43.35755996691485}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    self.evaluator.pred_dir = os.path.join(self.output_dir, \"predictions\")\n    if not os.path.exists(self.evaluator.pred_dir):\n        os.makedirs(self.evaluator.pred_dir)\n    if not os.path.exists(self.output_dir):\n        os.makedirs(self.output_dir)\n    if not os.path.exists(self.evaluator.pred_dir):\n        os.makedirs(self.evaluator.pred_dir)\n    self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file)\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \".npy\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged.npy\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged.txt\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged.json\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged.pt\")\n    if not os.path.exists(self.evaluator.pred_file):\n        self.evaluator.pred_file = os.path.join(self.evaluator.pred_dir, output_file + \"_merged.pkl\")", "CodeBleu_score": 40.294016533278736}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"Rank@1: {:.4f} - Rank@5: {:.4f} - mIoU: {:.4f}\".format(rank1, rank5, miou))\n", "CodeBleu_score": 38.379869422977805}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\n        \"Rank@1: {:.4f} - Rank@5: {:.4f} - mIoU: {:.4f}\".format(\n            metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"miou\"]\n        )\n    )\n\n", "CodeBleu_score": 28.956922137512915}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"Rank1: {:.4f} - Rank5: {:.4f} - mIoU: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )\n\n\n\n    def _eval_predictions(self, predictions, targets):\n        \"\"\"This function evaluates the predictions and targets using the provided metrics.\n\n        Args:\n            predictions (list): A list of predicted values.\n            targets (list): A list of target values.\n\n        Returns:\n            tuple: A tuple containing the rank1, rank5, and miou values.\n        \"\"\"\n        # Compute the rank1, rank5, and miou values\n        rank1, rank5, miou = 0, 0, 0\n        for i in range(len(predictions)):\n            pred_idx = np.argsort(predictions[i])[::-1]\n            target_idx = np.where(targets[i] == 1)[0]\n            rank1 += 1 if pred_idx[0] in target_idx else 0\n            rank5 += 1 if len(np.intersect1d(pred_idx[:5], target_idx)) > 0 else 0\n            miou += np.max(np.minimum(predictions[i], targets[i]))\n        rank1 /= len(predictions)\n        rank5 /= len(predictions)\n        miou /= len(predictions)\n        return rank1, rank5, miou\n\n", "CodeBleu_score": 40.997572332060486}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for pred, gt in zip(segments, data):\n        ious.append(self._iou(pred, gt))\n        ranks.append(self._rank(pred, gt))\n    rank1 = sum(1 for r in ranks if r == 1)/len(ranks)\n    rank5 = sum(1 for r in ranks if r <= 5)/len(ranks)\n    miou = sum(ious)/len(ious)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 23.825984554396356}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Compute IoU and rank for each segment-data pair\n    iou_scores = []\n    ranks = []\n    for segment, data_item in zip(segments, data):\n        # Calculate IoU for each ground truth segment\n        ious = [self._iou(segment, gt_segment) for gt_segment in data_item['segments']]\n        # Calculate rank for each ground truth segment\n        ranks.append(self._rank(ious, data_item['segments']))\n        # Calculate IoU for the best prediction\n        iou_scores.append(max(ious))\n    # Compute average rank\n    rank1 = np.mean([1.0 if rank == 1 else 0.0 for rank in ranks])\n    rank5 = np.mean([1.0 if rank <= 5 else 0.0 for rank in ranks])\n    # Compute average IoU\n    mean_iou = np.mean(iou_scores)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 31.639216030483947}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Calculate IoUs and ranks\n    ious = []\n    ranks = []\n    for segment, datum in zip(segments, data):\n        iou = self._iou(segment, datum)\n        rank = self._rank(segment, datum)\n        ious.append(iou)\n        ranks.append(rank)\n    # Calculate averages\n    rank1 = np.mean(np.array(ranks) == 1)\n    rank5 = np.mean(np.array(ranks) <= 5)\n    miou = np.mean(ious)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 27.3739785653509}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda(0)\n    for data in tqdm(eval_dataloader, desc=\"predicting\"):\n        data = {k: v.cuda(0) for k, v in data.items()}\n        with torch.no_grad():\n            output = model(**data)\n        output[\"input\"] = data\n        scores.append(self.process_prediction(output))\n    return self.finalize(scores, output_file)\n\n", "CodeBleu_score": 44.59778094770616}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda()\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(eval_dataloader)):\n            data = self._transfer_data_to_context(data, 0)\n            inputs = data[0]\n            outputs = model(inputs)\n            scores.append(outputs)\n            if i % 100 == 0:\n                print(i, len(scores))\n    results = self._process_scores(scores)\n    if output_file is not None:\n        self._save_predictions(results, output_file)\n    return results\n\n", "CodeBleu_score": 35.904301920058124}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda()\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(eval_dataloader, desc=\"Predicting\")):\n            batch = [t.cuda() for t in batch]\n            output = model(*batch)\n            scores.append(output)\n            if i == 0:\n                # process the first batch to get the prediction keys\n                self.pred_keys = output.keys()\n                if output_file is not None:\n                    # create the output file\n                    output_file = os.path.join(self.pred_dir, output_file)\n                    with open(output_file, \"w\") as f:\n                        json.dump(self.pred_keys, f)\n    scores = torch.cat(scores, dim=0)\n    return scores\n\n", "CodeBleu_score": 32.397032944974605}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores[0] = np.concatenate(scores[0], axis=0)\n    scores[1] = np.concatenate(scores[1], axis=0)\n    return scores[0].dot(scores[1].T)\n\n", "CodeBleu_score": 29.536286689487284}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores[0] = np.concatenate(scores[0], axis=0)\n    scores[1] = np.concatenate(scores[1], axis=0)\n    return np.dot(scores[0], scores[1].T)\n\n", "CodeBleu_score": 35.674621565395135}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [np.concatenate(scores[0], axis=0), np.concatenate(scores[1], axis=0)]\n    self.full_scores = []\n    return np.dot(scores[0], scores[1].T)\n\n", "CodeBleu_score": 52.043343096957514}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if len(Y_pred)!= len(Y_true):\n        print(f\"Error: the number of predictions ({len(Y_pred)}) does not match the number of true labels ({len(Y_true)})\")\n    else:\n        print(f\"Successfully predicted {len(Y_pred)} labels\")\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 51.34767720509128}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL\n            )\n    # concatenate all the predictions and true labels\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    # check for prediction errors\n    if (Y_pred!= Y_true).any():\n        print(\"Prediction error found!\")\n        # print samples of the errors\n        for i in range(len(Y_pred)):\n            if Y_pred[i]!= Y_true[i]:\n                print(f\"Sample {i}: Predicted {Y_pred[i]}, True {Y_true[i]}\")\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 60.29117427374575}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n    if Y_pred.shape!= Y_true.shape:\n        print(\"Prediction error. Pred shape: {}. True shape: {}\".format(Y_pred.shape, Y_true.shape))\n        print(\"Predictions: {}\".format(Y_pred[:10]))\n        print(\"True labels: {}\".format(Y_true[:10]))\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 52.27663937404561}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"loss\\\", loss_sum / sample_size, sample_size, round=3)\n\n    # TODO (huxu): log the other metrics.\n\n", "CodeBleu_score": 58.4915240125448}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2) if sample_size > 0 else 0.0, sample_size, round=3\n    )\n\n", "CodeBleu_score": 54.003403578960565}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"loss\\\", loss_sum / sample_size, sample_size, round=3)\n\n    metrics.log_derived(\\\"loss\\\", lambda meters: round(meters[\\\"loss\\\"].avg, 3))\n\n    total_ntokens = sum(log.get(\\\"ntokens\\\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"ntokens\\\", total_ntokens)\n\n    total_nsentences = sum(log.get(\\\"nsentences\\\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"nsentences\\\", total_nsentences)\n\n", "CodeBleu_score": 49.359778879407955}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        # attention_mask is a tensor of shape [batch_size, sequence_length]\n        # with 1 for real tokens and 0 for padding tokens.\n        # attention_mask = torch.cat(\n        #     [vmasks.unsqueeze(-1), cmasks.unsqueeze(-1)], dim=-1\n        # )\n        attention_mask = cmasks.unsqueeze(-1)\n    # token_type_ids is a tensor of shape [batch_size, sequence_length]\n    # with 0 for visual tokens and 1 for text tokens.\n    token_type_ids = torch.cat(\n        [vmasks.new_zeros(vmasks.shape), cmasks.new_ones(cmasks.shape)], dim=-1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 28.37685337192582}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        # generate attention mask\n        attention_mask = torch.cat(\n            [cmasks[:, None, :], vmasks[:, None, :]], dim=1\n        ).to(cmasks.device)\n        attention_mask = attention_mask.view(\n            attention_mask.size(0), -1\n        )\n    # generate token type IDs\n    token_type_ids = torch.cat(\n        [torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1\n    ).to(cmasks.device)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 45.371673868156606}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = torch.cat([vmasks, cmasks], dim=-1)\n    token_type_ids = torch.cat([torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 25.03590587310737}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t if input_ids is not None and input_video_embeds is not None:\n        # adjust the sequence length to match the combined length of input_ids and input_video_embeds\n        combined_length = input_ids.size(1) + input_video_embeds.size(1) + 1  # add 1 for [CLS] token\n        input_ids = input_ids[:, :combined_length - 1]  # remove the last token\n        input_video_embeds = input_video_embeds[:, :combined_length - 1]  # remove the last token\n        # modify the attention_mask and token_type_ids to fit the new sequence length\n        attention_mask = torch.cat(\n            [attention_mask, torch.ones(attention_mask.size(0), input_video_embeds.size(1), device=attention_mask.device)],\n            dim=1\n        )\n        token_type_ids = torch.cat(\n            [token_type_ids, torch.ones(token_type_ids.size(0), input_video_embeds.size(1), device=token_type_ids.device)],\n            dim=1\n        )\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 43.17017121610929}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t if input_ids is None:\n        input_ids = torch.full((input_video_embeds.size(0), 1), self.config.bos_token_id, dtype=torch.long, device=input_video_embeds.device)\n    else:\n        assert input_ids.size(1) == 1, \"input_ids should have one token only.\"\n\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.size(), dtype=torch.long, device=input_ids.device)\n    else:\n        assert attention_mask.size(1) == 1, \"attention_mask should have one token only.\"\n\n    if token_type_ids is None:\n        token_type_ids = torch.full(input_ids.size(), 0, dtype=torch.long, device=input_ids.device)\n    else:\n        assert token_type_ids.size(1) == 1, \"token_type_ids should have one token only.\"\n\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n    attention_mask = torch.cat([attention_mask, torch.ones(input_video_embeds.size(0), input_video_embeds.size(1), dtype=torch.long, device=input_ids.device)], dim=1)\n    token_type_ids = torch.cat([token_type_ids, torch.full(input_video_embeds.size(), 1, dtype=torch.long, device=input_ids.device)], dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 41.54742730500286}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t if input_ids is not None:\n        assert input_ids.size(1) == 3\n    if input_video_embeds is not None:\n        assert input_video_embeds.size(1) == self.config.dataset.max_video_len\n\n    if input_ids is not None and input_video_embeds is not None:\n        combined_length = input_ids.size(1) + input_video_embeds.size(1) - 1\n        if attention_mask is not None:\n            attention_mask = torch.cat(\n                [\n                    attention_mask[:, :1],\n                    attention_mask[:, 1:input_ids.size(1)],\n                    attention_mask[:, input_ids.size(1):]\n                ],\n                dim=1\n            )\n        if token_type_ids is not None:\n            token_type_ids = torch.cat(\n                [\n                    token_type_ids[:, :1],\n                    token_type_ids[:, 1:input_ids.size(1)],\n                    token_type_ids[:, input_ids.size(1):]\n                ],\n                dim=1\n            )\n    else:\n        combined_length = input_ids.size(1) if input_ids is not None else input_video_embeds.size(1)\n\n    if attention_mask is not None and attention_mask.size(1)!= combined_length:\n        attention_mask = attention_mask[:, :combined_length]\n    if token_type_ids is not None and token_type_ids.size(1)!= combined_length:\n        token_type_ids = token_type_ids[:, :combined_length]\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 50.07539507650368}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t_pipeline = cfg.test_dataloader.dataset.pipeline\n    if 'img_id' in test_pipeline[0]:\n        test_pipeline[0].pop('keys')\n    load_img_idx = None\n    for i, transform in enumerate(test_pipeline):\n        if isinstance(transform, LoadImageFromFile):\n            load_img_idx = i\n            break\n    assert load_img_idx is not None, 'LoadImageFromFile is not found'\n    test_pipeline[load_img_idx] = dict(type='mmdet.InferencerLoader')\n    return Compose(test_pipeline)\n", "CodeBleu_score": 29.980510928637667}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tline_cfg = copy.deepcopy(cfg.test_dataloader.dataset.pipeline)\n    if 'img_id' in pipeline_cfg[0].get('meta_keys', []):\n        pipeline_cfg[0]['meta_keys'].remove('img_id')\n    load_img_idx = None\n    for idx, transform in enumerate(pipeline_cfg):\n        if transform['type'] == 'LoadImageFromFile':\n            load_img_idx = idx\n            break\n    if load_img_idx is None:\n        raise ValueError('LoadImageFromFile is not found in the pipeline')\n    pipeline_cfg[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(pipeline_cfg)\n\n\n", "CodeBleu_score": 38.847569732664674}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t_pipeline = cfg.test_dataloader.dataset.pipeline\n    if 'img_id' in test_pipeline[0]['meta_keys']:\n        test_pipeline[0]['meta_keys'].remove('img_id')\n    load_img_idx = None\n    for idx, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_img_idx = idx\n            break\n    if load_img_idx is None:\n        raise ValueError('LoadImageFromFile not found in test pipeline.')\n    test_pipeline[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 33.47143623953113}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and os.path.isdir(inputs):\n        inputs = list(mmengine.scandir(inputs, suffix=IMG_EXTENSIONS))\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 36.1115631470065}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        if not os.path.isdir(inputs):\n            raise ValueError(f'The input must be a directory, but got {inputs}')\n        inputs = sorted(\n            [o for o in mmengine.scandir(inputs) if o.suffix.lower() in IMG_EXTENSIONS])\n        inputs = [str(o) for o in inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 39.344126450325945}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = list(sorted(mmengine.utils.scandir(inputs, IMG_EXTENSIONS)))\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 40.48104104854469}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    inputs = list(inputs)\n    if len(inputs) == 0:\n        return\n    if isinstance(inputs[0], dict):\n        if 'img' in inputs[0] or 'img_path' in inputs[0]:\n            inputs = self.pipeline(inputs)\n        else:\n            raise ValueError(\n                'The input is a dict, but it does not contain the key '\n                '\\'img\\' or \\'img_path\\'. Please check the input data.')\n    for start in range(0, len(inputs), chunk_size):\n        yield inputs[start:start + chunk_size]\n\n", "CodeBleu_score": 25.125417748899736}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            chunk.append(self.pipeline(input))\n        else:\n            chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n    def _get_chunk_data(self, inputs: Iterable, chunk_size: int):\n        \\\"\\\"\\\"The function processes input data in chunks.\n\n        It iterates through the inputs, collecting a specified chunk size of\n        data. For each input, it checks if the input is a dictionary\n        containing an 'img' or 'img_path' key and processes it with the\n        pipeline function. It yields each chunk of processed data until all\n        inputs are processed. If there are remaining inputs after the last\n        chunk, it yields the final chunk.\n\n        Args:\n            inputs (Iterable): Input data.\n            chunk_size (int): Chunk size.\n\n        Yields:\n            Any: Processed data.\n        \\\"\\\"\\\"\n        chunk = []\n        for input in inputs:\n            if isinstance(input, dict) and ('img' in input or\n                                             'img_path' in input):\n                chunk.append(self.pipeline(input))\n            else:\n                chunk.append(input)\n            if len(chunk) == chunk_size:\n                yield chunk\n                chunk = []\n        if chunk:\n            yield chunk\n\n    def _get_chunk_data(self, inputs: Iterable, chunk_size: int):\n        \\\"\\\"\\\"The function processes input data in chunks.\n\n        It iterates through the inputs, collecting a specified chunk size of\n        data. For each input, it checks if the input is a dictionary\n        containing an 'img' or 'img_path' key and processes it with the\n        pipeline function. It yields each chunk of processed data until all\n        inputs are processed. If there are remaining inputs after the last\n        chunk, it yields the final chunk.\n\n        Args:\n            inputs (Iterable):", "CodeBleu_score": 37.968736431691156}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for i in range(0, len(inputs), chunk_size):\n        chunk_data = []\n        for input in inputs[i:i + chunk_size]:\n            if isinstance(input, dict) and 'img' in input:\n                chunk_data.append(input)\n            elif isinstance(input, dict) and 'img_path' in input:\n                chunk_data.append(input)\n            else:\n                chunk_data.append({'img': input})\n        if chunk_data:\n            yield self.pipeline(chunk_data)\n\n", "CodeBleu_score": 29.168258301297236}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir!= '' and return_datasamples is True:\n        warnings.warn('Saving datasamples is not supported. Please set '\n                     'return_datasamples to False.')\n\n    results = dict()\n\n    if return_datasamples is False:\n        for pred in preds:\n            results.update(pred.to_dict())\n    else:\n        results['predictions'] = preds\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result is True:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 48.0141677488632}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n    if pred_out_dir!= '' and return_datasamples is True:\n        warnings.warn('Saving datasamples is not supported in the '\n                      'postprocess function. The results will not be saved.')\n    results = dict()\n    if return_datasamples:\n        results['predictions'] = preds\n    else:\n        results['predictions'] = self.format_results(preds)\n    if visualization is not None:\n        results['visualization'] = visualization\n    if print_result:\n        print_log(results['predictions'], logger='root')\n    return results\n\n", "CodeBleu_score": 47.409198272279}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir!= '' and return_datasamples:\n        warnings.warn('Saving datasamples is not supported. '\n                      'Please set `return_datasamples` to False or '\n                     'set `no_save_pred` to True.')\n\n    if pred_out_dir == '' and not return_datasamples:\n        results = []\n    else:\n        results = dict()\n\n    if return_datasamples:\n        results['predictions'] = preds\n    else:\n        results['predictions'] = self.format_results(preds)\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 54.520522715175446}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_cfg = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline = []\n    for transform in dataset_cfg:\n        if transform['type'] == 'LoadImageFromFile':\n            transform = copy.deepcopy(transform)\n            transform['type'] = 'LoadImageFromNDArray'\n        if transform['type'] == 'Resize':\n            transform = copy.deepcopy(transform)\n            transform['keep_ratio'] = False\n        test_pipeline.append(transform)\n    test_pipeline.append(copy.deepcopy(dataset_cfg[-1]))\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 34.550034014647615}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_cfg = copy.deepcopy(cfg.data.test.pipeline)\n    # Modify the first transform in the dataset pipeline to include only the 'Resize' transform\n    first_transform = dataset_cfg[0]\n    first_transform.type = 'Resize'\n    first_transform.keep_ratio = False\n    first_transform.scale = (1333, 800)\n    # Copy the last transform in the pipeline\n    last_transform = copy.deepcopy(dataset_cfg[-1])\n    # Combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = [first_transform, last_transform]\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 25.32028717310399}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_cfg = copy.deepcopy(cfg.test_dataloader.dataset)\n    dataset_cfg['pipeline'][0]['type'] = 'LoadImageFromNDArray'\n    dataset_cfg['pipeline'][0]['key'] = 'img'\n    dataset_cfg['pipeline'][0]['to_float32'] = True\n    dataset_cfg['pipeline'][0]['color_type'] = 'color_ignore_orientation'\n    dataset_cfg['pipeline'][0]['imdecode_backend'] = 'cv2'\n    dataset_cfg['pipeline'][0]['backend_args'] = None\n    dataset_cfg['pipeline'].append(dict(type='PackDetInputs'))\n    return Compose(dataset_cfg['pipeline'])\n\n", "CodeBleu_score": 18.420704104616313}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        img_shape=img.shape[:2],\n        img_id=frame_id,\n        video_len=video_len)\n\n    test_pipeline = Compose(model.cfg.test_dataloader.dataset.pipeline)\n\n    if model.data_preprocessor.device.type == 'cpu':\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    with torch.no_grad():\n        results = model.test_step(data)\n\n    return results\n\n", "CodeBleu_score": 47.29488997889435}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape[:2],\n                img_id=0, video_len=video_len)\n    data = test_pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 30.509672084722933}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n\n    if isinstance(img, np.ndarray):\n        # TODO: remove img_id.\n        data_ = dict(img=img, img_id=0)\n    else:\n        # TODO: remove img_id.\n        data_ = dict(img_path=img, img_id=0)\n\n    # build the data pipeline\n    data_ = test_pipeline(data_)\n\n    data_['inputs'] = [data_['inputs']]\n    data_['data_samples'] = [data_['data_samples']]\n\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data_)[0]\n\n    return results\n\n", "CodeBleu_score": 32.136593256764534}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\n            \"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 92.91212545357729}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 95.00892113723604}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 95.00892113723604}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.19712621274037}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 60.19712621274037}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 60.45344108153367}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    tensor_shape = get_shape_list(input_tensor)\n    if len(tensor_shape) > 2:\n        raise ValueError(\\\"Rank should be <= 2 but is %d\\\" % (len(tensor_shape)))\n    else:\n        return input_tensor\n", "CodeBleu_score": 12.401010414393204}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 37.07504670833283}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[ndims - 1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 71.66417162628814}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \\\"train-%d\\\" % (i)\n        text_a = tokenization.convert_to_unicode(line[0])\n        label = tokenization.convert_to_unicode(line[-1])\n        if label == \\\"contradictory\\\":\n            label = \\\"contradiction\\\"\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n", "CodeBleu_score": 39.17885247940767}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \\\"multinli_1.0_train.{}.tsv\\\".format(self.language)))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \\\"train-%d\\\" % (i)\n        text_a = tokenization.convert_to_unicode(line[0])\n        text_b = tokenization.convert_to_unicode(line[1])\n        if line[2] == \\\"contradictory\\\":\n            label = \\\"contradiction\\\"\n        else:\n            label = tokenization.convert_to_unicode(line[2])\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 46.89049423257}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 62.41740489696974}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.34592285434499}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The weights are of shape [2, bert_config.hidden_size] because there are\n    # two outputs: `is_next` and `not_next`\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(\n            one_hot_labels * log_probs, axis=-1\n        )\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 80.30374172636321}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The weights are of shape [2, bert_config.hidden_size], which is the shape of the hidden state of the BERT model.\n    # The bias is of shape [2], which is the number of next sentence prediction labels.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n\n    # The logits are calculated by multiplying the input tensor with the output weights and adding the output bias.\n    # The log probabilities are calculated by applying a log softmax to the logits.\n    # The labels are reshaped and converted to one-hot encoding.\n    # The per-example loss is calculated as the negative sum of the product of one-hot labels and log probabilities.\n    # The mean loss is computed by taking the sum of the per-example loss and dividing it by the number of examples.\n    # The function returns the mean loss, per-example loss, and log probabilities.\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 83.23873944643509}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 83.61831819352633}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # Get the shape of the input tensor\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n\n    # Flatten the input tensor and compute the offsets\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1]\n    )\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, -1])\n\n    # Gather the vectors at the specified positions\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 50.33655945308495}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 79.11608801603197}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 79.11608801603197}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 79.11608801603197}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\\\n        while True:\\\n            token = convert_to_unicode(reader.readline())\\\n            if not token:\\\n                break\\\n            token = token.strip()\\\n            vocab[token] = index\\\n            index += 1\\\n    return vocab\n\n", "CodeBleu_score": 45.05263786996016}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.89402347184635}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.89402347184635}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\\\n        split_tokens.append(sub_token)\\\n\n    return split_tokens\n\n", "CodeBleu_score": 58.964292839289676}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.7888382274633}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        data, _ = self.load(path, offset, duration, sample_rate, dtype)\n        return data, False\n    except Exception as e:\n        logger.error(e)\n        return -1.0, True\n\n        # Load audio data.\n        data, error = tf.py_function(safe_load, [audio_descriptor, offset, duration, sample_rate, dtype], [tf.float64, tf.bool])\n\n        # Set waveform shape.\n        data.set_shape([None, None])\n\n        # Return waveform.\n        return {waveform_name: data, f\\\"{waveform_name}_error\\\": error}\n\n    @abstractmethod\n    def save(\n        self,\n        audio_descriptor: AudioDescriptor,\n        signals: List[Signal],\n        sample_rate: int,\n        codec: Codec,\n        bitrate: int,\n        offset: Optional[float] = None,\n        duration: Optional[float] = None,\n    ) -> None:\n        \\\"\\\"\\\"\n        Saves the given signals to the audio file denoted by the given\n        audio descriptor. Aims to be implemented by client.\n\n        Parameters:\n            audio_descriptor (AudioDescriptor):\n                Describe song to save, in case of file based audio adapter,\n                such descriptor would be a file path.\n            signals (List[Signal]):\n                List of signals to save.\n            sample_rate (int):\n                Sample rate of the signals.\n            codec (Codec):\n                Codec to use when saving the file.\n            bitrate (int):\n                Bitrate to use when saving the file.\n            offset (Optional[float]):\n                (Optional) Start offset to save from in seconds.\n            duration (Optional[float]):\n                (Optional) Duration to save in seconds.\n        \\\"\\\"\\\"\n        pass\n\n    @abstractmethod\n    def save_waveform(\n        self,\n        audio_descriptor: AudioDescriptor,\n        waveform: np.ndarray,\n        sample_rate: int,\n        codec: Codec,\n        bitrate: int,\n        offset: Optional[float] = None,\n        duration: Optional[float]", "CodeBleu_score": 25.789865147026692}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        logger.info(\\\"Loading audio from %s\\\", path)\n        wf, sr = self.load(path, offset, duration, sample_rate, dtype)\n        return wf, False\n    except Exception as e:\n        logger.error(\\\"Error while loading audio from %s\\\", path)\n        logger.debug(e)\n        return -1.0, True\n\n\n    @abstractmethod\n    def save(\n        self,\n        audio_descriptor: AudioDescriptor,\n        signals: List[Signal],\n        codec: Codec,\n        sample_rate: int,\n        bitrate: int,\n        extension: str,\n        offset: Optional[float] = None,\n        duration: Optional[float] = None,\n    ) -> None:\n        \\\"\\\"\\\"\n        Saves the given audio signals to the file denoted by the given\n        audio descriptor. Aims to be implemented by client.\n\n        Parameters:\n            audio_descriptor (AudioDescriptor):\n                Describe song to save, in case of file based audio adapter,\n                such descriptor would be a file path.\n            signals (List[Signal]):\n                List of signals to save.\n            codec (Codec):\n                Codec to use for saving.\n            sample_rate (int):\n                Sample rate to save audio with.\n            bitrate (int):\n                Bitrate to use for saving.\n            extension (str):\n                Extension to use for saving.\n            offset (Optional[float]):\n                (Optional) Start offset to save from in seconds.\n            duration (Optional[float]):\n                (Optional) Duration to save in seconds.\n        \\\"\\\"\\\"\n        pass\n\n    @abstractmethod\n    def save_waveform(\n        self,\n        audio_descriptor: AudioDescriptor,\n        waveform: np.ndarray,\n        sample_rate: int,\n        codec: Codec,\n        bitrate: int,\n        extension: str,\n        offset: Optional[float] = None,\n        duration: Optional[float] = None,\n    ) -> None:\n        \\\"\\\"\\\"\n        Saves the given waveform to the file denoted by the given audio\n        descriptor. Aims to be implemented by client.\n\n       ", "CodeBleu_score": 27.583076926468664}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        logger.debug(f\"Loading audio file from path: {path}\")\n        return self.load(path, offset, duration, sample_rate, dtype), False\n    except Exception as e:\n        logger.error(f\"Failed to load audio file from path: {path}. Error: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 23.171009886840917}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    if waveform.shape[2] == n_channels:\n        return waveform\n    elif waveform.shape[2] > n_channels:\n        return waveform[:, :, :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels // waveform.shape[2]])[:, :, :n_channels]\n\n", "CodeBleu_score": 47.128298370627185}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    waveform_channels = tf.shape(waveform)[-1]\n    if waveform_channels >= n_channels:\n        return tf.slice(waveform, [0, 0, 0], [-1, -1, n_channels])\n    return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 50.361579944739596}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[2]\n    if waveform_channels >= n_channels:\n        return waveform[:, :, :n_channels]\n    waveform = tf.tile(waveform, [1, 1, n_channels])\n    return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 49.55262601392058}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    return from_float32_to_uint8(\n        tf.clip_by_value(\n            gain_to_db(spectrogram, **kwargs),\n            tf.math.reduce_max(gain_to_db(spectrogram, **kwargs)) - db_range,\n            tf.math.reduce_max(gain_to_db(spectrogram, **kwargs)),\n        ),\n        **kwargs,\n    )\n\n", "CodeBleu_score": 41.44712966464258}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    return from_float32_to_uint8(\n        tf.clip_by_value(\n            gain_to_db(spectrogram, **kwargs),\n            tf.reduce_max(gain_to_db(spectrogram, **kwargs)) - db_range,\n            tf.reduce_max(gain_to_db(spectrogram, **kwargs)),\n        ),\n        **kwargs,\n    )\n\n", "CodeBleu_score": 40.4335577416648}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    # pylint: disable=unused-argument\n    max_db = tf.math.reduce_max(gain_to_db(spectrogram))\n    return from_float32_to_uint8(\n        tf.clip_by_value(gain_to_db(spectrogram), max_db - db_range, max_db)\n    )\n\n", "CodeBleu_score": 37.041992065162894}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # pylint: disable=line-too-long\n    # pylint: disable=too-many-arguments\n    # pylint: disable=invalid-name\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=no-else-return\n    # pylint: disable=no-else-raise\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-boolean-expressions\n    # pylint: disable=no-member\n    # pylint: disable=no-value-for-parameter\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=useless-suppression\n    # pylint: disable=use-list-literal\n    # pylint: disable=use-a-generator\n    # pylint: disable=consider-using-enumerate\n    # pylint: disable=consider-using-with\n    # pylint: disable=consider-using-f-string\n    # pylint: disable=consider-using-ternary\n    # pylint: disable=consider-using-dict-comprehension\n    # pylint: disable=consider-using-set-comprehension\n    # pylint: disable=consider-using-generator\n    # pylint: disable=consider-using-conditional-expression\n    # pylint: disable=consider-using-in\n    # pylint: disable=consider-using-get\n    # pylint: disable=consider-using-reversed\n    # pylint: disable=consider-using-join\n    # pylint: disable=consider-using-min-max\n    # pylint: disable=consider-using-min\n    # pylint: disable=consider-using-max\n    # pylint: disable=consider-using-f-string\n    # pylint: disable=consider-using-ternary\n    # pylint: disable=consider-using-dict-comprehension\n    # pylint: disable=consider-using-set-comprehension\n    # pylint: disable=consider-using-generator\n    # pylint: disable=consider-using-conditional-expression\n    # pylint: disable=consider-using-in\n    # pylint: disable=consider-using-get\n    # pylint: disable=consider-using-reversed\n    # pylint: disable=consider-using-join\n    # pylint: disable=consider-using-min-max\n    # pylint: disable=consider-using-min\n    # pylint: disable=consider-", "CodeBleu_score": 17.449139280125195}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # pylint: disable=invalid-name\n    # Compute the spectrogram with STFT\n    spectrogram = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: hann_window(x, periodic=True) ** window_exponent,\n    )\n    # Transpose the spectrogram and take the absolute value\n    spectrogram = tf.abs(tf.transpose(spectrogram))\n    # Apply the exponent to the spectrogram\n    if spec_exponent!= 1.0:\n        spectrogram = spectrogram ** spec_exponent\n    # pylint: enable=invalid-name\n    return spectrogram", "CodeBleu_score": 40.08054231379398}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the window tensor\n    window = hann_window(frame_length, periodic=True, dtype=tf.float32)\n    window = tf.pow(window, window_exponent)\n    # Compute the STFT\n    stft_result = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x, y: x * y,\n        pad_end=False,\n    )\n    # Transpose the STFT result and compute the spectrogram\n    spectrogram = tf.abs(tf.transpose(stft_result))\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    return spectrogram\n\n", "CodeBleu_score": 42.61700982104564}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(tf.shape(spectrogram)[0] * factor, tf.int32)\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    if new_time_dim > tf.shape(spectrogram)[0]:\n        # Pad the spectrogram if the new time dimension is larger than the original\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, new_time_dim - tf.shape(spectrogram)[0]], [0, 0]],\n        )\n    else:\n        # Crop the spectrogram if the new time dimension is smaller than the original\n        resized_spectrogram = resized_spectrogram[\n            : tf.shape(spectrogram)[0], :\n        ]\n    # Return the resized and adjusted spectrogram\n    return resized_spectrogram\n\n", "CodeBleu_score": 38.138789927007785}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    adjusted_spectrogram = tf.image.resize_with_crop_or_pad(\n        resized_spectrogram, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n\n    return adjusted_spectrogram\n\n", "CodeBleu_score": 38.534163175097895}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(spectrogram, [None, new_time_dim], method=method)\n    # Adjust the resized spectrogram to the original dimensions\n    adjusted_spectrogram = tf.image.resize_with_crop_or_pad(\n        resized_spectrogram, tf.shape(spectrogram)[1], tf.shape(spectrogram)[2]\n    )\n    return adjusted_spectrogram\n\n", "CodeBleu_score": 38.88578909499106}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(\n        tf.cast(F, tf.float32) * 2.0 ** (semitone_shift / 12.0), tf.int32\n    )[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [F_ps, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, F_ps, F)\n\n", "CodeBleu_score": 37.28311307070112}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * 2.0 ** (semitone_shift / 12.0), tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [T, F_ps], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, T, F)\n\n", "CodeBleu_score": 45.48747268509821}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(\n        tf.cast(F, tf.float32) * 2.0 ** (semitone_shift / 12.0), tf.int32\n    )[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [F_ps, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, F, F)\n\n", "CodeBleu_score": 37.11679250068857}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"elu\":\n        return ELU(alpha=1.0)\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    return ReLU()\n\n", "CodeBleu_score": 42.535429180236605}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    if params[\"conv_activation\"] == \"ReLU\":\n        return ReLU()\n    if params[\"conv_activation\"] == \"ELU\":\n        return ELU()\n    return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 44.17981947690176}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    if params.get(\"conv_activation\") == \"leaky_relu\":\n        return LeakyReLU(alpha=params.get(\"leaky_relu_slope\", 0.2))\n    if params.get(\"conv_activation\") == \"elu\":\n        return ELU()\n    return ReLU()\n\n", "CodeBleu_score": 41.282310279344536}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Get the number of instruments.\n    n_instruments = len(instruments)\n    # Get the number of channels in the input tensor.\n    n_channels = input_tensor.shape[-1]\n    # Initialize the output dictionary.\n    output_dict = {}\n    # Loop over the instruments.\n    for i, instrument in enumerate(instruments):\n        # Create a copy of the input tensor.\n        input_tensor_copy = tf.identity(input_tensor)\n        # Apply the U-Net model to the input tensor for the current instrument.\n        output_tensor = apply_unet(input_tensor_copy, instrument, params)\n        # Get the logit masks from the output tensor.\n        logit_masks = output_tensor[..., 1]\n        # Apply softmax function along the channel axis.\n        softmax_masks = tf.nn.softmax(logit_masks, axis=-1)\n        # Multiply the softmax masks with the input tensor.\n        output_dict[instrument] = tf.math.multiply(\n            input_tensor[..., i], softmax_masks[..., i]\n        )\n    # Return the output dictionary.\n    return output_dict\n\n", "CodeBleu_score": 38.12571530646327}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    return apply(\n        apply_softmax_unet,\n        input_tensor,\n        instruments,\n        params,\n        output_mask_logit=True,\n    )\n\n", "CodeBleu_score": 11.09878457490164}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    instruments = sorted(instruments)\n    unet_output = unet(input_tensor, instruments, params)\n    output = {}\n    for instrument in instruments:\n        output[instrument] = softmax(unet_output[instrument], axis=-1) * input_tensor\n    return output\n\n", "CodeBleu_score": 20.431617469206703}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t   hash_sha256 = hashlib.sha256()\n    with open(path, \\\"rb\\\") as f:\n        for chunk in iter(lambda: f.read(4096), b\\\"\\\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n\n", "CodeBleu_score": 39.02874077803143}, {"code": "def compute_file_checksum(path)\n\t   hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 57.21584989170029}, {"code": "def compute_file_checksum(path)\n\t   hash_sha256 = hashlib.sha256()\n    with open(path, \\\"rb\\\") as f:\n        for chunk in iter(lambda: f.read(4096), b\\\"\\\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n\n", "CodeBleu_score": 39.02874077803143}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t   url = f\"{self._host}/{self._repository}/{self._release}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Unable to retrieve checksum index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in {url}\")\n    return index[name]\n\n", "CodeBleu_score": 44.213795905549375}, {"code": "def checksum(self, name: str) -> str\n\t   url: str = f\"{self._host}/{self._repository}/{self._release}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    try:\n        response = httpx.get(url)\n        if response.status_code!= 200:\n            raise ValueError(f\"Unable to retrieve checksum index from {url}\")\n        index: Dict[str, str] = response.json()\n        if name not in index:\n            raise ValueError(f\"Unable to find {name} in checksum index\")\n        return index[name]\n    except httpx.RequestError as error:\n        raise ValueError(f\"Unable to retrieve checksum index from {url}\") from error\n\n", "CodeBleu_score": 47.242304849934555}, {"code": "def checksum(self, name: str) -> str\n\t   url = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code!= 200:\n        raise ValueError(\n            f\"Failed to retrieve checksum index from {url}, got status code {response.status_code}.\"\n        )\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"No checksum for model {name} found in {url}.\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 41.78280420431946}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    with NamedTemporaryFile(suffix=\".tar.gz\") as stream:\\\n        response: httpx.Response = httpx.get(url, stream=stream)\\\n        response.raise_for_status()\\\n        stream.flush()\\\n        stream.seek(0)\\\n        checksum: str = compute_file_checksum(stream.name)\\\n        expected_checksum: str = self.checksum(name)\\\n        if checksum!= expected_checksum:\\\n            raise ValueError(f\"Invalid checksum for model {name}\")\\\n        logger.info(f\"Extracting model {name} to {path}\")\\\n        with tarfile.open(stream.name, \"r:gz\") as archive:\\\n            archive.extractall(path)\\\n        os.remove(stream.name)\\\n        logger.info(f\"Model {name} extracted to {path}\")\\\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 44.038644263078794}, {"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name + \".tar.gz\",\n        )\n    )\n    logger.debug(f\"Downloading {name} model from {url}\")\n    with NamedTemporaryFile(delete=False) as stream:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                stream.write(chunk)\n        stream.flush()\n        checksum: str = compute_file_checksum(stream.name)\n        expected_checksum: str = self.checksum(name)\n        if checksum!= expected_checksum:\n            raise ValueError(\n                f\"Invalid checksum for {name} model, expected {expected_checksum}, got {checksum}\"\n            )\n        logger.debug(f\"Extracting {name} model to {path}\")\n        with tarfile.open(stream.name, \"r:gz\") as archive:\n            archive.extractall(path)\n        os.remove(stream.name)\n        logger.debug(f\"Model {name} downloaded and extracted\")", "CodeBleu_score": 52.3283020857068}, {"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            f\"{name}.tar.gz\",\n        )\n    )\n    logger.info(f\"Downloading {name} model from {url}...\")\n    with NamedTemporaryFile() as stream:\n        with httpx.stream(\"GET\", url) as response:\n\n            if response.status_code!= 200:\n\n                raise RuntimeError(f\"Failed to download {name} model from {url}\")\n\n            for chunk in response.iter_bytes():\n\n                stream.write(chunk)\n\n            stream.flush()\n\n            checksum: str = compute_file_checksum(stream.name)\n\n            expected_checksum: str = self.checksum(name)\n\n            if checksum!= expected_checksum:\n\n                raise RuntimeError(f\"Invalid checksum for {name} model\")\n\n            with tarfile.open(stream.name) as archive:\n\n                archive.extractall(path)\n\n    logger.info(f\"Successfully extracted {name} model to {path}\")\n\n\n", "CodeBleu_score": 49.19433841367203}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \\\"\\\")\n        try:\n            return json.loads(loader.read_text(resources, configuration_name))\n        except loader.FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration '{configuration_name}' not found.\\\")\n    elif exists(descriptor):\n        try:\n            with open(descriptor, \\\"r\\\") as configuration_file:\n                return json.load(configuration_file)\n        except IOError:\n            raise SpleeterError(f\\\"Configuration file '{descriptor}' not found.\\\")\n    else:\n        raise SpleeterError(f\\\"Invalid configuration descriptor '{descriptor}'.\\\")\n\n", "CodeBleu_score": 43.80443011071506}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        config_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            config_data = loader.read_text(resources, config_name)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration '{config_name}' not found.\\\")\n    else:\n        if not exists(descriptor):\n            raise SpleeterError(f\\\"Configuration file '{descriptor}' not found.\\\")\n        with open(descriptor) as f:\n            config_data = f.read()\n    try:\n        return json.loads(config_data)\n    except json.JSONDecodeError:\n        raise SpleeterError(f\\\"Configuration file '{descriptor}' is not a valid JSON file.\\\")\n\n", "CodeBleu_score": 44.21350747629143}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        config_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        try:\n            config_data = loader.read_text(resources, config_name)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration {config_name} not found\\\")\n    else:\n        if not exists(descriptor):\n            raise SpleeterError(f\\\"Configuration file {descriptor} not found\\\")\n        with open(descriptor) as config_file:\n            config_data = config_file.read()\n    return json.loads(config_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.488195472879376}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n    concat_tensors = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    concat_tensors = func(concat_tensors)\n    tensor_dict = dict(\n        zip(tensor_dict.keys(), tf.split(concat_tensors, len(tensor_dict), axis=concat_axis))\n    )\n    return tensor_dict\n\n", "CodeBleu_score": 34.24294397821487}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(f\"concat_axis should be either 0 or 1, got {concat_axis}\")\n    concat_tensor = tf.concat(list(tensor_dict.values()), concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        return dict(zip(tensor_dict.keys(), tf.split(processed_tensor, len(tensor_dict))))\n    return dict(zip(tensor_dict.keys(), tf.split(processed_tensor, len(tensor_dict), axis=1)))\n\n", "CodeBleu_score": 34.83843787319355}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, got {}\".format(concat_axis)\n        )\n\n    tensor_list = [tensor_dict[key] for key in sorted(tensor_dict.keys())]\n    tensor = tf.concat(tensor_list, axis=concat_axis)\n    tensor = func(tensor)\n    tensor_list = tf.split(tensor, len(tensor_dict), axis=concat_axis)\n\n    return {key: tensor for key, tensor in zip(sorted(tensor_dict.keys()), tensor_list)}\n\n", "CodeBleu_score": 38.29977706693158}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    normalized_tensor = tf.cast(\n        tf.clip_by_value((tensor - min_value) / (max_value - min_value), 0.0, 1.0),\n        tf.uint8,\n    )\n    return {\n        tensor_key: normalized_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.49834078280709}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_val = tf.reduce_min(tensor)\n    max_val = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            (tensor - min_val) / (max_val - min_val) * 255, dtype=tf.uint8\n        ),\n        min_key: min_val,\n        max_key: max_val,\n    }\n\n", "CodeBleu_score": 56.04114900939775}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.math.reduce_min(tensor, axis=[1, 2], keepdims=True)\n    tensor_max = tf.math.reduce_max(tensor, axis=[1, 2], keepdims=True)\n    tensor_range = tf.math.subtract(tensor_max, tensor_min)\n    tensor_range = tf.math.maximum(tensor_range, 1e-5)\n    scaled_tensor = tf.math.divide(tf.math.subtract(tensor, tensor_min), tensor_range)\n    scaled_tensor = tf.math.multiply(scaled_tensor, 255.0)\n    scaled_tensor = tf.cast(scaled_tensor, tf.uint8)\n    return {tensor_key: scaled_tensor, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 38.73165293837574}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = tf.math.floormod(segment_len - tf.math.floormod(tensor_len, segment_len), segment_len)\n    pad_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(pad_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 30.452914023810713}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = segment_len - (tensor_len % segment_len)\n    padding = tf.zeros((padding_len, *tf.shape(tensor)[1:]))\n    padded_tensor = tf.concat([tensor, padding], axis=0)\n    return tf.reshape(padded_tensor, (-1, segment_len, *tf.shape(tensor)[1:]))\n\n", "CodeBleu_score": 28.08614396753738}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = tf.cast(tf.math.ceil(tensor_len / segment_len), tf.int32) * segment_len - tensor_len\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 28.922540991319252}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = frame_length - tf.math.floormod(tf.shape(instr_spec)[0], frame_length)\n    instr_spec_pad = tf.concat([instr_spec, tf.zeros([pad_size, F])], axis=0)\n    return tf.reshape(instr_spec_pad, [-1, frame_length, F])\n\n", "CodeBleu_score": 13.243792480257635}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[0] % frame_length)\n    # create an extension of zeros\n    extension = tf.zeros((num_extra_rows, instr_spec.shape[1]))\n    # concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = tf.concat([instr_spec, extension], axis=0)\n    # reshape the extended spectrogram by merging the first two dimensions\n    reshaped_spec = tf.reshape(extended_spec, (1, -1, instr_spec.shape[1], 1))\n    return reshaped_spec\n\n", "CodeBleu_score": 20.049777669189503}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = frame_length - tf.math.floormod(instr_spec.shape[0], frame_length)\n    pad_size = tf.math.floormod(pad_size, frame_length)\n    instr_spec = tf.pad(instr_spec, [[0, pad_size]] + [[0, 0]] * (len(instr_spec.shape) - 1))\n    instr_spec = tf.reshape(instr_spec, [F, -1, instr_spec.shape[-1]])\n    return instr_spec\n\n", "CodeBleu_score": 15.324848865635829}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    tensor_dict = {col: tf.constant(df[col].values) for col in df.columns}\n    return tf.data.Dataset.from_tensor_slices(tensor_dict)\n\n", "CodeBleu_score": 31.12867128060338}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    # pylint: disable=line-too-long\n    # pylint: disable=unused-argument\n    # pylint: disable=missing-docstring\n    # pylint: disable=redefined-outer-name\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=invalid-name\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-function-args\n    # pylint: disable=consider-using-enumerate\n    # pylint: disable=unsubscriptable-object\n    # pylint: disable=unsupported-membership-test\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=useless-suppression\n    # pylint: disable=too-many-boolean-expressions\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks", "CodeBleu_score": 5.9454191033138395}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n\n", "CodeBleu_score": 21.276760631811044}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = result and tf.equal(tf.shape(tensor_tf)[dim], length)\n    return result\n\n", "CodeBleu_score": 38.112346615956476}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for index, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and tensor_tf.shape[index] == dim\n    return result\n\n", "CodeBleu_score": 30.04549288630332}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, target_dim in zip(tensor_tf.shape, target_shape):\n        if target_dim is not None:\n            result = tf.math.logical_and(result, tf.equal(dim, target_dim))\n    return result\n\n", "CodeBleu_score": 33.566523543506975}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize dataset builder.\n    dataset_builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=10,\n        random_seed=0,\n    )\n    # Build dataset.\n    dataset = dataset_builder.build(\n        train_csv_path=\"train.csv\",\n        cache_dir=\"cache\",\n        batch_size=16,\n        chunks_per_song=10,\n        data_augmentation=True,\n        cache=True,\n    )\n    return dataset\n\n", "CodeBleu_score": 29.649194169858422}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, 10, 1000000000\n    )\n    return builder.build(\n        train_csv=audio_path + SEPARATOR + \"train.csv\",\n        cache_dir=audio_path + SEPARATOR + \"cache\",\n        batch_size=4,\n        num_chunks_per_song=100,\n        augmentation=True,\n        cache=True,\n    )\n\n", "CodeBleu_score": 34.09792047287049}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize a DatasetBuilder with the provided audio parameters, audio adapter, and audio path.\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    # Specify the chunk duration and random seed.\n    chunk_duration = 10\n    random_seed = 42\n    # Build the dataset using the builder with additional parameters like train_csv path, cache directory, batch size, number of chunks per song, and options for data augmentation and caching.\n    dataset = builder.build(\n        train_csv=audio_path + SEPARATOR + \"train.csv\",\n        cache_dir=audio_path + SEPARATOR + \"cache\",\n        batch_size=16,\n        num_chunks_per_song=10,\n        data_augmentation=True,\n        cache=True,\n        random_seed=random_seed,\n        chunk_duration=chunk_duration,\n    )\n    return dataset\n\n", "CodeBleu_score": 32.78584851884309}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    instrument,\n                    self._audio_params,\n                    self._audio_adapter,\n                    self._audio_path,\n                    self._random_seed,\n                )\n            )\n        self._instrument_builders = tf.data.Dataset.from_generator(\n            lambda: self._instrument_builders,\n            output_types=InstrumentDatasetBuilder,\n        )\n    yield from self._instrument_builders\n\n    def build(self, n_chunks_per_song: int = 1) -> Any:\n        \\\"\\\"\\\"\n        Builds dataset.\n\n        Parameters:\n            n_chunks_per_song (int, optional):\n                Number of segment per song to compute.\n                Defaults to 1.\n\n        Returns:\n            Any:\n                Dataset.\n        \\\"\\\"\\\"\n        dataset = self._instrument_builders.interleave(\n            lambda instrument_dataset_builder: instrument_dataset_builder.build()\n        )\n        dataset = dataset.map(self.expand_path)\n        dataset = dataset.filter(self.filter_error)\n        dataset = dataset.map(self.filter_waveform)\n        dataset = dataset.map(self.harmonize_spectrogram)\n        dataset = dataset.filter(self.filter_short_segments)\n        dataset = dataset.map(self.random_time_crop)\n        dataset = dataset.map(self.random_time_stretch)\n        dataset = dataset.map(self.random_pitch_shift)\n        dataset = dataset.map(self.map_features)\n        dataset = self.compute_segments(dataset, n_chunks_per_song)\n        return dataset\n\n", "CodeBleu_score": 42.96228998160606}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            builder = InstrumentDatasetBuilder(\n                instrument,\n                self._audio_adapter,\n                self._audio_params,\n                self._audio_path,\n                self._random_seed,\n                self._chunk_duration,\n            )\n            self._instrument_builders.append(builder)\n        self._instrument_builders = tuple(self._instrument_builders)\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 51.20719419377886}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_adapter,\n                    self._audio_params,\n                    instrument,\n                    self._audio_path,\n                    self._random_seed,\n                )\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n    @property\n    def audio_adapter(self) -> Any:\n        \\\"\\\"\\\"AudioAdapter instance.\\\"\\\"\\\"\n        return self._audio_adapter\n\n    @property\n    def audio_params(self) -> Any:\n        \\\"\\\"\\\"Audio parameters.\\\"\\\"\\\"\n        return self._audio_params\n\n    @property\n    def audio_path(self) -> Any:\n        \\\"\\\"\\\"Path to audio files.\\\"\\\"\\\"\n        return self._audio_path\n\n    @property\n    def random_seed(self) -> Any:\n        \\\"\\\"\\\"Random seed.\\\"\\\"\\\"\n        return self._random_seed\n\n    @property\n    def chunk_duration(self) -> Any:\n        \\\"\\\"\\\"Duration of chunk.\\\"\\\"\\\"\n        return self._chunk_duration\n\n    @property\n    def T(self) -> Any:\n        \\\"\\\"\\\"Number of frames in a chunk.\\\"\\\"\\\"\n        return self._T\n\n    @property\n    def F(self) -> Any:\n        \\\"\\\"\\\"Number of frequency bins in a chunk.\\\"\\\"\\\"\n        return self._F\n\n    @property\n    def sample_rate(self) -> Any:\n        \\\"\\\"\\\"Sample rate of audio.\\\"\\\"\\\"\n        return self._sample_rate\n\n    @property\n    def frame_length(self) -> Any:\n        \\\"\\\"\\\"Length of a frame in samples.\\\"\\\"\\\"\n        return self._frame_length\n\n    @property\n    def frame_step(self) -> Any:\n        \\\"\\\"\\\"Number of samples between consecutive frames.\\\"\\\"\\\"\n        return self._frame_step\n\n    @property\\", "CodeBleu_score": 57.22838954060122}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None or not os.path.exists(cache):\n        return dataset\n    if wait:\n        while not os.path.exists(os.path.join(cache, \\\"index\\\")):\n            logging.info(\\\"Waiting for cache to be created...\\\")\n            time.sleep(5)\n    return dataset.cache(cache)\n\n", "CodeBleu_score": 46.52390687468516}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    cache = os.path.join(cache, self._mix_name)\n    if not os.path.exists(cache):\n        os.makedirs(cache)\n    if wait:\n        while not os.path.exists(os.path.join(cache, \\\"index.json\\\")):\n            logging.info(\\\"Waiting for cache to be created\\\")\n            time.sleep(5)\n    return dataset.cache(cache)\n\n\n", "CodeBleu_score": 50.73005518683399}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    cache_path = os.path.join(cache, self.name)\n    if wait:\n        while not os.path.exists(os.path.join(cache_path, \\\"_index_file\\\")):\n            logging.info(\n                \\\"Waiting for cache to be generated at %s\\\", os.path.join(cache_path)\n            )\n            time.sleep(10)\n    if not os.path.exists(cache_path):\n        os.makedirs(cache_path)\n    return dataset.cache(cache_path)\n\n\n", "CodeBleu_score": 46.7504435579708}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_dir = ModelProvider.get_model_dir(params[\"model_dir\"], params[\"model_name\"])\n    params = ModelProvider.update_params(params, model_dir)\n    session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.95\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 31.453347957093776}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_directory = ModelProvider.get_model_path(params[\"model_name\"])\n    params.update(load_configuration(model_directory))\n    session_config = tf.ConfigProto(allow_soft_placement=True)\n    session_config.gpu_options.allow_growth = True\n    config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn, model_directory, params, config)\n\n", "CodeBleu_score": 29.195485556630345}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_provider = ModelProvider()\n    model_provider.load_model_directory(params[\\\"model_dir\\\"])\n    params.update(model_provider.get_params())\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=params[\\\"model_dir\\\"], params=params, config=config)\n\n\n", "CodeBleu_score": 30.294648979380582}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\taveform.ndim == 1:\n        waveform = to_stereo(waveform)\n    self._data_generator.update_data({\n        \"waveform\": waveform,\n        \"audio_id\": audio_descriptor.audio_id,\n    })\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n\n\n", "CodeBleu_score": 52.636524782008564}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tform = to_stereo(waveform, self._sample_rate)\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n    )\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 42.10438680690123}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tform = to_stereo(waveform, self._sample_rate)\n    audio_descriptor = audio_descriptor.copy()\n    audio_descriptor.sample_rate = self._sample_rate\n    audio_descriptor.duration = len(waveform) / self._sample_rate\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": str(audio_descriptor)}\n    )\n    prediction = next(self._get_prediction_generator())\n    audio_descriptor = AudioDescriptor.from_string(prediction.pop(\"audio_id\"))\n    return prediction, audio_descriptor\n\n", "CodeBleu_score": 43.4296209813375}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone is not a resnet\"\n    return np.array(\n        [\n            [\n                int(math.ceil(image_shape[0] / stride)),\n                int(math.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 76.62386736337}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n        )\n\n", "CodeBleu_score": 89.23569476063783}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone is not a resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 87.08713643768971}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\\\"clipped_boxes\\\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 57.76000690557442}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\\\"clipped_boxes\\\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n\n", "CodeBleu_score": 57.76000690557442}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    rpn_class_logits = KL.Conv2D(2 * anchors_per_location, (3, 3), padding='same',\n                                activation='linear', name='rpn_class_raw')\n    # Reshape to [batch, anchors, 2] for class predictions\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(rpn_class_logits)\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    rpn_bbox = KL.Conv2D(anchors_per_location * 4, (1, 1), activation='linear',\n                         name='rpn_bbox_pred')\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 51.015994692323396}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride, name='rpn_conv_shared')(\n        feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(\n        shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(tf.nn.softmax, name=\\\"rpn_class_xxx\\\")(\n        rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\\\"valid\\\",\n                  activation='linear', name='rpn_bbox_pred')(\n        shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 75.95760462800075}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff ** 2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 58.60419868806381}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t    K.clear_session()\n    input_shape = x_train.shape[1:]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 25.753503270729826}, {"code": "def build_model(x_train, num_classes)\n\t    # Reset the graph\n    K.clear_session()\n    # Define the input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    # Define the model\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(input_layer)\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=128, activation=\"relu\")(x)\n    output_layer = KL.Dense(units=num_classes, activation=\"softmax\")(x)\n    # Define the model\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n    # Compile the model\n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=keras.optimizers.Adam(lr=0.001),\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n", "CodeBleu_score": 32.31731781441777}, {"code": "def build_model(x_train, num_classes)\n\t    # Reset the graph\n    tf.reset_default_graph()\n    # Define the input layer\n    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n    # Convolutional layers\n    conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(input_layer)\n    conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(conv1)\n    # Max-pooling layer\n    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv2)\n    # Flatten the output\n    flatten = tf.keras.layers.Flatten()(pool)\n    # Dense layers\n    dense1 = tf.keras.layers.Dense(128, activation=\"relu\")(flatten)\n    output_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(dense1)\n    # Define the model\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n    # Compile the model\n    model.compile(\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 24.360180953538325}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes\n\n", "CodeBleu_score": 77.91363107789368}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n\n        boxes[i] = np.array([y1, x1, y2, x2])\n\n    return boxes\n\n", "CodeBleu_score": 77.91363107789368}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n\n        boxes[i] = np.array([y1, x1, y2, x2])\n\n    return boxes\n\n", "CodeBleu_score": 77.91363107789368}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas", "CodeBleu_score": 1.8523002421307506}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes", "CodeBleu_score": 0.6291343111878056}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = boxes1[..., 2] * boxes1[..., 3]\n    area2 = boxes2[..., 2] * boxes2[..., 3]\n\n    # Compute overlaps to generate the matrix [boxes1 count, boxes2 count] \n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 35.944096984236005}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = boxes1[..., 2] * boxes1[..., 3]\n    area2 = boxes2[..., 2] * boxes2[..., 3]\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count] \n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 35.944096984236005}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = Image.open(info[\"path\"])\n    # If grayscale. Convert to RGB for consistency.\n    if image.mode!= \"RGB\":\n        image = image.convert(\"RGB\")\n    # If has an alpha channel, remove it for consistency\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n    image = np.asarray(image)\n    return image\n\n", "CodeBleu_score": 42.725570662103166}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = io.imread(info[\"path\"])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim!= 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 81.33285217322333}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info[\"path\"]\n    image = Image.open(path)\n    if image.mode!= \"RGB\":\n        image = image.convert(\"RGB\")\n    if len(image.getbands()) == 4:\n        image = image.convert(\"RGB\")\n    return image\n\n", "CodeBleu_score": 41.043879842880216}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"No Title\\\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm,\n                   interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.41815068638795}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\" for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 79.32204722974599}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\" for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 79.32204722974599}]}
