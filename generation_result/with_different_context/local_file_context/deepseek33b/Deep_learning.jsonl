{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    model_path = self.model_path\n    config_path = self.config_path\n\n    # Check if a cached model is available and valid\n    cached_model_path = self.get_cached_model_path()\n    if cached_model_path is not None and os.path.exists(cached_model_path):\n        try:\n            model = torch.load(cached_model_path)\n            if isinstance(model, torch.nn.Module):\n                model.eval()\n                self.model = model\n                return {\"model\": model}\n        except Exception as e:\n            print(f\"Error loading cached model: {e}\")\n\n    # Load the model from the specified path\n    if model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load_file(model_path, device=\"cpu\")\n    else:\n        model = torch.load(model_path, map_location=\"cpu\")\n\n    # Configure the model using settings from a YAML file\n    config = OmegaConf.load(config_path)\n    model = instantiate_from_config(config.model)\n\n    # Move the model to the appropriate device\n    model.to(devices.device)\n\n    # Optional settings\n    if half_attention:\n        model.half_attention()\n    if devices.device.type == \"cuda\":\n        model.model.to(memory_format=torch.channels_last)\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Cache the loaded model for future use\n    self.cache_model(model)\n\n    return {\"model\": model}\n\n", "CodeBleu_score": 38.54286391187036}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    if self.ldsr_model is not None:\n        return self.ldsr_model\n\n    # Check if the cached model is available and valid\n    cached_ldsr_model_path = os.path.join(\n        shared.models_path, \"ldsr_model.safetensors\"\n    )\n    if os.path.exists(cached_ldsr_model_path):\n        try:\n            cached_ldsr_model = safetensors.torch.load_file(cached_ldsr_model_path)\n            if (\n                isinstance(cached_ldsr_model, dict)\n                and \"state_dict\" in cached_ldsr_model\n                and \"config\" in cached_ldsr_model\n            ):\n                self.ldsr_model = cached_ldsr_model\n                return self.ldsr_model\n        except Exception as e:\n            print(f\"Error loading cached LDSR model: {e}\")\n\n    # Load the model from the specified path\n    model_path = os.path.join(shared.models_path, \"ldsr.yaml\")\n    config = OmegaConf.load(model_path)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(\n        torch.load(os.path.join(shared.models_path, \"ldsr.ckpt\"), map_location=\"cpu\"),\n        strict=False,\n    )\n    model.eval()\n\n    # Configure the model\n    device = devices.get_optimal_device()\n    model.to(device)\n    if half_attention:\n        model.mid_block.attentions[0].to(devices.cpu)\n        model.mid_block.attentions[1].to(devices.cpu)\n    model.to(devices.device, memory_format=torch.channels_last)\n\n    # Cache the loaded model\n    self.ldsr_model = model\n    return self.ldsr_model\n\n", "CodeBleu_score": 40.47139858090858}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    global cached_ldsr_model\n\n    # Load from cache if available\n    if cached_ldsr_model is not None:\n        print(\"Loading LDSR model from cache...\")\n        return cached_ldsr_model\n\n    # Load from path\n    print(\"Loading LDSR model from path...\")\n    config = OmegaConf.load(\"models/ldsr/ldsr.yaml\")\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(\"models/ldsr/ldsr.ckpt\")[\"state_dict\"], strict=False)\n\n    # Configure model\n    device = devices.device\n    if half_attention:\n        model = model.half()\n    model = model.to(device)\n    model.eval()\n\n    # Cache model\n    cached_ldsr_model = model\n    return model\n\n", "CodeBleu_score": 27.242636741566393}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # make sure model is in eval mode\n    model.eval()\n\n    # get the batch size from the shape\n    bs = shape[0]\n\n    print(f'eta: {eta} steps: {steps}')\n\n    # create the sampler\n    sampler = DDIMSampler(model)\n\n    # initialize the sampler\n    samples, intermediates = sampler.sample(steps, batch_size=bs, shape=shape,\n                                            conditioning=cond, eta=eta, callback=callback,\n                                            normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                            quantize_x0=quantize_x0, temperature=temperature,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                            x_T=x_t)\n\n    # return the samples and intermediate values\n    return samples, intermediates\n\n", "CodeBleu_score": 57.60134783226963}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n    samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape,\n                                          conditioning=cond, eta=eta, callback=callback,\n                                          normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                          quantize_x0=quantize_x0, temperature=temperature,\n                                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                          x_T=x_t)\n    return samples, intermediates\n\n", "CodeBleu_score": 63.51669213278222}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    print(f\"Beginning sampling with eta {eta} and {steps} steps\")\n    samples_ddim, intermediates = ddim.sample(steps, batch_size=bs, shape=shape,\n                                              conditioning=cond, eta=eta, callback=callback,\n                                              normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                              quantize_x0=quantize_x0, temperature=temperature,\n                                              score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                              x_T=x_t)\n    return samples_ddim, intermediates\n\n", "CodeBleu_score": 61.71598181734973}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n    else:\n        if context is not None:\n            print(f\"{context}: using EMA weights\")\n        self.model_ema.swap()\n        try:\n            yield\n        finally:\n            if context is not None:\n                print(f\"{context}: restoring model weights\")\n            self.model_ema.swap()\n\n", "CodeBleu_score": 56.95668282840024}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.model_ema.store(self.parameters())\n        self.model_ema.copy_to(self.parameters())\n        print(f\"Switched to EMA weights - {context}\")\n        try:\n            yield\n        finally:\n            self.model_ema.restore(self.parameters())\n            print(f\"Restored original weights - {context}\")\n    else:\n        yield\n\n", "CodeBleu_score": 49.74726327511405}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            print(f\"{context}: Switching to EMA weights.\")\n        self.model_ema.swap()\n        yield\n        if context is not None:\n            print(f\"{context}: Restoring model weights.\")\n        self.model_ema.swap()\n    else:\n        yield\n\n", "CodeBleu_score": 54.24334595873616}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if len(sd.keys()) > 0:\n        if ignore_keys is not None:\n            sd = {k: v for k, v in sd.items() if not k.startswith(tuple(ignore_keys))}\n            self.load_state_dict(sd, strict=False)\n            print(f\"Restored from {path} with {len(sd.keys())} keys.\")\n\n", "CodeBleu_score": 30.8521833196745}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys:\n        sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n    self.load_state_dict(sd)\n    print(f\"Loaded state_dict from [{path}]\")\n\n", "CodeBleu_score": 18.59896337212342}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del sd[k]\n    self.load_state_dict(sd)\n    print(f\"Loaded state_dict from [{path}]\")\n\n", "CodeBleu_score": 15.574654634181565}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, torch.argmax(quant, dim=1)\n    return dec, diff\n\n", "CodeBleu_score": 50.2573765675113}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        pred_indices = self.post_quant_conv(quant).argmax(1)\n        return dec, diff, pred_indices\n    return dec, diff\n\n", "CodeBleu_score": 50.69146345652903}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant_t, diff, _ = self.encode(input)\n    dec = self.decode(quant_t)\n    if return_pred_indices:\n        pred_indices, _ = self.quantize.get_codebook_indices(quant_t)\n        return dec, diff, pred_indices\n    return dec, diff\n\n", "CodeBleu_score": 41.08266976692129}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if x.shape[1] != 3:\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format)\n    if self.batch_resize_range is not None:\n        if self.training:\n            if self.global_step < self.batch_resize_range[0]:\n                target_size = self.batch_resize_range[0]\n            elif self.global_step >= self.batch_resize_range[1]:\n                target_size = self.batch_resize_range[1]\n            else:\n                target_size = int(self.batch_resize_range[0] + (self.global_step - self.batch_resize_range[0]) / (self.batch_resize_range[1] - self.batch_resize_range[0]) * (self.batch_resize_range[1] - self.batch_resize_range[0]))\n            x = torch.nn.functional.interpolate(x, (target_size, target_size), mode='bicubic', align_corners=False)\n        else:\n            target_size = self.batch_resize_range[1]\n            x = torch.nn.functional.interpolate(x, (target_size, target_size), mode='bicubic', align_corners=False)\n    return x.contiguous()\n\n", "CodeBleu_score": 49.95744845424907}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if x.shape[1] != 3:\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format)\n    if self.batch_resize_range is not None:\n        if self.training:\n            if self.batch_resize_range is not None:\n                if self.batch_resize_range[0] == self.batch_resize_range[1]:\n                    bsz = self.batch_resize_range[0]\n                else:\n                    bsz = random.randint(self.batch_resize_range[0], self.batch_resize_range[1])\n            else:\n                bsz = self.batch_resize_range[1]\n            x = torch.nn.functional.interpolate(x, (bsz, bsz), mode='bicubic', align_corners=False)\n    return x.contiguous()\n\n", "CodeBleu_score": 52.785830624099106}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 3:\n        x = x[..., None]\n    if x.shape[1] != 3:\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format)\n    if self.batch_resize_range is not None:\n        if self.training:\n            if isinstance(self.batch_resize_range, float):\n                x = TF.resize(x, [int(self.batch_resize_range * x.shape[2]), int(self.batch_resize_range * x.shape[3])], interpolation=InterpolationMode.BICUBIC, antialias=True)\n            else:\n                rnd_resize = random.randint(*self.batch_resize_range) / 100\n                x = TF.resize(x, [int(rnd_resize * x.shape[2]), int(rnd_resize * x.shape[3])], interpolation=InterpolationMode.BICUBIC, antialias=True)\n    return x.contiguous()\n\n", "CodeBleu_score": 47.90347547650529}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    if optimizer_idx == 0:\n        # autoencode\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n    if optimizer_idx == 1:\n        # discriminator\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return discloss\n\n", "CodeBleu_score": 65.89219297905812}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n    self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n    return aeloss\n\n", "CodeBleu_score": 23.855121048033336}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\\\"train\\\")\n    self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n    return aeloss\n\n", "CodeBleu_score": 22.646837151940883}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = dict()\n    x = self.get_input(batch, self.image_key)\n    x = x.to(self.device)\n    xrec, _, _ = self(x)\n    if x.shape[1] > 3:\n        # colorize with dummy data\n        x_color = einops.repeat(x, 'b c ... -> b (c new_c) ...', new_c=3)\n        xrec_color = einops.repeat(xrec, 'b c ... -> b (c new_c) ...', new_c=3)\n        if self.use_ema:\n            with self.ema_scope():\n                xrec_color_ema, _, _ = self(x)\n        if not only_inputs:\n            log['samples'] = self.get_image_grid(x_color, xrec_color)\n            if self.use_ema and plot_ema:\n                log['samples_ema'] = self.get_image_grid(x_color, xrec_color_ema)\n    else:\n        log['samples'] = self.get_image_grid(x, xrec)\n        if self.use_ema and plot_ema:\n            with self.ema_scope():\n                xrec_ema, _, _ = self(x)\n                log['samples_ema'] = self.get_image_grid(x, xrec_ema)\n    return log\n", "CodeBleu_score": 40.5713033344349}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = dict()\n    x = self.get_input(batch, self.image_key)\n    x = x.to(self.device)\n    xrec, _, _ = self(x)\n    if x.shape[1] > 3:\n        # colorize with dummy data\n        x_color = einops.repeat(x, 'b c ... -> b (3 c) ...', c=3)\n        _, _, [_, _, xrec_color] = self(x_color)\n        xrec_color = einops.rearrange(xrec_color, 'b (3 c) ... -> b c ...', c=3)\n        key = f'{self.image_key}_orig'\n        log[key] = self.log_image_reconstruction(x, xrec, xrec_color, key)\n    else:\n        key = self.image_key\n        log[key] = self.log_image_reconstruction(x, xrec, xrec, key)\n\n    if plot_ema:\n        with self.ema_scope():\n            xrec_ema, _, _ = self(x)\n            if x.shape[1] > 3:\n                _, _, [_, _, xrec_color_ema] = self(x_color)\n                xrec_color_ema = einops.rearrange(xrec_color_ema, 'b (3 c) ... -> b c ...', c=3)\n                key = f'{self.image_key}_orig_ema'\n                log[key] = self.log_image_reconstruction(x, xrec_ema, xrec_color_ema, key)\n            else:\n                key = self.image_key + '_ema'\n                log[key] = self.log_image_reconstruction(x, xrec_ema, xrec_ema, key)\n    return log\n\n", "CodeBleu_score": 41.89142534098629}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = dict()\n    x = self.get_input(batch, self.image_key)\n    x = x[:self.num_log_images]\n    if self.global_step > 0 and self.use_ema and plot_ema:\n        with self.ema_scope():\n            xrec, _, _ = self(x)\n            xrec = xrec[:self.num_log_images]\n            if self.colorize_nrow is not None:\n                xrec = self.colorize(xrec)\n            log[\\\"samples_ema\\\"] = xrec\n    if not only_inputs:\n        xrec, _, _ = self(x)\n        xrec = xrec[:self.num_log_images]\n        if self.colorize_nrow is not None:\n            xrec = self.colorize(xrec)\n        log[\\\"samples\\\"] = xrec\n    if self.colorize_nrow is not None:\n        x = self.colorize(x)\n    log[\\\"input\\\"] = x\n    return log\n\n", "CodeBleu_score": 39.73944162621493}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 54.252106166325575}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError()\n    if mean:\n        loss = loss.mean()\n    return loss\n\n", "CodeBleu_score": 43.72152746383857}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError()\n    if mean:\n        loss = loss.mean()\n    return loss\n\n", "CodeBleu_score": 43.72152746383857}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, c, *spatial = x.shape\n    x_in = x.type(self.dtype)\n    if self.use_timesteps:\n        if self.timesteps is None:\n            if self.use_ddim:\n                t = torch.randint(0, self.num_timesteps, (b,), device=x.device).long()\n            else:\n                t = torch.randint(self.num_timesteps // 2, self.num_timesteps, (b,), device=x.device).long()\n        else:\n            t = self.timesteps\n        x, t_emb = self.q_sample(x_in, t)\n    else:\n        t = None\n        x = x_in\n    if self.cond_stage_key in kwargs:\n        c = kwargs[self.cond_stage_key]\n    if self.cond_stage_trainable:\n        if self.cond_stage_key in kwargs:\n            c = kwargs[self.cond_stage_key]\n        if c is None:\n            assert self.uncond_stage_key in kwargs, 'If no c is provided, you have to provide an unconditional c'\n            c = kwargs[self.uncond_stage_key]\n        if self.cond_stage_model is not None:\n            c = self.cond_stage_model(c)\n        if self.extra_cond_stage:\n            c = torch.cat([c, x], dim=1)\n        if self.cond_stage_key in kwargs:\n            kwargs[self.cond_stage_key] = c\n    if self.use_scale_shift_norm:\n        x = self.norm(x, t)\n    else:\n        x = self.norm(x)\n    if self.use_timesteps:\n        if self.timesteps is None:\n            if self.use_ddim:\n                t = torch.randint(0, self.num_timesteps, (b,), device=x.device).long()\n            else:\n", "CodeBleu_score": 40.77313556322573}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # parse args\n    c_crossattn = kwargs.pop(\"c_crossattn\", None)\n    c_concat = kwargs.pop(\"c_concat\", None)\n    c_emb = kwargs.pop(\"c_emb\", None)\n    t = kwargs.pop(\"t\", None)\n    if c_crossattn is not None:\n        c_crossattn = list(c_crossattn)\n    if c_concat is not None:\n        c_concat = list(c_concat)\n    if c_emb is not None:\n        c_emb = list(c_emb)\n    if t is not None:\n        t = list(t)\n\n    # choose a random timestep for each sample in the batch\n    timesteps = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n\n    # calculate the embeddings for the conditioning\n    if c_emb is not None:\n        c_emb = [self.transformer.positional_embedding(t) for t in c_emb]\n\n    # apply the model to get the predicted noise\n    return self.apply_model(x, timesteps, c, c_emb=c_emb, c_concat=c_concat, c_crossattn=c_crossattn)\n\n", "CodeBleu_score": 46.40612562159428}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, *_, device = *x.shape, x.device\n\n    if self.training and self.p2_loss_weight > 0:\n        img_e = self.q_sample(x, t=torch.randint(0, self.n_steps, (b,), device=device).long())\n        img_q = self.q_sample(x, t=torch.flip(torch.arange(0, self.n_steps, device=device), dims=(0,)))\n        if not self.cond_stage_key == 'hybrid':\n            c = None\n        loss_uncond, loss_cond = self.p2_losses(img_e, img_q, c=c, t=torch.randint(0, self.n_steps, (b,), device=device).long(),\n                                                noise=torch.randn_like(x))\n        loss = loss_uncond + self.p2_loss_weight * loss_cond\n    else:\n        loss = self.p_losses(x, c=c, t=torch.randint(0, self.n_steps, (b,), device=device).long(), noise=torch.randn_like(x))\n\n    return loss\n\n", "CodeBleu_score": 40.207979351687804}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}.\")\n    else:\n        for i in range(1, int(dimension ** 0.5) + 1):\n            if dimension % i == 0:\n                return i, dimension // i\n\n", "CodeBleu_score": 20.98547198931738}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor <= 0:\n            factor = dimension\n        for i in range(1, int(factor ** 0.5) + 1):\n            if dimension % i == 0:\n                return i, dimension // i\n        return dimension, 1\n", "CodeBleu_score": 24.12058366959229}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor < 0:\n            factor = dimension\n        for i in range(1, int(factor ** 0.5) + 1):\n            if dimension % i == 0:\n                if i <= dimension // i and i <= factor:\n                    return i, dimension // i\n        return dimension, 1\n", "CodeBleu_score": 30.647399240700864}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.dora_scale is not None:\n        orig_weight = orig_weight * self.dora_scale\n\n    if self.scale is not None:\n        updown = updown * self.scale\n\n    if self.alpha is not None:\n        updown = updown / self.alpha\n\n    if self.bias is not None:\n        updown = updown + self.bias\n\n    updown = updown - orig_weight\n\n    if self.dora_scale is not None:\n        updown = updown / self.dora_scale\n\n    if self.scale is not None:\n        updown = updown / self.scale\n\n    if self.alpha is not None:\n        updown = updown * self.alpha\n\n    if self.bias is not None:\n        updown = updown - self.bias\n\n    return updown\n\n", "CodeBleu_score": 23.35866408876849}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.dora_scale is not None:\n        return\n\n    if self.scale is None and self.alpha is None:\n        return\n\n    if orig_weight is None:\n        return\n\n    if updown == 'down':\n        return\n\n    if self.dim is None:\n        return\n\n    if self.alpha is None:\n        return\n\n    if self.scale is None:\n        return\n\n    if self.bias is None:\n        return\n\n    if self.bias is not None:\n        orig_weight = orig_weight - self.bias\n\n    if self.scale is not None:\n        orig_weight = orig_weight * self.scale\n\n    if self.alpha is not None:\n        orig_weight = orig_weight / self.alpha\n\n    if self.scale is not None:\n        orig_weight = orig_weight * self.scale\n\n    if self.bias is not None:\n        orig_weight = orig_weight + self.bias\n\n    return orig_weight\n", "CodeBleu_score": 16.56638432195003}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.dora_scale is not None:\n        scale = self.dora_scale\n        if self.dora_norm_dims is not None:\n            scale = scale / self.dora_norm_dims\n        else:\n            scale = scale / len(self.shape)\n    else:\n        scale = self.calc_scale()\n\n    if self.dim is not None:\n        updown = updown.reshape(self.shape)\n\n    if self.bias is not None:\n        updown = updown.add(self.bias)\n\n    if orig_weight is not None:\n        updown = orig_weight + scale * (updown - orig_weight)\n\n    return updown\n", "CodeBleu_score": 28.685130335475765}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias.to(updown.dtype)\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias is not None:\n        updown = updown * self.bias.to(updown.dtype)\n    if self.scale is not None:\n        updown = updown * self.scale.to(updown.dtype)\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 58.875651313680486}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n    if len(orig_weight.shape) == len(updown.shape):\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias is not None:\n        updown = updown + self.bias.reshape(updown.shape)\n    if self.scale is not None:\n        updown = updown * self.scale\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 58.77276199494441}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.calc_scale()\n\n    if updown.shape == orig_weight.shape:\n        updown = updown.reshape(orig_weight.shape)\n\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    if output_shape != updown.shape:\n        updown = updown.reshape(output_shape)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 54.11493520788009}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.ex_bias:\n        weight = self.weight.to(orig_weight.device)\n        bias = self.bias.to(orig_weight.device)\n        return self.finalize_updown(orig_weight, weight, bias)\n    else:\n        weight = self.weight.to(orig_weight.device)\n        return self.finalize_updown(orig_weight, weight)\n\n", "CodeBleu_score": 46.272260838339804}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.ex_bias is not None:\n        self.weight = torch.cat([self.weight, self.ex_bias], 0)\n    self.weight = self.weight.to(orig_weight.device)\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 42.4293532695569}, {"code": "def calc_updown(self, orig_weight)\n\t\n    self.weight.data = orig_weight.to(self.weight.device)\n    if self.ex_bias is not None:\n        self.ex_bias.data = orig_weight.to(self.ex_bias.device)\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 37.27455083863005}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global networks\n    if len(networks) > shared.opts.max_networks:\n        while len(networks) > shared.opts.max_networks:\n            # remove the oldest network\n            oldest_network = list(networks.keys())[0]\n            del networks[oldest_network]\n\n        gc.collect()\n\n", "CodeBleu_score": 36.951323356947164}, {"code": "def purge_networks_from_memory()\n\t\n    if shared.opts.network_cache_size == 0:\n        return\n\n    while len(shared.sd_model.network_layer_mapping) > shared.opts.network_cache_size:\n        shared.sd_model.network_layer_mapping.popitem(last=False)\n\n    gc.collect()\n\n", "CodeBleu_score": 29.123552213429782}, {"code": "def purge_networks_from_memory()\n\t\n    if len(networks) > shared.opts.networks_max_count:\n        # purge oldest networks first\n        for network_key in sorted(networks.keys(), key=lambda k: networks[k].mtime)[:shared.opts.networks_max_count - len(networks)]:\n            del networks[network_key]\n\n        gc.collect()\n\n", "CodeBleu_score": 33.87031142759825}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n\n    if self.backup_weights is not None and self.backup_biases is not None:\n        self.weight.copy_(self.backup_weights)\n        self.bias.copy_(self.backup_biases)\n        self.backup_weights = None\n        self.backup_biases = None\n        self.backup_weights_alpha = None\n        self.backup_biases_alpha = None\n    elif self.backup_weights is not None:\n        self.weight.copy_(self.backup_weights)\n        self.backup_weights = None\n        self.backup_weights_alpha = None\n    elif self.backup_biases is not None:\n        self.bias.copy_(self.backup_biases)\n        self.backup_biases = None\n        self.backup_biases_alpha = None\n    else:\n        self.bias = None\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.q_proj.weight.copy_(self.q_proj.backup_weights)\n        self.k_proj.weight.copy_(self.k_proj.backup_weights)\n        self.v_proj.weight.copy_(self.v_proj.backup_weights)\n        self.q_proj.bias.copy_(self.q_proj.backup_biases)\n        self.k_proj.bias.copy_(self.k_proj.backup_biases)\n        self.v_proj.bias.copy_(self.v_proj.backup_biases)\n        self.q_proj.backup_weights = None\n        self.k_proj.backup_weights = None\n        self.v_proj.backup_weights = None\n        self.q_proj.backup_biases = None\n        self.k_proj.backup_biases = None\n        self.v_proj.backup_biases = None\n        self.q_proj.backup_weights_alpha = None\n        self.k_proj.backup_weights_alpha = None\n        self.v", "CodeBleu_score": 36.647413250095575}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights is not None and self.backup_bias is not None:\n        self.weight.data = self.backup_weights\n        self.bias.data = self.backup_bias\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.proj_weight.data = self.backup_proj_weights\n            self.proj_bias.data = self.backup_proj_bias\n        else:\n            self.proj_weight.data = None\n            self.proj_bias.data = None\n    else:\n        self.bias = None\n\n\n\n", "CodeBleu_score": 32.515501001872934}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights is not None and self.backup_biases is not None:\n        self.weight.data.copy_(self.backup_weights)\n        if self.bias is not None:\n            self.bias.data.copy_(self.backup_biases)\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.weight.data.copy_(self.backup_projection_weights)\n            self.out_proj.bias.data.copy_(self.backup_projection_biases)\n    else:\n        self.bias = None\n\n", "CodeBleu_score": 32.174919759333264}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n\n    network_layer_name = getattr(org_module, 'network_layer_name', None)\n    if network_layer_name is None:\n        return original_forward(input)\n\n    input = input.to(devices.device, devices.dtype, non_blocking=True)\n\n    network_restore_weights_from_backup(org_module)\n\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad() if devices.device.type == 'cuda' else nullcontext():\n                    output = module.forward(output, org_module)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n    return output\n\n", "CodeBleu_score": 50.18358355646053}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n\n    network_layer_name = getattr(org_module, 'network_layer_name', None)\n    if network_layer_name is None:\n        return original_forward(input)\n\n    input = input.to(devices.device)\n    network_restore_weights_from_backup(org_module)\n\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(network_layer_name, None)\n        if module is not None:\n            try:\n                output = module.forward(output, org_module)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n    return output\n\n", "CodeBleu_score": 50.77235964226185}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if extra_network_lora.loaded_networks is None:\n        return original_forward(input)\n\n    device = devices.device\n    dtype = devices.dtype\n\n    if isinstance(input, torch.Tensor):\n        input = input.to(device, dtype=dtype)\n    elif isinstance(input, tuple):\n        input = tuple((x.to(device, dtype=dtype) for x in input))\n\n    network_restore_weights_from_backup(org_module)\n\n    output = original_forward(input)\n\n    for net in extra_network_lora.loaded_networks:\n        if net.name not in extra_network_lora.errors:\n            module = net.modules.get(getattr(org_module, 'network_layer_name', None), None)\n            if module is not None:\n                try:\n                    output = module.forward(output, net.name)\n                except RuntimeError as e:\n                    logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                    extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n    return output\n\n", "CodeBleu_score": 39.04034670028149}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainable_params.append(layer.weight)\n            trainable_params.append(layer.bias)\n    return trainable_params\n\n", "CodeBleu_score": 56.84994507797919}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainable_params.append(layer.weight)\n            trainable_params.append(layer.bias)\n    return trainable_params\n\n", "CodeBleu_score": 56.84994507797919}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded_hypernetworks = [hn for hn in shared.loaded_hypernetworks if hn.name in names]\n    shared.loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        hn = load_hypernetwork(name)\n        if hn is None:\n            continue\n        hn.multiplier = multiplier\n        shared.loaded_hypernetworks.append(hn)\n    shared.loaded_hypernetworks.extend(already_loaded_hypernetworks)\n", "CodeBleu_score": 29.575199293710114}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded = [h for h in shared.hypernetworks if h.name in names]\n    shared.hypernetworks.clear()\n    for name in names:\n        h = load_hypernetwork(name)\n        if h is None:\n            continue\n        if multipliers is not None and name in multipliers:\n            h.multiplier = multipliers[name]\n        shared.hypernetworks.append(h)\n\n    for h in already_loaded:\n        if h.name in names:\n            shared.hypernetworks.append(h)\n\n", "CodeBleu_score": 33.40209569669293}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    shared.loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        hn = load_hypernetwork(name)\n        if hn is not None:\n            hn.multiplier = multiplier\n            shared.loaded_hypernetworks.append(hn)\n\n", "CodeBleu_score": 23.54742083800978}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 77.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 77.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 70.5905906630193}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context) * self.scale\n    v = self.to_v(context)\n    del context\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n    q, k, v = apply_hypernetworks(self.hypernetworks, (q, k, v))\n    sim = einsum('b i d, b j d -> b i j', q, k)\n    if exists(self.mask):\n        mask = F.pad(mask, (self.left_pad, self.right_pad), value=True)\n        mask = rearrange(mask, 'b j -> b 1 1 j')\n        sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n    if exists(self.bias):\n        sim = sim + self.bias\n    attn = sim.softmax(dim=-1)\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 60.800437832795964}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context)\n    v = self.to_v(context)\n    q, k, v = apply_hypernetworks(self.hypernetworks, (q, k, v))\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n    if exists(mask):\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = rearrange(mask, 'b j -> b () j')\n        sim = sim.masked_fill(~mask, max_neg_value)\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 64.54518868536825}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context)\n    v = self.to_v(context)\n    q, k, v = apply_hypernetworks(self.hypernetworks, context, layer=self)\n    b, c, *spatial = q.shape\n    q = q.reshape(b, c, -1)\n    k = k.reshape(b, c, -1)\n    v = v.reshape(b, c, -1)\n    q = q.permute(0, 2, 1)\n    k = k.permute(0, 2, 1)\n    v = v.permute(0, 2, 1)\n    q = q.view(b, -1, h, c // h)\n    k = k.view(b, -1, h, c // h)\n    v = v.view(b, -1, h, c // h)\n    q = q * (c ** (-0.5))\n    context = k.shape[1]\n    if self.use_flash_attention:\n        q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\n        sim = torch.einsum('b i d, b j d -> b i j', q, k)\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n    else:\n        q = q.softmax(dim=-2)\n        out = torch.einsum('b i j, b j d -> b i d', q, v)\n    out = out.view(b, -1, c)\n    out = self.to_out(out)\n    return out\n\n", "CodeBleu_score": 40.275310210515016}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_state = hypernetwork.state_dict()\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.hash = checkpoint['hash']\n    hypernetwork.sd_checkpoint_name = checkpoint['sd_checkpoint_name']\n    hypernetwork.sd_checkpoint = checkpoint['sd_checkpoint']\n    hypernetwork.state_dict = checkpoint['state_dict']\n    hypernetwork.clip_skip = checkpoint['clip_skip']\n    hypernetwork.clip_max_token_length = checkpoint['clip_max_token_length']\n    hypernetwork.clip_layers = checkpoint['clip_layers']\n    hypernetwork.clip_dim = checkpoint['clip_dim']\n    hypernetwork.clip_max_length = checkpoint['clip_max_length']\n    hypernetwork.clip_dim_latent = checkpoint['clip_dim_latent']\n    hypernetwork.clip_max_length_latent = checkpoint['clip_max_length_latent']\n    hypernetwork.clip_cond_unet_number = checkpoint['clip_cond_unet_number']\n    hypernetwork.clip_uncond_unet_number = checkpoint['clip_uncond_unet_number']\n    hypernetwork.clip_cond_text_number = checkpoint['clip_cond_text_number']\n    hypernetwork.clip_uncond_text_number = checkpoint['clip_uncond_text_number']\n    hypernetwork.clip_cond_crossattn_number = checkpoint['clip_cond_crossattn_number']\n    hypernetwork.clip_uncond_crossattn_number = checkpoint['clip_uncond_crossattn_number']\n    hypernetwork.clip_cond_crossattn_k = checkpoint['clip_cond_crossattn_k']\n    hypernetwork.clip_uncond_crossattn_k = checkpoint['clip_uncond_crossattn_k']\n    hypernetwork.clip_cond_crossattn_v = checkpoint['clip_cond_crossattn_v']\n    hypernetwork.clip_uncond_crossattn_v = checkpoint['clip_uncond_crossattn_v']\n   ", "CodeBleu_score": 14.95112468342265}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_name = hypernetwork.name\n    old_filename = hypernetwork.filename\n    old_hash = hypernetwork.hash\n    old_prompt = hypernetwork.prompt\n    old_description = hypernetwork.description\n    old_notes = hypernetwork.notes\n    old_style = hypernetwork.style\n    old_version = hypernetwork.version\n    old_state_dict = hypernetwork.state_dict()\n\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.filename = filename\n    hypernetwork.hash = checkpoint['hash']\n    hypernetwork.prompt = checkpoint['prompt']\n    hypernetwork.description = checkpoint['description']\n    hypernetwork.notes = checkpoint['notes']\n    hypernetwork.style = checkpoint['style']\n    hypernetwork.version = checkpoint['version']\n    hypernetwork.load_state_dict(checkpoint['state_dict'])\n\n    try:\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.name = old_name\n        hypernetwork.filename = old_filename\n        hypernetwork.hash = old_hash\n        hypernetwork.prompt = old_prompt\n        hypernetwork.description = old_description\n        hypernetwork.notes = old_notes\n        hypernetwork.style = old_style\n        hypernetwork.version = old_version\n        hypernetwork.load_state_dict(old_state_dict)\n        raise\n\n", "CodeBleu_score": 24.439972101357366}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_hypernetwork_name = hypernetwork.hypernetwork_name\n    old_state_dict = hypernetwork.state_dict()\n    old_step = hypernetwork.step\n    old_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    old_sd_checkpoint_hash = hypernetwork.sd_checkpoint_hash\n    old_hypernetwork_name_for_save = hypernetwork.hypernetwork_name_for_save\n    old_optimizer_name = hypernetwork.optimizer_name\n    old_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    old_sd_checkpoint_config = hypernetwork.sd_checkpoint_config\n\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    hypernetwork.step = checkpoint.get('step', 0)\n    hypernetwork.sd_checkpoint_name = checkpoint.get('sd_checkpoint_name', None)\n    hypernetwork.sd_checkpoint_hash = checkpoint.get('sd_checkpoint_hash', None)\n    hypernetwork.hypernetwork_name_for_save = checkpoint.get('hypernetwork_name_for_save', None)\n    hypernetwork.optimizer_name = checkpoint.get('optimizer_name', None)\n    hypernetwork.optimizer_state_dict = checkpoint.get('optimizer_state_dict', None)\n    hypernetwork.sd_checkpoint_config = checkpoint.get('sd_checkpoint_config', None)\n\n    try:\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.hypernetwork_name = old_hypernetwork_name\n        hypernetwork.step = old_step\n        hypernetwork.sd_checkpoint_name = old_sd_checkpoint_name\n        hypernetwork.sd_checkpoint_hash = old_sd_checkpoint_hash\n        hypernetwork.hypernetwork_name_for_save = old_hypernetwork_name_for_save\n        hypernetwork.optimizer_name = old_optimizer_name\n        hypernetwork.optimizer_state_dict = old_optimizer_state_dict\n        hypernetwork.sd_checkpoint_config = old_sd_checkpoint", "CodeBleu_score": 35.05331703611933}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n    else:\n        with torch.no_grad():\n            if context is not None:\n                print(f\"EMA: {context}\")\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model.parameters())\n        try:\n            yield\n        finally:\n            with torch.no_grad():\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\"EMA: {context}\")\n\n", "CodeBleu_score": 60.52281983791777}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.original_params = deepcopy(self.model.state_dict())\n        self.model.load_state_dict(self.model_ema.state_dict())\n        if self.log_every_t is not None:\n            self.logger.experiment.info(f\"{context}: Switched to EMA weights.\")\n        try:\n            yield\n        finally:\n            self.model.load_state_dict(self.original_params)\n            if self.log_every_t is not None:\n                self.logger.experiment.info(f\"{context}: Restored model parameters.\")\n    else:\n        yield\n\n", "CodeBleu_score": 48.45537748665044}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        # Store the current model parameters\n        self.model_params = deepcopy(self.model.state_dict())\n\n        # Copy EMA weights to the model\n        self.model.load_state_dict(self.model_ema.state_dict())\n\n        # Log the switch to EMA weights (if specified)\n        if self.log_ema_switch:\n            self.log(f\"Switching to EMA weights in {context}\")\n\n    try:\n        # Yield control to the code block\n        yield\n    finally:\n        # Restore the original weights\n        self.model.load_state_dict(self.model_params)\n\n        # Log the restoration of original weights (if specified)\n        if self.log_ema_switch:\n            self.log(f\"Restoring original weights in {context}\")\n\n", "CodeBleu_score": 45.71422503377007}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.p_mean_variance(x, t)\n    if self.parameterization == \"eps\":\n        x_recon = self.predict_start_from_noise(x, t=t, noise=model_output)\n    elif self.parameterization == \"x0\":\n        x_recon = model_output\n    else:\n        raise NotImplementedError(self.parameterization)\n\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 70.97099534608927}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    if self.parameterization == \"eps\":\n        x_recon = self.predict_start_from_noise(x, t=t, noise=self.model(x, t))\n    elif self.parameterization == \"x0\":\n        x_recon = self.predict_start_from_noise(x_t=x, t=t, noise=self.model(x, t))\n    else:\n        raise NotImplementedError(self.parameterization)\n\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 66.81538624612635}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type == \"fixedlarge\":\n        x_recon = self.predict_start_from_noise(x, t=t, noise=model_output)\n    elif self.model_var_type == \"fixedsmall\":\n        x_recon = self.predict_start_from_noise(x=self.predict_start_from_noise(x, t=t, noise=model_output), t=t, noise=model_output)\n    else:\n        raise NotImplementedError(self.model_var_type)\n\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 60.44939804039464}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 53.19007099959852}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 53.19007099959852}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean([1, 2, 3])\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum([1, 2, 3])\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 52.775096690937104}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, dtype=torch.long, device=x_start.device)\n    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n    return torch.mean(kl_prior) / math.log(2.0)\n\n", "CodeBleu_score": 62.238238788170634}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, dtype=torch.long, device=x_start.device)\n    mean, logvar = self.q_mean_logvar(x_start, t)\n    return torch.nn.functional.kl_div(\n        normal_kl(mean, logvar),\n        normal_kl(torch.zeros_like(mean), torch.zeros_like(logvar)),\n        reduction='sum'\n    ) / np.log(2.) / batch_size\n\n", "CodeBleu_score": 37.06317524503815}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, _, log_variance = self.q_posterior_mean_variance(x_start=x_start, x_t=x_start, t=t)\n    kl_prior = normal_kl(mean, log_variance)\n    return kl_prior.mean() / math.log(2.0)\n\n", "CodeBleu_score": 45.671921709909896}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(cache_dir, subsection)\n    return diskcache.Cache(cache_path, size_limit=4e9, disk_min_file_size=256000)\n\n", "CodeBleu_score": 45.80926879851037}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 53.03546987218357}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, eviction_policy='least-recently-stored', sqlite_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 53.519409371546026}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir, exist_ok=True)\n\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n", "CodeBleu_score": 65.09437474972341}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is not None:\n        return cache_obj\n\n    with cache_lock:\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        if os.path.isfile(cache_filename):\n            convert_old_cached_data()\n\n        cache_obj = caches.get(subsection)\n        if cache_obj is None:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.17227096670307}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir, exist_ok=True)\n\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n", "CodeBleu_score": 65.09437474972341}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if cache_obj.get(title):\n        cached_mtime = cache_obj.get(title + '-mtime')\n        if cached_mtime and mtime > cached_mtime:\n            cache_obj.delete(title)\n            cache_obj.delete(title + '-mtime')\n\n    if cache_obj.get(title) is None:\n        data = func()\n        cache_obj[title] = data\n        cache_obj[title + '-mtime'] = mtime\n        dump_cache()\n\n    return cache_obj[title]\n\n", "CodeBleu_score": 32.43624540820311}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached = cache_obj.get(title)\n    if cached is not None:\n        if mtime > cached[1]:\n            cache_obj.delete(title)\n            cached = None\n\n    if cached is None:\n        data = func()\n        cache_obj[title] = (data, mtime)\n        dump_cache()\n    else:\n        data = cached[0]\n\n    return data", "CodeBleu_score": 28.03483147783042}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    data = cache_obj.get(title, None)\n    if data and data[1] > mtime:\n        cache_obj.delete(title)\n        data = None\n    if data is None:\n        data = func()\n        cache_obj[title] = (data, mtime)\n        dump_cache()\n    return data[0]\n\n", "CodeBleu_score": 19.949056664152458}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)", "CodeBleu_score": 70.73041784590174}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)", "CodeBleu_score": 73.6174005969053}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 73.6174005969053}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    prompt_success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not prompt_success or not negative_prompt_success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 59.53607852378144}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    extracted_positive_prompt = False\n    extracted_negative_prompt = False\n\n    if style.prompt:\n        extracted_positive_prompt, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n\n    if style.negative_prompt:\n        extracted_negative_prompt, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_positive_prompt or not extracted_negative_prompt:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 68.56575814717752}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    extracted_positive, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    extracted_negative, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_positive or not extracted_negative:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 62.135631708914985}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 94.50652737693086}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 89.68877129783917}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 89.68877129783917}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 95.0256034169634}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\\\"Model was not loaded. Call load_model() before inference.\\\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 80.87015022062592}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\\\"Model was not loaded. Call load_model() before inference.\\\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n\n\n", "CodeBleu_score": 80.87015022062592}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1\n    assert 0 < min_pad_coverage < 1\n\n    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 97.67006232562707}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1\n    assert 0 < min_pad_coverage < 1\n\n    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 97.67006232562707}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1\n    assert 0 < min_pad_coverage < 1\n\n    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 97.67006232562707}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n\n    ax.remove_xticks()\n    ax.remove_yticks()\n    ax.set_title(title)\n\n", "CodeBleu_score": 80.39159489469004}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n    ax.remove_xticks()\n    ax.remove_yticks()\n    ax.set_title(title)\n\n", "CodeBleu_score": 80.39159489469004}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n    ax.remove_xticks()\n    ax.remove_yticks()\n    ax.set_title(title)\n\n\n", "CodeBleu_score": 72.6692623525441}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[0], embeds.shape[1]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds[:,1:,:], dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_excl = (torch.sum(embeds[:,1:,:], dim=1, keepdim=True) - embeds[:,1:,:]) \\\n        / (torch.norm(embeds[:,1:,:], dim=2, keepdim=True) * (utterances_per_speaker - 1) + 1e-5)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed embeddings is simply the dot\n    # product of these embeddings (which is just an elementwise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    for j in range(speakers_per_batch):\n        mask = torch.ones_like(sim_matrix[j, :, :])\n        mask[j, :, j] = 0\n        centroids_incl_n = centroids_incl[j, :]\n        centroids_incl_n = centroids_incl_n.repeat(utterances_per_speaker, 1)\n        centroids_excl_n = centroids_excl[j, :]\n        centroids_excl", "CodeBleu_score": 44.19704902530695}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[0], embeds.shape[1]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for backprop\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\\\n        .expand(speakers_per_batch, utterances_per_speaker, -1)\\\n        .clone(embeds.device)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\\\n        .clone(embeds.device)\\\n        .view(speakers_per_batch, utterances_per_speaker, -1) / (utterances_per_speaker - 1)\n\n    # Similarity matrix. The cosine similarity of already 2-normed embeddings is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    cos_sim = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    for j in range(speakers_per_batch):\n        mask = torch.ones_like(cos_sim[j])\\\n            .to(self.loss_device)\\\n            .scatter(1, torch.tensor(j).repeat(utterances_per_speaker).view(-1, 1).to(self.loss_device), 0)\\\n            .to(self.loss_device)\n        centroids_incl_n = centroids_incl[j]\\\n            .detach()\\\n            .clone()\\\n            .view(1, -1)\\\n            .expand(utterances_per_speaker, -1)\\\n            .to(self.loss_device)\n        embeds_n = embeds[j]\\\n            .detach()\\\n            .clone()\\\n            .view(-1, 1)\\\n            .expand(-1, speakers_per_batch)\\", "CodeBleu_score": 35.0778247300182}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[0], embeds.shape[1]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds[:, 1:, :], dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_excl = (torch.sum(embeds[:, 1:, :], dim=1, keepdim=True) - embeds[:, 1:, :]) / (\n            speakers_per_batch - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed embeddings is simply the dot\n    # product of these embeddings (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = torch.ones_like(sim_matrix)\n    mask_matrix.to(self.loss_device)\n    for j in range(speakers_per_batch):\n        mask_matrix[j, :, j] = 0\n\n    sim_matrix = mask_matrix * embeds @ centroids_incl.transpose(1, 2)\n    sim_matrix += embeds @ centroids_excl.transpose(1, 2)\n    sim_matrix = sim_matrix / self.similarity_weight.exp()\n    sim_matrix += self.similarity_bias\n\n    return sim_matrix\n\n", "CodeBleu_score": 47.57738394119727}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, speakers_per_batch))\n\n    # Ground truth\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n\n    # Loss\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, target)\n\n    # EER (not backpropagated)\n    with torch.no_grad():\n        # Snippet from https://yangcha.github.io/EER-ROC/\n        fpr, tpr, thresholds = roc_curve(ground_truth, sim_matrix.detach().cpu().numpy())\n        eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n\n    return loss, eer\n\n", "CodeBleu_score": 59.11364014551358}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, speakers_per_batch))\n\n    # Ground truth\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n\n    # Loss\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, target)\n\n    # EER (not backpropagated)\n    with torch.no_grad():\n        # noinspection PyUnresolvedReferences\n        out_avg = torch.mean(sim_matrix, dim=0)\n        eer = brentq(lambda x: 1.0 - x - interpolate.interp1d(np.sort(out_avg), np.linspace(0, 1, len(out_avg)), kind=\"linear\", fill_value=\"extrapolate\")(x), 0.0, 1.0)\n\n    return loss, eer\n\n", "CodeBleu_score": 56.271315487581}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, speakers_per_batch))\n\n    # Ground truth\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n\n    # Calculate loss\n    loss = self.loss_fn(sim_matrix, target)\n\n    # EER (not backpropagated)\n    with torch.no_grad():\n        ### Derived from https://github.com/clovaai/voxceleb_trainer ###\n        scores, labels = sim_matrix.detach().cpu().numpy(), target.detach().cpu().numpy()\n\n        # Ensure the input is sorted\n        scores = np.sort(scores, axis=1)\n\n        N = scores.shape[0]\n        ground_truth = np.tile(np.arange(N), (N, 1))\n        mask = np.triu(np.ones(scores.shape), 1).astype(np.bool)\n        mask[ground_truth[mask]] = False\n        mask = np.logical_not(mask)\n\n        # Find the index of the score closest to the desired TPR\n        desired_tpr = 0.05\n        tprs = np.zeros((N, N))\n        fprs = np.zeros((N, N))\n        for i in range(N):\n            gt = np.repeat(ground_truth[i], N)\n            gt[mask[i]] = -1\n            tpr, fpr = roc_curve(gt, scores[i])\n            tprs[i] = tpr\n            fprs[i] = fpr\n\n        # Find the threshold closest to desired TPR\n        idx =", "CodeBleu_score": 58.55752650360391}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Import the parameter configurations\n    import yaml\n    import encoder.params_data\n    import encoder.params_model\n\n    # Create the parameter string\n    params_string = \\\"\\\"\n    for name, value in yaml.safe_load(open(\\\"params.yaml\\\")).items():\n        params_string += \\\"%s: \\\\t%s\\", "CodeBleu_score": 8.075695341602483}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    # Import the parameter configurations\n    from config.data_config import DataConfig\n    from config.model_config import ModelConfig\n\n    # Create a formatted string for the model and data parameters\n    model_params = \\\"\\\"\\\"\n    # Model Parameters\n    | Parameter | Value |\n    | --- | --- |\n    \\\"\\\"\\\"\n    data_params = \\\"\\\"\\\"\n    # Data Parameters\n    | Parameter | Value |\n    | --- | --- |\n    \\\"\\\"\\\"\n    for param_name, param_value in ModelConfig().__dict__.items():\n        model_params += \\\"    | %s | %s |\\", "CodeBleu_score": 14.858465212986479}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    # Import parameter configurations\n    from config.data_configs import DataConfigs\n    from config.model_configs import ModelConfigs\n\n    # Create the parameter string\n    params = \"<h2>Parameters</h2>\"\n\n    # Iterate through the model parameters\n    for param_name in [attr for attr in dir(ModelConfigs) if not attr.startswith(\"__\")]:\n        value = getattr(ModelConfigs, param_name)\n        params += \"<b>%s</b>: %s<br>\" % (param_name, value)\n\n    # Iterate through the data parameters\n    for param_name in [attr for attr in dir(DataConfigs) if not attr.startswith(\"__\")]:\n        value = getattr(DataConfigs, param_name)\n        params += \"<b>%s</b>: %s<br>\" % (param_name, value)\n\n    # Display the parameters in the visdom window\n    self.vis.text(params, opts={\"title\": \"Parameters\"})\n\n", "CodeBleu_score": 52.5225572948548}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    import umap\n\n    # Only show the first <max_speakers> speakers to avoid clutter\n    embeds = embeds[: max_speakers * utterances_per_speaker]\n    n_speakers = len(embeds) // utterances_per_speaker\n\n    # Flatten the embeddings for visualization\n    flattened_embeds = embeds.reshape((-1, embeds.shape[-1]))\n\n    # Assign a color to each speaker\n    colors = np.array([colormap[i] for i in range(n_speakers)])\n    colors = np.repeat(colors, utterances_per_speaker, axis=0)\n\n    # Reduce dimensionality using UMAP\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(flattened_embeds)\n\n    # Plot the embeddings\n    if out_fpath is not None:\n        plt.scatter(projected[:, 0], projected[:, 1], c=colors)\n        plt.title(f\"UMAP projection (step {step})\")\n        plt.savefig(out_fpath)\n        plt.clf()\n    else:\n        self.projection_win = self.vis.scatter(\n            X=projected,\n            Y=colors,\n            opts=dict(\n                legend=[\"Speaker\"],\n                markersize=5,\n                title=f\"UMAP projection (step {step})\",\n            ),\n            win=self.projection_win,\n        )\n", "CodeBleu_score": 57.33415271484107}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    import umap\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.lines import Line2D\n    from sklearn.manifold import TSNE\n    from sklearn.decomposition import PCA\n    embeds = embeds.detach().cpu().numpy()\n    embeds_flat = embeds.reshape((-1, embeds.shape[-1]))\n    n_speakers = len(utterances_per_speaker)\n    if n_speakers > max_speakers:\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n        embeds_flat = embeds_flat[:max_speakers * utterances_per_speaker[0]]\n    labels = np.repeat(range(len(utterances_per_speaker)), utterances_per_speaker)\n    speakers = [f\"Speaker {s}\" for s in range(len(utterances_per_speaker))]\n    reducer = umap.UMAP(random_state=42)\n    projections = reducer.fit_transform(embeds_flat)\n    plt.figure(figsize=(10, 8))\n    plt.title(f\"UMAP: {step} steps\")\n    for i, speaker in enumerate(speakers):\n        indices = labels == i\n        plt.scatter(projections[indices, 0], projections[indices, 1], label=speaker, s=1)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.tight_layout()\n    plt.xticks([]), plt.yticks([])\n    plt.legend(loc=\"upper right\")\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n    plt.clf()\n    plt.close()\n    if not self.disabled:\n        self.projection_win = self.vis.matplot(plt, win=self", "CodeBleu_score": 47.71184944166198}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    import umap\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.preprocessing import minmax_scale\n    from random import shuffle\n    embeds = embeds.detach().cpu().numpy()\n    embeds = minmax_scale(embeds)\n    speakers = np.repeat(np.arange(len(utterances_per_speaker)), utterances_per_speaker)\n    if len(speakers) > max_speakers:\n        speakers = np.random.choice(speakers, max_speakers, replace=False)\n    colors = [colormap[x] for x in speakers]\n    reducer = umap.UMAP(n_components=2, min_dist=0.15, metric=\"cosine\")\n    projected = reducer.fit_transform(embeds)\n    plt.figure(figsize=(12, 8))\n    for spk in np.unique(speakers):\n        idx = np.where(speakers == spk)[0]\n        plt.scatter(projected[idx, 0], projected[idx, 1], s=1, c=[colors[spk]])\n    plt.title(f\"UMAP projection (step {step})\")\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        plt.close()\n    else:\n        self.projection_win = self.vis.matplot(plt, win=self.projection_win)\n        plt.close()\n", "CodeBleu_score": 52.172446829130294}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_num_frames, x_embed_dim = x.size()[:-1]\n    speaker_embed_dim = speaker_embedding.size()[-1]\n    # (B, T_x, 1)\n    speaker_embedding = speaker_embedding.unsqueeze(1)\n    # (B, T_x, speaker_embed_dim)\n    speaker_embedding = speaker_embedding.repeat(1, x_num_frames, 1)\n    # (B, T_x, speaker_embed_dim) -> (B, T_x, speaker_embed_dim)\n    speaker_embedding = speaker_embedding.reshape(x_num_frames, x_embed_dim)\n    # (B, T_x, speaker_embed_dim) -> (B, speaker_embed_dim, T_x)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    # (B, speaker_embed_dim + embed_dim, T_x)\n    x = torch.cat((x, speaker_embedding), dim=1)\n    return x\n\n", "CodeBleu_score": 38.97148294720473}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    seq_len, n_conv_outputs, hidden_size = x.size()\n    seq_len = seq_len\n    n_conv_outputs = n_conv_outputs\n    hidden_size = hidden_size\n    if speaker_embedding is not None:\n        x = x.transpose(0, 1)\n        speaker_embedding_size = speaker_embedding.size()\n        speaker_embedding_size = speaker_embedding_size\n        speaker_embedding = speaker_embedding.expand(seq_len, -1, -1)\n        x = torch.cat((x, speaker_embedding), dim=2)\n        x = x.transpose(0, 1)\n    return x\n\n", "CodeBleu_score": 36.91637456848484}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    seq_len, batch_size, _ = x.size()\n    seq_len = int(seq_len)\n    batch_size = int(batch_size)\n    spk_emb_expanded = speaker_embedding.expand(seq_len, -1, -1)\n    x = torch.cat((x, spk_emb_expanded), dim=-1)\n    return x\n\n", "CodeBleu_score": 12.651871563558666}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    b, t, c = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(b, t, device=device)\n    self.attention = torch.zeros(b, t, device=device)\n\n", "CodeBleu_score": 53.156838929412864}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    b, t, c = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(b, t, device=device)\n    self.attention = torch.zeros(b, t, device=device)\n\n", "CodeBleu_score": 53.156838929412864}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    b, t, c = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(b, t, device=device)\n    self.attention = torch.zeros(b, t, device=device)\n\n", "CodeBleu_score": 53.156838929412864}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    ckpt_dict = torch.load(path, map_location=device)\n    self.load_state_dict(ckpt_dict['model_state'])\n    if optimizer:\n        optimizer.load_state_dict(ckpt_dict['optimizer_state'])\n\n", "CodeBleu_score": 27.561826671887545}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    ckpt = torch.load(path, map_location=device)\n    self.load_state_dict(ckpt['model_state'])\n    if optimizer is not None:\n        optimizer.load_state_dict(ckpt['optimizer_state'])\n    self.step = ckpt['step']\n\n", "CodeBleu_score": 32.472803002965684}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device  # use same device as parameters\n    ckpt_dict = torch.load(path, map_location=device)\n    self.load_state_dict(ckpt_dict[\\'model\\'])\n    if optimizer:\n        optimizer.load_state_dict(ckpt_dict[\\'optimizer\\'])\n\n", "CodeBleu_score": 24.57504502464907}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\"model_state\": self.state_dict(), \"optimizer_state\": optimizer.state_dict()}, str(path))\n    else:\n        torch.save({\"model_state\": self.state_dict()}, str(path))\n\n", "CodeBleu_score": 69.9144528494304}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\"model_state\": self.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict()}, str(path))\n    else:\n        torch.save({\"model_state\": self.state_dict()}, str(path))\n\n", "CodeBleu_score": 72.2179387730591}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n    m = _curly_re.match(text)\n    if not m:\n        sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n        break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n    # Append EOS token:\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 77.47481770016314}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            clean_text = _clean_text(text, cleaner_names)\n            if cmudict is not None:\n                clean_text = [word for word in clean_text.split(\" \") if word != \"\"]\n                sequence += _arpabet_to_sequence(_check_cmudict(clean_text))\n            else:\n                sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token:\n    sequence.append(_symbol_to_id[\\\"~\\\"])\n    return sequence\n\n", "CodeBleu_score": 60.88587641505949}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token:\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n\n\n", "CodeBleu_score": 77.20973299761631}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \\\"{\\\" and s[-1] == \\\"}\\\":\n                s = \\\"{%s}\\\" % s[1:-1].replace(\\\" \\\", \\\"\\\")\n            result += s\n    return result.replace(\\\"}{ \\\", \\\" \\\")\n\n\n\n", "CodeBleu_score": 51.47985338397979}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \\\"{\\\" and s[-1] == \\\"}\\\":\n                s = \\\"{%s}\\\" % s[1:-1].replace(\\\" \\\", \\\"\\\")\n            result += s\n    return result.replace(\\\"}{}\\\", \\\" \\\")\n\n", "CodeBleu_score": 51.47985338397979}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \\\"{\\\" and s[-1] == \\\"}\\\":\n                s = \\\"{%s}\\\" % s[1:-1].replace(\\\" \\\", \\\"\\\")\n            result += s\n    return result.replace(\\\"}{}\\\", \\\" \\\")\n\n", "CodeBleu_score": 51.47985338397979}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Create the synthesizer directory if it doesn't exist yet\n    if not synthesizer_root.exists():\n        synthesizer_root.mkdir(parents=True)\n\n    # Generate speaker embeddings\n    embed_dir = synthesizer_root.joinpath(\\\"embeds\\\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Create a metadata file if it doesn't exist yet\n    metadata_fpath = synthesizer_root.joinpath(\\\"train.txt\\\")\n    if not metadata_fpath.exists():\n        with metadata_fpath.open(\\\"w\\\", encoding=\\\"utf-8\\\") as metadata_file:\n            for dataset in [librispeech_datasets, vctk_datasets]:\n    for dataset_name, dataset_path in dataset_paths.items():\n        if not dataset_path.exists():\n            print(\\\"%s dataset is not found. Skipping. (See datasets.py for more information.)\\\" % dataset_name)\n            continue\n        with dataset_path.joinpath(\\\"metadata.csv\\\").open(\\\"r\\\", encoding=\\\"utf-8\\\") as metadata_file:\n            metadata = [line.split(\\\"|\\\") for line in metadata_file][1:]\n        for m in metadata:\n            m[0] = dataset_name + \\\"|\\\" + m[0]\n            metadata_file.write(\\\"|\\\".join(m))\n\n    # Gather the input wave filepath and the target output embed filepath\n    with metadata_fpath.open(\\\"r\\\", encoding=\\\"utf-8\\\") as metadata_file:\n        metadata = [line.split(\\\"|\\\") for line in metadata_file]\n    fpaths = [(wav_dir.joinpath(m[1]), embed_dir.joinpath(m[2])) for m in metadata]\n\n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model_fpath=encoder_model_", "CodeBleu_score": 41.98151291194582}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create the synthesizer directory if it doesn't exist yet\n    if not synthesizer_root.exists():\n        synthesizer_root.mkdir(exist_ok=True)\n\n    # Create the directories and files needed by the synthesizer\n    create_synthesizer_directories(synthesizer_root)\n\n    # Verify that the encoder model is present\n    if not encoder_model_fpath.exists():\n        raise Exception(\\\"You must copy the encoder model to %s before training the synthesizer.\\\" % synthesizer_root)\n\n    # Load the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\\\"train.txt\\\")\n    if not metadata_fpath.exists():\n        raise Exception(\\\"You must create a file train.txt with your training sentences in %s, by calling python prepare_dataset.py --wav-dir <path to your wavs> --datasets-root <path to your datasets>\\\" % synthesizer_root)\n\n    # Load the metadata\n    metadata = read_metadata(metadata_fpath)\n\n    # Create the speaker embeddings\n    create_speaker_embeddings(synthesizer_root, metadata, encoder_model_fpath, n_processes)\n\n", "CodeBleu_score": 30.045000767109414}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check for required directories and files\n    if not encoder_model_fpath.exists():\n        raise Exception(\\\"Error: encoder model not found. You need to download the pretrained encoder model\\\"\n               \\\"(\\\"%s\\\") and copy it into the synthesizer/saved_models/ directory.\\\" % encoder_model_fpath)\n    embed_dir = synthesizer_root.joinpath(\\\"embeds\\\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Gather the input wave filepath and the target output embed filepath\n    with meta_file_reader(synthesizer_root.joinpath(\\\"train.txt\\\")) as metadata:\n        fpaths = [(wav_path, embed_dir.joinpath(\\\"%s_embedding.npy\\\" % wav_path.stem)) for wav_path, _ in metadata]\n\n    # Create the process pool to encode the utterances in parallel\n    with Pool(n_processes) as pool:\n        futures = [pool.submit(embed_utterance, fpath, encoder_model_fpath) for fpath in fpaths]\n        rets = [future.result() for future in tqdm(futures)]\n        lengths = [ret[4] for ret in rets]\n\n    frames = sum(lengths)\n    hours = frames * hparams.frame_shift_ms / (3600 * 1000)\n    print(\\\"Wrote %d utterances, %d frames (%.2f hours)\\\" % (len(lengths), frames, hours))\n\n\n\n", "CodeBleu_score": 36.349414640622925}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    attention_path = plot_dir / f\\\"att-step{step}.png\\\"\n    save_attention(attention, attention_path)\n\n    mel_prediction_path = mel_output_dir / f\\\"mel-prediction-step{step}.npy\\\"\n    np.save(mel_prediction_path, mel_prediction)\n\n    mel_target_path = mel_output_dir / f\\\"mel-target-step{step}.npy\\\"\n    np.save(mel_target_path, target_spectrogram)\n\n    fig = plot_spectrogram(target_spectrogram, prediction=mel_prediction,\n                           attention=attention, step=step, plot_dir=plot_dir,\n                           target_spectrogram=target_spectrogram,\n                           hparams=hparams)\n    fig.savefig(plot_dir / f\\\"mel-spectrogram-step{step}.png\\\")\n    plt.close()\n\n    wav_path = wav_dir / f\\\"wav-step{step}.wav\\\"\n    wav = inv_spectrogram(mel_prediction)\n    save_wav(wav, wav_path)\n\n    print(\\\"Saved outputs at step %d\\\" % step)\n    print(\\\"\\\")\n\n", "CodeBleu_score": 25.638426226374555}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    attention_path = plot_dir / f\\\"attention-{step}-{sample_num}.png\\\"\n    plot_spectrogram(attention, attention_path, title=\\\"Attention\\\", ylabel=\\\"Spectrogram\\\")\n\n    mel_prediction_path = mel_output_dir / f\\\"mel-{step}-{sample_num}.png\\\"\n    plot_spectrogram(mel_prediction, mel_prediction_path, title=\\\"Predicted Mel-Spectrogram\\\", ylabel=\\\"log(mel)\\\")\n\n    wav_path = wav_dir / f\\\"wav-{step}-{sample_num}.wav\\\"\n    wav = inv_spectrogram(mel_prediction.T)\n    save_wav(wav, wav_path, hparams)\n\n    # Save the plot of predicted vs target mel spectrogram\n    mel_target_path = mel_output_dir / f\\\"mel-target-{step}-{sample_num}.png\\\"\n    plot_spectrogram(target_spectrogram, mel_target_path, title=\\\"Target Mel-Spectrogram\\\", ylabel=\\\"log(mel)\\\")\n\n    fig, ax = plt.subplots(figsize=(12, 3))\n    img = librosa.display.specshow(target_spectrogram, y_axis=\\\"mel\\\", x_axis=\\\"time\\\", ax=ax)\n    ax.set_title(\\\"Predicted Mel-Spectrogram\\\")\n    fig.colorbar(img, ax=ax, format=\\\"%+2.0f dB\\\")\n\n    fig, ax = plt.subplots(figsize=(12, 3))\n    img = librosa.display.specshow(mel_prediction, y_axis=\\\"mel\\\", x_axis=\\\"time\\\", ax=ax)\n    ax.set_title(\\\"Target Mel-Spectrogram\\\")\n    fig.colorbar(img, ax=ax, format=\\\"%+2.0f dB\\\")\n\n    plot_path = plot_dir / f\\\"mel-comp-{step}-{sample_", "CodeBleu_score": 33.37366168142164}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot the attention\n    attention_path = os.path.join(plot_dir, f\\\"att-step{step}.png\\\")\n    plot_attention(attention, attention_path, \\\"Tacotron\\\")\n\n    # Save the predicted mel spectrogram\n    mel_output_path = os.path.join(mel_output_dir, f\\\"mel-output-step{step}-sample{sample_num}.npy\\\")\n    np.save(mel_output_path, mel_prediction, allow_pickle=False)\n\n    # Save the reconstructed waveform\n    wav_path = os.path.join(wav_dir, f\\\"wav-step{step}-sample{sample_num}.wav\\\")\n    wav = audio.inv_mel_spectrogram(mel_prediction, hparams)\n    audio.save_wav(wav, wav_path, hparams.sample_rate)\n\n    # Save the target mel spectrogram\n    target_mel_path = os.path.join(mel_output_dir, f\\\"mel-target-step{step}-sample{sample_num}.npy\\\")\n    np.save(target_mel_path, target_spectrogram, allow_pickle=False)\n\n    # Plot the predicted and target mel spectrograms\n    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n    axes[0].imshow(target_spectrogram, aspect=\\\"auto\\\", origin=\\\"lower\\\", interpolation=\\\"none\\\")\n    axes[1].imshow(mel_prediction, aspect=\\\"auto\\\", origin=\\\"lower\\\", interpolation=\\\"none\\\")\n    fig.suptitle(f\\\"{hparams.tts_model} - {datetime.now().strftime('%H:%M')}, Step {step}, Loss {loss.item():.4f}\\\")\n    plt.savefig(os.path.join(plot_dir, f\\\"mel-comparison-step{step}-sample{sample_num}.png\\\"))\n    plt.close()\n\n    #", "CodeBleu_score": 37.46406846876857}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params /= 1000000\n    print('Number of trainable parameters: %.3fM' % num_params)\n\n", "CodeBleu_score": 18.643134624747507}, {"code": "def num_params(self)\n\t\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    total_params = total_params / 1e6\n    print('Trainable Params: %.3fM' % total_params)\n\n", "CodeBleu_score": 18.74353592751933}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in params])\n    return \"{:.3f}M\".format(params / 1e6)\n\n", "CodeBleu_score": 34.69075850097509}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Calculate the total size of the tensor after padding\n    if side == 'both':\n        new_size = x.shape[1] + pad * 2\n    elif side == 'after':\n        new_size = x.shape[1] + pad\n    elif side == 'before':\n        new_size = x.shape[1] + pad\n    else:\n        raise ValueError(\"side must be 'before', 'after', or 'both'\")\n\n    # Create a new tensor filled with zeros on either the CPU or GPU\n    if torch.cuda.is_available():\n        new_x = torch.zeros(x.shape[0], new_size, x.shape[2], device=x.device)\n    else:\n        new_x = torch.zeros(x.shape[0], new_size, x.shape[2])\n\n    # Place the original tensor into the new padded tensor according to the specified side\n    if side == 'after':\n        new_x[:, pad:, :] = x\n    elif side == 'before':\n        new_x[:, :-pad, :] = x\n    else:\n        new_x[:, pad:-pad, :] = x\n\n    return new_x\n\n", "CodeBleu_score": 43.5884185163668}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        l = x.size(0)\n        if l >= pad:\n            return x\n        else:\n            new_l = pad - l\n            if torch.cuda.is_available():\n                return torch.cat([torch.zeros((new_l // 2, *x.size()[1:])).cuda(), x, torch.zeros((new_l - new_l // 2, *x.size()[1:])).cuda()])\n            else:\n                return torch.cat([torch.zeros((new_l // 2, *x.size()[1:])), x, torch.zeros((new_l - new_l // 2, *x.size()[1:]))])\n    elif side == 'after':\n        l = x.size(0)\n        if l >= pad:\n            return x\n        else:\n            new_l = pad - l\n            if torch.cuda.is_available():\n                return torch.cat([x, torch.zeros((new_l, *x.size()[1:])).cuda()])\n            else:\n                return torch.cat([x, torch.zeros((new_l, *x.size()[1:]))])\n    elif side == 'before':\n        l = x.size(0)\n        if l >= pad:\n            return x\n        else:\n            new_l = pad - l\n            if torch.cuda.is_available():\n                return torch.cat([torch.zeros((new_l, *x.size()[1:])).cuda(), x])\n            else:\n                return torch.cat([torch.zeros((new_l, *x.size()[1:])), x])\n    else:\n        raise ValueError(\"side argument must be one of 'before', 'after', or 'both'\")\n\n", "CodeBleu_score": 25.78271198379969}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Calculate the total size of the tensor after padding\n    length = x.size(0)\n    total = max(length, pad)\n\n    # Determine the padding size for each side\n    if side == 'both':\n        pl = (total - length) // 2\n        pr = total - length - pl\n    elif side == 'after':\n        pl = 0\n        pr = total - length\n    elif side == 'before':\n        pl = total - length\n        pr = 0\n    else:\n        raise ValueError(\"Padding side must be one of 'before', 'after', or 'both'.\")\n\n    # Create a new tensor filled with zeros on the appropriate device\n    if torch.cuda.is_available():\n        padded = torch.zeros((total, x.size(1), x.size(2)), dtype=x.dtype).cuda()\n    else:\n        padded = torch.zeros((total, x.size(1), x.size(2)), dtype=x.dtype).cpu()\n\n    # Place the original tensor into the padded tensor\n    if side == 'both':\n        padded[pl:pl + length, :, :] = x\n    elif side == 'after':\n        padded[:length, :, :] = x\n    elif side == 'before':\n        padded[-length:, :, :] = x\n\n    return padded\n\n", "CodeBleu_score": 41.49929176108588}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 62.46286889331226}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 62.46286889331226}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    from matplotlib.ticker import MaxNLocator\n\n    # Reshape the attention matrix to a 2D array\n    attn = attn.reshape(attn.shape[1], attn.shape[-1])\n\n    # Create a figure and plot the attention matrix\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(attn, cmap='viridis', vmin=0, vmax=1)\n\n    # Set the aspect ratio and turn off the frame\n    ax.set_aspect('auto')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Add a colorbar\n    fig.colorbar(ax=ax)\n\n    # Save the figure as an image file\n    plt.savefig(path, bbox_inches='tight')\n\n    # Close the figure\n    plt.close(fig)\n\n", "CodeBleu_score": 57.19883553069101}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color(\"grey\")\n    ax.yaxis.label.set_color(\"grey\")\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis=\"x\", colors=\"grey\", labelsize=23)\n    ax.tick_params(axis=\"y\", colors=\"grey\", labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 64.9396240586664}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer\n        writer = csv.DictWriter(csvfile, fieldnames=['image_name', 'prediction', 'confidence'])\n\n        # Write the header if the file is new\n        if csvfile.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 49.18914304448004}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        'image_name': image_name,\n        'prediction': prediction,\n        'confidence': confidence\n    }\n    with open('predictions.csv', mode='a', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        if file.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n\n", "CodeBleu_score": 51.18585705044083}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \\\"Image\\\": image_name,\n        \\\"Prediction\\\": prediction,\n        \\\"Confidence\\\": confidence\n    }\n    file_exists = os.path.isfile(csv_path)\n    with open(csv_path, \\\"a\\\", newline=\\\"\\\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n        if not file_exists:\n            writer.writeheader()\n        writer.writerow(data)\n\n\n", "CodeBleu_score": 49.9232135844651}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"", "CodeBleu_score": 6.951754385967379}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"", "CodeBleu_score": 6.951754385967379}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")  # filename\n    if optimize:\n        LOGGER.info(f\"{prefix} using optimize=True, which optimizes for mobile gpu inference\")\n        ts = torch.jit.optimize_for_mobile(torch.jit.trace(model, im))\n    else:\n        LOGGER.info(f\"{prefix} using optimize=False, which optimizes for cpu inference\")\n        ts = torch.jit.trace(model, im)\n    torch.jit.save(ts, f)\n    LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n    return f, None\n\n", "CodeBleu_score": 44.59284658966398}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools>=6.0\")  # CoreML tools\n    import coremltools as ct\n    import coremltools.proto.FeatureTypes_pb2 as ft\n    from coremltools.models.neural_network import quantization_utils\n    from coremltools.models.neural_network.quantization_utils import AdvancedQuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import AdvancedQuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import QuantizedLayerSelector\n   ", "CodeBleu_score": 13.589251828650667}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools>=6.0\")  # CoreML quantization and NMS support\n    import coremltools as ct\n    from coremltools.models.neural_network import quantization_utils\n    from coremltools.models.neural_network import quantization_utils as cu\n\n    LOGGER.info(f\"", "CodeBleu_score": 12.24276735218385}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools>=6.0\")  # CoreML export\n    import coremltools as ct\n    import torch\n    from coremltools.models.neural_network import quantization_utils\n    from coremltools.models.neural_network.quantization_utils import AdvancedQuantizedLayerSelector\n\n    LOGGER.info(f\"", "CodeBleu_score": 12.139723640821995}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[_][0], input_ranges[_][1]))\n    return individual\n\n", "CodeBleu_score": 41.79375459387889}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[_][0], input_ranges[_][1]))\n    return individual\n\n", "CodeBleu_score": 41.79375459387889}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        # For discrete-time DPMs, we use a piecewise linear interpolation to compute the log(alpha_t)\n        log_alpha_t = self.log_alpha_array.gather(-1, (t * self.total_N).long())\n        return log_alpha_t\n    elif self.schedule == 'linear':\n        # For continuous-time DPMs, we use a quadratic expression to compute the log(alpha_t)\n        log_alpha_t = -1.5 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        return log_alpha_t\n    else:\n        # For continuous-time DPMs, we apply a cosine-based formula to compute the log(alpha_t)\n        log_alpha_t = -math.log(math.cos((t + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.)) + self.cosine_log_alpha_0\n        return log_alpha_t\n\n", "CodeBleu_score": 64.07311771568843}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        return self.log_alpha_array[:, int(t.item() * self.total_N)].reshape((-1, 1))\n    elif self.schedule == 'linear':\n        log_alpha = - 0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        return log_alpha.reshape((-1, 1))\n    elif self.schedule == 'cosine':\n        import math\n        log_alpha = torch.log(torch.tensor(math.cos((t + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))) - self.cosine_log_alpha_0\n        return log_alpha.reshape((-1, 1))\n\n", "CodeBleu_score": 67.55895642441148}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        t_floor = t.floor()\n        t_ceil = t_floor + 1\n        alpha_floor = torch.exp(self.log_alpha_array[:, t_floor.long()])\n        alpha_ceil = torch.exp(self.log_alpha_array[:, t_ceil.long()])\n        alpha_t = alpha_floor + (t - t_floor) * (alpha_ceil - alpha_floor)\n        log_alpha_t = torch.log(alpha_t)\n    elif self.schedule == 'linear':\n        beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n        log_alpha_t = - 0.5 * torch.log(beta_t)\n    elif self.schedule == 'cosine':\n        log_alpha_t = self.cosine_log_alpha_0 - torch.log(self.cosine_beta_max * torch.cos((t + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.) ** 2 + 1. - self.cosine_s)\n    return log_alpha_t\n\n", "CodeBleu_score": 52.302915383903546}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        if self.beta_0 == self.beta_1:\n            return lamb\n        return (torch.sqrt(self.beta_0 ** 2 + 2. * (self.beta_1 - self.beta_0) * torch.exp(2. * lamb)) - self.beta_0) / (self.beta_1 - self.beta_0)\n    elif self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    else:\n        log_alpha = torch.log(torch.cos((lamb + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.)) - self.cosine_log_alpha_0\n        return torch.arccos(torch.exp(log_alpha)) / ((1. + self.cosine_s) * math.pi / 2.)\n\n", "CodeBleu_score": 37.50710366886737}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        return (torch.sqrt(self.beta_0 ** 2 + 2. * (lamb + self.beta_0 * self.beta_1)) - self.beta_0) / self.beta_1\n    elif self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'cosine':\n        log_alpha = torch.log(torch.cos((torch.arccos(torch.exp(-0.5 * lamb)) / self.cosine_s) * self.cosine_s))\n        return interpolate_fn(log_alpha.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n\n", "CodeBleu_score": 31.673681516576856}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        return torch.sqrt(1. / (1. + self.beta_0 ** 2 - 2. * self.beta_0 * (torch.exp(2. * lamb) + 1.)))\n    elif self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    else:\n        log_alpha = torch.log(torch.cos((torch.arccos(torch.exp(-0.5 * lamb)) / (self.cosine_s + 1.) * math.pi / 2.) - self.cosine_s / (self.cosine_s + 1.) * math.pi / 2.))\n        return (log_alpha - self.cosine_log_alpha_0) / self.cosine_s\n\n    def marginal_log_mean_coeff(self, t):\n        \\\"\\\"\\\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \\\"\\\"\\\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\\", "CodeBleu_score": 39.765916204917}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t_input = get_model_input_time(t_continuous, noise_schedule)\n    t_input = torch.full((batch_size,), t_input, device=x.device, dtype=x.dtype)\n    if model_type == \"noise\":\n        model_output = model(x, t_input, **model_kwargs)\n        return model_output\n    elif model_type == \"x_start\":\n        model_output = model(x, t_input, **model_kwargs)\n        return model_output * noise_schedule.beta_t(t_continuous) + x * noise_schedule.alpha_bar_t(t_continuous) * (1. - noise_schedule.beta_t(t_continuous))\n    elif model_type == \"v\":\n        model_output = model(x, t_input, **model_kwargs)\n        return model_output * noise_schedule.alpha_bar_t(t_continuous)\n    elif model_type == \"score\":\n        model_output = model(x, t_input, **model_kwargs)\n        return -model_output * noise_schedule.beta_t(t_continuous) / noise_schedule.alpha_t(t_continuous)\n    else:\n        raise ValueError(f\"Unknown model_type: {model_type}\")\n\n", "CodeBleu_score": 34.65704651668207}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t = get_model_input_time(t_continuous, noise_schedule)\n    t_input = torch.ones(batch_size, device=x.device, dtype=torch.long) * t\n    if model_type == 'noise':\n        model_output = model(x, t_input, **model_kwargs)\n        return model_output\n    elif model_type == 'x_start':\n        model_output = model(x, t_input, **model_kwargs)\n        return model_output\n    elif model_type == 'v':\n        model_output = model(x, t_input, **model_kwargs)\n        pred_x_0 = (x - noise_schedule.beta_0 * model_output) / (torch.sqrt(1. - noise_schedule.alpha_bar(t_continuous)))\n        return pred_x_0\n    elif model_type == 'score':\n        model_output = model(x, t_input, **model_kwargs)\n        pred_x_0 = (x - noise_schedule.sigma_hat(t_continuous) * model_output) / torch.sqrt(noise_schedule.alpha_bar(t_continuous))\n        return pred_x_0\n    else:\n        raise ValueError(f'Invalid model_type: {model_type}')\n\n", "CodeBleu_score": 38.10017117946218}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t_input = get_model_input_time(t_continuous, noise_schedule)\n    t_input = torch.full((batch_size,), t_input, device=x.device, dtype=torch.long)\n    if model_type == 'noise':\n        noise_pred = model(x, t_input, **model_kwargs)\n        noise_pred = noise_pred.to(x.dtype)\n    elif model_type == 'x_start':\n        x_start_pred = model(x, t_input, **model_kwargs)\n        noise_pred = (x - x_start_pred) / extract(noise_schedule.sigma_t, t_input, x.shape)\n    elif model_type == 'v':\n        v_pred = model(x, t_input, **model_kwargs)\n        noise_pred = (x - extract(noise_schedule.alpha_t, t_input, x.shape) * v_pred) / extract(noise_schedule.sigma_t, t_input, x.shape)\n    elif model_type == 'score':\n        score_pred = model(x, t_input, **model_kwargs)\n        noise_pred = -score_pred / extract(noise_schedule.sigma_t, t_input, x.shape)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n    return noise_pred\n\n", "CodeBleu_score": 33.5191932507154}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.noise_schedule.marginal_std(torch.zeros(x0.shape[0]).to(x0.device))\n    s = x0.shape\n    if self.thresholding:\n        if self.variant == 'bh1':\n            t = t.reshape((-1, 1, 1, 1))\n            x0 = x0 / t\n        elif self.variant == 'bh2':\n            t = t.reshape((-1, 1, 1, 1))\n            x0 = x0 / t\n        else:\n            x0 = x0 / self.noise_schedule.marginal_std(torch.zeros(x0.shape[0]).to(x0.device)).reshape((-1, 1, 1, 1))\n        x0 = torch.clamp(x0, -self.max_val, self.max_val)\n    if self.variant == 'bh1':\n        x0 = x0 * t\n    elif self.variant == 'bh2':\n        x0 = x0 * t\n    else:\n        x0 = x0 * self.noise_schedule.marginal_std(torch.zeros(x0.shape[0]).to(x0.device)).reshape((-1, 1, 1, 1))\n    assert x0.shape == s\n    return x0\n\n", "CodeBleu_score": 32.043673548321024}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = torch.zeros(x0.shape[0], device=x0.device)\n    with torch.no_grad():\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), 0.8, dim=1)\n        s = s / self.max_val\n        s = s.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        s = s.expand(x0.shape)\n        x0 = torch.clamp(x0 / s, -1, 1) * s\n    return x0\n\n", "CodeBleu_score": 48.9191527694542}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = torch.tensor(0.0, device=x0.device, dtype=x0.dtype)\n    if self.thresholding:\n        N = x0.shape[0]\n        L = x0.view(N, -1).shape[1]\n        q = max(1, L // 1000)\n        s = torch.abs(x0).view(N, -1).topk(q, dim=1)[0]\n        s = torch.mean(s, dim=1, keepdim=True)\n        s = s / self.max_val\n        x0 = torch.clamp(x0 / s, -self.max_val, self.max_val)\n    return x0\n\n", "CodeBleu_score": 40.60374968581246}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.predict_x0:\n        x0 = self.model(x, t)\n        if self.thresholding:\n            x0 = self.dynamic_thresholding_fn(x0, t)\n        return x0\n    else:\n        return self.model(x, t)\n\n", "CodeBleu_score": 13.79098857079056}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.thresholding:\n        x = self.dynamic_thresholding_fn(x, t)\n    return x\n\n", "CodeBleu_score": 9.50416757276741}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    if isinstance(input, dict):\n        input = input[\"sample\"]\n    if isinstance(input, list):\n        input = input[0]\n    if isinstance(sigma_in, list):\n        sigma_in = sigma_in[0]\n    if isinstance(sigma_in, float):\n        sigma_in = torch.tensor(sigma_in, device=input.device)\n    if isinstance(sigma_in, int):\n        sigma_in = torch.tensor(sigma_in, device=input.device)\n    if isinstance(sigma_in, torch.Tensor):\n        sigma_in = sigma_in.to(input.device)\n    if isinstance(sigma_in, np.ndarray):\n        sigma_in = torch.from_numpy(sigma_in).to(input.device)\n    if isinstance(sigma_in, torch.Tensor):\n        sigma_in = sigma_in.to(input.device)\n    sigma_in = sigma_in.flatten()\n    if len(sigma_in.shape) == 0:\n        sigma_in = sigma_in.expand(input.shape[0])\n    if len(sigma_in.shape) == 1:\n        if sigma_in.shape[0] == 1:\n            sigma_in = sigma_in.expand(input.shape[0])\n        else:\n            sigma_in = sigma_in.view(-1, 1)\n            sigma_in = sigma_in.expand(-1, input.shape[1])\n    if len(sigma_in.shape) == 2:\n        sigma_in = sigma_in.view(-1, 1)\n        sigma_in = sigma_in.expand(-1, input.shape[1])\n    sigma_in = sigma_in.to(input.device)\n    adjusted_sigma = sigma_in\n    timesteps = sigma_in\n    model_output = model(input * adjusted_sigma, timesteps, **kwargs)\n    return model_output\n\n", "CodeBleu_score": 28.941091665109415}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    if isinstance(sigma_in, float):\n        sigma_in = torch.tensor(sigma_in)\n    if isinstance(sigma_in, torch.Tensor):\n        sigma_in = sigma_in.to(input.device)\n    if sigma_in.shape == torch.Size([]):\n        sigma_in = sigma_in.repeat(input.shape[0])\n    sigma_in = sigma_in.view(-1, 1, 1, 1)\n    sigma_in = sigma_in.to(input.device)\n    scaled_input = input / sigma_in\n    eps = model(scaled_input, sigma_in, **kwargs)\n    return input + sigma_in * eps\n\n", "CodeBleu_score": 35.08546210060287}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    if sigma_in is None:\n        return model(input, **kwargs)\n    else:\n        sigma_in = sigma_in.to(input.device)\n        sigma_in = expand_dims(sigma_in, input.ndim)\n        adjusted_sigma = sigma_in * (1.0 if model.sigma_data == 1.0 else (sigma_in / model.sigma_data) ** 2)\n        eps = model((input / adjusted_sigma), **kwargs)\n        return eps * adjusted_sigma\n\n", "CodeBleu_score": 30.068340639443825}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(retain_graph=True)\n    integrand = (taus.grad * alpha).detach()\n    return integrand\n", "CodeBleu_score": 14.060407680460957}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.requires_grad = True\n    taus.requires_grad = True\n    log_alpha.backward()\n    integrand = (taus.grad * alpha).detach()\n    return integrand\n", "CodeBleu_score": 29.691503949329658}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.requires_grad_(True)\n    log_alpha.retain_grad()\n    log_alpha.backward()\n    return (log_alpha.data * log_alpha.grad).detach()\n", "CodeBleu_score": 19.574525740274904}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        if sigma_down > 0:\n            x = x + (sigma_down - sigmas[i]) * d\n        x = x + noise_sampler(x, sigma_up) * s_noise\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i + 1], 'sigma_hat': sigmas[i + 1], 'denoised': denoised})\n    return x\n\n", "CodeBleu_score": 66.06223582009504}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        sigma_hat = sigmas[i] + sigma_up\n        if s_noise != 1 or sigma_up != 0:\n            x = x + noise_sampler(x, sigma_hat) * s_noise * sigma_up\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigma_down - sigma_hat\n        # Euler method\n        x = x + d * dt\n    return x\n\n", "CodeBleu_score": 61.697824531157394}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_down': sigma_down, 'sigma_up': sigma_up, 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        x = x + noise_sampler(x, sigma_down) * s_noise\n        x = x + noise_sampler(x, sigmas[i + 1]) * (s_noise - s_noise * (sigma_up / sigmas[i + 1]) ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 68.44048135254096}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 1:\n        raise ValueError('order must be >= 1')\n    def product_term(k):\n        if k == j:\n            return 1\n        return (t - t[i] - k) / (t[i + order] - t[i])\n    integrated = integrate.quad(product_term, 0, order - 1)[0]\n    return integrated * np.polygamma(order + 1, j - i)\n\n", "CodeBleu_score": 31.156825207695395}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(f'Order {order} too high for step {i}')\n\n    def prod_term(k):\n        return t(j) / t(i + k + 1) if k != j else 0\n\n    return integrate.quad(prod_term, 0, order - 1)[0]\n\n", "CodeBleu_score": 31.600568693351356}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 1:\n        raise ValueError('order must be at least 1')\n\n    def prod_term(t, i, j):\n        return t ** order / (order * (t[i] - t[j]))\n\n    if order == 1:\n        return prod_term(t, i, j)\n    else:\n        numerator = prod_term(t, i, j)\n        denominator = 0\n        for k in range(j + 1, i + order):\n            term = prod_term(t, i, k)\n            for l in range(j, k):\n                term *= (t[i] - t[l]) / (t[k] - t[l])\n            denominator += term\n        return numerator / denominator\n\n", "CodeBleu_score": 26.017326389773928}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    with torch.enable_grad():\n        for i in trange(len(sigmas) - 1, disable=disable):\n            denoised = model(x, sigma_fn(sigmas[i]) * s_in, **extra_args)\n            if callback is not None:\n                callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n            d = (x - denoised) / sigma_fn(sigmas[i])\n            grad = (d * x.flatten(1).norm(dim=-1, keepdim=True) - noise_sampler(x, sigma_fn(sigmas[i]) * s_in) * sigma_fn(sigmas[i + 1])).flatten(1).norm(dim=-1, keepdim=True)\n            grad = grad * (sigma_fn(sigmas[i]) / sigma_fn(sigmas[i + 1]))\n            x = x + grad * (sigma_fn(sigmas[i + 1]) - sigma_fn(sigmas[i]))\n            if sigmas[i + 1] > 0:\n                x = x / sigma_fn(sigmas[i + 1])\n    return x\n\n", "CodeBleu_score": 62.64016432092656}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = sigmas[0]\n    for i in trange(len(sigmas), disable=disable):\n        x_in = x * sigma_fn(i)\n        denoised = model(x_in, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = denoised + (x - denoised) * sigma_fn(i + 1) / sigma_fn(i)\n    return x\n\n", "CodeBleu_score": 55.23278709815331}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            x = denoised\n        else:\n            x = x + sigmas[i + 1] * (denoised - x) / sigmas[i]\n    return x\n\n", "CodeBleu_score": 72.29113870530885}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    n_dims_to_append = target_dims - len(x.shape)\n    if n_dims_to_append < 0:\n        raise ValueError('Passed in tensor has more dimensions than target')\n    for _ in range(n_dims_to_append):\n        x = torch.unsqueeze(x, dim=0)\n    if x.device.type == 'mps':\n        return x.detach().clone()\n    else:\n        return x\n\n", "CodeBleu_score": 38.09397956915264}, {"code": "def append_dims(x, target_dims)\n\t\n    num_new_dims = target_dims - x.dim()\n    if num_new_dims < 0:\n        raise ValueError('target_dims must be greater than the current number of dimensions')\n    elif num_new_dims == 0:\n        return x\n    else:\n        return x[(...,) + (None,) * num_new_dims].expand_as(x).detach().clone()\n\n", "CodeBleu_score": 28.022748814705693}, {"code": "def append_dims(x, target_dims)\n\t\n    ndims_to_append = target_dims - x.ndim\n    if ndims_to_append < 0:\n        raise ValueError(\n            f\"Target dimensions ({target_dims}) cannot be fewer than the current dimensions ({x.ndim}).\"\n        )\n    for _ in range(ndims_to_append):\n        x = x[..., None]\n    if x.device.type == \"mps\":\n        return x.detach().clone()\n    return x\n\n", "CodeBleu_score": 38.16195268352818}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as source, path.open('wb') as output:\n            shutil.copyfileobj(source, output)\n    if digest is not None:\n        file_hash = hashlib.sha256(path.read_bytes()).hexdigest()\n        if file_hash != digest:\n            raise RuntimeError(f'{path}: SHA-256 hash mismatch')\n    return path\n\n", "CodeBleu_score": 52.7842473393975}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if path.exists() and (digest is None or hashlib.sha256(path.read_bytes()).hexdigest() == digest):\n        return path\n    with urllib.request.urlopen(url) as source, path.open('wb') as output:\n        shutil.copyfileobj(source, output)\n    if digest is not None and hashlib.sha256(path.read_bytes()).hexdigest() != digest:\n        path.unlink()\n        raise RuntimeError('model has been downloaded but the SHA256 checksum does not not match')\n    return path\n\n", "CodeBleu_score": 40.77259087328322}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if path.exists() and path.is_file():\n        if digest is not None:\n            if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n                return path\n            path.unlink()\n    with urllib.request.urlopen(url) as source, path.open('wb') as output:\n        shutil.copyfileobj(source, output)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest() != digest:\n            raise RuntimeError('invalid hash')\n    return path\n\n", "CodeBleu_score": 42.66985757699653}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    modes = {}\n    for module in model.modules():\n        modes[module] = module.training\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in modes.items():\n            module.train(mode)\n\n", "CodeBleu_score": 38.52726477911279}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = {}\n    for module in model.modules():\n        modes[module] = module.training\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in modes.items():\n            module.train(mode)\n\n", "CodeBleu_score": 38.52726477911279}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = {}\n    try:\n        for module in model.modules():\n            modes[module] = module.training\n            module.train(mode)\n        yield model\n    finally:\n        for module, mode in modes.items():\n            module.train(mode)\n\n", "CodeBleu_score": 37.27036655787025}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    if decay > 1 or decay < 0:\n        raise ValueError(f'decay must be between 0 and 1, got {decay}')\n    if isinstance(model, nn.Module):\n        model = model.parameters()\n    if isinstance(averaged_model, nn.Module):\n        averaged_model = averaged_model.parameters()\n    for model_parameter, averaged_parameter in zip(model, averaged_model):\n        if isinstance(model_parameter, nn.Parameter):\n            if model_parameter.requires_grad:\n                averaged_parameter.copy_(model_parameter.lerp(averaged_parameter, 1 - decay))\n        else:\n            warnings.warn(f'{type(model_parameter)} is not a parameter, skipping')\n\n", "CodeBleu_score": 31.5782812574853}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    # Check if both model parameters and buffers have the same keys\n    assert model_params.keys() == averaged_params.keys()\n    for name, param in model_params.items():\n        if name in averaged_params:\n            averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    # Check if both model buffers have the same keys\n    assert model_buffers.keys() == averaged_buffers.keys()\n    for name, buf in model_buffers.items():\n        if name in averaged_buffers:\n            averaged_buffers[name].copy_(buf)\n\n", "CodeBleu_score": 78.97124063479176}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    # Check if both model classes match\n    if model.__class__ != averaged_model.__class__:\n        raise ValueError(f'averaged_model must match model in class, but model is {model.__class__} and averaged_model is {averaged_model.__class__}.')\n    # Get model state\n    model_state = model.state_dict()\n    averaged_model_state = averaged_model.state_dict()\n    # Check if both model parameters match\n    if model_state.keys() != averaged_model_state.keys():\n        raise ValueError('The model and averaged_model must match in parameters.')\n    # Check if both model buffers match\n    for buf_name in model.__dict__.keys():\n        if ('_buffers' in buf_name) and (model.__dict__[buf_name].keys() != averaged_model.__dict__[buf_name].keys()):\n            raise ValueError('The model and averaged_model must match in buffers.')\n    # EMA update\n    for name, param in model_state.items():\n        if 'num_batches_tracked' in name:\n            averaged_model_state[name].copy_(param)\n        else:\n            averaged_model_state[name].copy_(param * (1.0 - decay) + averaged_model_state[name] * decay)\n    # Copy BN running stats\n    for buf_name, buf in model.__dict__.items():\n        if 'num_batches_tracked' in buf_name:\n            averaged_model.__dict__[buf_name].copy_(buf)\n        else:\n            for bn_name in buf.keys():\n                averaged_model.__dict__[buf_name][bn_name].copy_(buf[bn_name])\n\n", "CodeBleu_score": 36.37527891288002}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_value = min_value.log()\n    max_value = max_value.log()\n    min_cdf = min_value.sigmoid()\n    max_cdf = max_value.sigmoid()\n    u = torch.rand(shape, dtype=dtype, device=device)\n    u = u * (max_cdf - min_cdf) + min_cdf\n    v = u.logit() * scale + loc\n    return v.exp()\n\n", "CodeBleu_score": 40.41362486867154}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_value = min_value.log()\n    max_value = max_value.log()\n    min_cdf = min_value.sigmoid()\n    max_cdf = max_value.sigmoid()\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    v = u.logit()\n    return v.mul(scale).add(loc).exp()\n\n\n", "CodeBleu_score": 44.13359466515684}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    min_value = min_value.log()\n    max_value = max_value.log()\n    min_cdf = min_value.sigmoid()\n    max_cdf = max_value.sigmoid()\n    u = torch.rand(shape, dtype=dtype, device=device) * (max_cdf - min_cdf) + min_cdf\n    return u.logit().mul_(scale).add_(loc).sigmoid().log().neg_().log()\n\n", "CodeBleu_score": 40.63006614604227}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"activation {activation} not supported\")\n\n", "CodeBleu_score": 40.69832709820199}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"activation {activation} not supported\")\n\n", "CodeBleu_score": 40.69832709820199}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels, alpha=1.0, alpha_trainable=True, alpha_logscale=True)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"activation {activation} not recognized\")\n\n", "CodeBleu_score": 39.02656115025889}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs is None:\n        return t\n    if t.shape[-1] % 2 == 1:\n        t = rearrange(t, '... (d j) -> ... d j', j=2)\n        t_rot, t_pass = t[..., :-1], t[..., -1:]\n    else:\n        t_rot, t_pass = t[..., ::2], t[..., 1::2]\n    (seq_len,) = t_rot.shape[:-1]\n    freqs = freqs.to(device=t.device)\n    t, t_pass = (t_rot.float(), t_pass.float())\n    freqs = freqs.to(dtype=t.dtype, device=t.device)\n    t = (t * scale * freqs.cos()) + (rotate_half(t) * scale * freqs.sin())\n    t = torch.cat((t, t_pass), dim=-1)\n    return t\n\n", "CodeBleu_score": 40.788611908457604}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.ndim == 1:\n        freqs = freqs.unsqueeze(0)\n    x = torch.view_as_real(t).float()\n    freqs = freqs.to(x)\n    x1, x2 = x[..., 0], x[..., 1]\n    rot1, rot2 = rotate_half(x1), rotate_half(x2)\n    x = torch.stack((rot1.real, rot1.imag, rot2.real, rot2.imag), dim=-1)\n    x = torch.view_as_complex(x.float().to(t.dtype))\n    return x * scale\n\n", "CodeBleu_score": 30.75805932009842}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs is None:\n        return t\n    seq_len, rotary_dim = freqs.shape\n    max_seq_len, num_channels = t.shape[-2:]\n    assert rotary_dim <= num_channels,\\\n        f'rotary dimension {rotary_dim} cannot be greater than channels {num_channels}'\n    assert seq_len <= max_seq_len,\\\n        f'sequence length {seq_len} cannot be greater than max sequence length {max_seq_len}'\n    t, t_pass = t[..., :seq_len, :].float(), t[..., seq_len:, :]\\\n        .float()\n    freqs = freqs.to(t)\n    t = (t * scale * freqs.cos()) + (rotate_half(t) * scale * freqs.sin())\n    t = torch.cat((t, t_pass), dim = -2)\n    return t\n\n", "CodeBleu_score": 40.2900680300604}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    quantized = self.codebook(idx)\n    if dim != -1:\n        quantized = quantized.transpose(dim, -1)\n    return quantized\n\n", "CodeBleu_score": 37.07548189806525}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).moveaxis(1, dim)\n\n", "CodeBleu_score": 18.52118445334911}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    quantized = self.codebook.weight[idx]\n    if dim != -1:\n        quantized = quantized.transpose(dim, -1)\n    return quantized\n\n", "CodeBleu_score": 30.780247396185047}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x, get_losses=True)\n        return x, (vq_loss, commit_loss), indices\n    else:\n        return x\n\n", "CodeBleu_score": 41.93462609922307}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x, get_losses=True)\n        return x, vq_loss, commit_loss, indices\n    return x\n\n", "CodeBleu_score": 39.99170580092055}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x, dim=1)\n        return x, (vq_loss, commit_loss), indices\n    else:\n        return x\n\n", "CodeBleu_score": 42.73865963225189}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n    else:\n        self.model_ema.store(self.parameters())\n        self.model_ema.copy_to(self.parameters())\n        if context is not None:\n            log.info(f\"{context} - Switching to EMA weights\")\n        try:\n            yield\n        finally:\n            self.model_ema.restore(self.parameters())\n            if context is not None:\n                log.info(f\"{context} - Restored model parameters\")\n\n", "CodeBleu_score": 71.19405111902563}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.model_ema(self, update_buffers=True)\n        if context is not None:\n            log.info(f\"{context}: using EMA weights\")\n        yield\n        self.model_ema.restore(self)\n        if context is not None:\n            log.info(f\"{context}: restored model weights from EMA\")\n    else:\n        yield\n\n", "CodeBleu_score": 48.07130335502114}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            self.log(f\"Using EMA weights for {context}\")\n        self.apply_ema_weights()\n        try:\n            yield\n        finally:\n            self.restore_original_weights()\n            if context is not None:\n                self.log(f\"Restored original weights for {context}\")\n    else:\n        yield\n\n", "CodeBleu_score": 44.529187456753824}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + modulate(attn, gate_msa, scale_msa=1.0)\n    x = self.norm2(x)\n    x = x + modulate(self.mlp(modulate(x, shift_mlp, scale_mlp)), gate_mlp, scale_mlp=1.0)\n    return x\n\n", "CodeBleu_score": 45.957777994360036}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + modulate(attn, gate_msa)\n    if not self.pre_only:\n        x = self.norm2(x)\n        x = x + modulate(self.mlp(modulate(self.norm1(x), shift_mlp, scale_mlp)), gate_mlp)\n    return x\n\n", "CodeBleu_score": 52.712582874789945}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + modulate(attn, gate_msa, 1.0)\n    if not self.pre_only:\n        x = self.norm2(x)\n        x = x + modulate(self.mlp(x), shift_mlp, scale_mlp)\n        x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 48.31902838186518}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    assert context.shape[0] == x.shape[0]\n    assert context.shape[1] == x.shape[1]\n    assert context.shape[2] == x.shape[2]\n    assert context_block.pre_only or x_block.pre_only\n    assert not context_block.scale_mod_only or not x_block.scale_mod_only\n    if context_block.pre_only:\n        qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    else:\n        qkv_context = context_block.attn.pre_attention(context_block.norm1(context))\n    if x_block.pre_only:\n        qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    else:\n        qkv_x = x_block.attn.pre_attention(x_block.norm1(x))\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    if context_block.pre_only:\n        context = context_block.post_attention(attn, *intermediates_context)\n    else:\n        context = context_block.norm2(context + context_block.attn.post_attention(attn))\n    if x_block.pre_only:\n        x = x_block.post_attention(attn, *intermediates_x)\n    else:\n        x = x_block.norm2(x + x_block.attn.post_attention(attn))\n    return context, x\n\n", "CodeBleu_score": 42.12518030176059}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context, x = attn.chunk(2, dim=1)\n    if context_block.pre_only:\n        x = x_block.post_attention(attn, *intermediates_x)\n        return context, x\n    else:\n        context = context_block.post_attention(attn, *intermediates_context)\n        x = x_block.post_attention(attn, *intermediates_x)\n        return context, x\n\n", "CodeBleu_score": 27.207690664696976}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    if context_block is not None:\n        qkv_context, intermediates_context = context_block.pre_attention(context, c)\n        qkv_x, intermediates_x = x_block.pre_attention(x, c)\n        qkv = torch.cat([qkv_context, qkv_x], dim=2)\n        attn = optimized_attention(\n            qkv,\n            num_heads=context_block.attn.num_heads,\n        )\n        if context_block.pre_only:\n            context = context_block.post_attention(attn[:, :, : context_block.attn.dim], context, *intermediates_context)\n            x = x_block.post_attention(attn[:, :, context_block.attn.dim :], x, *intermediates_x)\n        else:\n            context, x = attn.chunk(2, dim=2)\n            context = context_block.post_attention(context, context, *intermediates_context)\n            x = x_block.post_attention(x, x, *intermediates_x)\n    else:\n        qkv, intermediates = x_block.pre_attention(x, c)\n        attn = optimized_attention(\n            qkv,\n            num_heads=x_block.attn.num_heads,\n        )\n        x = x_block.post_attention(attn, x, *intermediates)\n    return context, x\n\n", "CodeBleu_score": 44.83979353579005}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.2112794103032}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.21369170165848}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.21369170165848}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(), (q, k, v))\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n", "CodeBleu_score": 59.65580779883081}, {"code": "def pytorch_attention(q, k, v)\n\t\n    B, C, H, W = q.shape\n    q, k, v = map(lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(), (q, k, v))\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n", "CodeBleu_score": 59.65580779883081}, {"code": "def pytorch_attention(q, k, v)\n\t\n    b, c, h, w = q.shape\n    q, k, v = map(lambda t: t.view(b, c, -1).transpose(1, 2).contiguous(), (q, k, v))\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(1, 2).reshape(b, c, h, w)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(b, -1, c), k.view(b, -1, c).transpose(1, 2), v.view(b, -1, c).transpose(1, 2)).reshape(b, c, h, w)\n    return out\n\n", "CodeBleu_score": 49.64513395814205}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if name in control and control[name] is not None:\n        control_value = control.pop(name)\n        try:\n            h = h + control_value\n        except:\n            logger.warning(f\"Could not add control {name} to {h}\")\n    return h\n\n", "CodeBleu_score": 30.487200087855935}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control and len(control[name]) > 0:\n        try:\n            h += control[name].pop(0)\n        except Exception as e:\n            print(f\\\"Failed to apply control {name} to {h.shape} with {control[name][0].shape}: {e}\\\")\n    return h\n\n", "CodeBleu_score": 49.92518317969512}, {"code": "def apply_control(h, control, name)\n\t\n    if name in control:\n        control_value = control.pop(name)\n        if control_value is not None:\n            try:\n                h += control_value\n            except ValueError:\n                logger.warning(f\"Could not add control {name} to {h}.\")\n    return h\n\n", "CodeBleu_score": 43.15016132697333}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n        min_encodings.scatter_(1, indices[:, None], 1)\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n        if shape is not None:\n            z_q = z_q.view(shape)\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n        return z_q\n\n", "CodeBleu_score": 69.4941476128895}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    min_encodings.scatter_(1, indices[:, None], 1)\n\n    # get quantized latent vectors\n    z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n\n    if shape is not None:\n        z_q = z_q.view(shape)\n\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n    return z_q\n", "CodeBleu_score": 71.19773778516759}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape optional\n    if not shape:\n        # gradient-based sample along dim=1\n        encoding_indices = indices\n    else:\n        # shape batch size\n        shape = shape if isinstance(shape, tuple) else (shape,)\n        # with random shape, sample entries\n        encoding_indices = indices.view(shape[0], -1)\n\n    # fetch quantized latent vectors\n    z_q = self.embedding(encoding_indices)\n\n    # reshape\n    if shape is not None:\n        z_q = z_q.view(shape)\n        # reshape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n    return z_q\n\n", "CodeBleu_score": 62.705178560329465}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    device = self.input.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n    return noises\n\n", "CodeBleu_score": 66.38745781484451}, {"code": "def make_noise(self)\n\t\n    noises = [torch.randn(1, 1, 4, 4, device=self.constant_input.weight.device)]\n\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=self.constant_input.weight.device))\n\n    return noises\n\n", "CodeBleu_score": 51.24193313056762}, {"code": "def make_noise(self)\n\t\n    device = self.input.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n\n    return noises\n\n", "CodeBleu_score": 62.6766143142443}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=x.device)\n    latent = self.style_mlp(latent_in)\n    latent = latent.mean(0)\n    return latent\n\n", "CodeBleu_score": 36.25100614597102}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(\n        num_latent, self.style_mlp.num_style_feat\n    )\n    latent = self.style_mlp(latent_in)\n    latent = latent.mean(0)\n    return latent\n\n", "CodeBleu_score": 22.16345892357032}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.style_dim, device=self.device)\n    latent = self.style_mlp(latent_in)\n    return latent.mean(0)\n\n", "CodeBleu_score": 28.8970158752631}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter = np.random.uniform(-shift, shift, size=img.shape)\n    img = img + jitter\n    return np.clip(img, 0, 1)\n\n", "CodeBleu_score": 39.02460616806957}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = np.random.uniform(-shift, shift, size=img.shape)\n    jittered_img = np.clip(img + jitter, 0, 1)\n    return jittered_img\n\n", "CodeBleu_score": 35.383522371129125}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3).mul_(2 * shift) - shift\n    jitter = jitter.reshape(1, 3, 1, 1)\n    return (img + jitter).clamp_(0, 1)\n\n", "CodeBleu_score": 24.971836895335006}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye_location' in data:\n        self.left_eye_location = data['left_eye_location'].to(self.device)\n    if 'right_eye_location' in data:\n        self.right_eye_location = data['right_eye_location'].to(self.device)\n    if 'mouth_location' in data:\n        self.mouth_location = data['mouth_location'].to(self.device)\n\n", "CodeBleu_score": 53.81378680819442}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data and 'right_eye' in data and 'mouth' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n        self.right_eye = data['right_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.04045369154063}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    if self.opt['use_pyramid']:\n        # construct image pyramid for training\n        self.img_pyramid = []\n        tmp_img = self.gt.detach()\n        downscale_factor = self.opt['scale']\n        for _ in range(self.opt['n_iter_init_downscale']):\n            tmp_img = F.interpolate(tmp_img, scale_factor=downscale_factor, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n            self.img_pyramid.insert(0, tmp_img)\n    else:\n        self.img_pyramid = [self.gt.detach()]\n\n", "CodeBleu_score": 25.58257318738469}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(1, self.pyramid_levels):\n        img_pyramid.insert(0, F.interpolate(self.gt, scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n\n", "CodeBleu_score": 27.840765669872784}, {"code": "def construct_img_pyramid(self)\n\t\n    pyramid_img = []\n    pyramid_img.append(self.gt)\n    for i in range(1, self.downscale):\n        pyramid_img.insert(0, F.interpolate(pyramid_img[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid_img\n\n", "CodeBleu_score": 28.179580330326417}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.net_g, 'net_g', current_iter)\n    self.save_network(self.net_g_ema, 'net_g_ema', current_iter)\n    self.save_network(self.net_d, 'net_d', current_iter)\n    if self.use_facial_disc:\n        self.save_network(self.net_d_left_eye, 'net_d_left_eye', current_iter)\n        self.save_network(self.net_d_right_eye, 'net_d_right_eye', current_iter)\n        self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 81.75887291560552}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['rank'] == 0:\n        network_G = self.net_g.module if self.opt['dist'] else self.net_g\n        network_G_ema = self.net_g_ema.module if self.opt['dist'] else self.net_g_ema\n        state_dict = network_G.state_dict()\n        for key, param in state_dict.items():\n            param.requires_grad = False\n        state_dict_ema = network_G_ema.state_dict()\n        for key, param in state_dict_ema.items():\n            param.requires_grad = False\n        state_dict_d = self.net_d.state_dict()\n        for key, param in state_dict_d.items():\n            param.requires_grad = False\n        if self.use_facial_disc:\n            state_dict_d_left_eye = self.net_d_left_eye.state_dict()\n            for key, param in state_dict_d_left_eye.items():\n                param.requires_grad = False\n            state_dict_d_right_eye = self.net_d_right_eye.state_dict()\n            for key, param in state_dict_d_right_eye.items():\n                param.requires_grad = False\n            state_dict_d_mouth = self.net_d_mouth.state_dict()\n            for key, param in state_dict_d_mouth.items():\n                param.requires_grad = False\n        state_dict_all = {\n            'network_G': state_dict,\n            'network_G_ema': state_dict_ema,\n            'network_D': state_dict_d,\n            'network_D_left_eye': state_dict_d_left_eye,\n            'network_D_right_eye': state_dict_d_right_eye,\n            'network_D_mouth': state_dict_d_mouth,\n            'optimizer_G': self.optimizer_g.state_dict(),\n            'optimizer_D': self.optimizer", "CodeBleu_score": 32.80139195338822}, {"code": "def save(self, epoch, current_iter)\n\t\n    # save networks\n    self.save_network(self.net_g, 'net_g', current_iter)\n    self.save_network(self.net_g_ema, 'net_g_ema', current_iter)\n    self.save_network(self.net_d, 'net_d', current_iter)\n    if self.use_facial_disc:\n        self.save_network(self.net_d_left_eye, 'net_d_left_eye', current_iter)\n        self.save_network(self.net_d_right_eye, 'net_d_right_eye', current_iter)\n        self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n\n    # save training state\n    state_dict = dict(epoch=epoch, iter=current_iter, schedulers=[], optimizers=[])\n    for s in self.schedulers:\n        state_dict['schedulers'].append(s.state_dict())\n    for o in self.optimizers:\n        state_dict['optimizers'].append(o.state_dict())\n    save_file_name = f'{self.opt[\"path\"][\"training_state\"]}/training_state_{current_iter}.state'\n    torch.save(state_dict, save_file_name)\n\n", "CodeBleu_score": 71.37686374438955}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.71577776852638}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in batch]\n    return batch\n", "CodeBleu_score": 65.89215820118145}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in batch]\n    return batch", "CodeBleu_score": 65.89215820118145}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    for exp in batch:\n        self.exps.remove(exp)\n    return batch\n", "CodeBleu_score": 49.88347961391807}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = [e['ds_config'] for e in self.exps]\n    all_configs_flattened = [flatten_config(c) for c in all_configs]\n    all_configs_flattened_numerical = [\n        [v if isinstance(v, numbers.Number) else 0 for v in c] for c in all_configs_flattened\n    ]\n    all_configs_flattened_numerical = np.array(all_configs_flattened_numerical)\n\n    # predict estimates\n    estimates = self.cost_model.predict(all_configs_flattened_numerical)\n\n    # sort estimates\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        # sort by ascending order\n        sorted_idx = np.argsort(estimates)\n    else:\n        # sort by descending order\n        sorted_idx = np.argsort(estimates)[::-1]\n\n    return sorted_idx\n\n", "CodeBleu_score": 31.013195129802263}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = [flatten_config(config) for config in self.all_configs]\n    num_configs = len(flattened_configs)\n    num_features = len(flattened_configs[0])\n    X = np.zeros((num_configs, num_features))\n    for i in range(num_configs):\n        X[i, :] = flattened_configs[i]\n    estimated_perf = self.cost_model.predict(X)\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        estimated_perf = -estimated_perf\n    sorted_idx = np.argsort(estimated_perf)\n    return sorted_idx\n\n", "CodeBleu_score": 28.968598339773965}, {"code": "def find_estimated_top_configs(self)\n\t\n    config_features = []\n    for config in self.all_configs:\n        config_feature = flatten_config(config)\n        config_feature = [config_feature[k] for k in self.dims]\n        config_features.append(config_feature)\n    config_features = np.array(config_features)\n\n    estimated_perf = self.cost_model.predict(config_features)\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        estimated_perf = -estimated_perf\n\n    top_config_indices = np.argsort(estimated_perf)\n\n    return top_config_indices\n\n", "CodeBleu_score": 36.26283592183713}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t)}\")\n        process = subprocess.Popen(self.optimal_cmd)\n        process.wait()\n        logger.info(f\"Successfully ran the optimal command found by autotuning: {' '.join(self.optimal_cmd)}\")\n    else:\n        logger.info(f\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 58.18418942237563}, {"code": "def run_after_tuning(self)\n\t   process = subprocess.Popen(self.optimal_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode == 0:\n            logger.info(\"Successfully ran the optimal command found by autotuning.\")\n        else:\n            logger.error(f\"Failed to run the optimal command found by autotuning. Return code: {process.returncode}\")\n            logger.error(f\"Standard output: {stdout}\")\n            logger.error(f\"Standard error: {stderr}\")\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 40.53135138140636}, {"code": "def run_after_tuning(self)\n\t      proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        for line in iter(proc.stdout.readline, b''):\n            logger.info(line.decode('utf-8').rstrip())\n        proc.wait()\n        logger.info(f\"Finished running the command after tuning: {cmd}\")\n    else:\n        logger.info(f\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 40.917214499711285}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    exp[\"master_port\"] = self.get_master_port()\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp[\"name\"])\n    os.makedirs(exp[\"result_dir\"], exist_ok=True)\n\n    # update user arguments based on argument mappings\n    if self.arg_mappings is not None:\n        for k, v in self.arg_mappings.items():\n            if k in exp[\"args\"]:\n                exp[\"args\"][k] = v\n\n    # start the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # store the running experiment details with a timestamp\n    self.running_experiments[exp_id] = (exp, time.time(), thread)\n\n", "CodeBleu_score": 30.074851473178555}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp[\"name\"])\n    exp[\"master_port\"] = self.get_free_port()\n    exp[\"stderr_file\"] = os.path.join(exp[\"result_dir\"], \"stderr.log\")\n    exp[\"model_info_file\"] = os.path.join(exp[\"result_dir\"], \"model_info.json\")\n    exp[\"metric_file\"] = os.path.join(exp[\"result_dir\"], \"metrics.json\")\n\n    # update user arguments\n    for k, v in self.arg_mappings.items():\n        if k in exp[\"user_args\"]:\n            exp[\"user_args\"][k] = v\n\n    # create result directory\n    if not os.path.exists(exp[\"result_dir\"]):\n        os.makedirs(exp[\"result_dir\"])\n\n    # start experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n    self.running_experiments[exp_id] = (exp, thread, time.time())\n\n", "CodeBleu_score": 31.444932017277605}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    exp[\"master_port\"] = self.get_available_port()\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp[\"name\"])\n    exp[\"stderr_file\"] = os.path.join(exp[\"result_dir\"], \"stderr.log\")\n    exp[\"model_info_file\"] = os.path.join(exp[\"result_dir\"], \"model_info.json\")\n    exp[\"metric_file\"] = os.path.join(exp[\"result_dir\"], \"metrics.json\")\n\n    # update user arguments based on argument mappings\n    if self.args.arg_mappings is not None:\n        for k, v in self.args.arg_mappings.items():\n            if k in exp[\"user_args\"]:\n                exp[\"user_args\"][k] = v\n\n    # create result directory\n    if not os.path.exists(exp[\"result_dir\"]):\n        os.makedirs(exp[\"result_dir\"])\n\n    # create experiment script\n    script_path = os.path.join(self.exps_dir, f\"exp_{exp_id}.sh\")\n    with open(script_path, \"w\") as fd:\n        fd.write(f\"#!/bin/bash", "CodeBleu_score": 30.327239268569343}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized and self.is_log_permitted():\n        if ranks is None:\n            ranks = [0]\n        for rank in ranks:\n            if rank == 0:\n                message[\"rank\"] = rank\n                self.log_to_file(message, path)\n\n", "CodeBleu_score": 20.85043181447682}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized and self.is_master_process():\n        if ranks is not None:\n            message = f'{ranks}: {message}'\n        if path is None:\n            path = self.args.result_dir\n        path = os.path.join(path, 'metrics.json')\n        with open(path, 'a+') as f:\n            f.write(json.dumps(message) + '", "CodeBleu_score": 25.842061767636043}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized:\n        if ranks is None:\n            ranks = [0]\n        if path is None:\n            path = os.path.join(self.args.result_dir, \\\"metrics.json\\\")\n        if self.args.rank in ranks:\n            with open(path, \\\"a+\\\") as fd:\n                fd.write(json.dumps(message) + \\\"\\", "CodeBleu_score": 21.720925680187758}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_file_list = self._get_layer_files(layer_index)\n    if len(layer_file_list) == 0:\n        return {}\n    prefix = self._get_layer_prefix(layer_index)\n    tp_to_files_map = self._partition_files(layer_file_list, prefix)\n    return tp_to_files_map\n\n", "CodeBleu_score": 27.243106554712238}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    tp_other_layer_map = {}\n    if self.has_layer_files:\n        assert layer_index < self.num_layers\n        layer_file_prefix = self.layer_file_prefixes[layer_index]\n        layer_file_list = [fname for fname in self.mp_rank_files if fname.startswith(layer_file_prefix)]\n        tp_other_layer_map = self._partition_files(layer_file_list, self.tp_degree)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 38.050248911943186}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    tp_other_layer_map = {}\n    tp_other_layer_files = self._get_files_with_prefix(self.layer_file_prefixes[layer_index])\n    if tp_other_layer_files:\n        assert len(tp_other_layer_files) == self.tp_degree, f'{len(tp_other_layer_files)} != {self.tp_degree}'\n        for tp_index in range(self.tp_degree):\n            tp_other_layer_map[tp_index] = [fname for fname in tp_other_layer_files if f'tp_rank_{tp_index}' in fname]\n    return tp_other_layer_map\n\n", "CodeBleu_score": 35.458134624232166}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    return {\n        \"OS\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"OS version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 75.65646408629524}, {"code": "def system_info()\n\t\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 89.24161063858953}, {"code": "def system_info()\n\t\n    system_info = {\n        \"OS\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"OS Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n    }\n    return system_info\n\n", "CodeBleu_score": 54.554780835124994}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    num_gpus = torch.cuda.device_count()\n    gpu_names = []\n    for i in range(num_gpus):\n        gpu_names.append(torch.cuda.get_device_name(i))\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return {\n        \"num_gpus\": num_gpus,\n        \"gpu_names\": gpu_names,\n        \"cuda_available\": cuda_available,\n        \"cuda_version\": cuda_version,\n    }\n\n", "CodeBleu_score": 48.88774315610431}, {"code": "def cuda_info()\n\t\n    num_gpus = torch.cuda.device_count()\n    gpu_names = []\n    for i in range(num_gpus):\n        gpu = torch.cuda.get_device_properties(i)\n        gpu_names.append(gpu.name)\n    cuda = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return {\"cuda\": cuda, \"cuda_version\": cuda_version, \"num_gpus\": num_gpus, \"gpu_names\": gpu_names}\n\n", "CodeBleu_score": 48.627266781973596}, {"code": "def cuda_info()\n\t\n    num_gpus = torch.cuda.device_count()\n    gpu_names = []\n    for i in range(num_gpus):\n        gpu_names.append(torch.cuda.get_device_name(i))\n    cuda = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return {\"num_gpus\": num_gpus, \"gpu_names\": gpu_names, \"cuda\": cuda, \"cuda_version\": cuda_version}\n\n", "CodeBleu_score": 49.69047690472226}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n    if class_name_key is None:\n        raise ValueError(\"class_name_key cannot be None.\")\n    if class_id_to_class_name is None:\n        class_id_to_class_name = {}\n    accuracy_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item[0]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        class_name = item[class_name_key]\n        if class_name not in accuracy_dict:\n            accuracy_dict[class_name] = {\"correct\": 0, \"total\": 0}\n        if encoder_manager.is_class_conditional():\n            class_id = item[encoder_manager.config.class_conditional_class_id_key]\n            class_name = class_id_to_class_name[class_id]\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            predicted_class_name = class_id_to_class_name[predicted_class_id]\n        else:\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n        if predicted_class_name == class_name:\n            accuracy_dict[class_name][\"correct\"] += 1\n        accuracy_dict[class_name][\"total\"] += 1\n    total_accuracy = 0\n    for class_name, class_accuracy in accuracy_dict.items():\n        class_accuracy = class_accuracy[\"correct\"] / class_accuracy[\"total\"]\n        total_accuracy += class_accuracy\n        print(f\"Accuracy for class {class_name}: {class_accuracy}\")\n    total_accuracy /= len(accuracy_dict)\n    print(f\"Total accuracy: {total_accuracy}\")\n\n", "CodeBleu_score": 38.288865933207}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    map_class_id_to_class_name = encoder_manager.config.map_class_id_to_class_name\n    if map_class_id_to_class_name:\n        class_ids = list(map_class_id_to_class_name.keys())\n    else:\n        class_ids = None\n    # compute the accuracy\n    accuracy_dict = {c: [] for c in class_ids}\n    for item in tqdm(dataset_items):\n        wav_file = item[0]\n        embedding = encoder_manager.compute_embedding(wav_file).detach().cpu().numpy()\n        class_name = item[2][class_name_key]\n        if map_class_id_to_class_name:\n            class_id = item[2][\"class_id\"]\n            class_name = map_class_id_to_class_name[class_id]\n        pred_class_name = encoder_manager.classify_embedding(embedding)\n        if pred_class_name == class_name:\n            accuracy_dict[class_name].append(1)\n        else:\n            accuracy_dict[class_name].append(0)\n    # print and save accuracy\n    for class_name, acc_list in accuracy_dict.items():\n        acc = sum(acc_list) / len(acc_list)\n        print(f\"Accuracy for class {class_name}: {acc}\")\n\n", "CodeBleu_score": 32.12061259210328}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    use_speaker_id = encoder_manager.config.use_speaker_id\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n\n    if use_speaker_id and class_id_to_class_name is None:\n        raise ValueError(\n            \"class_id_to_class_name must be defined in the encoder config if use_speaker_id is True.\"\n        )\n\n    if use_speaker_id:\n        speaker_manager = SpeakerManager()\n\n    accuracy_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item[encoder_manager.config.audio_key]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        class_name = item[class_name_key]\n        if use_speaker_id:\n            speaker_id = item[encoder_manager.config.speaker_id_key]\n            speaker_name = speaker_manager.get_speaker_name(speaker_id)\n            predicted_class_id = encoder_manager.classify_speaker(embedding, speaker_name)\n            predicted_class_name = class_id_to_class_name[predicted_class_id]\n        else:\n            predicted_class_id = encoder_manager.classify_utterance(embedding)\n            predicted_class_name = class_id_to_class_name[predicted_class_id]\n        if class_name not in accuracy_dict:\n            accuracy_dict[class_name] = {\"total\": 0, \"correct\": 0}\n        accuracy_dict[class_name][\"total\"] += 1\n        if class_name == predicted_class_name:\n            accuracy_dict[class_name][\"correct\"] += 1\n\n    for class_name, class_accuracy in accuracy_dict.items():\n        class_accuracy = class_accuracy[\"correct\"] / class_accuracy[\"total\"]\n        print(f\"Accuracy for class {class_name", "CodeBleu_score": 37.613633028807286}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    model, utils = get_vad_model_and_utils()\n    wav = utils.load_audio(audio_path)\n    speech_timestamps = utils.get_speech_timestamps(wav, model, return_seconds=False)\n    if len(speech_timestamps) > 0:\n        speech_timestamps = [\n            [st[0], st[1]] for st in speech_timestamps if st[1] - st[0] > min_dur_in_sec * 16000]\n        wav = remove_silence(wav, speech_timestamps, 200, 100)\n        utils.save_wav(wav, output_path)\n        return output_path, True\n    else:\n        return output_path, False\n\n", "CodeBleu_score": 43.24469992703086}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    try:\n        vad_model, vad_utils = get_vad_model_and_utils()\n        wav = vad_utils.read_wave(audio_path)\n        segments = vad_utils.get_voice_segments(wav, vad_model)\n        if len(segments) == 0:\n            return output_path, False\n        speech_timestamps = vad_utils.process_wave(wav, vad_model, segments=segments)\n        wav_processed = remove_silence(wav, speech_timestamps, threshold=threshold, min_dur=min_dur, padding=padding)\n        vad_utils.save_wav(output_path, wav_processed, vad_utils.SAMPLE_RATE)\n        return output_path, True\n    except Exception as e:\n        print(f\"Error processing file {audio_path}: {str(e)}\")\n        return output_path, False\n\n", "CodeBleu_score": 40.950774668848965}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    if os.path.exists(output_path) and not args.force:\n        print(f\"{output_path} already exists. Skipping.\")\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    model, utils = get_vad_model_and_utils()\n    wav = utils.read_audio(audio_path)\n    segments = utils.get_voice_segments(wav, model, args.aggressive)\n    wav = remove_silence(wav, segments, args.threshold)\n    if len(wav) > 0:\n        utils.save_audio(output_path, wav, 16000)\n        return output_path, True\n    return output_path, False\n\n", "CodeBleu_score": 42.66049083350999}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir, dirs_exist_ok=True)\n        input_dir = output_dir\n\n    audio_files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, [(filename, output_sr) for filename in audio_files]), total=len(audio_files)))\n\n    print(\"Done.\")\n\n", "CodeBleu_score": 34.663292702399}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir, dirs_exist_ok=True)\n        input_dir = output_dir\n\n    files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    with Pool(n_jobs) as p:\n        list(\n            tqdm(\n                p.imap(resample_file, [(f, output_sr) for f in files]),\n                total=len(files),\n            )\n        )\n\n    print(\"Completed resampling.\")\n\n", "CodeBleu_score": 36.85854544457388}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir:\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n    print(f\"Resampling files in {input_dir} to {output_sr} Hz\")\n    files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    print(f\"Found {len(files)} files\")\n    func_args = [(file, output_sr) for file in files]\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, func_args), total=len(files)))\n    print(\"Done\")\n\n", "CodeBleu_score": 43.75405077210417}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n\n    model.eval()\n    eval_loss = 0.0\n    eval_steps = 0\n    num_samples = 0\n    eval_start_time = time.time()\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            labels = labels.view(-1, c.num_utter_per_class)\n            inputs = inputs.view(\n                labels.size(0),\n                c.num_utter_per_class,\n                inputs.size(1),\n                inputs.size(2),\n            )\n            inputs = inputs.view(-1, inputs.size(2), inputs.size(3))\n            labels = labels.view(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            eval_steps += 1\n            num_samples += inputs.size(0)\n\n            # plot embeddings\n            if i == len(data_loader) - 1:\n                embeddings = outputs.cpu().numpy()\n                class_ids = labels.cpu().numpy()\n                plot_embeddings(embeddings, class_ids, global_step, \"eval\")\n\n    eval_loss = eval_loss / eval_steps\n    eval_time = time.time() - eval_start_time\n    eval_time = str(datetime.timedelta(seconds=eval_time)).split(\".\")[0]\n    logger.info(\n        f\"Eval loss: {eval_loss:.4f} - Eval time: {eval_time} - Steps per second: {eval_steps / eval_time}\"\n    )\n    return eval_loss\n\n", "CodeBleu_score": 37.62006075039188}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_steps = 0\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(data_loader):\n            # get classes\n            classes = labels.unique()\n            if len(classes) < c.num_classes_in_batch:\n                raise RuntimeError(\n                    f\"config.num_classes_in_batch ({c.num_classes_in_batch}) need to be <= {len(classes)} (Number total of Classes in the Train dataset) !\"\n                )\n            # reshape inputs and labels\n            inputs = inputs.reshape(c.num_classes_in_batch, c.num_utter_per_class, -1)\n            labels = labels.reshape(c.num_classes_in_batch, c.num_utter_per_class)\n            # dispatch to GPU\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            # forward\n            outputs = model(inputs)\n            # compute loss\n            loss = criterion(outputs, labels)\n            # compute evaluation metrics\n            eval_loss += loss.item()\n            eval_steps += 1\n            # log\n            if idx % 50 == 0:\n                print(\n                    f\"[{global_step}] Evaluation: {idx}/{len(data_loader)} - Loss: {loss.item()} - Eval Loss: {eval_loss / eval_steps}\"\n                )\n\n    # compute average evaluation loss\n    eval_loss = eval_loss / eval_steps\n    # log\n    print(f\"[{global_step}] Evaluation: {eval_loss}\")\n    # log evaluation loss\n    writer.add_scalar(\"eval_loss\", eval_loss, global_step)\n    # generate and log embeddings\n    if c.model_params.get(\"use_torch_spec\", False):\n        # get embeddings\n        embeddings = model.module.model.get_embeddings(inputs.reshape(-1, inputs.shape[-1]))\n        # generate and log U", "CodeBleu_score": 37.78706161246108}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_steps = 0\n    eval_steps_per_epoch = len(data_loader)\n    with torch.no_grad():\n        for eval_data in data_loader:\n            # get the data\n            eval_inputs, eval_labels = eval_data[0], eval_data[1]\n            eval_inputs = eval_inputs.cuda() if use_cuda else eval_inputs\n            eval_labels = eval_labels.cuda() if use_cuda else eval_labels\n            # compute the embeddings\n            embeds = model(eval_inputs)\n            eval_loss += criterion(embeds, eval_labels)\n            eval_steps += 1\n        eval_loss = eval_loss / eval_steps\n        eval_ppl = torch.exp(eval_loss).item()\n        print(\" > Eval loss: {}\".format(round(eval_loss.item(), 4)))\n        print(\" > Eval ppl: {}\".format(round(eval_ppl, 4)))\n        print(\" > Eval steps/epoch: {}\".format(eval_steps_per_epoch))\n        # generate and log UMAP plot\n        plot_embeddings(\n            embeds,\n            eval_labels,\n            c.eval_output_path,\n            \"eval_umap_epoch_\" + str(global_step) + \".png\",\n        )\n        # log eval stats\n        writer.add_scalar(\"eval_loss\", eval_loss.item(), global_step)\n        writer.add_scalar(\"eval_ppl\", eval_ppl, global_step)\n        writer.add_scalar(\"eval_steps_per_epoch\", eval_steps_per_epoch, global_step)\n    return eval_loss.item()\n\n", "CodeBleu_score": 30.769936437048695}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # construct the config class name\n    config_class_name = f\"{model_name.title()}Config\"\n\n    # if the model is xtts, import its specific configuration\n    if model_name == \"xtts\":\n        from TTS.vocoder.configs.xtts_config import XTTSConfig as Config\n    else:\n        # search through predefined paths to find the configuration class\n        for path in [\n            f\"TTS.vocoder.configs.{model_name}_config\",\n            f\"TTS.vocoder.configs.{model_name}.config\",\n        ]:\n            try:\n                Config = find_module(path, verbose=False)\n                break\n            except ModuleNotFoundError:\n                pass\n        else:\n            raise ModuleNotFoundError(f\"Config for {model_name} could not be found.\")\n\n    # assign the configuration class\n    return Config\n\n", "CodeBleu_score": 37.11573792984253}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"_\", \"\") + \"Config\"\n    config_class = None\n    if model_name == \"xtts\":\n        config_class = \"XTTSConfig\"\n    else:\n        paths = [\n            f\"TTS.tts.configs.{model_name.replace('_', '')}.{config_class_name}\",\n            f\"TTS.vocoder.configs.{model_name.replace('_', '')}.{config_class_name}\",\n        ]\n        for path in paths:\n            try:\n                config_class = find_module(path)\n                break\n            except ModuleNotFoundError:\n                pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class\n\n", "CodeBleu_score": 47.78298659937838}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = f\"{model_name.replace('_', '')}(Config)\"\n    if model_name == \"xtts\":\n        config_class_name = f\"{model_name.upper()}(Config)\"\n    try:\n        config_class = find_module(config_class_name, \"TTS.config.shared_configs\")\n        return config_class\n    except ModuleNotFoundError:\n        try:\n            config_class = find_module(config_class_name, \"TTS.config.shared_configs\")\n            return config_class\n        except ModuleNotFoundError:\n            try:\n                config_class = find_module(config_class_name, \"TTS.config.shared_configs\")\n                return config_class\n            except ModuleNotFoundError:\n                try:\n                    config_class = find_module(config_class_name, \"TTS.config.shared_configs\")\n                    return config_class\n                except ModuleNotFoundError:\n                    raise ModuleNotFoundError(f\"Config for {model_name} not found.\")\n\n", "CodeBleu_score": 29.453639979715433}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    _, ext = os.path.splitext(config_path)\n    if ext.lower() in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = yaml.safe_load(f)\n    elif ext.lower() in (\".json\"):\n        data = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown type {ext}\")\n\n    # model_name = data[\"model\"] if \"model\" in data else data[\"generator_model\"]\n    model_name = _process_model_name(data)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(data)\n    return config", "CodeBleu_score": 51.740307232852444}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict = yaml.safe_load(yaml_file)\n    elif ext == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type. Use .json or .yaml\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 57.499363720378106}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict = yaml.safe_load(yaml_file)\n    elif ext == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type. Use .json or .yaml\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 57.499363720378106}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not (xtts_checkpoint and xtts_config and xtts_vocab):\n        return \"Please provide the paths to the XTTS checkpoint, config, and vocab files.\"\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config, None, None)\n    model.load_checkpoint(xtts_checkpoint)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 48.50721404606125}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide all required paths for XTTS model.\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, eval=True)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 41.180612174500595}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide all required paths for XTTS model.\"\n    print(\"Loading XTTS model...\")\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 53.02235122679275}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        hop_length = self.torch_spec_config[\"hop_length\"]\n        num_frames = min(num_frames, x.shape[-1] // hop_length + 1)\n    offsets = np.linspace(0, x.shape[-1] - x.shape[-1] % hop_length, num=num_frames, endpoint=False).astype(int)\n    frames = [x[..., o : o + self.window] for o in offsets]\n    frames = torch.stack(frames, dim=0)\n    embeddings = self.inference(frames, l2_norm=l2_norm)\n    if return_mean:\n        embeddings = embeddings.mean(0, keepdim=True)\n    return embeddings\n\n", "CodeBleu_score": 41.746902862941084}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torchaudio_spec:\n        hop_length = self.torch_mel_spectrogram.n_stft // 2\n        num_frames = min(num_frames, x.shape[-1] // hop_length)\n        offsets = np.linspace(0, x.shape[-1] - self.torch_mel_spectrogram.n_stft, num=num_frames)\n        offsets = np.round(offsets).astype(np.int32)\n        x = [x[..., offset : offset + self.torch_mel_spectrogram.n_stft] for offset in offsets]\n        x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n        x = self.torch_mel_spectrogram(x)[..., :-1]\n    else:\n        x, _ = self.compute_linear_spec(x, num_frames=num_frames, l2_norm=l2_norm)\n    if num_eval is not None:\n        num_eval = min(num_eval, x.shape[1])\n        eval_idxs = np.linspace(0, x.shape[1] - 1, num=num_eval).astype(np.int32)\n        x = x[:, eval_idxs, :]\n    x = self.forward(x, l2_norm=l2_norm)\n    if return_mean:\n        x = torch.mean(x, dim=1)\n    return x\n\n", "CodeBleu_score": 39.4647472828627}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        hop_length = self.torch_mel_spectrogram.n_stft // 2\n        num_frames = min(num_frames, x.shape[-1] // hop_length)\n    else:\n        num_frames = min(num_frames, x.shape[-1] // self.hop_length)\n    offsets = np.linspace(0, x.shape[-1] - self.hop_length, num=num_frames)\n    new_offsets = np.linspace(0, x.shape[-1] - self.hop_length, num=num_eval)\n    # compute the mean of the embeddings\n    if return_mean:\n        offsets = np.concatenate([offsets, new_offsets])\n    # extract the frames\n    frames = np.array([x[..., int(offset) : int(offset) + self.mel_window_size] for offset in offsets])\n    # compute the embeddings\n    embeddings = self.forward(torch.from_numpy(frames), l2_norm)\n    # return the mean embedding\n    if return_mean:\n        return embeddings.mean(axis=0)\n    return embeddings\n\n", "CodeBleu_score": 39.021652933278844}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.47011482327022}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.47011482327022}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.47011482327022}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = len(embeddings) // num_classes_in_batch\n    if num_classes > 10:\n        num_classes = 10\n    embeddings = np.concatenate(embeddings[: num_classes * num_classes_in_batch])\n    labels = np.concatenate(\n        [np.full(num_classes_in_batch, i) for i in range(num_classes)]\n    )\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(embeddings)\n    plt.figure(figsize=(10, 10))\n    plt.scatter(\n        embedding[:, 0],\n        embedding[:, 1],\n        c=labels[: len(embedding)],\n        cmap=matplotlib.colors.ListedColormap(colormap[:num_classes]),\n        s=5,\n    )\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection\", fontsize=24)\n    plt.savefig(\"umap\")\n    return plt.gcf()\n", "CodeBleu_score": 44.67900492437517}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_utterances_per_class = [\n        sum(embeddings[:, 1] == i) for i in range(num_classes_in_batch)\n    ]\n    num_classes = len(num_utterances_per_class)\n    if num_classes > 10:\n        num_classes = 10\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    embeddings = reducer.fit_transform(embeddings)\n\n    fig, ax = plt.subplots()\n    for i in range(num_classes):\n        indices = np.where(embeddings[:, 1] == i)[0]\n        ax.scatter(\n            embeddings[indices, 0],\n            embeddings[indices, 1],\n            label=i,\n            c=colormap[i],\n        )\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    fig.savefig(\"umap\")\n    return fig\n", "CodeBleu_score": 39.23463798080417}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate number of utterances per class\n    num_utterances_per_class = np.bincount(num_classes_in_batch)\n\n    # Limit the number of classes to 10\n    num_classes = len(num_utterances_per_class)\n    if num_classes > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes = 10\n\n    # Calculate cumulative sum of utterances per class\n    cumulative_utterances = np.cumsum(num_utterances_per_class)\n\n    # UMAP transform\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(embeddings)\n\n    # Create scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 10))\n    plt.scatter(\n        embedding[:, 0],\n        embedding[:, 1],\n        c=[\n            colormap[num_classes_in_batch[i]]\n            for i in range(embeddings.shape[0])\n        ],\n    )\n\n    # Add legend\n    legend_handles = []\n    for i in range(num_classes):\n        legend_handles.append(\n            plt.Line2D(\n                [],\n                [],\n                marker=\"o\",\n                color=\"w\",\n                markerfacecolor=colormap[i],\n                markersize=10,\n            )\n        )\n    plt.legend(legend_handles, range(num_classes))\n\n    # Set plot title and adjust aspect ratio\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n\n    # Save plot and return figure\n    plt.savefig(\"umap.png\")\n    return plt.gcf()\n", "CodeBleu_score": 39.172075699919304}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr_idx in range(len(dvecs)):\n        # Calculate the cosine similarity matrix w.r.t. each centroid\n        for utt_idx in range(len(dvecs[spkr_idx])):\n            # Calculate the cosine similarity\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n            cos_sim = F.cosine_similarity(dvecs[spkr_idx, utt_idx], new_centroids)\n            # Avoid the cosine similarity to be 1.0\n            # This would make the exclusion in the softmax not effective\n            cos_sim = cos_sim + 0.00001\n            # Add a clamp\n            cos_sim = torch.clamp(cos_sim, 1e-7, 1 - (1e-7))\n            # Add to list of cosine similarity values\n            cos_sim_matrix.append(cos_sim)\n    cos_sim_matrix = torch.stack(cos_sim_matrix)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 35.689809864700194}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_matrix = []\n    for spkr_idx in range(dvecs.size(0)):\n        for utt_idx in range(dvecs.size(1)):\n            # Calculate new centroids excluding the utterance\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n            # Compute the cosine similarity matrix to each centroid\n            # with every utterance centroid vector\n            # [num_speakers, num_utterances, num_speakers]\n            centroids_expand = new_centroids.unsqueeze(1).expand(\n                new_centroids.size(0), dvecs.size(1), new_centroids.size(1)\n            )\n            # [num_speakers, num_utterances, num_speakers]\n            utterance_expand = dvecs[spkr_idx, :].unsqueeze(1).expand(\n                new_centroids.size(0), dvecs.size(1), dvecs.size(2)\n            )\n            # [num_speakers, num_utterances]\n            sim_vec = F.cosine_similarity(\n                utterance_expand, centroids_expand, dim=2\n            )\n            # [num_speakers, num_utterances]\n            sim_matrix.append(sim_vec)\n    # [num_speakers, num_speakers, num_utterances]\n    sim_matrix = torch.stack(sim_matrix)\n    # [num_speakers, num_speakers, num_utterances]\n    sim_matrix = sim_matrix.permute(1, 2, 0)\n    # [num_speakers, num_utterances, num_speakers]\n    return sim_matrix\n\n", "CodeBleu_score": 39.946085599623295}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    # Calculate the cosine similarity matrix\n    cos_sim_matrix = []\n    for spkr_idx in range(dvecs.size(0)):\n        # Calculate the cosine similarity matrix for the current speaker\n        spkr_cos_sim = []\n        for utt_idx in range(dvecs.size(1)):\n            # Calculate the new centroids excluding the current utterance\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n\n            # Calculate the cosine similarity with the new centroids\n            cos_sim = F.cosine_similarity(dvecs[spkr_idx, utt_idx], new_centroids)\n            cos_sim = cos_sim.clamp(min=1e-6)\n            spkr_cos_sim.append(cos_sim)\n        spkr_cos_sim = torch.stack(spkr_cos_sim)\n        cos_sim_matrix.append(spkr_cos_sim)\n    cos_sim_matrix = torch.stack(cos_sim_matrix)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 40.42862404789924}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0).unsqueeze(0))\n        L_row = torch.cat(L_row, dim=0)\n        L.append(L_row)\n    return torch.stack(L)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrast.", "CodeBleu_score": 61.94707634203389}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0).unsqueeze(0))\n        L_row = torch.cat(L_row, dim=0)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 68.8544495767305}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0).unsqueeze(0))\n        L_row = torch.cat(L_row, dim=0)\n        L.append(L_row.unsqueeze(0))\n    return torch.cat(L, dim=0)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function computes the embedding loss using contrastive loss.", "CodeBleu_score": 56.8953847641352}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.logsigmoid(cos_sim_matrix[j, i, j]))\n            L_row.append(torch.stack(L_row))\n        L.append(L_row)\n    return torch.stack(L)\n\n    def forward(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the GE2E loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n        \\\"\\\"\\\"\n        # Calculate the centroids\n        centroids = torch.mean(dvecs, 1)\n\n        # Calculate the scatter matrix\n        X = dvecs - centroids.unsqueeze(1)\n        A = torch.sum(X * X, -1)\n\n        # Calculate the cosine similarity matrix\n        N, M, _ = dvecs.shape\n        cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n\n        # Calculate the loss\n        loss = self.embed_loss(dvecs, cos_sim_matrix)\n        loss = loss.squeeze(dim=-1)\n        loss = torch.mean(loss)\n        return loss\n\n    def similarity_matrix(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the similarity matrix for the embeds.\n        \\\"\\\"\\\"\n        return self.calc_cosine_sim(dvecs, torch.mean(dvecs, 1))\n\n", "CodeBleu_score": 45.710950505047606}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            excl_centroids = torch.cat((dvecs[j, :i], dvecs[j, i + 1 :]))\n            centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i, :])\n            excl_centroids_sigmoids = torch.sigmoid(torch.sum(excl_centroids * dvecs[j, i], dim=1))\n            contrast_loss = -torch.log(centroids_sigmoids) + torch.log(1 - excl_centroids_sigmoids)\n            L_row.append(contrast_loss)\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 54.75366637789267}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n            excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:j], centroids_sigmoids[j + 1 :]))\n            L_row.append(1.0 - torch.sigmoid(cos_sim_matrix[j, i, j]) + torch.max(excl_centroids_sigmoids))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 80.53236934326931}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # Ensure inputs have the right shape\n    assert x.ndim == 3\n    (num_speakers, num_utts_per_speaker, dvec_feats) = x.shape\n    assert num_speakers >= 2\n    assert num_utts_per_speaker >= 2\n    assert dvec_feats >= 1\n\n    # Calculate centroids\n    centroids = torch.mean(x, dim=1)\n\n    # Calculate the cosine similarity matrix\n    # Shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # Calculate the loss\n    sim_matrix = self.w * cos_sim_matrix.to(self.device) + self.b\n    loss = self.embed_loss(x, sim_matrix)\n    loss = torch.sum(loss)\n    loss /= num_speakers * num_utts_per_speaker\n    return loss", "CodeBleu_score": 44.850418073898325}, {"code": "def forward(self, x, _label=None)\n\t\n    (N, M, D) = x.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n            excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:j], centroids_sigmoids[j + 1 :]))\n            L_row.append(1.0 - torch.sigmoid(cos_sim_matrix[j, i, j]) + torch.max(excl_centroids_sigmoids))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 29.705131196731603}, {"code": "def forward(self, x, _label=None)\n\t\n    (N, M, D) = x.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n            excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:j], centroids_sigmoids[j + 1 :]))\n            L_row.append(1.0 - torch.sigmoid(cos_sim_matrix[j, i, j]) + torch.max(excl_centroids_sigmoids))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 29.705131196731603}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        if os.path.isdir(extra_voice_dir):\n            for root, dirs, files in os.walk(extra_voice_dir):\n                subdir = os.path.basename(root)\n                voices[subdir] = []\n                for file in files:\n                    if file.endswith(\".npz\"):\n                        voices[subdir].append(os.path.join(root, file))\n                if len(voices[subdir]) == 0:\n                    for file in files:\n                        if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                            voices[subdir].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 29.840349211674088}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            if not files:\n                continue\n            if not any(file.endswith(\".npz\") for file in files):\n                files = [file for file in files if file.endswith(\".wav\") or file.endswith(\".mp3\")]\n            if not files:\n                continue\n            voices[os.path.basename(subdir)] = [os.path.join(subdir, file) for file in files]\n    return voices\n\n", "CodeBleu_score": 27.976388724493262}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    subdir = os.path.basename(root)\n                    if subdir not in voices:\n                        voices[subdir] = []\n                    voices[subdir].append(os.path.join(root, file))\n            if not voices:\n                for file in files:\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        subdir = os.path.basename(root)\n                        if subdir not in voices:\n                            voices[subdir] = []\n                        voices[subdir].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 23.988903724817696}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voices(extra_voice_dirs)\n    if voice not in voice_paths:\n        raise ValueError(f\"voice {voice} not found in {extra_voice_dirs}\")\n    if len(voice_paths[voice]) > 1:\n        raise ValueError(f\"voice {voice} has multiple options in {extra_voice_dirs}\")\n    voice_path = voice_paths[voice][0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"voice {voice} not found in {extra_voice_dirs}\")\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        # generate npz file\n        npz_file = voice_path.replace(os.path.splitext(voice_path)[1], \".npz\")\n        if not os.path.exists(npz_file):\n            logger.info(f\"Generating npz file for {voice_path}\")\n            generate_npz(model, voice_path, npz_file)\n        semantic, coarse, fine = load_npz(npz_file)\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 49.34428521066844}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"voice '{voice}' not found in {extra_voice_dirs}\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"voice '{voice}' has multiple options in {extra_voice_dirs}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"voice '{voice}' not found in {extra_voice_dirs}\")\n    if voice_path.endswith(\".npz\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        # generate .npz file\n        voice_npz_path = voice_path.replace(voice_path.split(\".\")[-1], \"npz\")\n        if not os.path.exists(voice_npz_path):\n            logger.info(f\"Generating {voice_npz_path}\")\n            generate_voice_npz(model, voice_path, voice_npz_path)\n        semantic, coarse, fine = load_npz(voice_npz_path)\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 51.95299528462678}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"voice '{voice}' not found in {extra_voice_dirs}\")\n\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"voice {voice} not found in {voice_path}\")\n\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n\n    # generate .npz file\n    npz_file = voice_path.replace(\".wav\", \".npz\")\n    npz_file = npz_file.replace(\".mp3\", \".npz\")\n\n    if not os.path.exists(npz_file):\n        wav, sr = librosa.load(voice_path, sr=16000)\n        wav = convert_audio(wav, sr, model.ap.sample_rate, model.ap.channels)\n        wav = wav / np.abs(wav).max() * 0.999\n        mel = model.ap.melspectrogram(torch.from_numpy(wav).unsqueeze(0))\n        x_tst = torch.randn(1, 10, mel.shape[2]).to(mel.device)\n        with torch.no_grad():\n            with inference_mode(model):\n                _, _, _, x_history = model.tts_model.infer(mel, x_tst, mask=None, sigmas=None, sigma_data=None)\n\n        # save the voice\n        x_history = x_history.squeeze().cpu().numpy()\n        np.savez(npz_file, semantic_prompt=x_history[2], coarse_", "CodeBleu_score": 47.65604126643825}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.08735369887231}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.08735369887231}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.08735369887231}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    context = context.transpose(1, 2)\n    output = []\n    for i in range(len(context)):\n        out = context[i][:, : lens[i]]\n        for conv in self.convolutions:\n            out = F.relu(conv(out))\n            out = self.dropout(out)\n        output.append(out)\n    output = nn.utils.rnn.pad_sequence(output, batch_first=True)\n    return output\n\n", "CodeBleu_score": 38.91787633238517}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context = context.transpose(1, 2)\n    max_len = context.size(2)\n    mask = torch.arange(max_len).unsqueeze(0) < lens.unsqueeze(1)\n    processed_contexts = []\n    for i in range(len(self.convolutions)):\n        conv = self.convolutions[i]\n        context = F.relu(conv(context))\n    if self.lstm_type != \"\":\n        context = context.transpose(1, 2)\n        lengths = torch.sum(mask, dim=1)\n        context = nn.utils.rnn.pack_padded_sequence(context, lengths, batch_first=True, enforce_sorted=False)\n        context, _ = self.bilstm(context)\n        context, _ = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n        context = context.transpose(1, 2)\n    if self.use_linear:\n        context = self.dense(context)\n    context = self.dropout(context)\n    processed_contexts.append(context)\n    context = torch.cat(processed_contexts, dim=1)\n    context = context.transpose(1, 2)\n    mask = mask.unsqueeze(2).repeat(1, 1, context.size(2))\n    return context, mask\n\n", "CodeBleu_score": 32.956019535069885}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # compute total output dim of convs\n    out_dim = self.conv_layers[-1].out_channels\n    # run through convs\n    conv_outputs = []\n    for i in range(context.size(0)):\n        conv_input = context[i, : lens[i], :]\n        conv_output = self.conv_layers(conv_input.unsqueeze(0)).squeeze(0)\n        conv_outputs.append(conv_output)\n    # pad and stack outputs\n    conv_outputs = torch.nn.utils.rnn.pad_sequence(conv_outputs, batch_first=True)\n    return conv_outputs\n\n", "CodeBleu_score": 20.51673795004561}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context[indices]\n    context_sorted_padded = nn.utils.rnn.pack_padded_sequence(context_sorted, lens_sorted.cpu(), batch_first=True)\n    context_sorted_padded = fn(context_sorted_padded)\n    context_sorted, _ = nn.utils.rnn.pad_packed_sequence(context_sorted_padded, batch_first=True)\n    _, indices_unsorted = torch.sort(indices, descending=False)\n    context = context_sorted[indices_unsorted]\n    return context\n\n", "CodeBleu_score": 21.100090279110063}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = nn.utils.rnn.pack_padded_sequence(context, lens.cpu(), batch_first=True)\n    context = fn(context)\n    context, _ = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    context = context[indices]\n    return context\n\n", "CodeBleu_score": 16.451462080273732}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = torch.nn.utils.rnn.pack_padded_sequence(context[indices], lens, batch_first=True)\n    context = fn(context)\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    context = context[indices]\n    return context\n\n", "CodeBleu_score": 14.2684710661698}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.remove_weight_norm(self.convt_pre[1])\n    for block in self.conv_blocks:\n        nn.utils.remove_weight_norm(block[1])\n\n", "CodeBleu_score": 42.55392119287791}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.remove_weight_norm(self.convt_pre[1])\n    for block in self.conv_blocks:\n        nn.utils.remove_weight_norm(block[1])\n\n", "CodeBleu_score": 42.55392119287791}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv in self.conv_blocks:\n        conv.remove_weight_norm()\n\n", "CodeBleu_score": 26.378073370575184}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    if 'se_' in name:\n        name = name.replace('se_', 'se')\n    if 'senet_' in name:\n        name = name.replace('senet_', 'senet')\n    if 'inceptionv3' in name:\n        name = 'inception_v3'\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 40.15824665134494}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    if 'se_' in name:\n        name = name.replace('se_', 'se')\n    if 'senet_' in name:\n        name = name.replace('senet_', 'senet')\n    if 'inceptionv3' in name:\n        name = name.replace('inceptionv3', 'inception_v3')\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 44.33447390180338}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name[3:]\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name[6:]\n    if mx_name.startswith('inceptionv3'):\n        mx_name = 'inception_v3'\n    torch_name = 'gluon_' + mx_name\n    return torch_name\n\n", "CodeBleu_score": 38.439280016867976}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = int(math.ceil(self.num_samples / self.global_num_workers))\n    if self.repeats > 0:\n        num_samples_per_worker *= self.repeats\n    if self.is_training:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size)\n    return num_samples_per_worker\n\n", "CodeBleu_score": 32.29209656997216}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    if self.repeats > 0 and not self.is_training and self.dist_num_replicas == 1:\n        num_samples = math.ceil(num_samples / self.repeats)\n    elif self.is_training and self.dist_num_replicas > 1:\n        num_samples = math.ceil(num_samples / self.dist_num_replicas)\n    if self.is_training and self.batch_size > 1:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 48.04124524669484}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    if self.repeats > 0:\n        num_samples_per_worker *= self.repeats\n    if self.is_training:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 37.04582734756002}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    labels = list()\n    filenames = list()\n    class_to_idx = dict()\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        if not class_to_idx.get(label):\n            class_to_idx[label] = len(class_to_idx)\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(class_to_idx[label])\n    if sort:\n        # Sorting *.npy entries messes up ordering when loading\n        # with multiple workers, so we sort the images instead\n        sorted_index = natural_key(filenames)\n        filenames = [filenames[i] for i in sorted_index]\n        labels = [labels[i] for i in sorted_index]\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 52.15685193522938}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    labels = list()\n    filenames = list()\n    class_to_idx = {}\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        if not class_to_idx.get(label):\n            class_to_idx[label] = len(class_to_idx)\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(class_to_idx[label])\n    if sort:\n        labels, filenames = (list(t) for t in zip(*sorted(zip(labels, filenames))))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 50.46511297308746}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    labels = []\n    filenames = []\n\n    if not types:\n        types = get_img_extensions()\n    assert len(types) > 0, 'at least one image type must be supplied.'\n    relabel = True\n\n    # NOTE: we don't care about case here, but preserve it for class names\n    relabel = relabel and not os.path.exists(os.path.join(folder, 'classes.txt'))\n\n    if relabel:\n        # building class index from folder structure\n        classes = sorted(entry.name for entry in os.scandir(folder) if entry.is_dir())\n        if not class_to_idx:\n            class_to_idx = {classes[i]: i for i in range(len(classes))}\n        num_classes = len(classes)\n        print(f'   >> Found {num_classes} classes.')\n        if not leaf_name_only:\n            print(f'   >> Treating subfolder names as class names.')\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        num_classes = len(classes)\n        print(f'   >> Found {num_classes} classes.')\n        if not leaf_name_only:\n            print(f'   >> Treating subfolder names as class names.')\n    else:\n        # try reading class names from existing file\n        classes, class_to_idx = load_class_map(folder, class_to_idx)\n        num_classes = len(classes)\n        print(f'   >> Found {num_classes} classes in {os.path.join(folder, \"classes.txt\")}.')\n\n    labels_set = set()\n    count = 0\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        if not label:\n            continue\n        for f", "CodeBleu_score": 44.02954555564497}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2 ** 32\n    else:\n        return dist.get_rank() % 2 ** 32\n\n", "CodeBleu_score": 49.229231377866206}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2 ** 32\n    else:\n        return dist.get_rank()\n\n", "CodeBleu_score": 50.429141002776255}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return 1234 + dist.get_rank()\n\n", "CodeBleu_score": 53.867082090252936}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"RGB\":\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 70.91797203070446}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == 'L':\n        return ImageOps.solarize(img, thresh)\n    elif img.mode == 'RGB':\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 60.664257838463186}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if _PIL_VER >= (7, 0):\n        return ImageOps.solarize(img, thresh=thresh).point(lambda x: x + add)\n    else:\n        lookup_table = [\n            int(x + add) if x < thresh else x for x in range(256)\n        ]  # for 3.x compatibility\n        return img.point(lookup_table)\n\n", "CodeBleu_score": 28.21857064210243}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probs = zip(*transforms.items())\n    sum_probs = sum(probs)\n    probs = [prob / sum_probs for prob in probs]\n    return transform_names, probs\n\n", "CodeBleu_score": 31.961825929713694}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for transform_name, transform_prob in transforms.items():\n        transform_names.append(transform_name)\n        transform_probs.append(transform_prob)\n    transform_probs = np.array(transform_probs) / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 31.99519129653508}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for transform_name, transform_prob in transforms.items():\n        transform_names.append(transform_name)\n        transform_probs.append(transform_prob)\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / transform_probs.sum()\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 32.72284938108453}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    ws = mixing_weights * m\n    for w in ws:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img = img.astype(np.float32) + img_aug.astype(np.float32) * w\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    return Image.fromarray(img)\n\n", "CodeBleu_score": 50.002467897453386}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img = np.zeros_like(img)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig.copy()\n        for op in ops:\n            img_aug = op(img_aug, m)\n        img = np.where(img == 0, img_aug, img * (1 - w) + img_aug * w)\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    img = Image.fromarray(img)\n    return Image.blend(img_orig, img, self.alpha)\n\n", "CodeBleu_score": 50.58593577687201}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_aug = img_orig.copy()\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        for op in ops:\n            img_aug = op(img_aug)\n        img = (1 - w) * img + w * img_aug\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    img = Image.fromarray(img)\n    return img\n\n", "CodeBleu_score": 47.85842842654008}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        return (x,) * n\n    if len(x) != n:\n        _logger.warning(\n            f\"Input channel length ({len(x)}) doesn't match model ({n}). Average channels to match input.\"\n        )\n        return (sum(x) / len(x),) * n\n    assert len(x) == n, f\"Input channel length ({len(x)}) doesn't match model ({n}).\"\n    return x\n\n", "CodeBleu_score": 33.68310240045515}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple([sum(x) / len(x)] * n)\n        _logger.warning('input channels for mean {} is not matching the required {} channels, '\n                        'adapting it with average channels'.format(len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 47.5978825565409}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple(x)\n        _logger.warning('mean {} channels of input is not equal to {}'.format(len(x), n))\n        x = [int(sum(x) / len(x))] * n\n        x = tuple(x)\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 48.038439042169465}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    assert (worker_info.num_workers > 0 and worker_info.id >= 0 and worker_info.id < worker_info.num_workers)\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif isinstance(worker_seeding, (str, bytes)):\n        if worker_seeding == 'all':\n            seed_set = (worker_info.seed + worker_info.id) % (2 ** 32 - 1)\n            random.seed(seed_set)\n            torch.manual_seed(seed_set)\n            np.random.seed(seed_set)\n        else:\n            raise ValueError(f'Invalid worker_seeding {worker_seeding} passed, only \\'all\\' or a callable')\n    else:\n        raise ValueError(f'Invalid worker_seeding {worker_seeding} passed, only \\'all\\' or a callable')\n\n", "CodeBleu_score": 55.47054805375122}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    worker_id = (worker_info.id + worker_info.num_workers) % worker_info.num_workers\n    np.random.seed(worker_info.seed % (2 ** 32 - 1))\n    if worker_seeding == 'all':\n        for i in range(100):\n            seed = (torch.initial_seed() + worker_id) % 2 ** 32\n            random.seed(seed)\n            if seed != torch.initial_seed():\n                break\n        else:\n            _logger.warning(f'worker_init failed to break seed, seed is {seed}')\n    elif isinstance(worker_seeding, Callable):\n        worker_seeding(seed)\n    elif isinstance(worker_seeding, (int, float)):\n        torch.manual_seed(worker_seeding)\n        random.seed(worker_seeding)\n        np.random.seed(worker_seeding)\n    else:\n        raise ValueError(f'Invalid worker_seeding {worker_seeding}')\n\n", "CodeBleu_score": 47.75440541398489}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed % (2 ** 32 - 1))\n    elif worker_seeding == 'all':\n        seed = worker_info.seed % (2 ** 32 - 1)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    else:\n        assert False\n\n", "CodeBleu_score": 66.58318949497092}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32,\n    )\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], resize_method)\n\n    return image\n\n", "CodeBleu_score": 59.692369126568764}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], resize_method)\n\n    return image\n\n", "CodeBleu_score": 62.06639120218206}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32,\n    )\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], resize_method)\n    return image\n\n", "CodeBleu_score": 59.692369126568764}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.898446301658076}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'area':\n        resize_method = tf.image.ResizeMethod.AREA\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n    if use_bfloat16:\n        image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n        image = tf.image.random_flip_left_right(image)\n        image = tf.reshape(image, [image_size, image_size, 3])\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n        image = _flip(image)\n        image = tf.reshape(image, [image_size, image_size, 3])\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 57.26358561429227}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        interpolation = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'lanczos3':\n        interpolation = tf.image.ResizeMethod.LANCZOS3\n    elif interpolation == 'lanczos5':\n        interpolation = tf.image.ResizeMethod.LANCZOS5\n    elif interpolation == 'bilinear':\n        interpolation = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'gaussian':\n        interpolation = tf.image.ResizeMethod.GAUSSIAN\n    elif interpolation == 'nearest':\n        interpolation = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Unknown interpolation method: %s' % interpolation)\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 66.28985212123098}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    self.predictor.model = model\n    self.predictor.eval_dataloader = eval_dataloader\n    self.predictor.pred_dir = self.predictor.pred_dir + \"_\" + output_file\n    self.predictor.run_prediction_loop()\n    return self()", "CodeBleu_score": 44.7769842789062}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor.eval_dataloader = eval_dataloader\n    self.predictor.output_file = output_file\n    self.predictor(model)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}", "CodeBleu_score": 51.383203834056765}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor(model, eval_dataloader, output_file=output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}", "CodeBleu_score": 58.403368500784005}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"R@1: {:.4f} - R@5: {:.4f} - R@10: {:.4f} - Median R: {}\".format(\n            rank1, rank5, rank5, miou\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.2669562037779}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"R@1: {:.4f} - R@5: {:.4f} - mIoU: {:.4f}\".format(rank1, rank5, miou))\n\n", "CodeBleu_score": 37.555458794426556}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"R@1: {:.4f} - R@5: {:.4f} - miou: {:.4f}\".format(rank1, rank5, miou))\n\n", "CodeBleu_score": 37.84722269072}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # For each segment-data pair, calculate IoUs and ranks.\n    # Then, calculate the percentage of segments with rank 1 and rank 5,\n    # and the mean IoU across all segments.\n    rank1 = 0\n    rank5 = 0\n    miou = 0\n    for i, segment in enumerate(segments):\n        # Calculate IoUs and ranks for each segment-data pair.\n        ious = [self._iou(segment, data[j]) for j in range(len(data))]\n        ranks = [self._rank(sorted(ious, reverse=True), iou) for iou in ious]\n\n        # Calculate the percentage of segments with rank 1 and rank 5.\n        rank1 += int(ranks[0] == 1)\n        rank5 += int(ranks[0] <= 5)\n\n        # Calculate the mean IoU across all segments.\n        miou += max(ious)\n\n    # Calculate the percentage of segments with rank 1 and rank 5.\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n\n    # Calculate the mean IoU across all segments.\n    miou /= len(segments)\n\n    return rank1, rank5, miou\n", "CodeBleu_score": 30.12939425177214}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # For each segment, calculate the IoU with all the ground-truth segments.\n    # Then, for each ground-truth segment, calculate the rank of the highest IoU.\n    # Finally, average the ranks across all ground-truth segments.\n    # This is the average rank.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is the average of the IoUs across all ground-truth segments.\n    # The average rank is the average of the ranks across all ground-truth segments.\n    # The average IoU is", "CodeBleu_score": 1.0374015748031495}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    mIoU = 0\n    for i, segment in enumerate(segments):\n        rank = []\n        iou = []\n        for j, datum in enumerate(data):\n            rank.append(_rank(segment, datum))\n            iou.append(_iou(segment, datum))\n        rank1 += min(1, rank.index(1) + 1)\n        rank5 += min(1, rank.index(5) + 1)\n        mIoU += max(iou)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mIoU /= len(segments)\n    return rank1, rank5, mIoU\n\n", "CodeBleu_score": 25.68525097230357}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda(0)\n    for i, data in enumerate(eval_dataloader):\n        data = data.cuda(0)\n        with torch.no_grad():\n            outputs = model(data)\n            outputs = self.output_transform(outputs, data)\n        scores.extend(outputs)\n    scores = self.finalize_output(scores)\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 44.371978175246504}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = model.to(torch.device(\"cuda:0\"))\n    for data in tqdm(eval_dataloader):\n        with torch.no_grad():\n            data = data.to(torch.device(\"cuda:0\"))\n            outputs = model(data)\n            scores.append(outputs)\n    scores = self.finalize_output(scores)\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 47.96362395394011}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.cuda(0)\n    scores = []\n    for i, data in tqdm(enumerate(eval_dataloader)):\n        with torch.no_grad():\n            data = self.transfer_batch_to_gpu(data, 0)\n            outputs = model(**data)\n            scores.append(outputs)\n    results = self.output_prediction(scores, eval_dataloader)\n    if output_file is not None:\n        self.save_predictions(results, output_file)\n    return results\n\n", "CodeBleu_score": 37.07782408607162}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [np.concatenate(scores[0], axis=0), np.concatenate(scores[1], axis=0)]\n    self.full_scores = []\n    return np.dot(scores[0], scores[1].T)\n\n", "CodeBleu_score": 52.043343096957514}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = np.concatenate(scores, axis=0)\n    self.full_scores = []\n    return scores[0].dot(scores[1].T)\n\n", "CodeBleu_score": 41.26749373301858}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = np.concatenate(scores, axis=0)\n    scores = np.dot(scores[0], scores[1].T)\n    self.full_scores = []\n    return scores\n", "CodeBleu_score": 45.418335688702136}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n    assert Y_pred.shape == Y_true.shape, \"{} != {}\".format(Y_pred.shape, Y_true.shape)\n    if output_file is not None:\n        with open(\n            os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\"\n        ) as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL\n            )\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 50.34218711067986}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 32.254311748218214}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred).cpu().numpy()\n    Y_true = torch.cat(Y_true).cpu().numpy()\n    # check for prediction errors\n    if np.any(Y_pred != Y_true):\n        print(\"Prediction error detected, printing samples\")\n        for i, (y_pred, y_true) in enumerate(zip(Y_pred, Y_true)):\n            if y_pred != y_true:\n                print(f\"Sample {i}: Predicted label = {y_pred}, True label = {y_true}\")\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL\n            )\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 53.26564038133852}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # round=3 is the same as np.round(loss_sum / sample_size, 3)\n\n", "CodeBleu_score": 52.40683697663986}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # round=3 is the same as fairseq's default\n\n", "CodeBleu_score": 53.528100668393975}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(cmasks),\n            torch.ones_like(vmasks)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 31.81578616362636}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(cmasks.size(), dtype=torch.long),\n            torch.ones(vmasks.size(), dtype=torch.long),\n        ],\n        dim=1,\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.67067586837754}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(cmasks),\n            torch.ones_like(vmasks)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 31.81578616362636}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if \"past_key_values\" in model_kwargs:\n        past_key_values = model_kwargs.pop(\"past_key_values\")\n    else:\n        past_key_values = None\n\n    if \"position_ids\" in model_kwargs:\n        position_ids = model_kwargs.pop(\"position_ids\")\n    else:\n        position_ids = None\n\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n\n    if token_type_ids is None:\n        if hasattr(self.bert, \"embeddings\"):\n            buffered_token_type_ids = self.bert.embeddings.token_type_ids[:, :input_ids.shape[-1]]\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                input_ids.size(0), -1\n            )\n            token_type_ids = buffered_token_type_ids_expanded\n        else:\n            token_type_ids = torch.zeros_like(input_ids)\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: torch.Tensor = self.bert.get_extended_attention_mask(\n        attention_mask, input_ids.shape, device=input_ids.device\n    )\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if attention_mask is not None:\n            encoder_attention_mask = self.invert_attention_mask(attention_mask)\n        else", "CodeBleu_score": 32.949813039663255}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # prepare inputs for generation\n    input_ids = input_ids.to(self.device)\n    input_video_embeds = input_video_embeds.to(self.device)\n    attention_mask = attention_mask.to(self.device)\n    token_type_ids = token_type_ids.to(self.device)\n    # adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    input_ids = input_ids[:, :combined_length]\n    attention_mask = attention_mask[:, :combined_length]\n    token_type_ids = token_type_ids[:, :combined_length]\n    # modify attention_mask and token_type_ids to fit the new sequence length\n    attention_mask = attention_mask.to(dtype=torch.float32)\n    attention_mask = attention_mask.masked_fill(attention_mask == 0, float('-inf'))\n    attention_mask = attention_mask.masked_fill(attention_mask == 1, float(0.0))\n    token_type_ids = token_type_ids.to(dtype=torch.float32)\n    token_type_ids = token_type_ids.masked_fill(token_type_ids == 0, float('-inf'))\n    token_type_ids = token_type_ids.masked_fill(token_type_ids == 1, float(0.0))\n    # return the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        'input_ids': input_ids,\n        'input_video_embeds': input_video_embeds,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 40.342112810108006}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids = input_ids.to(self.device)\n    # input_video_embeds = input_video_embeds.to(self.device)\n    # attention_mask = attention_mask.to(self.device)\n    # token_type_ids = token_type_ids.to(self.device)\n\n    # combine the text and video embeddings\n    combined_input_ids = torch.cat((input_ids, input_video_embeds), dim=1)\n    combined_attention_mask = torch.cat((attention_mask, torch.ones_like(input_video_embeds)), dim=1)\n    combined_token_type_ids = torch.cat((token_type_ids, torch.ones_like(input_video_embeds)), dim=1)\n\n    # adjust the sequence length to match the combined length\n    combined_input_ids = combined_input_ids[:, :self.max_length]\n    combined_attention_mask = combined_attention_mask[:, :self.max_length]\n    combined_token_type_ids = combined_token_type_ids[:, :self.max_length]\n\n    # update the input_ids, input_video_embeds, attention_mask, and token_type_ids\n    input_ids = combined_input_ids\n    input_video_embeds = combined_input_ids\n    attention_mask = combined_attention_mask\n    token_type_ids = combined_token_type_ids\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 33.20818085813717}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = cfg.deepcopy()\n    meta_keys = cfg.test_dataloader.dataset.pipeline.meta_keys\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    transforms = cfg.test_dataloader.dataset.pipeline.transforms\n    for transform in transforms:\n        if transform.type == 'LoadImageFromFile':\n            transform.type = 'mmdet.InferencerLoader'\n            break\n    else:\n        raise ValueError('The pipeline does not contain `LoadImageFromFile`')\n    return Compose(cfg.test_dataloader.dataset.pipeline)\n\n", "CodeBleu_score": 29.928867104452987}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = copy.deepcopy(cfg)\n    test_pipeline = cfg.test_dataloader.dataset.pipeline\n    for i, transform in enumerate(test_pipeline):\n        if 'meta_keys' in transform:\n            if 'img_id' in transform['meta_keys']:\n                transform['meta_keys'].remove('img_id')\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise ValueError('LoadImageFromFile is not found in the pipeline.')\n    test_pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 35.18348518718402}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    test_pipeline = cfg.test_pipeline\n    for transform in test_pipeline:\n        if 'meta_keys' in transform and 'img_id' in transform['meta_keys']:\n            transform['meta_keys'].remove('img_id')\n    # find the index of LoadImageFromFile\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise ValueError('LoadImageFromFile is not found in the pipeline')\n    test_pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 35.0954287159638}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if not osp.isfile(inputs):\n            inputs = list(Path(inputs).rglob('*'))\n            inputs = [str(f) for f in inputs if f.suffix.lower() in IMG_EXTENSIONS]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 47.96895231369736}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if not is_filepath(inputs):\n            inputs = list(Path(inputs).glob('*'))\n            inputs = [str(input) for input in inputs]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.66203480509444}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith(IMG_EXTENSIONS)]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 43.19916553900241}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for chunk in chunk_iter(inputs, chunk_size):\n        chunk = [self.pipeline(data) for data in chunk]\n        yield chunk\n\n", "CodeBleu_score": 12.385324320584093}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for data in inputs:\n        if isinstance(data, dict) and ('img' in data or 'img_path' in data):\n            data = self.pipeline(data)\n        chunk.append(data)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 23.59095908329675}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for chunk_data in chunk_iter(inputs, chunk_size):\n        chunk_data = [\n            self.pipeline(data)\n            for data in chunk_data\n            if ('img' in data or 'img_path' in data)\n        ]\n        yield chunk_data\n\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 19.819214656974083}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    results = {'predictions': [], 'visualization': visualization}\n\n    if return_datasamples is False:\n        for pred in preds:\n            results['predictions'].append(pred.to_dict())\n    else:\n        if pred_out_dir != '':\n            warnings.warn(\n                'Saving datasamples is not supported in this model. '\n                'If you want to save the results, please set '\n                '`return_datasamples=False`.'\n            )\n        results['predictions'] = preds\n\n    if print_result:\n        print_log(results, logger='mmengine')\n\n    return results\n\n", "CodeBleu_score": 50.276711433466495}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir != '' and return_datasamples is True:\n        warnings.warn('Saving datasamples is not supported.')\n\n    results = {'predictions': []}\n\n    if return_datasamples is False:\n        preds = [pred.to_dict() for pred in preds]\n\n    results['predictions'].extend(preds)\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result is True:\n        print_log(results, logger=self.logger)\n\n    return results\n\n", "CodeBleu_score": 43.284899414049406}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir != '' and return_datasamples is True:\n        warnings.warn('Saving datasamples is not supported in the current version.')\n\n    results = {'predictions': []}\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if return_datasamples is False:\n        preds = self.postprocess_predictions(preds, **kwargs)\n        results['predictions'].extend(preds)\n\n    if pred_out_dir != '':\n        mmengine.mkdir_or_exist(pred_out_dir)\n        if return_datasamples is False:\n            self.save_predictions(preds, pred_out_dir)\n        else:\n            warnings.warn('Saving datasamples is not supported in the current version.')\n\n    if print_result is True:\n        self.print_predictions(preds)\n\n    return results\n\n", "CodeBleu_score": 45.48870099177669}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0] = test_pipeline[0].copy()\n    test_pipeline[0].type = 'mmdet.Resize'\n    test_pipeline[0].transforms = [test_pipeline[0].transforms[0]]\n    test_pipeline.append(test_pipeline[-1].copy())\n    test_pipeline[-1].type = 'mmdet.Pad'\n    test_pipeline[-1].transforms = [test_pipeline[-1].transforms[0]]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 25.10142127052116}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0].type = 'LoadImageFromFile'\n    test_pipeline[0].transforms[0].type = 'Resize'\n    test_pipeline.append(cfg.data.test.pipeline[-1])\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 22.321381402312905}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n\n    test_pipeline = cfg.data.test.pipeline\n    # TODO: find a better way to support both `LoadImageFromFile`\n    # and `LoadImageFromNDArray`\n    for i, transform in enumerate(test_pipeline):\n        if transform.type == 'LoadImageFromFile':\n            test_pipeline[i] = dict(type='LoadImageFromNDArray')\n        elif transform.type == 'LoadImageFromWebcam':\n            test_pipeline[i] = dict(type='LoadImageFromWebcam')\n        elif transform.type == 'LoadImageFromMMDet':\n            test_pipeline[i] = dict(type='LoadImageFromMMDet')\n\n    # replace `Resize` transform with `Resize` and\n    # `Normalize` transform (if exists)\n    resize_transform = dict(type='Resize', scale=(1333, 800), keep_ratio=True)\n    test_pipeline[0] = resize_transform\n    for i, transform in enumerate(test_pipeline):\n        if transform.type == 'Normalize':\n            test_pipeline[i - 1] = resize_transform\n            test_pipeline = test_pipeline[:i + 1]\n            break\n\n    return test_pipeline\n\n", "CodeBleu_score": 28.726145504174195}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # prepare data\n    data = dict(img=img, img_id=frame_id, ori_shape=img.shape, img_id=frame_id,\n                img_ids=frame_id, original_shape=img.shape, video_len=video_len)\n\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_dataloader.dataset.pipeline)\n    data = test_pipeline(data)\n\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model.test_step(data)[0]\n\n    return result\n\n", "CodeBleu_score": 55.017488440909204}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        img_id=0,\n        img_shape=img.shape,\n        ori_shape=img.shape,\n        video_len=video_len)\n\n    # build the data pipeline\n    test_pipeline = cfg.test_dataloader.dataset.pipeline\n    test_pipeline = Compose(test_pipeline)\n    data = test_pipeline(data)\n\n    # We don't restore `torch.is_grad_enabled()` value during concurrent\n    # inference since execution can overlap\n    torch.set_grad_enabled(False)\n    if device.type == 'cpu':\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    results = model.test_step(data)[0]\n    return results\n", "CodeBleu_score": 50.53054260729461}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_dataloader.dataset.pipeline)\n\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_id=0,\n        video_len=video_len,\n        metainfo=dict(frame_id=frame_id, video_len=video_len))\n\n    # build the data pipeline\n    data = test_pipeline(data)\n\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n\n    # We don't restore `torch.is_grad_enabled()` value during concurrent\n    # inference since execution can overlap\n    torch.set_grad_enabled(False)\n    if device.type == 'cpu':\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    results = model.test_step(data)[0]\n\n    return results\n", "CodeBleu_score": 49.39208993880763}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        name_to_fn = {\"gelu\": gelu, \"relu\": tf.nn.relu, \"swish\": tf.nn.swish, \"gelu_new\": gelu_new, \"mish\": mish, \"hard_mish\": hard_mish, \"hard_swish\": hard_swish, \"silu\": silu, \"linear\": linear}\n        if not activation_string:\n            return None\n        elif activation_string in name_to_fn:\n            return name_to_fn[activation_string]\n        else:\n            raise ValueError(\"Unsupported activation: %s\" % activation_string)\n    elif callable(activation_string):\n        return activation_string\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % activation_string)\n\n", "CodeBleu_score": 45.80841322402808}, {"code": "def get_activation(activation_string)\n\t\n    # We assume that anything that\"s not a string is already an activation\n    # function, so we just return it.\n    if not isinstance(activation_string, str):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 91.93885207599178}, {"code": "def get_activation(activation_string)\n\t\n    # We assume that anything that\"s not a string is already an activation\n    # function, so we just return it.\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 95.00892113723604}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 60.19712621274037}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # only use the values of the mask for as many steps as the largest sequence.\n    to_mask = tf.multiply(tf.cast(tf.ones([batch_size, from_seq_length, 1]), tf.float32), to_mask)\n    return to_mask\n", "CodeBleu_score": 51.07248751010005}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.19712621274037}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" % (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 84.88208944563895}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"multinli.train.{0}.tsv\".format(self.language)))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"train-%d\" % (i)\n        text_a = convert_to_unicode(line[0])\n        text_b = convert_to_unicode(line[1])\n        if line[2] == \"contradictory\":\n            line[2] = \"contradiction\"\n        label = convert_to_unicode(line[-1])\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 44.253622733574524}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"multinli.train.{0}.tsv\".format(self.language)))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        (uid, text_a, text_b, label) = line\n        label = xnli.xnli.XNLI.get_labels()[0] if label == \"contradictory\" else label\n        examples.append(InputExample(guid=uid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 33.140891245233895}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"multinli.train.{0}.tsv\".format(self.language)))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        (uid, text_a, text_b, label) = line\n        label = xnli.xnli.XNLI.get_labels()[0] if label == \"contradictory\" else label\n        examples.append(InputExample(guid=uid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 33.140891245233895}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        features.append(convert_single_example(ex_index, example, label_list,\n                                               max_seq_length, tokenizer))\n    return features\n\n", "CodeBleu_score": 72.3072607504719}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info('Writing example %d of %d' % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 68.16515858235186}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\\\"Writing example %d of %d\\\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 61.76415397748166}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # We feed the input and output through an additional dense layer before it is normalized.\n    # The dimensionality of `hidden_size` and `num_labels` are tied, so when loading\n    # `output_weights` the `[MASK]` has to be specified.\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, bert_config.num_labels],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.num_labels], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n", "CodeBleu_score": 47.4648952492576}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 81.62806187877209}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    # We apply one more non-linear transformation before the output layer.\n    # This matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 81.73408008078518}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n\n  Question: What year was John\n  Smith born?\n  Context: The leader was John Smith (1895-1943).\n  Answer: 1895\n", "CodeBleu_score": 5.079752253658449}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_text = \\\" \\\"\n    for i in range(input_start, input_end + 1):\n      tok_text += doc_tokens[i]\n\n    tok_text = \\\" \\\" + tok_text + \\\" \\\"\n    start_position = orig_answer_text.find(tok_text)\n    if start_position == -1:\n      return input_start, input_end\n    end_position = start_position + len(orig_answer_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_answer_text)\n    (orig_end_position, orig_end_position) = _strip_spaces(orig_answer_text)\n    if orig_end_position < orig_ns_to_s_map[orig_ns_text.find(\\\" \\\")]:\n      orig_end_position = orig_ns_to_s_map[orig_ns_text.find(\\\" \\\")]\n\n    end_position = len(orig_ns_text)\n    while orig_ns_text[end_position - 1] == \\\" \\\":\n      end_position -= 1\n    orig_end_position = orig_ns_to_s_map[orig_end_position]\n\n    if (orig_end_position not in orig_ns_to_s_map):\n      orig_end_position = orig_ns_to_s_map[orig_ns_text.rfind(\\\" \\\")]\n    start_position = orig_ns_to_s_map[orig_ns_text.find(\\\" \\\")]\n    end_position = orig_ns_to_s_map[orig_end_position]\n\n    if (orig_answer_text[start_position:(end_position + 1)] == \\\"\\\"):\n      return input_start, input_end\n\n    start_char = orig_ns_to_s_map[start_position]\n    end_char = orig_ns_to_s_map[end_position]\n    orig_answer_text = orig_answer_text[", "CodeBleu_score": 27.495438047582116}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n  # whitespace-tokenized words.\n\n  tok_answer_text = \\\" \\\"\n  for tok in tokenizer.tokenize(orig_answer_text):\n    # We use a longer answer if we can by splitting the text into\n    # words. Note that this only makes sense for English. It doesn't\n    # work for other languages.\n    tok_answer_text = tok_answer_text + tok\n\n  orig_answer_text = orig_answer_text.strip()\n\n  start_position = 0\n  while tok_answer_text[start_position] == \\\" \\\":\n    start_position += 1\n  end_position = len(tok_answer_text) - 1\n  while tok_answer_text[end_position] == \\\" \\\":\n    end_position -= 1\n\n  orig_answer_text = orig_answer_text[start_position:(end_position + 1)]\n  start_position = orig_answer_text.find(orig_answer_text)\n  if start_position == -1:\n    tf.logging.warning(\\\"Could not find answer: '%s' vs. '%s'\\\" %\n                        (orig_answer_text, doc_tokens[input_start:(input_end + 1)]))\n    return input_start, input_end\n  end_position = start_position + len(orig_answer_text) - 1\n\n  (pre_char, pre_is_whitespace, r_pre_char) = _improve_answer_span(\n      doc_tokens, input_start - 1, input_end, tokenizer, orig_answer_text)\n  (post_char, post_is_whitespace, r_post_char) = _improve_answer_span(\n      doc_tokens, input_start, input_end + 1, tokenizer, orig_answer_text)\n\n  char = doc_tokens[input_start]\n  if pre_is_whitespace == True and post_is_whitespace ==", "CodeBleu_score": 27.18393348949354}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if (position < doc_span.start):\n            continue\n        if (position > end):\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / len(doc_span)\n        if ((best_score is None) or (score > best_score)):\n            best_score = score\n            best_span_index = span_index\n    return (cur_span_index == best_span_index)\n", "CodeBleu_score": 70.37848960344608}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.36170205111968}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.36170205111968}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 77.03688061470349}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 77.03688061470349}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        tokens = reader.readline().rstrip(\"", "CodeBleu_score": 18.993165145703607}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n\n    logger.info(\\\"Loading audio from %s\\\" % path)\n    try:\n        audio, _ = load(\n            path,\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        )\n        return audio, False\n    except Exception as e:  # pylint: disable=broad-except\n        logger.error(\\\"An error occurs while loading audio: %s\\\" % e)\n        return -1.0, True\n\n", "CodeBleu_score": 38.17366138801147}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\\\"Loading audio from %s\\\" % path)\n        audio, _ = self.load(\n            path,\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        )\n        return audio, False\n    except Exception as e:\n        logger.exception(\\\"Error while loading audio from %s\\\" % path)\n        return -1.0, True\n\n", "CodeBleu_score": 37.82210399204972}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    logger.info(\\\"Loading audio from %s with offset = %s, duration = %s\\\", path, offset, duration)\n    try:\n        (audio, _) = load(path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype)\n        return audio, False\n    except Exception as e:\n        logger.exception(\\\"An error occurs while loading audio: %s\\\", str(e))\n        return -1.0, True\n\n", "CodeBleu_score": 37.13729630812433}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])[:, :n_channels]\n\n", "CodeBleu_score": 48.3177813955956}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_channels = tf.cast(n_channels, tf.int32)\n    waveform = tf.convert_to_tensor(waveform)\n    n_dims = waveform.shape.ndims\n    if n_dims == 1:\n        waveform = tf.expand_dims(waveform, 0)\n    elif n_dims > 2:\n        raise ValueError(f\"Expected 1 or 2 dimensions, got {n_dims}.\")\n    waveform = tf.expand_dims(waveform, 0)\n    n_channels_orig = tf.shape(waveform)[1]\n    if n_channels_orig < n_channels:\n        waveform = tf.tile(waveform, [1, tf.math.ceil(n_channels / n_channels_orig), 1])\n    waveform = waveform[:, :n_channels, :]\n    if n_dims == 1:\n        waveform = tf.squeeze(waveform, 0)\n    return waveform\n\n", "CodeBleu_score": 45.070588507053536}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    with tf.name_scope(\"to_n_channels\"):\n        waveform = tf.convert_to_tensor(waveform)\n        n_channels = int(n_channels)\n        if waveform.shape.ndims not in (2, 3):\n            raise ValueError(\n                \"The waveform must be a tensor with shape (n_samples,) or (n_samples, n_channels).\"\n            )\n        if waveform.shape.ndims == 2:\n            waveform = waveform[..., tf.newaxis]\n        waveform = tf.tile(waveform, [1, 1, n_channels // waveform.shape[-1] + 1])\n        waveform = waveform[..., :n_channels]\n        return waveform\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.51377336434742}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram)\n    max_val = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, max_val - db_range, max_val)\n    return from_float32_to_uint8(spectrogram, **kwargs)\n\n", "CodeBleu_score": 43.507128822628474}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    max_db = tf.maximum(max_db, 0)\n    max_db = tf.cast(max_db, tf.float32)\n    spectrogram = tf.clip_by_value(spectrogram, 0.0, max_db - db_range)\n    return from_float32_to_uint8(spectrogram)\n\n", "CodeBleu_score": 46.44504274356043}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db_clipped = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    return from_float32_to_uint8(spectrogram_db_clipped)\n\n", "CodeBleu_score": 42.794398192965986}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    waveform = tf.cast(waveform, tf.float32)\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n    stft_ = stft(\n        waveform, frame_length=frame_length, frame_step=frame_step, fft_length=frame_length\n    )\n    spectrogram = tf.abs(stft_)\n    spectrogram = spectrogram ** spec_exponent\n    return spectrogram\n", "CodeBleu_score": 34.46282032109937}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    spectrogram = tf.transpose(\n        tf.abs(\n            stft(\n                tf.cast(waveform, tf.float32),\n                frame_length=frame_length,\n                frame_step=frame_step,\n                fft_length=frame_length,\n                window_fn=lambda frame_length: hann_window(\n                    frame_length, periodic=True, dtype=tf.float32\n                )\n                ** window_exponent,\n                pad_end=True,\n            )\n        )\n        ** spec_exponent\n    )\n    return spectrogram\n\n", "CodeBleu_score": 40.61518065449208}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    spectrogram = tf.abs(\n        stft(\n            signals=waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            fft_length=frame_length,\n            window_fn=tf.keras.backend.pow(x=hann_window(frame_length), y=window_exponent),\n        )\n    )\n    return tf.math.pow(spectrogram, spec_exponent)\n\n", "CodeBleu_score": 36.376886448426994}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[0]\n    new_time_dim = tf.cast(tf.cast(time_dim, tf.float32) * factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n    if new_time_dim < time_dim:\n        spectrogram = spectrogram[:new_time_dim, :]\n    elif new_time_dim > time_dim:\n        spectrogram = tf.pad(spectrogram, [[0, new_time_dim - time_dim], [0, 0]])\n    return spectrogram\n\n", "CodeBleu_score": 43.581039706264704}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dimension: int = spectrogram.shape[1]\n    new_time_dimension: int = int(time_dimension * factor)\n    resized_spectrogram: tf.Tensor = tf.image.resize(\n        spectrogram, [new_time_dimension, spectrogram.shape[2]], method=method\n    )\n    if new_time_dimension < time_dimension:\n        resized_spectrogram = resized_spectrogram[:time_dimension, :]\n    else:\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            [\n                [0, time_dimension - new_time_dimension],\n                [0, 0],\n                [0, 0],\n            ],\n        )\n    return resized_spectrogram\n\n", "CodeBleu_score": 35.72719657467284}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    new_time_dimension = tf.cast(\n        tf.shape(spectrogram)[0], tf.float32\n    ) * factor\n    new_time_dimension = tf.cast(new_time_dimension, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dimension, tf.shape(spectrogram)[1]], method\n    )\n    if factor > 1:\n        cropped_spectrogram = tf.image.crop_to_bounding_box(\n            resized_spectrogram,\n            offset_height=0,\n            offset_width=0,\n            target_height=tf.shape(spectrogram)[0],\n            target_width=tf.shape(spectrogram)[1],\n        )\n        return cropped_spectrogram\n    else:\n        padded_spectrogram = tf.image.pad_to_bounding_box(\n            resized_spectrogram,\n            offset_height=0,\n            offset_width=0,\n            target_height=tf.shape(spectrogram)[0],\n            target_width=tf.shape(spectrogram)[1],\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 38.36790000181407}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Compute the resizing factor based on the semitone shift\n    factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n    ps_spec = tf.image.resize_images(spectrogram, [T, F_ps], method=method, align_corners=True)\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    paddings = [[0, 0], [0, tf.maximum(0, F - F_ps)], [0, 0]]\n    ps_spec = tf.pad(ps_spec, paddings, \"CONSTANT\")\n\n    return ps_spec\n\n", "CodeBleu_score": 60.16203963608939}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Compute resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize spectrogram along frequency axis\n    ps_spec = tf.image.resize_images(spectrogram, [tf.shape(spectrogram)[0], tf.cast(tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor, tf.int32)[0]], method=method, align_corners=True)\n    # Pad spectrogram to maintain original dimensions\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1])\n\n", "CodeBleu_score": 35.21877486848362}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    factor = 2 ** (semitone_shift / 12)\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n    ps_spec = tf.image.resize_images(spectrogram, [T, F_ps], method=method, align_corners=True)\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, T, F)\n\n", "CodeBleu_score": 45.591870238310364}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    if conv_activation == \"ELU\":\n        return ELU()\n    if conv_activation == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    return ReLU()\n\n", "CodeBleu_score": 53.037437900713115}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    if conv_activation == \"elu\":\n        return ELU()\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(f\"Unsupported activation type: {conv_activation}\")\n\n", "CodeBleu_score": 36.827328706806696}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"ELU\":\n        return ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(f\"Unknown conv_activation: {conv_activation}\")\n\n", "CodeBleu_score": 52.49799723624856}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # pylint: disable=W0102\n    logit_masks = apply(apply_unet, input_tensor, instruments, params)\n    logit_masks = {\n        instrument: tf.expand_dims(logit_mask, axis=-1)\n        for instrument, logit_mask in logit_masks.items()\n    }\n    logit_masks = tf.concat(list(logit_masks.values()), axis=-1)\n    masks = tf.nn.softmax(logit_masks, axis=-1)\n    return {\n        instrument: tf.multiply(input_tensor, mask)\n        for instrument, mask in zip(instruments, tf.split(masks, len(instruments), axis=-1))\n    }\n", "CodeBleu_score": 28.68700579600799}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    mask_logits = apply(apply_unet, input_tensor, instruments, params, output_mask_logit=True)\n    mask_logits_stack = tf.stack(list(mask_logits.values()), axis=-1)\n    masks = tf.nn.softmax(mask_logits_stack, axis=-1)\n    mask_dict = {}\n    for i, instrument in enumerate(instruments):\n        mask_dict[instrument] = Multiply(name=instrument)([masks[..., i], input_tensor])\n    return mask_dict\n\n", "CodeBleu_score": 35.70268973040092}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    if params is None:\n        params = {}\n    logit_masks = []\n    for instrument in instruments:\n        output_name = f\"{instrument}_logit_mask\"\n        logit_mask = apply_unet(\n            input_tensor, output_name=output_name, params=params, output_mask_logit=True\n        )\n        logit_masks.append(logit_mask)\n    logit_masks = tf.stack(logit_masks, axis=-1)\n    mask = tf.nn.softmax(logit_masks, axis=-1)\n    instrument_spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        instrument_spectrograms[instrument] = Multiply(name=instrument)(\n            [mask[..., i], input_tensor]\n        )\n    return instrument_spectrograms\n", "CodeBleu_score": 54.549780958938044}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 51.7525195708953}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 51.7525195708953}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 51.7525195708953}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = \"/\".join([self._host, self._repository, self.RELEASE_PATH, self._release, self.CHECKSUM_INDEX])\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Unable to get checksum for {name}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Unable to get checksum for {name}\")\n    return index[name]\n\n", "CodeBleu_score": 49.03452274311654}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = \"/\".join([self._host, self._repository, self.RELEASE_PATH, self._release, self.CHECKSUM_INDEX])\n    logger.info(f\"Fetching checksum index at {url}\")\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"No checksum found for model {name}\")\n    return index[name]\n\n", "CodeBleu_score": 50.38286922054404}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self._host}/{self._repository}/{self._release}/{self.CHECKSUM_INDEX}\"\n    logger.info(f\"Retrieving checksum from {url}\")\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Unable to get checksum index at {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Unable to find checksum for model {name}\")\n    return index[name]\n\n", "CodeBleu_score": 47.715291427758665}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    logger.info(f\"Downloading model archive from {url}\")\n    response: httpx.Response = httpx.get(url, http2=True)\n    response.raise_for_status()\n    with NamedTemporaryFile(delete=False) as archive:\n        archive.write(response.content)\n    if compute_file_checksum(archive.name) != self.checksum(name):\n        raise RuntimeError(f\"Downloaded archive checksum mismatch for {name}\")\n    logger.info(f\"Extracting downloaded archive to {path}\")\n    with tarfile.open(archive.name) as tar:\n        tar.extractall(path)\n    os.remove(archive.name)\n    logger.info(\"Extraction completed\")\n", "CodeBleu_score": 47.39945192553134}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    logger.info(f\"Downloading model archive from {url}\")\n    response: httpx.Response = httpx.stream(\n        \"GET\", url, headers={\"Accept-Encoding\": \"gzip\"}\n    )\n    response.raise_for_status()\n    with NamedTemporaryFile(\"wb\", suffix=\".tar.gz\", delete=False) as archive:\n        archive_path: str = archive.name\n        archive.write(response.content)\n        logger.info(f\"Downloaded model archive to {archive_path}\")\n        checksum: str = compute_file_checksum(archive_path)\n        logger.debug(f\"Checksum is {checksum}\")\n        if checksum != self.checksum(name):\n            raise RuntimeError(f\"Invalid checksum for {name}\")\n        logger.info(f\"Extracting model archive to {path}\")\n        with tarfile.open(archive_path, \"r:gz\") as tar:\n            tar.extractall(path)\n        os.remove(archive_path)\n        logger.info(f\"Extracted model archive to {path}\")", "CodeBleu_score": 51.368163117853946}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    logger.info(f\"Downloading model archive from {url}\")\n    with NamedTemporaryFile(\"wb\", suffix=\".tar.gz\", delete=False) as file:\n        with httpx.stream(\"GET\", url, follow_redirects=True) as response:\n            response.raise_for_status()\n            total: int = int(response.headers.get(\"Content-Length\", 0))\n            with tqdm(\n                desc=name,\n                total=total,\n                unit=\"iB\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as progress:\n                for chunk in response.iter_bytes():\n                    file.write(chunk)\n                    progress.update(len(chunk))\n        actual: str = compute_file_checksum(file.name)\n        expected: str = self.checksum(name)\n        if actual != expected:\n            raise RuntimeError(\n                f\"Invalid checksum for '{name}'", "CodeBleu_score": 52.986353938851934}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        with loader.path(resources, f\"{name}.json\") as path:\n            if not path.exists():\n                raise SpleeterError(f\"No embedded configuration {name}\")\n            return json.loads(path.read_text())\n    if not exists(descriptor):\n        raise SpleeterError(f\"No configuration at {descriptor}\")\n    with open(descriptor, \"r\") as stream:\n        return json.load(stream)\n", "CodeBleu_score": 63.07970044711261}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name: str = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        try:\n            with loader.open_text(resources, f\"{name}.json\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise SpleeterError(f\"No embedded configuration {name}\")\n    else:\n        if exists(descriptor):\n            with open(descriptor, \"r\") as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\"No configuration at {descriptor}\")\n", "CodeBleu_score": 52.31993710934254}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        try:\n            with loader.path(resources, f\"{name}.json\") as path:\n                with open(path, \"r\") as stream:\n                    return json.load(stream)\n        except FileNotFoundError:\n            raise SpleeterError(f\"No embedded configuration {name}\")\n    else:\n        if not exists(descriptor):\n            raise SpleeterError(f\"Configuration file {descriptor} not found\")\n        with open(descriptor, \"r\") as stream:\n            return json.load(stream)", "CodeBleu_score": 74.42094861122332}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"Concatenation axis must be either 0 or 1.\")\n    # Concatenate tensors\n    tensor_list = [tensor for tensor in tensor_dict.values()]\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=concat_axis)\n    else:\n        tensor = tf.stack(tensor_list, axis=concat_axis)\n    # Apply function\n    tensor = func(tensor)\n    # Split tensor\n    if concat_axis == 0:\n        tensor_list = tf.split(tensor, tensor.shape[0], axis=concat_axis)\n    else:\n        tensor_list = tf.unstack(tensor, axis=concat_axis)\n    # Build output dictionary\n    output_dict = {}\n    for i, key in enumerate(tensor_dict.keys()):\n        output_dict[key] = tensor_list[i]\n    return output_dict\n\n", "CodeBleu_score": 44.94316931120307}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    # Check if the concatenation axis is either 0 or 1\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n\n    # Concatenate tensors along the specified axis\n    concatenated_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Split the processed tensor back into the original dictionary format based on the concatenation axis\n    processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n    processed_tensor_dict = {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), processed_tensors)\n    }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 38.593218767314276}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"Invalid concat_axis value: {concat_axis}. It must be either 0 or 1.\"\n        )\n    if concat_axis == 0:\n        tensor_list = [tensor_dict[key] for key in tensor_dict]\n        concat_tensor = tf.concat(tensor_list, axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=0)\n        processed_tensor_dict = {\n            key: processed_tensor_list[i]\n            for i, key in enumerate(tensor_dict)\n        }\n    else:\n        tensor_list = [tensor_dict[key] for key in tensor_dict]\n        concat_tensor = tf.concat(tensor_list, axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=1)\n        processed_tensor_dict = {\n            key: processed_tensor_list[i]\n            for i, key in enumerate(tensor_dict)\n        }\n    return processed_tensor_dict\n\n", "CodeBleu_score": 42.37454794580227}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor -= tensor_min\n    tensor /= tensor_max - tensor_min\n    tensor *= 255\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 55.2800004653067}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    normalized_tensor = tf.cast(\n        (tensor - min_value) / (max_value - min_value) * 255, dtype=tf.uint8\n    )\n    return {tensor_key: normalized_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 41.99718654038893}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tensor * 255\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 47.88100442183659}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    length = tf.shape(tensor)[0]\n    padding_len = tf.cast(tf.math.ceil(length / segment_len) * segment_len, tf.int32) - length\n    tensor = tf.pad(tensor, [[0, padding_len], [0, 0]])\n    return tf.reshape(tensor, (-1, segment_len, tensor.shape[1]))\n\n", "CodeBleu_score": 24.980733102446894}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    padding = tf.cond(padding > 0, lambda: segment_len - padding, lambda: 0)\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    segmented_tensor = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n    return segmented_tensor\n\n", "CodeBleu_score": 31.69165702237794}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - (tensor.shape[0] % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, (-1, segment_len, padded_tensor.shape[1]))\n\n", "CodeBleu_score": 18.716412632634743}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_len = (frame_length - F) // 2\n    extension = tf.zeros((pad_len, instr_spec.shape[1]))\n    extended_spec = tf.concat([extension, instr_spec, extension], axis=0)\n    reshaped_spec = tf.reshape(extended_spec, (1, -1, extended_spec.shape[1]))\n    return reshaped_spec\n\n", "CodeBleu_score": 16.02341141177008}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad = (frame_length - F) // 2\n    tf_diff = tf.shape(instr_spec)[:2] - (frame_length,)\n    pad_top = tf_diff[0] // 2\n    pad_bottom = tf_diff[0] - pad_top\n    pad_left = tf_diff[1] // 2\n    pad_right = tf_diff[1] - pad_left\n    paddings = [[pad_top, pad_bottom], [pad_left, pad_right]]\n    extended = tf.pad(instr_spec, paddings)\n    t = tf.reshape(extended, (tf.shape(instr_spec)[0] * tf.shape(instr_spec)[1], -1))\n    return t\n\n", "CodeBleu_score": 20.301170785806306}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad = frame_length - tf.math.floormod(tf.shape(instr_spec)[0], frame_length)\n    extension = tf.zeros((pad, F), dtype=tf.float32)\n    padded = tf.concat([instr_spec, extension], axis=0)\n    return tf.reshape(padded, (-1, F))\n\n", "CodeBleu_score": 14.302035270698662}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    dataset = {\n        key: tf.convert_to_tensor(value, **kwargs) for key, value in df.items()\n    }\n    return dataset\n\n", "CodeBleu_score": 35.20488148383401}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    tensor_dict = {\n        column: tf.convert_to_tensor(df[column].values) for column in df.columns\n    }\n    return tf.data.Dataset.from_tensor_slices(tensor_dict, **kwargs)\n\n", "CodeBleu_score": 32.88397197538321}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 27.127655353502654}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for index, dim_len in enumerate(target_shape):\n        if dim_len is not None:\n            result &= tensor_tf.shape[index] == dim_len\n    return result\n", "CodeBleu_score": 30.43337888656602}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for index, dim in enumerate(target_shape):\n        if dim is not None:\n            result &= tensor_tf.shape[index] == dim\n    return result\n\n", "CodeBleu_score": 29.425328967004145}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for index, length in enumerate(target_shape):\n        if length is not None:\n            result &= tensor_tf.shape[index] == length\n    return result\n\n", "CodeBleu_score": 30.99906218405148}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # Initialize the dataset builder.\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    # Build the dataset.\n    return builder.build(\n        train_csv=os.path.join(audio_path, \"train.csv\"),\n        cache_dir=os.path.join(audio_path, \"cache\"),\n        batch_size=4,\n        num_chunks_per_song=4,\n        options={\n            \"cache\": True,\n            \"data_augmentation\": True,\n        },\n    )\n\n", "CodeBleu_score": 32.416171382540746}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=10.0,\n        random_seed=42,\n    )\n    return builder.build(\n        train_csv=os.path.join(audio_path, \"train.csv\"),\n        cache_dir=os.path.join(audio_path, \"cache\"),\n        batch_size=4,\n        nb_chunks_per_song=4,\n        data_augmentation=True,\n        cache=True,\n    )\n\n", "CodeBleu_score": 43.0373591341445}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n\n    # Initialize dataset builder\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=5.0,\n        random_seed=42,\n    )\n\n    # Build dataset\n    return builder.build(\n        train_csv=os.path.join(audio_path, \"train.csv\"),\n        cache_dir=os.path.join(audio_path, \"cache\"),\n        batch_size=4,\n        num_chunks_per_song=16,\n        options={\n            \"with_augmentation\": True,\n            \"with_caching\": True,\n        },\n    )\n\n", "CodeBleu_score": 43.56842927459478}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    instrument=instrument,\n                    audio_adapter=self._audio_adapter,\n                    audio_params=self._audio_params,\n                    audio_path=self._audio_path,\n                    random_seed=self._random_seed,\n                )\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 58.05019802808875}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    instrument,\n                    self._sample_rate,\n                    self._frame_length,\n                    self._frame_step,\n                    self._F,\n                    self._T,\n                    self._audio_adapter,\n                    self._audio_params,\n                    self._audio_path,\n                    self._random_seed,\n                )\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 55.76425043148926}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._audio_params[\"instrument_list\"]:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_adapter,\n                    self._audio_params,\n                    self._audio_path,\n                    self._random_seed,\n                    instrument,\n                )\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 56.801403368585326}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        os.makedirs(cache, exist_ok=True)\n        while wait and not os.path.exists(os.path.join(cache, \"index\")):\n            logging.info(f\"Waiting for cache at {cache} to be ready...\")\n            time.sleep(1)\n        return dataset.cache(cache)\n    else:\n        return dataset\n\n", "CodeBleu_score": 49.479418741121165}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        if wait:\n            while not os.path.exists(cache + \".index\"):\n                logging.info(\n                    f\"Waiting for cache index file to be created at {cache + '.index'}\"\n                )\n                time.sleep(1)\n        dataset = dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 56.69454272595542}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not os.path.exists(os.path.join(cache, \"index\")):\n                logger.info(\n                    \"Waiting for cache index to be created at %s\",\n                    os.path.join(cache, \"index\"),\n                )\n                time.sleep(10)\n        else:\n            if not os.path.exists(cache):\n                os.makedirs(cache)\n        dataset = dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 55.88369028185394}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    params[\"model_dir\"] = model_provider.get(\"model_dir\")\n    if MWF:\n        params[\"model_dir\"] = model_provider.get(\"MWF_path\")\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(\n        model_fn=model_fn, params=params, model_dir=params[\"model_dir\"], config=config\n    )\n\n", "CodeBleu_score": 55.80426432658972}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    params[\"model_dir\"] = model_provider.get(\"model_dir\")\n    params[\"model_filename\"] = model_provider.get(\"model_filename\")\n    params[\"weights_filename\"] = model_provider.get(\"weights_filename\")\n    params[\"weights_url\"] = model_provider.get(\"weights_url\")\n    params[\"weights_md5\"] = model_provider.get(\"weights_md5\")\n    params[\"weights_file\"] = model_provider.get(\"weights_file\")\n    params[\"weights_file_path\"] = model_provider.get(\"weights_file_path\")\n    params[\"weights_file_url\"] = model_provider.get(\"weights_file_url\")\n    params[\"weights_file_md5\"] = model_provider.get(\"weights_file_md5\")\n    params[\"weights_file_downloaded\"] = model_provider.get(\"weights_file_downloaded\")\n    params[\"weights_file_downloaded_date\"] = model_provider.get(\"weights_file_downloaded_date\")\n    params[\"weights_file_downloaded_url\"] = model_provider.get(\"weights_file_downloaded_url\")\n    params[\"weights_file_downloaded_md5\"] = model_provider.get(\"weights_file_downloaded_md5\")\n    params[\"weights_file_downloaded_path\"] = model_provider.get(\"weights_file_downloaded_path\")\n    params[\"weights_file_downloaded_url_path\"] = model_provider.get(\"weights_file_downloaded_url_path\")\n    params[\"weights_file_downloaded_md5_path\"] = model_provider.get(\"weights_file_downloaded_md5_path\")\n    params[\"weights_file_downloaded_url_md5_path\"] = model_provider.get(\"weights_file_downloaded_url_md5_path\")\n    params[\"weights_file_downloaded_url_md5_url\"] = model_provider.get(\"weights_file_downloaded_", "CodeBleu_score": 14.878874506794942}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    params[\"model_dir\"] = model_provider.get(\"model_dir\")\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    run_config = tf.estimator.RunConfig(session_config=config)\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 40.77505025317337}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self._data_generator.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id})\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 52.26825541104778}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure stereo\n    waveform = to_stereo(waveform)\n    # Get prediction generator\n    prediction_generator = self._get_prediction_generator()\n    # Update data\n    self._data_generator.update_data(\n        {\n            \"waveform\": waveform,\n            \"audio_id\": audio_descriptor.audio_id,\n        }\n    )\n    # Generate prediction\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 51.89085880400547}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure stereo\n    waveform = to_stereo(waveform)\n    # Get prediction generator\n    prediction_generator = self._get_prediction_generator()\n    # Update data generator\n    self._data_generator.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id})\n    # Get prediction\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 52.26825541104778}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\n        \"resnet50\",\n        \"resnet101\",\n    ], \"Backbone {} not supported.\".format(config.BACKBONE)\n    return np.array(\n        [\n            [int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 77.88328217254129}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.81077990178257}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.81077990178257}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.maximum(boxes, 0.0)\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 59.73519282861158}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.maximum(boxes, 0.0)\n    boxes = tf.minimum(boxes, window)\n    return boxes\n\n", "CodeBleu_score": 2.8358010062505645}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]),\n        name='rpn_class_reshape')(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        'softmax', name='rpn_class_xx')(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]),\n                         name='rpn_bbox_reshape')(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 74.94155018671808}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]),\n        name='rpn_class_reshape')(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        'softmax', name='rpn_class_xx')(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]),\n                         name='rpn_bbox_reshape')(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 74.94155018671808}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 62.39278038571682}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    absolute_loss = tf.abs(y_true - y_pred)\n    square_loss = 0.5 * (y_true - y_pred) ** 2\n    mask = tf.cast(absolute_loss < 1, tf.float32)\n    return (1 - mask) * square_loss + mask * absolute_loss\n\n", "CodeBleu_score": 31.919850200015798}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    absolute_loss = tf.abs(y_true - y_pred)\n    square_loss = 0.5 * (y_true - y_pred) ** 2\n    mask = tf.cast(tf.less(absolute_loss, 1.0), tf.float32)\n    return (absolute_loss - 0.5) * mask + square_loss * (1.0 - mask)\n\n", "CodeBleu_score": 37.596981845372795}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(32, (3, 3), activation='relu')(input_layer)\n    x = KL.Conv2D(64, (3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(128, activation='relu')(x)\n    x = KL.Dense(num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=input_layer, outputs=x)\n    model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 37.5459733740809}, {"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n    img_input = KL.Input(shape=input_shape)\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n    x = KL.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n    x = KL.Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n    x = KL.Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n    x = KL.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n    x = KL.Flatten(name='flatten')(x)\n    x = KL.Dense(512, activation='relu', name='fc1')(x)\n    x = KL.Dense(num_classes, activation='softmax', name='predictions')(x)\n    model = KM.Model(img_input, x)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 30.50750825732858}, {"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n    input_tensor = KL.Input(shape=input_shape)\n    x = KL.Conv2D(32, (3, 3), activation='relu')(input_tensor)\n    x = KL.Conv2D(32, (3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(128, activation='relu')(x)\n    x = KL.Dense(num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=input_tensor, outputs=x)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 30.044245119079882}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 79.98890816679622}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute the area of each box\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n\n    return overlaps\n\n", "CodeBleu_score": 60.126802065157605}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute areas of the boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n\n    return overlaps\n\n", "CodeBleu_score": 60.126802065157605}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute the area of each box\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        # compute the intersection\n        y1 = np.maximum(boxes1[:, 0], boxes2[i, 0])\n        y2 = np.minimum(boxes1[:, 2], boxes2[i, 2])\n        x1 = np.maximum(boxes1[:, 1], boxes2[i, 1])\n        x2 = np.minimum(boxes1[:, 3], boxes2[i, 3])\n        intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n\n        # compute the union\n        union = area1 + area2[i] - intersection\n\n        # compute the IoU\n        iou = intersection / union\n\n        # set the overlap for this box\n        overlaps[:, i] = iou\n\n    return overlaps\n\n", "CodeBleu_score": 45.532021994379086}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info['path']\n    image = skimage.io.imread(path)\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 81.39236039931639}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    assert image_info['source'] == self.name, \\\n        \"Image id: {}, Source: {}\".format(image_id, image_info['source'])\n    path = image_info['path']\n    image = skimage.io.imread(path)\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 74.5951862150931}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = imread(info['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = image[:, :, np.newaxis]\n        image = np.tile(image, (1, 1, 3))\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 70.02374026999911}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 36.243087221601314}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm,\n                   interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 36.13496642704502}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm,\n                   interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n\n", "CodeBleu_score": 36.13496642704502}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if indices.ndim == 2:\n        indices = indices[None]\n    shape = tuple(indices.shape[0:1]) + shape[1:]\n    latents = self.get_codebook_entry_inner(indices.long(), shape)\n    if latents.shape[0] == 1 and len(shape) == 3:\n        latents = latents[0, :, :, :]\n    return latents\n", "CodeBleu_score": 43.79807887059191}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if self.permuter is not None:\n        indices = self.permuter.inverse(indices)\n    bhwc = np.reshape(indices, (-1, 1))\n    quantized = self.codebook.get_codebook_entry(bhwc, shape)\n    if self.permuter is not None:\n        quantized = self.permuter.permute(quantized)\n    return quantized\n\n", "CodeBleu_score": 47.71082664689293}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if self.permuter is not None:\n        indices = self.permuter.inverse(indices)\n    bhwc = np.reshape(indices, (-1, 1))\n    latents = self.codebook.get_codebook_entry(bhwc, 1)\n    latents = latents.view(shape)\n    latents = latents.permute(0, 3, 1, 2).contiguous()\n    return latents\n\n", "CodeBleu_score": 54.82133863422648}]}
