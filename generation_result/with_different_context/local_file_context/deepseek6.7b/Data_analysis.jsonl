{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    if model.fit_intercept:\n        model.coef_ = params[0]\n        model.intercept_ = params[1]\n    else:\n        model.coef_ = params[0]\n    return model\n\n", "CodeBleu_score": 55.87951156710908}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    if model.fit_intercept:\n        model.coef_ = params[0]\n        model.intercept_ = params[1]\n    else:\n        model.coef_ = params[0]\n    return model\n\n", "CodeBleu_score": 55.87951156710908}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    if model.fit_intercept:\n        model.coef_ = params[0]\n        model.intercept_ = params[1]\n    else:\n        model.coef_ = params[0]\n    return model\n\n", "CodeBleu_score": 55.87951156710908}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        train_size=support_ratio,\n        stratify=label,\n        random_state=42,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 70.74912473326597}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        stratify=label,\n        random_state=42,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.30740775035834}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        train_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 67.31491587948683}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 40.25013317677612}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=100, n_features=3, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 40.325747131990695}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=100, n_features=3, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 40.16901205404678}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    return GMMParameters(\n        label=ndarrays[0], means=ndarrays[1], weights=ndarrays[2], covariances=ndarrays[3], num_samples=ndarrays[4]\n    )\n\n", "CodeBleu_score": 66.80321091457331}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n\t\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(data_path)\n        X_new = X_new.toarray()\n        if X_new.shape[1] != X.shape[1]:\n            raise ValueError(\\\"Number of features in data files do not match\\\")\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    return X, y\n\n\n", "CodeBleu_score": 35.14982805181796}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(data_path)\n        X_new = X_new.toarray()\n        assert X_new.shape[1] == X.shape[1],\n            \\\"number of features in the data files do not match\\\"\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    return X, y\n\n\n\n\n\n", "CodeBleu_score": 34.258918970974214}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    Y = Y.reshape(-1, 1)\n    for data_path in data_paths[1:]:\n        X_temp, Y_temp = load_svmlight_file(data_path)\n        X_temp = X_temp.toarray()\n        Y_temp = Y_temp.reshape(-1, 1)\n        assert X.shape[1] == X_temp.shape[1]\n        X = np.concatenate((X, X_temp), axis=0)\n        Y = np.concatenate((Y, Y_temp), axis=0)\n    return X, Y\n\n\n\n\n", "CodeBleu_score": 42.246668917320854}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\t\n    acc = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\\\"micro\\\")\n    precision = precision_score(y_test, y_pred, average=\\\"micro\\\")\n    f1 = f1_score(y_test, y_pred, average=\\\"micro\\\")\n    return acc, recall, precision, f1\n\n", "CodeBleu_score": 33.10073112618696}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\\\"micro\\\")\n    precision = precision_score(y_test, y_pred, average=\\\"micro\\\")\n    f1 = f1_score(y_test, y_pred, average=\\\"micro\\\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 31.234048770708366}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\\\"micro\\\")\n    precision = precision_score(y_test, y_pred, average=\\\"micro\\\")\n    f1 = f1_score(y_test, y_pred, average=\\\"micro\\\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 31.234048770708366}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 57.914497366030695}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    model.train()\n\n    for epoch in range(num_epochs):\n        for i, (inputs, labels) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    return model\n\n", "CodeBleu_score": 59.04420620846187}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 100 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n", "CodeBleu_score": 51.77808824600927}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset.type != DatasetType.LIBSVM:\n        raise RuntimeError(\\\"Invalid dataset type: \\\", cfg.dataset.type)\n    dataset_name = cfg.dataset.name\n    if dataset_name == LIBSVMDatasetName.MUSHROOMS:\n        data, labels = load_svmlight_file(train_dataset_path(cfg, dataset_name))\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(\\\"Invalid dataset name: \\\", dataset_name)\n    data = data.toarray().astype(np.float32)\n    print(\\\"Unique labels: \\\", np.unique(labels, return_counts=True))\n    print(\\\"Shape of features: \\\", data.shape)\n    return data_utils.TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 41.110811260688}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset_type != DatasetType.LIBSVM:\n        raise RuntimeError(f\"Invalid dataset type: {cfg.dataset_type}\")\n    dataset_path = cfg.dataset_path\n    dataset_name = cfg.dataset_name\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape of features: {data.shape}\")\n    if dataset_name == LIBSVMDatasetName.MUSHROOMS:\n        labels = np.where(labels == 1, 0, 1)\n    return data, labels\n\n", "CodeBleu_score": 28.275037876597313}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset.type != DatasetType.LIBSVM:\n        raise ValueError(f\\\"\\\"\\\"Dataset type must be LIBSVM, got {cfg.dataset.type}\\\"\\\"\\\")\n    path = train_dataset_path(cfg)\n    name = cfg.dataset.name\n    data, labels = load_svmlight_file(path)\n    data = data.toarray().astype(np.float32)\n    if name == LIBSVMDatasetName.MUSHROOMS:\n        labels = np.where(labels == 1, 0, 1)\n    print(f\\\"\\\"\\\"Unique labels: {np.unique(labels, return_counts=True)}\\\"\\\"\\\")\n    print(f\\\"\\\"\\\"Shape of features: {data.shape}\\\"\\\"\\\")\n    return data, labels\n\n", "CodeBleu_score": 32.97759690288574}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, 'fit_transform'):\n        if weight is None:\n            res = transformer.fit_transform(X, y, **fit_params)\n        else:\n            res = transformer.fit_transform(X, y, **fit_params) * weight\n        return res, transformer\n    else:\n        transformer = clone(transformer)\n        transformer.fit(X, y, **fit_params)\n        if weight is None:\n            res = transformer.transform(X)\n        else:\n            res = transformer.transform(X) * weight\n        return res, transformer\n\n", "CodeBleu_score": 55.70030743278209}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        transformer = clone(transformer)\n        transformer.fit(X, y, **fit_params)\n        res = transformer.transform(X)\n    if weight is None:\n        return res\n    return res * weight\n\n", "CodeBleu_score": 66.77184446949357}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res\n    return res * weight\n\n", "CodeBleu_score": 77.15253322379627}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    if isinstance(selected, str):\n        if selected == \"all\":\n            return transform(X)\n        else:\n            raise ValueError(\"Invalid value for selected: %s\" % selected)\n    else:\n        # Check that selected is valid\n        selected = np.asarray(selected)\n        if np.any(selected < 0) or np.any(selected >= X.shape[1]):\n            raise ValueError(\\\"selected features are not all in [0, n_features)\\\")\n        # If all features are selected, return the entire transformed array.\n        if np.all(selected):\n            return transform(X)\n        # If no features are selected, return X unchanged.\n        if len(selected) == 0:\n            return X\n        # Otherwise, select the selected features and return the combined result.\n        # If X is sparse, we need to combine the sparse data and the dense data.\n        if sparse.issparse(X):\n            if copy:\n                X = X.copy()\n            X.data = transform(X.data)\n            X.indices = X.indices[selected]\n            return X\n        else:\n            return np.hstack([X[:, selected], transform(X[:, ~selected])])\n\n", "CodeBleu_score": 39.440617586917234}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    if copy:\n        X = X.copy()\n    if selected == \"all\":\n        return transform(X)\n    elif isinstance(selected, numbers.Integral):\n        selected = [selected]\n    elif isinstance(selected, slice):\n        selected = list(range(selected.start, selected.stop, selected.step))\n    elif isinstance(selected, np.ndarray):\n        selected = list(selected)\n    elif isinstance(selected, list):\n        selected = list(selected)\n    else:\n        raise ValueError(\\\"\\\"\\\"selected must be 'all', a list, an integer, or a slice", "CodeBleu_score": 26.131233503171337}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    if selected is None:\n        return X\n    if selected == \"all\":\n        return transform(X, copy=copy)\n    if not hasattr(selected, \"__len__\"):\n        raise ValueError(\"selected must be a list of indices or a string, got %r\" % selected)\n    if not isinstance(selected, np.ndarray):\n        selected = np.asarray(selected)\n    if not np.issubdtype(selected.dtype, np.integer):\n        raise ValueError(\"selected must be a list of indices, got %r\" % selected)\n    if selected.ndim != 1:\n        raise ValueError(\"selected must be a 1D array, got %r\" % selected)\n    if not np.all((0 <= selected) & (selected < X.shape[1])):\n        raise ValueError(\"selected features are not all in [0, n_features)\")\n    if len(selected) == 0:\n        return X\n    if copy:\n        X = X.copy()\n    if sparse.issparse(X):\n        X_selected = X[:, selected]\n        X_rest = X[:, np.setdiff1d(np.arange(X.shape[1]), selected)]\n        X_selected = transform(X_selected, copy=copy)\n        X = sparse.hstack([X_selected, X_rest]).tocsr()\n    else:\n        X_selected = X[:, selected]\n        X_rest = X[:, np.setdiff1d(np.arange(X.shape[1]), selected)]\n        X_selected = transform(X_selected, copy=copy)\n        X[:, selected] = X_selected\n        X = np.hstack([X_selected, X_rest])\n    return X\n\n", "CodeBleu_score": 41.0887615263388}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, dtype=np.int, force_all_finite=False)\n    if np.any(X < 0):\n        raise ValueError(\"X needs to contain only non-negative integers.\")\n    n_samples, n_features = X.shape\n    if (isinstance(self.n_values, six.string_types) and self.n_values == 'auto'):\n        n_values = np.max(X, axis=0) + 1\n    elif isinstance(self.n_values, numbers.Integral):\n        if (np.max(X, axis=0) >= self.n_values).any():\n            raise ValueError(\"Feature out of bounds for n_values=%d\" % self.n_values)\n        n_values = np.empty(n_features, dtype=np.int)\n        n_values.fill(self.n_values)\n    else:\n        try:\n            n_values = np.asarray(self.n_values, dtype=int)\n        except (ValueError, TypeError):\n            raise TypeError(\"Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got %r\" % type(X))\n        if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:\n            raise ValueError(\"Shape mismatch: if n_values is an array, it has to be of shape (n_features,)\")\n    self.n_values_ = n_values\n    self.categories_ = [np.arange(n_val - 1, dtype=self.dtype) for n_val in n_values]\n    n_values = np.hstack([[0], n_values])\n    indices = np.cumsum(n_values)\n    self.feature_indices_ = indices\n    column_indices = (X + indices[:-1]).ravel()\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32), n_features)\n    data = np.ones(n_samples * n_features", "CodeBleu_score": 37.26392834530594}, {"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, dtype=np.int)\n    if np.any(X < 0):\n        raise ValueError(\"X needs to contain only non-negative integers.\")\n    n_samples, n_features = X.shape\n    n_values = np.max(X, axis=0) + 1\n    if (isinstance(self.n_values, six.string_types) and self.n_values == 'auto'):\n        n_values = np.max(X, axis=0) + 1\n        mask = np.array(X.sum(axis=0)).ravel() != 0\n        active_features = np.where(mask)[0]\n        X = X[:, active_features]\n        self.categories_ = [np.unique(X[:, i]).astype(self.dtype) for i in range(n_features)]\n    else:\n        try:\n            n_values = np.asarray(self.n_values, dtype=int)\n        except (ValueError, TypeError):\n            raise TypeError(\"Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got %r\" % type(X))\n        if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:\n            raise ValueError(\"Shape mismatch: if n_values is an array, it has to be of shape (n_features,)\")\n    indices = np.cumsum(np.hstack([[0], n_values]))\n    self.feature_indices_ = indices\n    column_indices = (X + indices[:-1]).ravel()\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32), n_features)\n    data = np.ones(n_samples * n_features)\n    out = sparse.coo_matrix((data, (row_indices, column_indices)), shape=(n_samples, indices[-1]), dtype=self.dtype).tocsr()\n    if (isinstance(self.n_values, six.string_", "CodeBleu_score": 41.19475361589212}, {"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, dtype=np.int)\n    if np.any(X < 0):\n        raise ValueError(\"X needs to contain only non-negative integers.\")\n    n_samples, n_features = X.shape\n    if (isinstance(self.n_values_, six.string_types) and self.n_values_ == 'auto'):\n        n_values = np.max(X, axis=0) + 1\n    elif isinstance(self.n_values_, numbers.Integral):\n        if (np.max(X, axis=0) >= self.n_values_).any():\n            raise ValueError(\"Feature out of bounds for n_values=%d\" % self.n_values_)\n        n_values = np.empty(n_features, dtype=np.int)\n        n_values.fill(self.n_values_)\n    else:\n        try:\n            n_values = np.asarray(self.n_values_, dtype=int)\n        except (ValueError, TypeError):\n            raise TypeError(\"Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got %r\" % type(X))\n        if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:\n            raise ValueError(\"Shape mismatch: if n_values is an array, it has to be of shape (n_features,)\")\n    self.n_values_ = n_values\n    self.categories_ = [np.arange(n_val - 1, dtype=X.dtype) for n_val in n_values]\n    n_values = np.hstack([[0], n_values])\n    indices = np.cumsum(n_values)\n    self.feature_indices_ = indices\n    column_indices = (X + indices[:-1]).ravel()\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32), n_features)\n    data = np.ones(n_samples * n_features)\n    out = sparse", "CodeBleu_score": 38.78454640772395}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n\t\n    from scipy.stats import uniform, loguniform\n    if self.log:\n            return loguniform(self.lower, self.upper)\n        else:\n            return uniform(self.lower, self.upper - self.lower)\n\n", "CodeBleu_score": 44.97555925253217}, {"code": "def convert_to_sklearn(self)\n\t\n    if self.log:\n            from scipy.stats import loguniform\n            return loguniform(self.lower, self.upper)\n        else:\n            from scipy.stats import uniform\n            return uniform(self.lower, self.upper - self.lower)\n\n", "CodeBleu_score": 42.79400333535892}, {"code": "def convert_to_sklearn(self)\n\t\n    import scipy.stats\n    if self.log:\n            return scipy.stats.uniform(loc=np.log(self.lower), scale=np.log(self.upper) - np.log(self.lower))\n        else:\n            return scipy.stats.uniform(loc=self.lower, scale=self.upper - self.lower)\n\n", "CodeBleu_score": 30.13491257260965}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_args = state.get_additional_args()\n    fig_args = self.fig_args\n        if self.fig_args is not None\n        else {}\n\n        if \\\"figsize\\\" not in fig_args:\n            fig_args[\\\"figsize\\\"] = (12, 10)\n\n        fig, ax = plt.subplots(**fig_args)\n\n        kwargs = {**additional_args, **self.kwargs}\n\n        if self.two_way:\n            data = state.pdp_data\n            data = data[data[self.features[0]].notnull() & data[self.features[1]].notnull()]\n\n        if self.headers:\n            self.render_header(ax)\n\n        PartialDependenceDisplay.from_estimator(\n            state.model, state.pdp_data, self.features, ax=ax, **kwargs\n        )\n\n        plt.tight_layout()\n        plt.show()\n\n    def render_header(self, ax):\n        ax.set_title(f\\\"Partial Dependence Plot (PDP) for {', '.join(self.features)}\\\")\n\n    def _render_interactive(self, state: AnalysisState) -> None:\n        pass\n\n", "CodeBleu_score": 35.396173990646155}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_args = self.get_additional_args(state)\n    fig_args = self.get_fig_args(state)\n    features = self.get_features(state)\n\n    if self.headers:\n        self.render_header(state)\n\n    fig, ax = plt.subplots(**fig_args)\n\n    kwargs = dict(additional_args, **self.PREDEFINED_KWARGS)\n\n    if self.two_way:\n        data = state.pdp_data.copy()\n        data = data[data[features[0]].notnull()]\n        data = data[data[features[1]].notnull()]\n\n    PartialDependenceDisplay.from_estimator(state.model, data, features, ax=ax, **kwargs)\n\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 44.48394324069324}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    # Retrieve additional arguments, figure arguments, and features to be plotted\n    additional_args = self.get_additional_args(state)\n    fig_args = self.get_fig_args(state)\n    features = self.features\n\n    # Render header for the PDP section\n    if self.headers:\n        self.render_header(state, \\\"PDP\\\")\n\n    # Create figure and axes with specified figure arguments\n    fig, ax = plt.subplots(**fig_args)\n\n    # Prepare keyword arguments by merging additional arguments and predefined keyword arguments\n    kwargs = dict(additional_args, **self.predefined_kwargs)\n\n    # Filter data for non-null values if two-way interaction plots are enabled\n    if self.two_way:\n        data = state.pdp_data[state.pdp_data[features[0]].notnull()]\n        data = data[data[features[1]].notnull()]\n    else:\n        data = state.pdp_data\n\n    # Create PDPs from the model and data, plotting them on the specified axes\n    display = PartialDependenceDisplay.from_estimator(\n        state.model, data, features, ax=ax, **kwargs\n    )\n\n    # Adjust layout with tight_layout for better spacing\n    plt.tight_layout()\n\n    # Display the plots\n    plt.show()\n\n    # Return the PartialDependenceDisplay object for further analysis\n    return display\n\n", "CodeBleu_score": 37.07732455724584}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X)\n\n    if self._feature_generator.features:\n        X = X.copy()\n        X[self._feature_generator.features] = self._feature_generator.transform(X[self._feature_generator.features])\n\n    X = X.fillna(0).astype(np.float32)\n    return X\n\n", "CodeBleu_score": 58.27628916669807}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X)\n    if self._feature_generator.features:\n        X = X.copy()\n        X.loc[:, self._feature_generator.features] = self._feature_generator.transform(X[self._feature_generator.features])\n    X.fillna(0, inplace=True)\n    X = X.to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 55.729202331191566}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X)\n    if self._feature_generator.features:\n        X = X.copy()\n        X.loc[:, self._feature_generator.features] = self._feature_generator.transform(X.loc[:, self._feature_generator.features])\n    X = X.fillna(0).astype(np.float32)\n    return X\n\n", "CodeBleu_score": 58.04798495324989}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    if self.problem_type == SOFTCLASS:\n        raise NotEnoughMemoryError(\n            \"SoftClass models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    if self.problem_type == QUANTILE:\n        raise NotEnoughMemoryError(\n            \"Quantile models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    # TODO: Remove this check once we have a better way to handle softclass/quantile\n    if self.problem_type == REGRESSION:\n        if self.num_classes is None:\n            self.num_classes = 1\n    if self.problem_type == MULTICLASS:\n        if self.num_classes is None:\n            self.num_classes = 2\n    if self.num_classes == 1:\n        raise NotEnoughMemoryError(\n            \"Binary classification models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    if self.num_classes == 2:\n        raise NotEnoughMemoryError(\n            \"Binary classification models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    if self.num_classes is None:\n        if self.problem_type == MULTICLASS:\n            self.num_classes = 2\n        elif self.problem_type == SOFTCLASS:\n            self.num_classes = 1\n    if self.num_classes == 1:\n        raise NotEnoughMemoryError(\n            \"Binary classification models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    if self.num_classes == 2:\n        raise NotEnoughMemoryError(\n            \"Binary classification models require more memory than the default 40 GB. Please increase the memory limit in the config.\"\n        )\n    if self.num_classes is None:\n        if self.problem_type == MULTICLASS:\n            self.num_classes = 2\n        elif self.problem_", "CodeBleu_score": 12.455521177181177}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    # Get model parameters\n        params = self.get_params()\n        if \\\"max_depth\\\" not in params:\n            params[\\\"max_depth\\\"] = 10\n        if \\\"max_features\\\" not in params:\n            params[\\\"max_features\\\"] = \\\"auto\\\"\n        if \\\"n_estimators\\\" not in params:\n            params[\\\"n_estimators\\\"] = 300\n        if \\\"criterion\\\" not in params:\n            params[\\\"criterion\\\"] = \\\"gini\\\"\n        if \\\"n_jobs\\\" not in params:\n            params[\\\"n_jobs\\\"] = -1\n        if \\\"random_state\\\" not in params:\n            params[\\\"random_state\\\"] = 0\n        if \\\"bootstrap\\\" not in params:\n            params[\\\"bootstrap\\\"] = True\n\n        # Get number of estimators\n        if self.search_space is not None:\n            num_estimators = self.search_space.get_param_space()[\\\"n_estimators\\\"]\n        else:\n            num_estimators = params[\\\"n_estimators\\\"]\n        if num_estimators < 40:\n            num_estimators = 40\n\n        # Get number of trees per estimator\n        num_trees_per_estimator = self._get_num_trees_per_estimator()\n\n        # Estimate bytes used per estimator\n        if self.problem_type == QUANTILE:\n            # TODO: Is this correct?\n            bytes_per_estimator = 4 * 2 * 10 ** 6  # 4 bytes per float32, 2 features, 10000 rows\n        elif self.problem_type == REGRESSION:\n            # TODO: Is this correct?\n            bytes_per_estimator = 4 * 2 * 10 ** 6  # 4 bytes per float32, 2 features, 10000 rows\n        else:\n            # TODO: Is this correct?\n            bytes_per_estimator = 4 * 2 * 10 ** 6  # ", "CodeBleu_score": 30.251882743037527}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    # TODO: Add in documentation", "CodeBleu_score": 2.4404761904761907}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = [ngram for ngram, _ in ngram_counter.most_common(vocab_size)]\n    top_ngrams.sort()\n    new_vocab = {ngram: idx for idx, ngram in enumerate(top_ngrams)}\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer", "CodeBleu_score": 32.44716784346369}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    ngram_freq_counter = Counter(ngram_freq)\n    top_ngrams = ngram_freq_counter.most_common(vocab_size)\n    top_ngrams_sorted = sorted([ngram for ngram, _ in top_ngrams])\n    new_vocab = {ngram: i for i, ngram in enumerate(top_ngrams_sorted)}\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n\n", "CodeBleu_score": 36.22227532861708}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = ngram_counter.most_common(vocab_size)\n    sorted_top_ngrams = sorted([ngram for ngram, freq in top_ngrams])\n    new_vocab = {ngram: index for index, ngram in enumerate(sorted_top_ngrams)}\n    vectorizer.vocabulary = new_vocab\n    return vectorizer", "CodeBleu_score": 33.943561335657144}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n\t\n    types_of_features = dict()\n    types_of_features.update(continuous=features)\n    return types_of_features\n\n\n def _select_bool(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either boolean or continuous based on their data type.", "CodeBleu_score": 14.900332874849937}, {"code": " def _select_continuous(self, df, features)\n\t\n    types_of_features = dict()\n    types_of_features.update(continuous=features)\n    return types_of_features\n\n\n def _select_bool(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either continuous or skewed based on their skewness.", "CodeBleu_score": 14.8676451829431}, {"code": " def _select_continuous(self, df, features)\n\t\n    types_of_features = dict()\n    skewness_threshold = self.params[\\\"skewness_threshold\\\"]\n    for feature in features:\n        if df[feature].dtype == \\\"float64\\\" or df[feature].dtype == \\\"int64\\\":\n            skewness = df[feature].skew()\n            if abs(skewness) >= skewness_threshold:\n                if feature not in types_of_features:\n                    types_of_features[\\\"skewed\\\"] = [feature]\n                else:\n                    types_of_features[\\\"skewed\\\"].append(feature)\n            else:\n                if feature not in types_of_features:\n                    types_of_features[\\\"continuous\\\"] = [feature]\n                else:\n                    types_of_features[\\\"continuous\\\"].append(feature)\n    return types_of_features\n\n\n def _select_bool(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either boolean or continuous based on their data type.", "CodeBleu_score": 31.971907541700368}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \\\"uniform\\\":\n        return None\n    elif weights == \\\"distance\\\":\n        weights = 1.0 / dist\n        weights[np.isinf(weights)] = 0\n        weights = weights / np.sum(weights)\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\\\"weights not recognized: should be 'uniform', 'distance', or a callable function\\\")\n\n\n", "CodeBleu_score": 45.14209156443052}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \\\"uniform\\\":\n        return None\n    elif weights == \\\"distance\\\":\n        weights = 1.0 / (dist + 1e-12)\n        weights[np.isinf(weights)] = 0\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\\\"weights not recognized: should be 'uniform', 'distance', or a callable function\\\")\n\n", "CodeBleu_score": 41.38417684715564}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        weights = 1 / (dist + 1e-6)\n        weights[np.isinf(weights)] = 0\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 45.631020942965165}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\t\n    neigh_dist, neigh_ind = self.kneighbors()\n    _y = self._y\n    if not self.outputs_2d_:\n        _y = self._y.reshape((-1, 1))\n    weights = _get_weights(neigh_dist, self.weights)\n    if weights is None:\n        y_pred = np.average(_y[neigh_ind], axis=1)\n    else:\n        y_pred = np.average(_y[neigh_ind], axis=1, weights=weights)\n    if not self.outputs_2d_:\n        y_pred = y_pred.ravel()\n    return y_pred\n\n", "CodeBleu_score": 41.16652409838368}, {"code": "def predict_loo(self)\n\t\n    neigh_dist, neigh_ind = self.kneighbors()\n    _y = self._y\n    if not self.outputs_2d_:\n        _y = self._y.reshape((-1, 1))\n    weights = _get_weights(neigh_dist, self.weights)\n    if weights is None:\n        weights = np.ones_like(neigh_ind)\n    y_pred = np.average(_y[neigh_ind], axis=1, weights=weights)\n    if not self.outputs_2d_:\n        y_pred = y_pred.ravel()\n    return y_pred\n\n", "CodeBleu_score": 37.3533495756638}, {"code": "def predict_loo(self)\n\t\n    neigh_dist, neigh_ind = self.kneighbors()\n    _y = self._y\n    if not self.outputs_2d_:\n        _y = self._y.reshape((-1, 1))\n    n_outputs = len(self.classes_)\n    n_queries = len(neigh_dist)\n    weights = _get_weights(neigh_dist, self.weights)\n    y_pred = np.empty((n_queries, n_outputs), dtype=_y.dtype)\n    if weights is None:\n        y_pred[:, :] = _y[neigh_ind, :].mean(axis=1)\n    else:\n        for k in range(n_outputs):\n            y_pred[:, k] = np.average(_y[neigh_ind, k], axis=1, weights=weights)\n    if not self.outputs_2d_:\n        y_pred = y_pred.ravel()\n    return y_pred\n\n", "CodeBleu_score": 49.00889923719093}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    output_data = np.zeros_like(input_data)\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds='clip')\n        ir.fit(quantile_list, input_data[i])\n        output_data[i] = ir.transform(input_data[i])\n    return output_data\n\n", "CodeBleu_score": 45.39434386824608}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.reshape(quantile_list, (-1, 1))\n    batch_size = input_data.shape[0]\n    transformed_data = np.zeros((batch_size, input_data.shape[1]))\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds='clip')\n        ir.fit(quantile_list, input_data[i])\n        transformed_data[i] = ir.transform(quantile_list)\n    return transformed_data\n\n", "CodeBleu_score": 41.36422341208102}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    transformed_data = np.zeros_like(input_data)\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds='clip')\n        ir.fit(quantile_list, input_data[i])\n        transformed_data[i] = ir.transform(input_data[i])\n    return transformed_data\n\n\n", "CodeBleu_score": 44.05697265750438}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.set_params(stop_words=None)\n    vectorizer.fit(text_data)\n    transform_matrix = vectorizer.transform(text_data)\n    return vectorizer, transform_matrix\n\n", "CodeBleu_score": 35.20745084764349}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    transform_matrix = vectorizer.transform(text_data)\n    return vectorizer, transform_matrix\n\n", "CodeBleu_score": 50.020838445437356}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.set_params(stop_words=None)\n    vectorizer.fit(text_data)\n    transform_matrix = vectorizer.transform(text_data)\n    return vectorizer, transform_matrix\n\n", "CodeBleu_score": 35.20745084764349}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n\t\n    df = df.replace(\\\"'--\\\", np.nan)\n    df = df.dropna()\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\\\"id\\\")], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\\\"shortcut\\\")], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\\\"death_date\\\")], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\\\"year_of_death\\\")], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\\\"days_to_death\\\")], axis=1)\n    train, test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train, test\n\n", "CodeBleu_score": 39.15293188955995}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    # Remove rows with missing entries marked by \"--\"\n    df = df.replace(\\\"--\\\", np.nan)\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one\n    unique_values = df.nunique()\n    columns_to_drop = unique_values[unique_values <= 1].index.tolist()\n    df = df.drop(columns=columns_to_drop)\n    # Drop columns with \"id\" in their name\n    df = df.drop(columns=[col for col in df.columns if \\\"id\\\" in col])\n    # Drop shortcut columns like \\\"days_to_death\\\" and \\\"year_of_death\\\"\n    df = df.drop(columns=[col for col in df.columns if \\\"death_\\\" in col])\n    # Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\\\"vital_status\\\"]), df[\\\"vital_status\\\"], test_size=test_size, shuffle=shuffle)\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 26.39466272559645}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    # Remove rows with missing entries marked by \"--\"\n    df = df[df.isin([\\\"--\\\"]).sum(axis=1) == 0]\n    # Remove columns with unique values less than or equal to one\n    df = df.loc[:, df.nunique() > 1]\n    # Remove columns with \"id\" in their name\n    df = df.loc[:, ~df.columns.str.contains(\\\"id\\\")]\n    # Remove shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.loc[:, ~df.columns.str.contains(\\\"days_to_death\\\")]\n    df = df.loc[:, ~df.columns.str.contains(\\\"year_of_death\\\")]\n    # Split data into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train_df, test_df\n\n", "CodeBleu_score": 23.311655751817952}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset,\n        make_sparse=sparse,\n    )\n\n    configuration_space = classifier.get_hyperparameter_search_space(\n        dataset_properties={\"sparse\": sparse}\n    )\n    default_config = configuration_space.get_default_configuration()\n\n    classifier = classifier(random_state=0, **default_config)\n\n    if hasattr(classifier, \"iterative_fit\"):\n\n        class counter(object):\n            def __init__(self, func):\n                self.n_calls = 0\n                self.func = func\n\n            def __call__(self, *args, **kwargs):\n                self.n_calls += 1\n                return self.func(*args, **kwargs)\n\n        classifier.iterative_fit = counter(classifier.iterative_fit)\n\n    predictor = classifier.fit(X_train, Y_train)\n\n    if hasattr(classifier, \"iterative_fit\"):\n        n_calls = classifier.iterative_fit.n_calls\n    else:\n        n_calls = None\n\n    predictions = predictor.predict(X_test)\n    return predictions, Y_test, n_calls\n\n", "CodeBleu_score": 40.37475996442028}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset,\n        make_sparse=sparse,\n        train_size_maximum=150,\n        make_multilabel=False,\n        make_binary=False,\n    )\n\n    configuration_space = classifier.get_hyperparameter_search_space(\n        dataset_properties={\"sparse\": sparse}\n    )\n    default_config = configuration_space.get_default_configuration()\n\n    classifier = classifier(random_state=0, **default_config)\n\n    if hasattr(classifier, \"iterative_fit\"):\n\n        class counter(object):\n            def __init__(self, func):\n                self.n_calls = 0\n                self.func = func\n\n            def __call__(self, *args, **kwargs):\n                self.n_calls += 1\n                return self.func(*args, **kwargs)\n\n        classifier.iterative_fit = counter(classifier.iterative_fit)\n\n    predictor = classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n\n    if hasattr(classifier, \"iterative_fit\"):\n        n_calls = classifier.iterative_fit.n_calls\n    else:\n        n_calls = None\n\n    for i in range(2, 100):\n        predictor = classifier.iterative_fit(\n            X_train, Y_train, n_iter=i, refit=True, current_predictor=predictor\n        )\n\n        if hasattr(classifier, \"iterative_fit\"):\n            n_calls += classifier.iterative_fit.n_calls\n        else:\n            n_calls = None\n\n    predictions = predictor.predict(X_test)\n    return predictions, Y_test, n_calls\n\n", "CodeBleu_score": 42.704556499811055}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset,\n        make_sparse=sparse,\n    )\n\n    configuration_space = classifier.get_hyperparameter_search_space(\n        dataset_properties={\"sparse\": sparse}\n    )\n    default_config = configuration_space.get_default_configuration()\n\n    classifier = classifier(random_state=0, **default_config)\n\n    if hasattr(classifier, \"iterative_fit\"):\n\n        class counter(object):\n            def __init__(self, func):\n                self.n_calls = 0\n                self.func = func\n\n            def __call__(self, *args, **kwargs):\n                self.n_calls += 1\n                return self.func(*args, **kwargs)\n\n        classifier.iterative_fit = counter(classifier.iterative_fit)\n\n    predictor = classifier.fit(X_train, Y_train)\n\n    if hasattr(classifier, \"iterative_fit\"):\n        n_calls = classifier.iterative_fit.n_calls\n    else:\n        n_calls = None\n\n    predictions = predictor.predict(X_test)\n    return predictions, Y_test, n_calls\n\n", "CodeBleu_score": 40.37475996442028}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n\tself):\n    \"\"\"# This function loads models based on the specified ensemble class and resampling strategy.", "CodeBleu_score": 0.26021191062026466}, {"code": "def _load_models(self)\n\tself):\n    \"\"\"# This function loads models based on the specified ensemble class and resampling strategy.", "CodeBleu_score": 0.26021191062026466}, {"code": "def _load_models(self)\n\tself):\n    \"\"\"# This function loads models based on the specified ensemble class and resampling strategy.", "CodeBleu_score": 0.26021191062026466}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y(\n        target=task.default_target_attribute, return_categorical_indicator=True, return_attribute_names=True)\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X[X_train]\n    X_test = X[X_test]\n    y_train = y[y_train]\n    y_test = y[y_test]\n    categorical_indicator = categorical_indicator[X_train.columns]\n    return X_train, X_test, y_train, y_test, categorical_indicator\n\n", "CodeBleu_score": 19.59737463703065}, {"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y(\n        return_categorical_indicator=True, return_attribute_names=True\n    )\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X[X_train]\n    X_test = X[X_test]\n    y_train = y[X_train]\n    y_test = y[X_test]\n    return X_train, X_test, y_train, y_test, categorical_indicator, attribute_names\n\n", "CodeBleu_score": 14.072126980142375}, {"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, _ = task.get_X_and_y(\n        target=task.default_target_attribute, dataset_format='dataframe')\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X.iloc[X_train]\n    X_test = X.iloc[X_test]\n    y_train = y.iloc[y_train]\n    y_test = y.iloc[y_test]\n    y_train = y_train.map(lambda x: 0 if x == task.class_labels[0] else 1)\n    y_test = y_test.map(lambda x: 0 if x == task.class_labels[0] else 1)\n    return X_train, X_test, y_train, y_test, categorical_indicator\n\n", "CodeBleu_score": 22.044171956139756}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 57.20031380903361}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 57.20031380903361}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 57.20031380903361}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n\t\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.indices_)\n\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    weights = np.zeros(self.num_input_models_)\n\n    # Calculate the weight for each ensemble member and assign it to the corresponding index in the weights array\n    for i, count in enumerate(counts):\n        if count > 0:\n            weights[i] = count / self.ensemble_size\n\n    # Normalize the weights if the sum is less than 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n\n    # Store the calculated weights in the instance variable\n    self.weights_ = weights\n\n", "CodeBleu_score": 32.96936807673577}, {"code": "def _calculate_weights(self) -> None\n\t\n    self.weights_ = np.zeros(self.num_input_models_)\n    for i in self.indices_:\n        self.weights_[i] += 1\n    self.weights_ /= len(self.indices_)\n    if np.sum(self.weights_) < 1:\n        self.weights_ /= np.sum(self.weights_)\n", "CodeBleu_score": 19.899560777062824}, {"code": "def _calculate_weights(self) -> None\n\t\n    # This function calculates the weights for ensemble members based on their occurrence frequency in the ensemble.\n    # It counts the occurrences of each ensemble member index and initializes a weights array with zeros, having a length equal to the number of input models.\n    # For each ensemble member, it calculates the weight as the frequency divided by the ensemble size and assigns it to the corresponding index in the weights array.\n    # If the sum of weights is less than 1, the weights are normalized to ensure their sum equals 1.\n    # The calculated weights are then stored in the instance variable self.weights_.\n    count = np.bincount(self.indices_)\n    self.weights_ = np.zeros(self.num_input_models_)\n    for i, c in enumerate(count):\n        self.weights_[i] = c / self.ensemble_size\n    if np.sum(self.weights_) < 1:\n        self.weights_ = self.weights_ / np.sum(self.weights_)\n\n", "CodeBleu_score": 23.02125623867243}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tthe predicted probabilities.", "CodeBleu_score": 7.363959305928505}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tthe predicted probabilities.", "CodeBleu_score": 7.363959305928505}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tthe predicted probabilities.", "CodeBleu_score": 7.363959305928505}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if len(Ybin.shape) != 1:\n        return np.dot(Ybin, np.arange(Ybin.shape[1]))\n    return Ybin\n\n", "CodeBleu_score": 40.970132706468114}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, np.arange(Ybin.shape[1]))\n\n", "CodeBleu_score": 30.43598530373655}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if Ybin.ndim != 1:\n        return Ybin.dot(np.arange(Ybin.shape[1]))\n    else:\n        return Ybin\n\n", "CodeBleu_score": 29.16257862489192}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = dict()\n    if self.config[\"balancing\"] == \"weighting\":\n        weights = self.get_balancing_weights(X, y)\n        fit_params[\"balancing_weights\"] = weights\n    self.init_params.update(fit_params)\n    self.set_hyperparams(self.config, self.init_params)\n    return super().fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 25.81818538462144}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = dict()\n    if self.config.get(\"balancing\", \"balancing_strategy\") == \"weighting\":\n        balancing = Balancing(self.config, self.dataset_properties, self.random_state)\n        weights = balancing.fit_transform(X, y)\n        fit_params = {**fit_params, **{\"sample_weight\": weights}}\n    self.init_params = {**self.init_params, **self.config}\n    self.set_hyperparameters(self.config, self.init_params)\n    fit_params = {**fit_params, **self.config}\n    return super().fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 39.23368537341103}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if self.config.get(\\\"balancing__strategy\\\") == \\\"weighting\\\":\n        balancing = Balancing(self.config, self.feat_type, self.dataset_properties, self.random_state)\n        self.init_params.update(balancing.get_params())\n        self.config.update(balancing.get_params())\n        self.config.update(self.init_params)\n        if fit_params is None:\n            fit_params = dict()\n        fit_params.update(balancing.get_params())\n    else:\n        if fit_params is None:\n            fit_params = dict()\n    return super().fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 35.452481645569094}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n\t\n    estimator = utils.StatsmodelsSklearnLikeWrapper(\n        sm.GLM,\n        dict(\n            init=dict(\n                family=sm.families.Tweedie(sm.families.links.Power(1))\n            ),\n            fit=dict(maxiter=1)\n        )\n    )\n    estimator = estimator.fit([[1], [2]], [0.1, 0.2])\n\n    assembler = assemblers.StatsmodelsModelAssemblerSelector(estimator)\n    actual = assembler.assemble()\n\n    expected = ast.BinNumExpr(\n        ast.NumVal(0.0),\n        ast.BinNumExpr(\n            ast.FeatureRef(0),\n            ast.NumVal(0.0020808009),\n            ast.BinNumOpType.MUL\n        ),\n        ast.BinNumOpType.ADD\n    )\n\n    assert utils.cmp_exprs(actual, expected)\n\n", "CodeBleu_score": 55.16447210657098}, {"code": "def test_glm_identity_link_func()\n\t\n    estimator = utils.StatsmodelsSklearnLikeWrapper(\n        sm.GLM,\n        dict(\n            init=dict(\n                family=sm.families.Tweedie(sm.families.links.Power(1))\n            ),\n            fit=dict(maxiter=1)\n        )\n    )\n    estimator = estimator.fit([[1], [2]], [0.1, 0.2])\n\n    assembler = assemblers.StatsmodelsModelAssemblerSelector(estimator)\n    actual = assembler.assemble()\n\n    expected = ast.BinNumExpr(\n        ast.NumVal(0.0),\n        ast.BinNumExpr(\n            ast.FeatureRef(0),\n            ast.NumVal(0.014700492832969888),\n            ast.BinNumOpType.MUL\n        ),\n        ast.BinNumOpType.ADD\n    )\n\n    assert utils.cmp_exprs(actual, expected)\n\n", "CodeBleu_score": 54.97652015538006}, {"code": "def test_glm_identity_link_func()\n\t\n    estimator = utils.StatsmodelsSklearnLikeWrapper(sm.GLM, dict(init=dict(family=sm.families.Tweedie(sm.families.links.Power(1))), fit=dict(maxiter=1)))\n    estimator = estimator.fit([[1], [2]], [0.1, 0.2])\n\n    assembler = assemblers.StatsmodelsModelAssemblerSelector(estimator)\n    actual = assembler.assemble()\n\n    expected = ast.BinNumExpr(\n        ast.NumVal(0.0),\n        ast.BinNumExpr(\n            ast.FeatureRef(0),\n            ast.NumVal(0.004545454545454545),\n            ast.BinNumOpType.MUL),\n        ast.BinNumOpType.ADD)\n\n    assert utils.cmp_exprs(actual, expected)\n\n", "CodeBleu_score": 54.647630766380814}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n    new_col_name = f\"{col.name}_{agg_name}\"\n    return self.group_by[col.name].agg(agg_func).rename(new_col_name)\n\n", "CodeBleu_score": 61.57008177024408}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        if isinstance(agg, str):\n            agg_name = agg\n        else:\n            agg_name = agg.__name__\n        agg_func = agg\n\n    col_name = col.name\n    new_col_name = f\"{col_name}_{agg_name}\"\n\n    return self.group_by[col_name].agg(agg_func).rename(new_col_name)\n\n", "CodeBleu_score": 50.201621485754586}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        if callable(agg):\n            agg_name = agg.__name__\n        else:\n            agg_name = agg\n        agg_func = agg\n\n    agg_name = agg_name.replace(\n            \\\" \\\", \\\"_\\\").replace(\n            \\\"-\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"\\\\\\\\\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"/\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\".\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\",\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\":\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"<\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\">\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"=\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"\\\\\\\\/\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"|\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"&\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"(\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\")\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\n            \\\"{\\\", \\\"_\\\"\n        )\n        agg_name = agg_name.replace(\\", "CodeBleu_score": 40.33725366389177}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    if self.hue is not None:\n        n_hue = len(self.hue_names)\n        if n_hue == 1:\n            ax = kws[\"ax\"]\n        else:\n            ax = kws[\"axes\"][self.hue_names.index(self.hue)]\n    else:\n        ax = kws[\"ax\"]\n    if r > 0.5:\n        font_size = 10\n    elif r > 0.4:\n        font_size = 10\n    elif r > 0.3:\n        font_size = 10\n    elif r > 0.2:\n        font_size = 10\n    elif r > 0.1:\n        font_size = 10\n    else:\n        font_size = 10\n    ax.annotate(f\"{r:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=font_size)\n\n", "CodeBleu_score": 25.44215480351092}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    # \u30c7\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u9664\u5916\n    if np.isnan(r):\n        return\n    # \u76f8\u95a2\u4fc2\u6570\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3057\u3066\u8868\u793a\n    if r > 0.5:\n        ax = plt.gca()\n        ax.annotate(f\"{r:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\",\n                    ha=\"center\", va=\"center\", fontsize=12, color=\"red\")\n    elif r > 0.2:\n        ax = plt.gca()\n        ax.annotate(f\"{r:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\",\n                    ha=\"center\", va=\"center\", fontsize=12, color=\"orange\")\n    else:\n        ax = plt.gca()\n        ax.annotate(f\"{r:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\",\n                    ha=\"center\", va=\"center\", fontsize=12, color=\"green\")\n\n", "CodeBleu_score": 23.229617222739634}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    if self.hue is not None:\n        hue_group_index = kws['hue_group']\n        if hue_group_index == 0:\n            # Adjust font size based on the number of hue groups\n            font_size = 'x-large' if len(self.hue_names) > 2 else 'large'\n        else:\n            # Adjust font size based on the correlation value\n            font_size = 'x-large' if abs(r) > 0.5 else 'large'\n        ax.annotate(f'{r:.2f}', xy=(0.5, 0.5), xycoords='axes fraction', ha='center', va='center', fontsize=font_size)\n\n", "CodeBleu_score": 19.90544345412663}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return decimal.Decimal(str(src)).quantize(decimal.Decimal('1e-{}'.format(rounddigit)))\n    elif method == 'format':\n        return float('{:.{}f}'.format(src, rounddigit))\n    else:\n        raise ValueError('Invalid method specified.')\n\n", "CodeBleu_score": 61.24928789601749}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if method == 'decimal':\n        if rounddigit is None:\n            return round(src)\n        else:\n            return round(src, rounddigit)\n    elif method == 'sig':\n        if rounddigit is None:\n            return decimal.Decimal(str(src)).quantize(decimal.Decimal('1e-1'))\n        else:\n            return decimal.Decimal(str(src)).quantize(decimal.Decimal('1e-' + str(rounddigit)))\n    elif method == 'format':\n        if rounddigit is None:\n            return '{:.1f}'.format(src)\n        else:\n            return '{:.{}f}'.format(src, rounddigit)\n\n", "CodeBleu_score": 52.29217576219776}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return decimal.Decimal(str(src)).quantize(decimal.Decimal('1e-{}'.format(rounddigit)))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n\n", "CodeBleu_score": 59.68946705490767}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    return {key: cls._round_digits(value, rounddigit, method) if isinstance(value, float) else value for key, value in srcdict.items()}\n\n", "CodeBleu_score": 24.031545677095167}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    newdict = srcdict.copy()\n    for key, value in srcdict.items():\n        if isinstance(value, float):\n            newdict[key] = cls._round_digits(value, rounddigit, method)\n    return newdict\n\n", "CodeBleu_score": 35.292049101242206}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    newdict = {}\n    for key, value in srcdict.items():\n        if isinstance(value, float):\n            newdict[key] = cls._round_digits(value, rounddigit, method)\n        else:\n            newdict[key] = value\n    return newdict\n\n", "CodeBleu_score": 42.465740364649264}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    if hue_data is not None:\n        df = pd.DataFrame(np.column_stack((x, y, hue_data)), columns=[x_name, y_name, hue_name])\n    else:\n        df = pd.DataFrame(np.column_stack((x, y)), columns=[x_name, y_name])\n    if legend_kws is None:\n        legend_kws = {}\n    if legend_kws.get('title') is None:\n        legend_kws['title'] = hue_name\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    return ax\n", "CodeBleu_score": 48.015961694745}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    # Create a DataFrame from the x and y values\n    data = pd.DataFrame({x_name: x.ravel(), y_name: y.ravel()})\n    # Add the hue field to the DataFrame if provided\n    if hue_data is not None:\n        data[hue_name] = hue_data.ravel()\n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    # Set the title for the legend if not specified\n    if legend_kws.get(\"title\") is None:\n        legend_kws[\"title\"] = hue_name\n    # Add the legend to the plot\n    ax.legend(**legend_kws)\n\n", "CodeBleu_score": 37.423536714524275}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    if hue_data is None:\n        df = pd.DataFrame({x_name: x.ravel(), y_name: y.ravel()})\n    else:\n        df = pd.DataFrame({x_name: x.ravel(), y_name: y.ravel(), hue_name: hue_data.ravel()})\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    if legend_kws.get('title') is None:\n        legend_kws['title'] = hue_name\n    ax.legend(**legend_kws)\n    return ax\n\n", "CodeBleu_score": 42.51333698609419}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(self.node.endog, self.node.exog, **fit_args)\n    return self\n\n", "CodeBleu_score": 37.23879714404107}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n    self.logger.info(f\"Fitting model for {self.node.name}\")\n\n    # Retrieve the main time series data\n    endog = self.node.get_main_data()\n\n    # Check for exogenous variables\n    exog = None\n    if self.node.exogenous is not None:\n        exog = self.node.get_exogenous_data()\n\n    # Fit the model\n    self.model = self.model.fit(endog, exog, **fit_args)\n\n    # Calculate in-sample predictions\n    self.in_sample_predictions = self.model.predict_in_sample(endog, exog)\n\n    # Calculate MSE\n    self.mse = ((self.in_sample_predictions - endog) ** 2).mean()\n\n    # Calculate residuals\n    self.residual = self.in_sample_predictions - endog\n\n    # Calculate forecast\n    self.forecast = self.model.predict(n_periods=self.node.forecast_steps, exogenous=exog)\n\n    # Reset the filter\n    warnings.resetwarnings()\n\n    return self\n\n", "CodeBleu_score": 43.15817752592896}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"Unable to import 'pmdarima'.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'pmdarima' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.statespace.sarimax' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_process' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=UserWarning,\n        message=\"The 'statsmodels.tsa.arima_model' module is not installed.*\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n       ", "CodeBleu_score": 11.910890855188846}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n\t\n    if self.kind == \"holt_winters\":\n        data = self._get_transformed_data()\n        model = ExponentialSmoothing(data, **kwargs).fit()\n    elif self.kind == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it to use auto_arima model\")\n            raise\n        data = self._get_transformed_data(as_series=True)\n        model = AutoARIMA(**kwargs).fit(data)\n    elif self.kind == \"sarimax\":\n        data = self._get_transformed_data()\n        exog = None\n        if self.node.exogenous is not None:\n            exog = data[self.node.exogenous]\n        model = SARIMAX(data, exog=exog, **kwargs).fit()\n    else:\n        raise InvalidArgumentException(f\"Model {self.kind} not valid. Pick one of: {', '.join(ModelT.names())}\")\n    return model\n\n", "CodeBleu_score": 56.83321849691292}, {"code": "def create_model(self, **kwargs)\n\t\n    if self.kind == \"holt_winters\":\n        transformed_data = self._get_transformed_data()\n        model = ExponentialSmoothing(endog=transformed_data, trend=\"add\", seasonal=\"add\", seasonal_periods=12)\n    elif self.kind == \"auto_arima\":\n        try:\n            from pmdarima import auto_arima\n        except ImportError:\n            logger.error(\"AutoARIMA not installed. Please install pmdarima to use this model\")\n            sys.exit(1)\n        transformed_data = self._get_transformed_data(as_series=True)\n        model = auto_arima(transformed_data, **kwargs)\n    elif self.kind == \"sarimax\":\n        transformed_data = self._get_transformed_data()\n        exog = None\n        if self.node.exogenous:\n            exog = transformed_data[self.node.exogenous]\n        model = SARIMAX(endog=transformed_data, exog=exog, **kwargs)\n    else:\n        raise Exception(\"Model not recognized\")\n    return model\n\n", "CodeBleu_score": 54.86480351481396}, {"code": "def create_model(self, **kwargs)\n\t\n    if self.kind == \"holt_winters\":\n        transformed = self._get_transformed_data()\n        return ExponentialSmoothing(transformed[self.node.key], trend=\"add\", seasonal=\"add\", seasonal_periods=12).fit()\n    elif self.kind == \"auto_arima\":\n        try:\n            from pmdarima import auto_arima\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it to use auto_arima model\")\n            exit(1)\n        transformed = self._get_transformed_data()\n        return auto_arima(transformed[self.node.key], exogenous=self._get_exogenous_data(), **kwargs).fit(transformed[self.node.key])\n    elif self.kind == \"sarimax\":\n        transformed = self._get_transformed_data()\n        exogenous = self._get_exogenous_data()\n        if exogenous is not None:\n            return SARIMAX(transformed[self.node.key], exog=exogenous, **kwargs).fit()\n        else:\n            return SARIMAX(transformed[self.node.key], **kwargs).fit()\n    else:\n        raise ValueError(\"Model type not recognized\")\n\n", "CodeBleu_score": 36.0940096928346}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n\t\n    if isinstance(array, chainerx.ndarray):\n        return array\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 82.4707901447282}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, chainerx.ndarray):\n        return array\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 73.71817877316793}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, chainerx.ndarray):\n        return array\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 82.4707901447282}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if _numpy_split_ok:\n        return ys\n    if len(ys) == 1:\n        return ys\n    y = numpy.empty(x.shape, dtype=x.dtype)\n    ys_ = numpy.split(y, indices_or_sections, axis=axis)\n    ys_ = [y_.reshape(y_.shape + (1,) * (x.ndim - y_.ndim)) for y_ in ys_]\n    return [y_.reshape(x.shape) for y_ in ys_]\n\n", "CodeBleu_score": 36.149628683889205}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if _numpy_split_ok:\n        return ys\n    if len(ys) != indices_or_sections:\n        raise ValueError('Number of sections does not match length of array')\n    if axis < 0:\n        axis += x.ndim\n    if (axis < 0) or (axis >= x.ndim):\n        raise ValueError('invalid axis')\n    if isinstance(indices_or_sections, int):\n        indices_or_sections = numpy.array_split(numpy.zeros(x.shape), indices_or_sections, axis=axis)\n    else:\n        indices_or_sections = numpy.array_split(numpy.zeros(x.shape), indices_or_sections, axis=axis)\n    for (i, y) in enumerate(ys):\n        if y.ndim == x.ndim:\n            continue\n        if y.ndim < x.ndim:\n            raise ValueError('cannot split array with fewer dimensions than axis')\n        if y.shape[axis] != indices_or_sections[i].shape[axis]:\n            raise ValueError('cannot split array along axis with unequal sizes')\n        ys[i] = y.reshape(indices_or_sections[i].shape)\n    return ys\n\n", "CodeBleu_score": 31.161507869432647}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if not _numpy_split_ok:\n        if (len(ys) == 1):\n            return ys\n        if (len(indices_or_sections) == 1):\n            return ys\n        if (indices_or_sections == (0, 1)):\n            return ys\n        if (indices_or_sections == (1, 0)):\n            return ys\n        if (indices_or_sections == (0, 1, 0)):\n            return ys\n        if (indices_or_sections == (0, 0, 1)):\n            return ys\n        if (indices_or_sections == (1, 0, 0)):\n            return ys\n        if (indices_or_sections == (1, 1, 0)):\n            return ys\n        if (indices_or_sections == (0, 1, 1)):\n            return ys\n        if (indices_or_sections == (1, 0, 1)):\n            return ys\n        if (indices_or_sections == (1, 1, 1)):\n            return ys\n        if (indices_or_sections == (1, 0, 0, 0)):\n            return ys\n        if (indices_or_sections == (0, 1, 0, 0)):\n            return ys\n        if (indices_or_sections == (0, 0, 1, 0)):\n            return ys\n        if (indices_or_sections == (0, 0, 0, 1)):\n            return ys\n        if (indices_or_sections == (1, 0, 0, 1)):\n            return ys\n        if (indices_or_sections == (0, 1, 0, 1)):\n            return ys\n        if (indices_or_sections == (0, 0, 1, 1)):\n            return ys\n        if (indices_or_sections == (1, 1, 0, 0)):\n            return ys\n       ", "CodeBleu_score": 10.930398852966158}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    if device < 0:\n        return [chainer.dataset.to_device(device, x)\n                for x in batch]\n    xp = chainer.cuda.get_array_module(*batch)\n    x_concat = xp.concatenate(batch, axis=0)\n    x_section = xp.cumsum(xp.array([len(x) for x in batch[:-1]], dtype=xp.int32))\n    x_split = xp.split(x_concat, x_section, axis=0)\n    return [xp.array(x, dtype=x.dtype) for x in x_split]\n\n", "CodeBleu_score": 64.6005567147275}, {"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    if device < 0:\n        return [to_device(b, device) for b in batch]\n    xp = chainer.cuda.get_array_module(batch[0])\n    x_batch = xp.concatenate([b[0] for b in batch], axis=0)\n    t_batch = xp.concatenate([b[1] for b in batch], axis=0)\n    x_batch = chainer.dataset.to_device(device, x_batch)\n    t_batch = chainer.dataset.to_device(device, t_batch)\n    x_section = numpy.cumsum([len(b[0]) for b in batch])\n    t_section = numpy.cumsum([len(b[1]) for b in batch])\n    x_batch = chainer.cuda.cupy.split(x_batch, x_section, axis=0)\n    t_batch = chainer.cuda.cupy.split(t_batch, t_section, axis=0)\n    return list(zip(x_batch, t_batch))\n\n", "CodeBleu_score": 49.54162209413175}, {"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = chainer.cuda.cupy if device >= 0 else numpy\n        x_batch = xp.concatenate(batch, axis=0)\n        x_batch = chainer.dataset.to_device(device, x_batch)\n        x_section = numpy.cumsum([len(x) for x in batch[:-1]])\n        batch_new = xp.split(x_batch, x_section, axis=0)\n        return batch_new\n\n", "CodeBleu_score": 68.51517688301023}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\t\n    np.random.seed(int(trainer.updater.get_all_optimizer('main').t))\n    n_images = 100\n    rows = int(np.sqrt(n_images))\n    cols = int(np.ceil(n_images / rows))\n    out_dir = 'out'\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    xp = gen.xp\n    z = Variable(xp.asarray(gen.make_hidden(n_images)))\n    with chainer.using_config('train', False):\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)\n    x = x.reshape((rows, cols, 3, 32, 32)).transpose(0, 3, 1, 4, 2, 5)\n    x = x.reshape((rows * 32, cols * 32, 3))\n    preview_path = '{}/preview-{}.png'.format(out_dir, trainer.updater.iteration)\n    Image.fromarray(x).save(preview_path)\n\n", "CodeBleu_score": 60.99424067079045}, {"code": "def make_image(trainer)\n\t\n    gen, dis = trainer.updater.get_all_models()\n    gen.to_cpu()\n    gen.disable_update()\n    dis.to_cpu()\n    dis.disable_update()\n    rows = cols = 4\n    np.random.seed(0)\n    n_images = rows * cols\n    xp = gen.xp\n    z = Variable(xp.asarray(gen.make_hidden(n_images)))\n    with chainer.using_config('train', False):\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)\n    x = x.reshape((rows, cols) + x.shape[1:])\n    x = x.transpose((0, 2, 1, 3, 4))\n    x = x.reshape((rows * x.shape[1], cols * x.shape[3]) + x.shape[4:])\n    preview_dir = '{}/preview'.format(trainer.out)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    Image.fromarray(x).save('{}/image{}.png'.format(preview_dir, trainer.updater.iteration))\n\n", "CodeBleu_score": 55.564953413079074}, {"code": "def make_image(trainer)\n\t\n    gen, dis = trainer.updater.get_all_models()\n    gen.to_cpu()\n    gen.train = False\n    dis.to_cpu()\n    dis.train = False\n    rows = cols = 4\n    np.random.seed(0)\n    n_images = rows * cols\n    xp = gen.xp\n    z = Variable(xp.asarray(gen.make_hidden(n_images)))\n    with chainer.using_config('train', False), chainer.no_backprop_mode():\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)\n    _, _, h, w = x.shape\n    x = x.reshape((rows, cols, 3, h, w))\n    x = x.transpose(0, 3, 1, 4, 2)\n    x = x.reshape((rows * h, cols * w, 3))\n    preview_dir = '{}/preview'.format(trainer.out)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    preview_path = '{}/preview_iteration_{:0>8}.png'.format(preview_dir, trainer.updater.iteration)\n    Image.fromarray(x).save(preview_path)\n\n", "CodeBleu_score": 63.80181081880719}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    from_char = _typechars.find(from_dtype.char)\n    to_char = _typechars.find(to_dtype.char)\n    return from_char >= 0 and to_char >= 0 and from_char <= to_char\n\n", "CodeBleu_score": 56.30274858064796}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    return _typechars.find(from_dtype.char) <= _typechars.find(to_dtype.char)\n\n", "CodeBleu_score": 74.17754101174233}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 65.35402012285344}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    ax = g.ax if isinstance(g, Visualizer) else g\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\\\"length of labels and colors must match\\\")\n    handles = [patches.Patch(color=c, label=l) for c, l in zip(colors, labels)]\n    return ax.legend(handles=handles, **legend_kwargs)\n\n\n\n\n", "CodeBleu_score": 43.236637410296545}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    ax = g.ax if isinstance(g, Visualizer) else g\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\\\"length of labels and colors must match\\\")\n    handles = [patches.Patch(color=c, label=l) for c, l in zip(colors, labels)]\n    return ax.legend(handles=handles, **legend_kwargs)\n\n", "CodeBleu_score": 43.236637410296545}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    ax = g.ax if isinstance(g, Visualizer) else g\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\\\"Number of labels must match number of colors\\\")\n    handles = [patches.Patch(color=c, label=l) for c, l in zip(colors, labels)]\n    return ax.legend(handles=handles, **legend_kwargs)\n\n\n\n\n", "CodeBleu_score": 43.74583386963774}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\t\n    fig, ax = plt.subplots()\n    visualizer = ParallelCoordinates(ax, fast=fast)\n    start_time = time.time()\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    elapsed_time = time.time() - start_time\n    plt.close(fig)\n    return elapsed_time\n\n", "CodeBleu_score": 24.73941722900573}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    plt.axis('off')\n    pcoords = ParallelCoordinates(fast=fast)\n    start_time = time.time()\n    pcoords.fit(X, y)\n    pcoords.transform(X)\n    elapsed_time = time.time() - start_time\n    plt.clf()\n    plt.close()\n    return elapsed_time\n\n", "CodeBleu_score": 44.16046148462651}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    fig, ax = plt.subplots(figsize=(12, 8))\n    pcoords = ParallelCoordinates(ax, fast=fast)\n    start_time = time.time()\n    pcoords.fit(X, y)\n    pcoords.transform(X)\n    elapsed_time = time.time() - start_time\n    plt.close(fig)\n    return elapsed_time\n\n", "CodeBleu_score": 27.255177801094714}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the alpha value from the object's attributes or set a default value\n        alpha = self.alpha or 0.25\n\n        # Loop through each instance and plot it on the axis\n        for i in range(X.shape[0]):\n            # Get the feature vector and label for the current instance\n            x = X[i]\n            label = y[i]\n\n            # Get the color for the current instance based on the label\n            color = self.colors_[label]\n\n            # Plot the feature vector against the increment sequence on the axis\n            self.ax.plot(self._increments, x, color=color, alpha=alpha)\n\n        # Return the axis object with the plotted instances\n        return self.ax\n\n\n", "CodeBleu_score": 49.172617741070844}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Retrieve the alpha value from the object's attributes or default to 0.25\n    alpha = self.alpha or 0.25\n\n    # Loop over each instance in X and y\n    for i, (x, label) in enumerate(zip(X, y)):\n        # Retrieve the color for the instance based on the label\n        color = self.colors_[label]\n\n        # Plot the feature vector against the predefined increment sequence on the axis\n        self.ax_.plot(self._increments, x, color=color, alpha=alpha, **kwargs)\n\n\n", "CodeBleu_score": 42.699265815293444}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the alpha value from the object's attributes or default to 0.25\n    alpha = self.alpha or 0.25\n\n    # Loop through each instance in X and plot its feature vector against a predefined increment sequence on the axis\n    for i, x in enumerate(X):\n        # Get the color for this instance based on its label\n        color = self.colors_[y[i]]\n\n        # Plot the feature vector against a predefined increment sequence on the axis\n        self.ax.plot(self._increments, x, color=color, alpha=alpha, **kwargs)\n\n\n\n", "CodeBleu_score": 45.422804127538754}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    # Get alpha from param or default\n    alpha = self.alpha or 0.5\n\n    # Add a column of ones to X to separate the instances visually\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\n\n\n    # Add a None to the increments list to create breaks between instances\n    self._increments = np.insert(self._increments, 0, None)\n\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n\n    # Iterate over the classes\n    for cls in classes:\n\n        # Determine the color for the class\n\n        color = self.get_colors([cls])[0]\n\n\n        # Select the instances of X belonging to the class\n\n        Xi = X[y == cls]\n\n\n        # Plot the instances as a single line plot with the specified color and alpha\n\n        self.ax.plot(self._increments, Xi, color=color, alpha=alpha, **kwargs)\n\n\n    return self.ax\n\n\n", "CodeBleu_score": 38.895351195196035}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    # Get alpha from param or default\n        alpha = self.alpha or 0.5\n\n        # Add a column of ones to X to separate the instances visually\n        X = np.c_[X, np.ones(len(X))]\n\n        # Add None to the increments list to create breaks between instances\n        self._increments = np.append(self._increments, None)\n\n        # Get the unique class labels\n        classes = np.unique(y)\n\n        # Iterate over the unique classes\n        for cls in classes:\n            # Determine the color for the class\n            color = self.get_colors([cls])[0]\n\n            # Select the instances of X belonging to the class\n            X_cls = X[y == cls]\n\n            # Plot the instances as a single line plot with the specified color and alpha\n            self.ax.plot(self._increments, X_cls.T, color=color, alpha=alpha, **kwargs)\n\n        return self.ax\n\n", "CodeBleu_score": 37.62773662105697}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    # Get alpha from param or default\n    alpha = self.alpha or 0.5\n\n    # Add column of ones to X to separate instances visually\n    X = np.c_[X, np.ones(len(X))]\n    # Append None to increments list to create breaks between instances\n    self._increments = list(self._increments)\n    self._increments.append(None)\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over classes\n    for class_label in classes:\n        # Determine color\n        color = self.get_colors([class_label])[0]\n\n        # Select instances of X belonging to class\n        Xi = X[y == class_label]\n\n        # Plot as single line plot with specified color and alpha\n        self.ax.plot(self._increments, Xi.T, color=color, alpha=alpha, **kwargs)\n\n    return self.ax\n\n\n\n", "CodeBleu_score": 38.55734123553215}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    # Create the figure and subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Perform RadViz analysis on occupancy data\n    occupancy = load_occupancy()\n    visualizer = RadViz(ax=ax1)\n    visualizer.fit(occupancy.X, occupancy.y)\n    visualizer.finalize()\n\n    # Perform Rank2D analysis on concrete data\n    concrete = load_concrete()\n    visualizer = Rank2D(ax=ax2)\n    visualizer.fit(concrete.X, concrete.y)\n    visualizer.transform(concrete.X)\n    visualizer.finalize()\n\n    # Save the figure\n    fig.savefig(os.path.join(FIGURES, fname))\n\n", "CodeBleu_score": 21.810974880317794}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n    # Load the data\n    data = load_occupancy()\n    X, y = data.X, data.y\n\n    # Perform RadViz analysis on the left subplot\n    visualizer = RadViz(ax=ax1)\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.finalize()\n\n    # Perform Rank2D analysis on the right subplot\n    data = load_concrete()\n    X, y = data.X, data.y\n\n    visualizer = Rank2D(ax=ax2)\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.finalize()\n\n    # Save the figure\n    plt.savefig(os.path.join(FIGURES, fname))\n\n", "CodeBleu_score": 25.619052469044934}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    # Create the figure and subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Create the RadViz visualization\n    visualizer = RadViz(ax=ax1)\n    X, y = load_occupancy()\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.finalize()\n\n    # Create the Rank2D visualization\n    visualizer = Rank2D(ax=ax2)\n    X, y = load_concrete()\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.finalize()\n\n    # Save the figure\n    fig.savefig(os.path.join(FIGURES, fname), dpi=300)\n\n\n\n", "CodeBleu_score": 22.44254218548463}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\t\n    # Create side-by-side axes grid\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n\n    # Load the concrete dataset\n    data = load_concrete(split=True)\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    alpha = np.logspace(-4, 4, 50)\n    lasso = LassoCV(alphas=alpha, cv=5)\n    visualizer = PredictionError(lasso, ax=axes[0])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=alpha, cv=5)\n    visualizer = ResidualsPlot(ridge, ax=axes[1])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    path = os.path.join(FIGURES, fname)\n    plt.savefig(path)\n\n    # Return the path to the saved figure\n    return path\n\n", "CodeBleu_score": 53.145399033099174}, {"code": "def regression(fname=\"regression.png\")\n\t\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n\n    # Load the concrete dataset\n    data = load_concrete(split=True)\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=np.logspace(-10, 10, 21), cv=5)\n    visualizer = PredictionError(lasso, ax=axes[0])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=np.logspace(-10, 10, 21), cv=5)\n    visualizer = ResidualsPlot(ridge, ax=axes[1])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    path = os.path.join(FIGURES, fname)\n    plt.savefig(path)\n    return path\n\n", "CodeBleu_score": 38.242681801706354}, {"code": "def regression(fname=\"regression.png\")\n\t\n    # Create side-by-side axes grid\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n\n    # Load the concrete dataset\n    data = load_concrete(split=True)\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=np.logspace(-3, 1, 100), cv=5)\n    visualizer = PredictionError(lasso, ax=axes[0])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=np.logspace(-3, 1, 100), cv=5)\n    visualizer = ResidualsPlot(ridge, ax=axes[1])\n    visualizer.fit(data.train.X, data.train.y)\n    visualizer.score(data.test.X, data.test.y)\n    visualizer.finalize()\n\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    path = os.path.join(FIGURES, fname)\n    plt.savefig(path)\n    return path\n\n", "CodeBleu_score": 38.54422366045983}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\t\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n\n    # Load the spam dataset with a train-test split\n    data = load_spam(split=True)\n\n    # Plot the classification report on the left\n    oz = ClassificationReport(\n        MultinomialNB(), ax=axes[0], classes=[\"not spam\", \"spam\"]\n    )\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n\n    # Plot the discrimination threshold on the right\n    oz = DiscriminationThreshold(\n        LogisticRegression(), ax=axes[1], classes=[\"not spam\", \"spam\"]\n    )\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n\n    # Save the figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)\n\n", "CodeBleu_score": 45.931251510833434}, {"code": "def classification(fname=\"classification.png\")\n\t\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n    data = load_spam(split=True)\n\n    # Plot classification report on the left\n    oz = ClassificationReport(MultinomialNB(), ax=axes[0])\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n\n    # Plot discrimination threshold on the right\n    oz = DiscriminationThreshold(LogisticRegression(), ax=axes[1])\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n\n    # Save figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)\n\n", "CodeBleu_score": 48.906621048475}, {"code": "def classification(fname=\"classification.png\")\n\t\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n\n    # Plot classification report on the left\n    data = load_spam(split=True)\n    oz = ClassificationReport(\n        MultinomialNB(), ax=axes[0]\n    )\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n\n    # Plot discrimination threshold on the right\n    data = load_spam(split=False)\n    oz = DiscriminationThreshold(\n        LogisticRegression(), ax=axes[1]\n    )\n    oz.fit(data.X, data.y)\n    oz.finalize()\n\n    # Save figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)\n\n", "CodeBleu_score": 50.96003523076338}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n\t\n    try:\n        # If we're using matplotlib 1.5.0 or greater, use the new color cycle API\n        colors = mpl.rcParams['axes.prop_cycle'].by_key()['color']\n    except KeyError:\n        # Fall back to the old color cycle API\n        colors = mpl.rcParams['axes.color_cycle']\n\n    return colors\n\n\n\n        colors = mpl.rcParams['axes.prop_cycle'].by_key()['color']\n    except KeyError:\n        # Fall back to the old color cycle API\n        colors = mpl.rcParams['axes.color_cycle']\n\n    return colors\n\n", "CodeBleu_score": 41.10424523972289}, {"code": "def get_color_cycle()\n\t\n    try:\n        if mpl_ge_150:\n            return mpl.rcParams['axes.prop_cycle'].by_key()['color']\n        else:\n            return mpl.rcParams['axes.color_cycle']\n    except KeyError:\n        warnings.warn(\\\"Matplotlib color cycle not found, returning default colors.\\\", UserWarning)\n        return [\\\"b\\\", \\\"g\\\", \\\"r\\\", \\\"c\\\", \\\"m\\\", \\\"y\\\", \\\"k\\\"]\n\n\n", "CodeBleu_score": 29.32245965965573}, {"code": "def get_color_cycle()\n\t\n    if mpl_ge_150:\n        try:\n            return mpl.rcParams['axes.prop_cycle'].by_key()['color']\n        except KeyError:\n            warnings.warn(\\\"Unable to retrieve color cycle from axes.prop_cycle\\\", UserWarning)\n            return mpl.rcParams['axes.color_cycle']\n    else:\n        return mpl.rcParams['axes.color_cycle']\n\n\n\n\n\n", "CodeBleu_score": 36.554782921600605}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111)\n\n    # plot normalized x and y values\n    ax.plot(self.x_normalized, self.y_normalized)\n\n    # plot distance curve in red\n    ax.plot(self.x_difference, self.y_difference, color=\\\"red\\\")\n\n    # set x and y ticks at intervals of 0.1 based on the minimum and maximum values of the respective data\n    x_min, x_max = min(self.x_normalized), max(self.x_normalized)\n    y_min, y_max = min(self.y_normalized), max(self.y_normalized)\n    x_ticks = np.arange(x_min, x_max + 0.1, 0.1)\n    y_ticks = np.arange(y_min, y_max + 0.1, 0.1)\n    ax.set_xticks(x_ticks)\n    ax.set_yticks(y_ticks)\n\n    # draw vertical line at knee point\n    if self.knee:\n        ax.axvline(self.knee, color=\\\"black\\\", linestyle=\\\"--\\\")\n\n    # set axis labels and title\n    ax.set_xlabel(\\\"Normalized x\\\")\n    ax.set_ylabel(\\\"Normalized y\\\")\n    ax.set_title(\\\"Normalized Difference Curve\\\")\n\n    # show the plot\n    plt.show()\n\n\n    def plot_knee_normalized_y(self):\n        \\\"\\\"\\\"This function plots the y values of the knee point and the normalized knee point.\\\"\\\"\\\"\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 8))\n        ax = fig.add_subplot(111)\n\n        # plot y values", "CodeBleu_score": 36.539097117400765}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n\n    plt.plot(self.x_normalized, self.y_normalized, label='Normalized Curve')\n    plt.plot(self.x_difference, self.y_difference, color='red', label='Distance Curve')\n    plt.scatter(self.norm_knee, self.norm_knee_y, color='black', label='Knee Point')\n\n    plt.xlabel('Normalized X')\n    plt.ylabel('Normalized Y')\n\n    plt.xticks(np.arange(min(self.x_normalized), max(self.x_normalized) + 0.1, 0.1))\n    plt.yticks(np.arange(min(self.y_normalized), max(self.y_normalized) + 0.1, 0.1))\n\n    plt.legend()\n\n    plt.show()\n\n", "CodeBleu_score": 48.26046306732127}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n\n    # plot the normalized x and y values\n    plt.plot(self.x_normalized, self.y_normalized, color='blue', label='Normalized Curve')\n\n    # plot the distance curve in red\n    plt.plot(self.x_difference, self.y_difference, color='red', label='Distance Curve')\n\n    # set the x and y ticks at intervals of 0.1 based on the minimum and maximum values of the respective data\n    plt.xticks(np.arange(min(self.x_normalized), max(self.x_normalized) + 0.1, 0.1))\n    plt.yticks(np.arange(min(self.y_normalized), max(self.y_normalized) + 0.1, 0.1))\n\n    # draw a vertical line at the knee point to highlight it on the plot\n    if self.knee is not None:\n        plt.axvline(x=self.knee, color='green', linestyle='--', label='Knee Point')\n\n    # add a legend to the plot\n    plt.legend()\n\n    # show the plot\n    plt.show()\n\n", "CodeBleu_score": 45.866669837580034}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\t\n\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1])\n\n", "CodeBleu_score": 69.02973240571076}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1])\n\n", "CodeBleu_score": 69.02973240571076}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1])\n\n", "CodeBleu_score": 69.02973240571076}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n\n    def show_decomp(i=0):\n        \"\"\"# This function plots the original data and its PCA components for a given index.", "CodeBleu_score": 49.10047483159955}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    def show_decomp(i=0):\n        \"\"\"# This function shows the decomposition of a given data instance in the PCA space.", "CodeBleu_score": 26.335438040179348}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n    pca = PCA(n_components=n_components)\n    transformed = pca.fit_transform(data)\n    def show_decomp(i=0):\n        \"\"\"# This function plots the original data and its PCA components for a given index.", "CodeBleu_score": 62.501328793087175}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n\t\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]\n\n\n    y = iris.target\n\n\n    h = .02  # step size in the mesh\n\n\n    \n\n\n    # Create color maps\n\n\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n\n\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n\n    \n\n\n    # we create an instance of Neighbours Classifier and fit the data.\n\n\n    clf = neighbors.KNeighborsClassifier(5, weights='uniform')\n\n\n    clf.fit(X, y)\n\n\n    \n\n\n    # Plot the decision boundary. For that, we will assign a color to each\n\n\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n\n\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n\n    \n\n\n    # Put the result into a color plot\n\n\n    Z = Z.reshape(xx.shape)\n\n\n    pl.figure()\n\n\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n\n    \n\n\n    # Plot also the training points\n\n\n    pl", "CodeBleu_score": 59.00060468280609}, {"code": "def plot_iris_knn()\n\t\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n\n    h = .02  # step size in the mesh\n\n\n    # Create color maps\n\n\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n\n\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n\n\n    # we create an instance of Neighbours Classifier and fit the data.\n\n\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n\n\n    clf.fit(X, y)\n\n\n\n    # Plot the decision boundary. For that, we will assign a color to each\n\n\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n\n\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n\n\n    # Put the result into a color plot\n\n\n    Z = Z.reshape(xx.shape)\n\n\n    pl.figure()\n\n\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n\n\n    # Plot also the training points\n\n\n    pl.scatter(X[:, 0], X[:", "CodeBleu_score": 61.71710740548658}, {"code": "def plot_iris_knn()\n\t\n    # import some data to play with", "CodeBleu_score": 0.2}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n\t\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n    plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolor='black', s=20)\n    plt.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 60.9297943335542}, {"code": "def plot_sgd_separator()\n\t\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n    xx = np.linspace(-1, 5, 100)\n    yy = np.linspace(-1, 5, 100)\n    XX, YY = np.meshgrid(xx, yy)\n    Z = np.empty(XX.shape)\n    for (i, j), val in np.ndenumerate(XX):\n        x1 = val\n        x2 = YY[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n    plt.contour(XX, YY, Z, levels=levels, colors=colors, linestyles=linestyles)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired, edgecolors='k')\n    plt.axis('tight')\n    return plt.show()\n\n", "CodeBleu_score": 54.037840980933936}, {"code": "def plot_sgd_separator()\n\t\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n    XX, YY = np.meshgrid(xx, yy)\n    Z = np.empty(XX.shape)\n    for (i, j), val in np.ndenumerate(XX):\n        x1 = val\n        x2 = YY[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n    plt.contour(XX, YY, Z, levels=levels, colors=colors, linestyles=linestyles)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n    plt.xlim(-1, 4)\n    plt.ylim(-1, 4)\n    plt.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 55.703173731426524}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n\t\n    X = np.array(self.data)\n    y = X[:, 2]\n        X = X[:, :2]\n        dump_svmlight_file(X, y, file)\n\n    def set_data(self, data):\n        self.data = data\n        self.changed('data')\n\n    def set_cls(self, cls):\n        self.cls = cls\n        self.changed('cls')\n\n    def set_surface_type(self, surface_type):\n        self.surface_type = surface_type\n        self.changed('surface_type')\n\n", "CodeBleu_score": 21.939971716921086}, {"code": "def dump_svmlight_file(self, file)\n\t\n    X = np.array(self.data)[:, :2]\n        y = np.array(self.data)[:, 2]\n        dump_svmlight_file(X, y, file)\n\n    def get_surface_type(self):\n        return self.surface_type\n\n    def set_surface_type(self, surface_type):\n        self.surface_type = surface_type\n        self.changed('surface_type')\n\n    def set_cls(self, cls):\n        self.cls = cls\n        self.changed('cls')\n\n    def get_cls(self):\n        return self.cls\n\n    def get_data(self):\n        return self.data\n\n    def set_data(self, data):\n        self.data = data\n        self.changed('data')\n\n    def add_data(self, data):\n        self.data.extend(data)\n        self.changed('data')\n\n    def clear_data(self):\n        self.data = []\n        self.changed('data')\n\n", "CodeBleu_score": 37.047407163617486}, {"code": "def dump_svmlight_file(self, file)\n\t\n    X = np.array(self.data)\n    y = X[:, 2]\n        X = X[:, 0:2]\n        dump_svmlight_file(X, y, file, zero_based=False)\n\n    def set_data(self, data):\n        self.data = data\n        self.changed('data')\n\n    def set_cls(self, cls):\n        self.cls = cls\n        self.changed('cls')\n\n    def set_surface_type(self, surface_type):\n        self.surface_type = surface_type\n        self.changed('surface_type')\n\n    def get_surface_type(self):\n        return self.surface_type\n\n    def get_cls(self):\n        return self.cls\n\n    def get_data(self):\n        return self.data\n\n    def clear(self):\n        self.data = []\n        self.changed('data')\n\n    def get_surface(self):\n        return self.surface\n\n    def get_surface_data(self):\n        return self.surface.get_surface_data()\n\n    def get_surface_data_x(self):\n        return self.surface.get_surface_data_x()\n\n    def get_surface_data_y(self):\n        return self.surface.get_surface_data_y()\n\n    def get_surface_data_z(self):\n        return self.surface.get_surface_data_z()\n\n    def get_surface_data_x_min(self):\n        return self.surface.get_surface_data_x_min()\n\n    def get_surface_data_x_max(self):\n        return self.surface.get_surface_data_x_max()\n\n    def get_surface_data_y_min(self):\n        return self.surface.get_surface_data_y_min()\n\n    def get_surface_data_y", "CodeBleu_score": 20.096532250848032}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n\t\n    print('Fitting...')\n        data = np.array(self.model.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        if np.all(y == y[0]):\n            clf = svm.OneClassSVM(nu=0.5, kernel=self.kernel.get(), gamma=0.1)\n        else:\n            clf = svm.SVC(C=1.0, kernel=self.kernel.get(), gamma=0.1, coef0=0.0)\n        clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print(clf.score(X, y))\n        self.model.surface = self.get_surface(clf)\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed('surface')\n\n    def get_surface(self, clf):\n        \\\"\\\"\\\"Calculate the decision surface of the model.\\\"\\\"\\\"\n        if self.surface_type.get() == 0:\n            # Plot decision function of a linear SVC\n            return self.plot_decision_function(clf)\n        elif self.surface_type.get() == 1:\n            # Plot decision function for a non-linear SVC\n            return self.plot_non_linear_decision_function(clf)\n        elif self.surface_type.get() == 2:\n            # Plot the decision surface of a SVC\n            return self.plot_decision_surface(clf)\n\n    def plot_decision_function(self, clf):\n        \\\"\\\"\\\"Plot the decision function of a linear SVC.\\\"\\\"\\\"\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()", "CodeBleu_score": 44.73485062931114}, {"code": "def fit(self)\n\t\n    print('Fitting model...')\n        data = np.array(self.model.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        if len(set(y)) == 1:\n            self.clf = svm.OneClassSVM(kernel=self.kernel.get(), gamma=0.001, degree=3, coef0=0.0)\n        else:\n            self.clf = svm.SVC(kernel=self.kernel.get(), gamma=0.001, degree=3, coef0=0.0)\n        self.clf.fit(X, y)\n        if hasattr(self.clf, 'score'):\n            print('Accuracy: %.2f' % self.clf.score(X, y))\n        self.model.surface = self.clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n        self.model.surface_type = 0\n        self.fitted = True\n        self.model.changed('surface')\n\n", "CodeBleu_score": 38.95365936365035}, {"code": "def fit(self)\n\t\n    print('Fitting model...')\n        data = np.array(self.model.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        clf = svm.SVC(kernel=self.kernel.get(), C=self.C.get(), gamma=self.gamma.get(), coef0=self.coef0.get(), degree=self.degree.get())\n        if np.unique(y).size == 1:\n            clf = svm.OneClassSVM(kernel=self.kernel.get(), gamma=self.gamma.get(), coef0=self.coef0.get(), degree=self.degree.get())\n        clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print('Accuracy: %.2f%%' % (clf.score(X, y) * 100))\n        self.model.surface = self.calculate_surface(clf)\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed('surface')\n\n    def calculate_surface(self, clf):\n        \"\"\"# This function calculates the decision surface for a given classifier.", "CodeBleu_score": 46.642128403970936}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n\t\n    import_string = \"\"\n    object_string = \"\"\n    obj = None\n    if sourcecode.startswith(\\\"tpot.\\\"):\n        import_string = sourcecode[5:]\n        object_string = sourcecode[5:]\n    else:\n        import_string = sourcecode\n        object_string = sourcecode\n    try:\n        obj = importlib.import_module(import_string)\n        obj = getattr(obj, object_string)\n    except Exception as e:\n        if verbose > 2:\n            raise ImportError(e)\n        elif verbose > 0:\n            print(\\\"Warning: Unable to import object {}.\\\".format(object_string))\n    return import_string, object_string, obj\n\n", "CodeBleu_score": 33.72988994441567}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    import sys\n    import importlib\n    import inspect\n    import traceback\n    import re\n    import os\n    import ast\n    import astor\n    import numpy as np\n    import pandas as pd\n    import sklearn\n    import tpot\n    import tpot.config\n    import tpot.operators\n    import tpot.operators.mutation\n    import tpot.operators.crossover\n    import tpot.operators.selection\n    import tpot.operators.replacement\n    import tpot.operators.evaluation\n    import tpot.operators.import_export\n    import tpot.operators.import_export.import_export\n    import tpot.operators.import_export.import_export_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils_utils.import_export_utils_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils_utils.import_export_utils_utils_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils_utils.import_export_utils_utils_utils_utils.import_export_utils_utils_utils_utils\n    import tpot.operators.import_export.import_export_utils.import_export_utils_utils.import_export_utils_utils_utils.import_export_utils_utils_utils_utils.import_export_utils_utils_utils_utils_", "CodeBleu_score": 10.205851048262234}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    if sourcecode.startswith('tpot.'):\n        module_name = sourcecode[5:].split('.')[0]\n        object_name = sourcecode[5:]\n    else:\n        module_name = sourcecode.split('.')[0]\n        object_name = sourcecode\n    try:\n        module = __import__(module_name)\n        object_string = object_name\n        object_name = object_name.split('.')[-1]\n        for submodule_name in object_name.split('.'):\n            module = getattr(module, submodule_name)\n        return module_name, object_string, module\n    except ImportError as e:\n        if verbose > 2:\n            raise\n        elif verbose > 1:\n            print('Failed to import {}'.format(sourcecode))\n            print(e)\n        return None, None, None\n\n", "CodeBleu_score": 42.06562157390038}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = dict()\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                if inspect.signature(step.fit).parameters.get(\"sample_weight\"):\n                    sample_weight_dict[step.__class__.__name__ + \"sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 51.73236661396498}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = dict()\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                signature = inspect.signature(step.fit)\n                if \"sample_weight\" in signature.parameters:\n                    sample_weight_dict[step.__class__.__name__ + \"sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 43.12158170272329}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                if inspect.signature(step.fit).parameters.get(\"sample_weight\") is not None:\n                    sample_weight_dict[step.__class__.__name__ + \":sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 53.52231869724625}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if not self._optimized_pipeline:\n            # pick one individual from evaluated pipeline for a error message\n            eval_ind_list = list(self.evaluated_individuals_.keys())\n            for pipeline, pipeline_scores in zip(\n                self._pareto_front.items, reversed(self._pareto_front.keys)\n            ):\n                if np.isinf(pipeline_scores.wvalues[1]):\n                    sklearn_pipeline = self._toolbox.compile(expr=pipeline)\n                    from sklearn.model_selection import cross_val_score\n\n                    cv_scores = cross_val_score(\n                        sklearn_pipeline,\n                        self.pretest_X,\n                        self.pretest_y,\n                        cv=self.cv,\n                        scoring=self.scoring_function,\n                        verbose=0,\n                        error_score=\\\"raise\\\",\n                    )\n                    break\n            raise RuntimeError(\n                \\\"There was an error in the TPOT optimization \\\"\n                \\\"process. This could be because the data was \\\"\n                \\\"not formatted properly, because a timeout \\\"\n                \\\"was reached or because data for \\\"\n                \\\"a regression problem was provided to the \\\"\n                \\\"TPOTClassifier object. Please make sure you \\\"\n                \\\"passed the data to TPOT correctly. If you \\\"\n                \\\"enabled PyTorch estimators, please check \\\"\n                \\\"the data requirements in the online \\\"\n                \\\"documentation: \\\"\n                \\\"https://epistasislab.github.io/tpot/using/\\\"\n            )\n        else:\n            if self.verbosity >= 1:\n                print(\n                    \\\"\\", "CodeBleu_score": 36.63283619206892}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    # Check if there is an optimized pipeline\n        if not self._optimized_pipeline:\n            raise RuntimeError(\n                \\\"A pipeline has not yet been optimized. Please call fit() first.\\\"\n            )\n\n        # Compile the best pipeline found by TPOT\n        sklearn_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)\n\n        # Fit the best pipeline found by TPOT\n        with warnings.catch_warnings():\n            warnings.simplefilter(\\\"ignore\\\")\n            sklearn_pipeline.fit(features, target)\n\n        # Print the best pipeline if verbosity is 1 or 2\n        if self.verbosity >= 1:\n            if self.verbosity >= 2:\n                print(\\\"\\\"\\\"\n", "CodeBleu_score": 20.94345049730906}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if self._optimized_pipeline:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\\\"ignore\\\")\n                self._optimized_pipeline.fit(features, target)\n                if self.verbosity >= 1:\n                    if self.verbosity >= 2:\n                        print(\n                            \\\"\\", "CodeBleu_score": 13.363537475361007}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n\t\n    if not self.fitted_pipeline_:\n        raise RuntimeError(\n            \"A pipeline has not yet been optimized. Please call fit() first.\"\n        )\n\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target, sample_weight=None\n    )\n\n    if self.scoring_function == \\\"accuracy\\\":\n        if self.scoring_function_params:\n            raise ValueError(\n                \\\"scoring_function_params must be None when scoring_function is \\\"\n                \\\"accuracy.\\\"\n            )\n        scorer = make_scorer(accuracy_score)\n    elif self.scoring_function == \\\"balanced_accuracy\\\":\n        if self.scoring_function_params:\n            raise ValueError(\n                \\\"scoring_function_params must be None when scoring_function is \\\"\n                \\\"balanced_accuracy.\\\"\n            )\n        scorer = make_scorer(balanced_accuracy_score)\n    elif self.scoring_function == \\\"f1\\\":\n        if self.scoring_function_params:\n            raise ValueError(\n                \\\"scoring_function_params must be None when scoring_function is \\\"\n                \\\"f1.\\\"\n            )\n        scorer = make_scorer(f1_score, average=\\\"weighted\\\")\n    elif self.scoring_function == \\\"log_loss\\\":\n        if self.scoring_function_params:\n            raise ValueError(\n                \\\"scoring_function_params must be None when scoring_function is \\\"\n                \\\"log_loss.\\\"\n            )\n        scorer = make_scorer(log_loss, labels=np.unique(testing_target))\n    elif self.scoring_function == \\\"precision\\\":\n        if self.scoring_function_params:\n            raise ValueError(\n                \\\"scoring_function_params must be None when scoring_function is \\\"\n                \\\"precision.\\\"\n            )\n        scorer = make_scorer(precision_score, average=\\\"weighted\\\")\n    elif self.scoring_function", "CodeBleu_score": 47.727963761035}, {"code": "def score(self, testing_features, testing_target)\n\t\n    if not self.fitted_pipeline_:\n        raise RuntimeError(\n            \"A pipeline has not yet been optimized. Please call fit() first.\"\n        )\n\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target, sample_weight=None\n    )\n\n    if self.scoring_function in SCORERS:\n        scorer = SCORERS[self.scoring_function]\n    else:\n        scorer = self.scoring_function\n\n    return scorer(self.fitted_pipeline_, testing_features, testing_target)\n\n", "CodeBleu_score": 49.23064083735689}, {"code": "def score(self, testing_features, testing_target)\n\t\n    if not self.fitted_pipeline_:\n        raise RuntimeError(\n            \"A pipeline has not yet been optimized. Please call fit() first.\"\n        )\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target, sample_weight=None\n    )\n    if self.scoring_function in SCORERS:\n        scorer = SCORERS[self.scoring_function]\n    else:\n        scorer = self.scoring_function\n    return scorer(self.fitted_pipeline_, testing_features, testing_target)\n\n", "CodeBleu_score": 49.23064083735689}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\\\"Imputing missing values...\\\")\n    if self._imputer is None:\n        self._imputer = SimpleImputer(strategy=\\\"median\\\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n\n def _check_dataset(self, features, target=None, sample_weight=None):\n    \\\"\\\"\\\"Check the features, target and sample_weight for correct format.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix of the training or testing set\n        target: array-like {n_samples}\n            Target values of the training set\n        sample_weight: array-like {n_samples}\n            Sample weights of the training set\n\n        Returns\n        -------\n        features: array-like {n_samples, n_features}\n            Feature matrix of the training or testing set\n        target: array-like {n_samples}\n            Target values of the training set\n        sample_weight: array-like {n_samples}\n            Sample weights of the training set\n        \\\"\\\"\\\"\n        if sample_weight is not None:\n            if len(sample_weight) != len(target):\n                raise ValueError(\\\"Sample weights must have the same length as target.\\\")\n        if target is not None:\n            if len(target) != len(features):\n                raise ValueError(\\\"Target must have the same length as features.\\\")\n        if not isinstance(features, np.ndarray):\n            features = np.array(features)\n        if target is not None:\n            if not isinstance(target, np.ndarray):\n                target = np.array(target)\n        if sample_weight is not None:\n            if not isinstance(sample_weight, np.ndarray):\n                sample_weight = np.array(sample_weight)\n\n        return features, target, sample_weight\n\n\n def _update_pbar(self, pbar_num=None", "CodeBleu_score": 36.114497621647565}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\\\"Imputing missing values...\\\")\n    if self._imputer is None:\n        self._imputer = SimpleImputer(strategy=\\\"median\\\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n\n def _check_dataset(self, features, target, sample_weight):\n    \\\"\\\"\\\"# This function checks if the input dataset is valid.", "CodeBleu_score": 47.849462634869134}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\\\"Imputing missing values...\\\")\n    if self._imputer is None:\n        self._imputer = SimpleImputer(strategy=\\\"median\\\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n\n def _check_dataset(self, features, target, sample_weight):\n    \\\"\\\"\\\"Check if the dataset is valid and convert it to a numpy array.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix of the dataset\n        target: array-like {n_samples}\n            Target values of the dataset\n        sample_weight: array-like {n_samples}\n            Sample weights of the dataset\n\n        Returns\n        -------\n        features: array-like {n_samples, n_features}\n            Feature matrix of the dataset\n        target: array-like {n_samples}\n            Target values of the dataset\n        sample_weight: array-like {n_samples}\n            Sample weights of the dataset\n        \\\"\\\"\\\"\n    if target is not None:\n        target = np.array(target)\n        if sample_weight is not None:\n            sample_weight = np.array(sample_weight)\n    features = np.array(features)\n    if features.shape[0] != target.shape[0]:\n        raise ValueError(\\\"The number of samples in the feature matrix and target vector must be the same.\\\")\n    if features.shape[0] != sample_weight.shape[0]:\n        raise ValueError(\\\"The number of samples in the feature matrix and sample weight vector must be the same.\\\")\n    return features, target, sample_weight\n\n\n def _check_target(self, target):\n    \\\"\\\"\\\"Check if the target is valid and convert it to a numpy array.\n\n        Parameters\n        ----------\n        target: array-like {n_samples}\n            Target values of the dataset\n\n        Returns\n        -------\n        target", "CodeBleu_score": 36.41118467732157}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\t\n    # generate the pipeline code string from the expression and a predefined set of operators\n    pipeline_code = self._compile_to_pipeline_code(expr, self.operators)\n\n    # evaluate the pipeline code string within a specific context to obtain the scikit-learn pipeline object\n    pipeline = eval(pipeline_code, self._eval_context)\n\n    # set the memory attribute of the pipeline\n    pipeline.memory = self.memory\n\n    # recursively set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        for step in pipeline.steps:\n            if hasattr(step[1], 'random_state'):\n                step[1].random_state = self.random_state\n\n    # return the configured scikit-learn pipeline\n    return pipeline\n\n", "CodeBleu_score": 35.66509290490995}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    # Generate pipeline code string\n    pipeline_code = self._generate_pipeline_code(expr)\n\n    # Evaluate pipeline code string within a specific context\n    pipeline = eval(pipeline_code, self._context)\n\n    # Set pipeline's memory attribute\n    pipeline.memory = self._memory\n\n    # Set random_state for each step in the pipeline\n    if self.random_state is not None:\n        for step in pipeline.steps:\n            if hasattr(step[1], 'random_state'):\n                step[1].random_state = self.random_state\n\n    return pipeline\n\n", "CodeBleu_score": 35.795905334840576}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    # Generate the pipeline code string from the expression and a predefined set of operators.\n    pipeline_code = _generate_pipeline_code(expr, self._pset)\n    # Evaluate the pipeline code string within a specific context to obtain the scikit-learn pipeline object.\n    pipeline = eval(pipeline_code, self._context)\n    # Set the memory attribute of the pipeline.\n    pipeline.memory = self._memory\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline.\n    if self.random_state is not None:\n        for _, step in pipeline.steps:\n            if hasattr(step, 'random_state'):\n                step.random_state = self.random_state\n    # Return the configured scikit-learn pipeline.\n    return pipeline\n\n", "CodeBleu_score": 37.932650232021814}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    # Create sets of primitive nodes for each individual and convert each individual to a string representation\n    primitive_sets = [set(individual.primitive_set) for individual in population]\n    individual_strings = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if len(primitive_sets[i] & primitive_sets[j]) > 0 and individual_strings[i] != individual_strings[j]:\n                eligible_pairs.append((i, j))\n    # If no eligible pairs are found, return None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n\n    # Randomly select one of the eligible pairs and return the corresponding individuals from the population\n    index1, index2 = eligible_pairs[np.random.choice(len(eligible_pairs))]\n    return population[index1], population[index2]\n\n", "CodeBleu_score": 24.00771786345251}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    # Create sets of primitive nodes for each individual\n    primitive_node_sets = [set(individual.primitives) for individual in population]\n    # Convert each individual to a string representation\n    individual_strings = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if len(primitive_node_sets[i] & primitive_node_sets[j]) > 0 and individual_strings[i] != individual_strings[j]:\n                eligible_pairs.append((i, j))\n    # If no eligible pairs are found, return None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n\n    # Randomly select one of the eligible pairs and return the corresponding individuals from the population\n    pair = eligible_pairs[np.random.choice(len(eligible_pairs))]\n    return population[pair[0]], population[pair[1]]\n\n\n\n\n", "CodeBleu_score": 25.260876288999217}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(individual.primitives) for individual in population]\n    # Convert each individual to a string representation\n    individuals_str = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = [(i, j) for i, j in zip(range(len(population)), range(len(population))) if i != j and len(primitive_sets[i].intersection(primitive_sets[j])) > 0]\n    # Consider pairs in both possible orders\n    eligible_pairs.extend([(j, i) for i, j in eligible_pairs])\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population\n    else:\n        i, j = eligible_pairs[np.random.randint(len(eligible_pairs))]\n        return population[i], population[j]\n\n", "CodeBleu_score": 25.94605685738752}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n    steps = _process_operator(pipeline_tree, operators)\n    if len(steps) > 1:\n        pipeline_text = \\\"make_pipeline(", "CodeBleu_score": 11.889080625843407}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n    steps = _process_operator(pipeline_tree, operators)\n    num_steps = len(steps)\n    if num_steps > 1:\n        pipeline_text = \\\"make_pipeline(", "CodeBleu_score": 15.718544255250539}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n    steps = _process_operator(pipeline_tree, operators)\n    num_steps = len(steps)\n    if num_steps > 1:\n        pipeline_text = \\\"make_pipeline(", "CodeBleu_score": 15.718544255250539}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\t\n    if operator == \\\"CombineDFs\\\":\n        return [\\\"pd.concat(X, axis=1)\\\"]\n    op_name = operator[0]\n    op = get_by_name(op_name, operators)\n    if op.root and depth > 0:\n        return [\\\"StackingEstimator(estimator={})\\\".format(_process_operator(operator[1], operators, depth=0)[0])]\n    if op.root:\n        return [_process_operator(operator[1], operators, depth=depth + 1)[0]]\n    if len(operator) == 2:\n        return [_process_operator(operator[1], operators, depth=depth + 1)[0]]\n    else:\n        return [_process_operator(operator[1], operators, depth=depth + 1)[0], _process_operator(operator[2], operators, depth=depth + 1)[0]]\n\n", "CodeBleu_score": 33.17323014770729}, {"code": "def _process_operator(operator, operators, depth=0)\n\t\n    operator_name = operator[0]\n    if operator_name == \\\"CombineDFs\\\":\n        return [\\\"make_union(*input_matrix)\\\"]\n    else:\n        tpot_operator = get_by_name(operator_name, operators)\n        if tpot_operator.root and depth > 0:\n            return [\\\"StackingEstimator(estimator={})\\\".format(operator_name)]\n        else:\n            if tpot_operator.input_matrix:\n                input_matrix = _process_operator(tpot_operator.input_matrix, operators, depth + 1)\n            else:\n                input_matrix = []\n            if tpot_operator.input_df:\n                input_df = _process_operator(tpot_operator.input_df, operators, depth + 1)\n            else:\n                input_df = []\n            if tpot_operator.input_series:\n                input_series = _process_operator(tpot_operator.input_series, operators, depth + 1)\n            else:\n                input_series = []\n            if tpot_operator.input_array:\n                input_array = _process_operator(tpot_operator.input_array, operators, depth + 1)\n            else:\n                input_array = []\n            if tpot_operator.input_dict:\n                input_dict = _process_operator(tpot_operator.input_dict, operators, depth + 1)\n            else:\n                input_dict = []\n            if tpot_operator.input_list:\n                input_list = _process_operator(tpot_operator.input_list, operators, depth + 1)\n            else:\n                input_list = []\n            if tpot_operator.input_tuple:\n                input_tuple = _process_operator(tpot_operator.input_tuple, operators, depth + 1)\n            else:\n                input_tuple = []\n            if tpot_operator.input_primitive:\n                input_primitive = _process_operator(tpot_operator.input_primitive, operators, depth + 1)\n            else:\n                input_primitive = []\n            if tpot_operator.input_", "CodeBleu_score": 31.128284713219266}, {"code": "def _process_operator(operator, operators, depth=0)\n\t\n    # Identify the operator's name\n    op_name = operator[0]\n\n    # Check if the operator is a \"CombineDFs\" operation\n    if op_name == \\\"CombineDFs\\\":\n        # Combine the dataframes\n        return [\\\"pd.concat(X_list, axis=1)\\\"]\n\n    # Retrieve the corresponding TPOT operator\n    tpot_op = get_by_name(op_name, operators)\n\n    # Process the input operators recursively if they are not the initial input matrix\n    if not tpot_op.is_input:\n        inputs = [_process_operator(input_, operators, depth=depth + 1) for input_ in operator[1:]]\n    else:\n        inputs = []\n\n    # Wrap the operator in a StackingEstimator if it is a root operator and the depth is greater than zero\n    if tpot_op.root and depth > 0:\n        # Wrap the operator in a StackingEstimator\n        inputs = [\\\"StackingEstimator(estimator={})\\\" .format(input_[0]) for input_ in inputs]\n\n    # Generate the step for the operator\n    step = \\\"{}({})\\\" .format(op_name, \\\", \\\".join(inputs))\n\n    # Return the list of generated steps for the pipeline\n    return [step]\n\n", "CodeBleu_score": 44.152166711302996}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n\t\n    X = check_array(X, accept_sparse='csr')\n    if self.minimum_fraction is not None:\n        if self.minimum_fraction < 0 or self.minimum_fraction > 1:\n            raise ValueError('minimum_fraction must be between 0 and 1')\n        X_selected = _X_selected(X, self.minimum_fraction)\n    else:\n        X_selected = auto_select_categorical_features(X, self.threshold)\n    if X_selected.size == 0:\n        raise ValueError('No categorical features found in input data')\n    return OneHotEncoder().fit_transform(X_selected)\n\n", "CodeBleu_score": 34.52866503505822}, {"code": "def transform(self, X)\n\t\n    X = check_array(X, accept_sparse='csr')\n    X_selected = _X_selected(X, self.threshold, self.minimum_fraction)\n        if X_selected.size == 0:\n            raise ValueError(\\\"No categorical features were selected.\\\")\n        else:\n            self.one_hot_encoder.fit(X_selected)\n            return self.one_hot_encoder.transform(X_selected)\n\n", "CodeBleu_score": 26.46848636364092}, {"code": "def transform(self, X)\n\t\n    X = check_array(X, accept_sparse='csr')\n    if X.shape[1] == 0:\n        raise ValueError('No features in input data.')\n    cat_features = auto_select_categorical_features(X, threshold=self.threshold, minimum_fraction=self.minimum_fraction)\n    if len(cat_features) == 0:\n        raise ValueError('No categorical features found.')\n    return _X_selected(X, cat_features, self.threshold, self.minimum_fraction)\n\n", "CodeBleu_score": 38.063918267150896}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    sensitivities = []\n    specificities = []\n    for c in classes:\n        true_positives = np.sum((y_true == c) & (y_pred == c))\n        true_negatives = np.sum((y_true != c) & (y_pred != c))\n        actual_positives = np.sum(y_true == c)\n        actual_negatives = np.sum(y_true != c)\n        sensitivity = true_positives / actual_positives if actual_positives > 0 else 0\n        specificity = true_negatives / actual_negatives if actual_negatives > 0 else 1\n        sensitivities.append(sensitivity)\n        specificities.append(specificity)\n    class_accuracies = [(sens + spec) / 2 for sens, spec in zip(sensitivities, specificities)]\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 24.945528324540568}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(np.concatenate([y_true, y_pred]))\n    tp = np.sum(np.logical_and(y_true == classes[1], y_pred == classes[1]))\n    tn = np.sum(np.logical_and(y_true == classes[0], y_pred == classes[0]))\n    fp = np.sum(np.logical_and(y_true == classes[0], y_pred == classes[1]))\n    fn = np.sum(np.logical_and(y_true == classes[1], y_pred == classes[0]))\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1\n    return np.mean([sensitivity, specificity])\n\n", "CodeBleu_score": 11.075425914439927}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(np.concatenate((y_true, y_pred)))\n    tp = np.sum(np.logical_and(y_true == classes[1], y_pred == classes[1]))\n    fp = np.sum(np.logical_and(y_true == classes[0], y_pred == classes[1]))\n    tn = np.sum(np.logical_and(y_true == classes[0], y_pred == classes[0]))\n    fn = np.sum(np.logical_and(y_true == classes[1], y_pred == classes[0]))\n    sensitivity = tp / float(tp + fn)\n    specificity = tn / float(tn + fp)\n    if len(classes) == 1:\n        return 1\n    return np.mean([sensitivity, specificity])\n\n", "CodeBleu_score": 12.518637406458977}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse=True, force_all_finite=False, ensure_2d=True, dtype=None, order='C', copy=True)\n    non_zero_count = np.count_nonzero(X, axis=1).reshape(-1, 1)\n    zero_count = np.count_nonzero(X == 0, axis=1).reshape(-1, 1)\n    return np.hstack((non_zero_count, zero_count, X))", "CodeBleu_score": 16.480859513553398}, {"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse=True)\n        X_new = np.hstack((X.count(axis=1, values=0).reshape(-1, 1), X.count(axis=1, values=1).reshape(-1, 1), X.count(axis=1, values=2).reshape(-1, 1), X.count(axis=1, values=3).reshape(-1, 1), X.count(axis=1, values=4).reshape(-1, 1), X.count(axis=1, values=5).reshape(-1, 1), X.count(axis=1, values=6).reshape(-1, 1), X.count(axis=1, values=7).reshape(-1, 1), X.count(axis=1, values=8).reshape(-1, 1), X.count(axis=1, values=9).reshape(-1, 1)))\n        return X_new\n\n", "CodeBleu_score": 15.918731447433254}, {"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse=True)\n        X_new = np.hstack((np.count_nonzero(X, axis=1)[:, np.newaxis], np.count_zero(X, axis=1)[:, np.newaxis], X))\n        return X_new\n\n", "CodeBleu_score": 13.327452338703939}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n\t\n    self.n_classes = len(np.unique(target))\n    if len(target) < self.n_classes:\n        raise ValueError('The number of classes ({}) is larger than the number of examples ({}).'.format(self.n_classes, len(target)))\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.n_classes, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) != self.n_classes:\n        self.pretest_y = _safe_indexing(target, np.unique(self.pretest_y))\n\n", "CodeBleu_score": 25.945657939548134}, {"code": "def _init_pretest(self, features, target)\n\t\n    self.n_classes_ = len(np.unique(target))\n    self.pretest_size_ = max(self.n_classes_, self.pretest_size)\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.pretest_size_, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.n_classes_:\n        self.pretest_y = _safe_indexing(self.pretest_y, np.unique(self.pretest_y))\n\n", "CodeBleu_score": 20.821266453482934}, {"code": "def _init_pretest(self, features, target)\n\t\n    n_classes = len(np.unique(target))\n    if len(features) < n_classes:\n        raise ValueError('The number of training examples must be at least as large as the number of classes.')\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=n_classes, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < n_classes:\n        self.pretest_y = _safe_indexing(target, np.unique(self.pretest_y))\n\n", "CodeBleu_score": 24.27183088487289}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n\n", "CodeBleu_score": 49.19748004987954}, {"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    else:\n        return dtype\n\n", "CodeBleu_score": 43.53879182400563}, {"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    else:\n        return dtype\n\n", "CodeBleu_score": 43.53879182400563}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    if batch_size is None:\n        return ds.numpy()\n    try:\n        return ds.batch(batch_size).numpy()\n    except AttributeError:\n        return np.concatenate([batch for batch in ds.batch(batch_size)])\n\n", "CodeBleu_score": 12.276028934779248}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    if batch_size is None:\n        return ds.as_numpy_iterator()\n    try:\n        return ds.batch(batch_size).as_numpy_iterator()\n    except AttributeError:\n        return ds.batch_as_numpy_iterator(batch_size)\n\n", "CodeBleu_score": 10.96548315444828}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    if batch_size is None:\n        return np.concatenate([_to_numpy(x) for x in tf.compat.v1.data.make_one_shot_iterator(ds).get_next()])\n    try:\n        return np.concatenate([_to_numpy(x) for x in tf.compat.v1.data.make_one_shot_iterator(ds.batch(batch_size)).get_next()])\n    except AttributeError:\n        return np.concatenate([_to_numpy(x) for x in tfds.as_numpy(ds)])\n\n", "CodeBleu_score": 23.588020381230255}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n\t\n    seed = np.asarray(seed, dtype=np.uint32)\n    if seed.size != 1:\n        raise ValueError(\\\"seed must be a scalar\\\")\n    return np.concatenate((np.array([np.uint32(seed)]), np.random.randint(0, 2**32, size=2, dtype=np.uint32)))\n\n", "CodeBleu_score": 21.10272634051165}, {"code": "def get_prng(seed)\n\t\n    seed = np.asarray(seed, dtype=np.uint32)\n    if seed.size != 1:\n        raise ValueError(\\\"Seed must be a scalar.\\\")\n    return np.array([seed, seed + 1]).view(np.uint64)\n\n", "CodeBleu_score": 15.973545816732933}, {"code": "def get_prng(seed)\n\t\n    seed = np.asarray(seed, dtype=np.uint32)\n    if seed.ndim != 0:\n        raise ValueError(f'seed must be a scalar, got shape {seed.shape}')\n    return np.random.default_rng(seed)\n\n\n", "CodeBleu_score": 17.23370874317495}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.timezone)\n    elif pdt.is_datetime64_any_dtype(s.dtype):\n        return s.tz_localize(dtype.timezone)\n    elif pdt.is_numeric_dtype(s.dtype):\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.timezone)\n    else:\n        return pd.to_datetime(s).dt.tz_localize(dtype.timezone)\n\n", "CodeBleu_score": 63.821553331846594}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.timezone)\n    elif pdt.is_datetime64_any_dtype(s.dtype):\n        return s.tz_localize(\"UTC\").tz_convert(dtype.timezone)\n    elif pdt.is_numeric_dtype(s.dtype):\n        return s.astype(\"datetime64[ns]\").tz_localize(\"UTC\").tz_convert(dtype.timezone)\n    else:\n        return s.astype(\"datetime64[ns]\").tz_localize(\"UTC\").tz_convert(dtype.timezone)\n\n", "CodeBleu_score": 53.66384673184892}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.timezone)\n    elif pdt.is_datetime64_any_dtype(s.dtype):\n        return s.tz_localize(dtype.timezone)\n    elif pdt.is_numeric_dtype(s.dtype):\n        return pd.to_datetime(s, utc=True).tz_localize(dtype.timezone)\n    else:\n        return pd.to_datetime(s, utc=True).tz_localize(None).tz_localize(dtype.timezone)\n\n", "CodeBleu_score": 65.32000010250712}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.tz is not None:\n        arg = arg.dt.tz_convert(None)\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    if lower_bound.tz is not", "CodeBleu_score": 15.888868567654402}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.dtype.tz == \"UTC\":\n        arg = arg.dt.tz_convert(None)\n    else:\n        arg = arg.dt.tz_localize(\"UTC\")\n\n    lower_bound = lower_bound.dt.tz_localize(\"UTC\")\n    upper_bound = upper_bound.dt.tz_localize(\"UTC\")\n\n    lower_bound = lower_bound.dt.time\n    upper_bound = upper_bound.dt.time\n\n    return cls.elementwise(\n        lambda x: (lower_bound <= x) & (x <= upper_bound),\n        dict(arg=arg),\n        name=op.name,\n        dtype=bool,\n    )\n\n", "CodeBleu_score": 31.71934834745812}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.tz is not None:\n        arg = arg.dt.tz_convert(None)\n        lower_bound = lower_bound.dt.tz_convert(None)\n        upper_bound = upper_bound.dt.tz_convert(None)\n\n    indexer = (lower_bound <= arg) & (arg <= upper_bound)\n    return cls.elementwise(lambda df: df.where(indexer, False), name=op.name)\n\n", "CodeBleu_score": 29.355292734885865}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, da.Series):\n        return value\n    elif isinstance(value, da.Scalar):\n        return da.from_array(value).to_series()\n    elif isinstance(value, pd.Series):\n        return da.from_array(value).to_series()\n    elif isinstance(value, tuple):\n        return da.from_array(value).to_series()\n    elif isinstance(value, list):\n        return da.from_array(value).to_series()\n    elif isinstance(value, dict):\n        return da.from_array(value).to_series()\n    elif isinstance(value, pd.DataFrame):\n        return da.from_array(value).to_series()\n    elif isinstance(value, da.DataFrame):\n        return da.from_array(value).to_series()\n    elif isinstance(value, np.ndarray):\n        return da.from_array(value).to_series()\n    elif isinstance(value, Callable):\n        return da.from_array(value).to_series()\n    elif isinstance(value, str):\n        return da.from_array(value).to_series()\n    elif isinstance(value, int):\n        return da.from_array(value).to_series()\n    elif isinstance(value, float):\n        return da.from_array(value).to_series()\n    elif isinstance(value, bool):\n        return da.from_array(value).to_series()\n    elif isinstance(value, pd.Index):\n        return da.from_array(value).to_series()\n    elif isinstance(value, da.Index):\n        return da.from_array(value).to_series()\n    elif isinstance(value, np.ndarray):\n        return da.from_array(value).to_series()\n    elif isinstance(value, Callable):\n        return da.from_array(value).to_series()\n    elif isinstance(value, str):\n        return da.from_array(value).to_series()\n    elif isinstance(value, int):\n        return da.from_array(value", "CodeBleu_score": 25.645473454690304}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, da.Series):\n        return value\n    elif isinstance(value, da.Scalar):\n        return da.from_array(value).to_series()\n    elif isinstance(value, pd.Series):\n        return da.from_array(value).to_series()\n    elif like is not None:\n        if isinstance(like, tuple):\n            return da.from_array(value).to_series()\n        elif isinstance(like, list):\n            return da.from_array(value).to_series()\n        elif isinstance(like, dict):\n            return da.from_array(value).to_series()\n    else:\n        return da.from_array(value).to_series()\n\n", "CodeBleu_score": 31.537626361964932}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, da.Series):\n        return value\n    elif isinstance(value, da.Scalar):\n        return da.from_array(value).to_series()\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, tuple):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, list):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, dict):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n\n", "CodeBleu_score": 36.250132685965035}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Value for key '{key}' is not a Dask or Pandas DataFrame\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 66.38333306442279}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    for name, table in dictionary.items():\n        if not isinstance(table, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(\n                f\"{name} is not a valid DataFrame. Expected Dask or Pandas DataFrame\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 55.801369690933036}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    for k, v in dictionary.items():\n        if not isinstance(v, dd.DataFrame):\n            raise TypeError(\n                f\"Value for key {k} is not a Dask DataFrame: {type(v)}\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 53.90415135986649}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for {estimator_name}: {list(classes[estimator_name].keys())}\")\n        logger.warning(f\"Multiple classes found for {estimator_name}, using the first one\")\n    selected_estimator = classes[estimator_name][list(classes[estimator_name].keys())[0]]\n    if not issubclass(selected_estimator, BaseEstimator):\n        logger.info(f\"{selected_estimator} is not a subclass of sklearn.BaseEstimator\")\n    return selected_estimator\n\n", "CodeBleu_score": 47.15261104652184}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} could not be found in {library_name} module.\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for estimator {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(f\"Multiple classes found for estimator {estimator_name}, using first one.\")\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"{estimator_class} is not a subclass of sklearn.base.BaseEstimator.\")\n    return estimator_class\n\n", "CodeBleu_score": 40.29563857889731}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(\n            f\"Estimator {estimator_name} could not be found in module {library_name}\"\n        )\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(f\"Multiple classes found for {estimator_name}, using first one\")\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"{estimator_class} is not a subclass of sklearn.BaseEstimator\")\n    return estimator_class\n\n", "CodeBleu_score": 40.92785724881401}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task.json\"), \"r\") as f:\n        estimator_to_task_map = json.load(f)\n    for postfix, task in estimator_to_task_map.items():\n        if estimator_name.endswith(postfix):\n            return task\n    return \\\"unknown\\\"\n\n", "CodeBleu_score": 42.58447663775547}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    mapping_file = os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\")\n    with open(mapping_file, \"r\") as f:\n        mapping = json.load(f)\n    for postfix, task in mapping.items():\n        if estimator_name.endswith(postfix):\n            return task\n    return \"unknown\"\n\n", "CodeBleu_score": 42.08774350483324}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    # Load the mapping from a JSON file\n    with open(\"mapping.json\", \"r\") as f:\n        mapping = json.load(f)\n\n    # Check if the estimator name matches any of the keys in the mapping\n    for key in mapping.keys():\n        if key in estimator_name:\n            return mapping[key]\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 26.2965181987234}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    logger.info(f'Loading data from cache {data_cache}')\n    data_files = get_filenames_by_prefix(data_cache, data_name)\n    data = dict()\n    for file in data_files:\n        if file.endswith(\\\".json\\\"):\n            continue\n        component, extension = file.split(\\\".\\\")[0].split(\\\"_\\\")[-2:]\n        data[component] = load_data_file(os.path.join(data_cache, file), extension)\n    return data\n\n\n\n\n\n", "CodeBleu_score": 47.64412136631444}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    data_dir = os.path.join(data_cache, data_name)\n    filenames = get_filenames_by_prefix(data_dir, data_name)\n    data = {}\n    for filename in filenames:\n        if filename.endswith(\\\".json\\\"):\n            continue\n        component, extension = filename.split(\\\".\\\")[-2:]\n        data[component] = load_data_file(os.path.join(data_dir, filename), extension)\n    return data\n\n\n\n\n", "CodeBleu_score": 40.79950865333401}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    filenames = get_filenames_by_prefix(data_cache, data_name)\n    logger.info(f'Loading data from cache: {data_cache}')\n    data = {}\n    for filename in filenames:\n        if filename.endswith(\\\".json\\\"):\n            continue\n        component, extension = filename.split(\\\".\\\")[:2]\n        data[component] = load_data_file(os.path.join(data_cache, filename), extension)\n    return data\n\n\n\n\n", "CodeBleu_score": 40.28266609830355}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\\\"http\\\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200\n        total_size = int(response.headers.get(\\\"Content-Length\\\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \\\"wb\\\") as f:\n            for data in response.iter_content(block_size):\n                f.write(data)\n                written_size += len(data)\n                if total_size:\n                    print(\\\"", "CodeBleu_score": 44.53794598841027}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\\\"http\\\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200\n        total_size = int(response.headers.get(\\\"content-length\\\", 0))\n        block_size = 1024\n        written = 0\n        with open(filename, \\\"wb\\\") as f:\n            for data in response.iter_content(block_size):\n                written += len(data)\n                f.write(data)\n                if total_size:\n                    print(\\\"", "CodeBleu_score": 44.40775953726755}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200, f\"Download failed with status code {response.status_code}\"\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n\n        with open(filename, \"wb\") as f:\n            for data in response.iter_content(block_size):\n                f.write(data)\n                written_size += len(data)\n\n        assert total_size == written_size, \"Downloaded size does not match the expected content length\"\n    else:\n        raise ValueError(f\"Unsupported URL scheme: {url}\")\n\n", "CodeBleu_score": 57.35656269142161}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    data_filename = \"covtype.csv.zip\"\n\n    data_path = download_kaggle_files(\n        \"uciml\",\n        \"forest-cover-type-prediction\",\n        [data_filename],\n        raw_data_cache,\n    )[data_filename]\n\n    data = pd.read_csv(data_path, index_col=0, compression=\"zip\", dtype=np.float32)\n    y = data.iloc[:, -1].to_numpy(dtype=np.int64) - 1\n    x = data.drop(labels=[data.columns[-1]], axis=1)\n\n    if dataset_params.get(\\\"binary_target\\\", False):\n        y = y == 1\n\n    data_desc = {\\\"n_classes\\\": 7, \\\"default_split\\\": {\\\"test_size\\\": 0.2, \\\"random_state\\\": 77}}\n    return {\\\"x\\\": x, \\\"y\\\": y}, data_desc\n\n", "CodeBleu_score": 31.032111091275183}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    data_filename = \"covtype.csv\"\n\n    data_path = download_kaggle_files(\n        \"uciml\",\n        \"mines-vs-rocks\",\n        [data_filename],\n        raw_data_cache,\n    )[data_filename]\n\n    data = pd.read_csv(data_path, index_col=0, compression=\"zip\", dtype=np.float32)\n    y = data.iloc[:, -1].to_numpy(dtype=np.int64)\n    x = data.drop(labels=[data.columns[-1]], axis=1)\n\n    if dataset_params.get(\\\"binary\\\", False):\n        y = (y == 1).astype(np.int64)\n\n    data_desc = {\n        \\\"n_classes\\\": 2,\n        \\\"default_split\\\": {\\\"test_size\\\": 0.2, \\\"random_state\\\": 77},\n    }\n    return {\\\"x\\\": x, \\\"y\\\": y}, data_desc\n\n", "CodeBleu_score": 34.445973377762996}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    data_filename = \"covtype.csv.zip\"\n\n    data_path = download_kaggle_files(\n        \"uci-machine-learning-repository-archive\",\n        \"covtype-data-for-multi-label-classification\",\n        [data_filename],\n        raw_data_cache,\n    )[data_filename]\n\n    data = pd.read_csv(data_path, index_col=0, compression=\"zip\", dtype=np.float32)\n    y = data.iloc[:, -1].to_numpy(dtype=np.int64) - 1\n    x = data.drop(labels=[data.columns[-1]], axis=1)\n\n    data_desc = {\"n_classes\": 7, \"default_split\": {\"test_size\": 0.2, \"random_state\": 77}}\n    return {\"x\": x, \"y\": y}, data_desc\n\n", "CodeBleu_score": 35.31874804957272}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    output_list = []\n    for el in input_list:\n        if isinstance(el, list):\n            output_list += flatten_list(el, ensure_type_homogeneity)\n        else:\n            output_list.append(el)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 43.675977689684586}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    output_list = []\n    for el in input_list:\n        if isinstance(el, list):\n            output_list.extend(flatten_list(el))\n        else:\n            output_list.append(el)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 40.7393594372694}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    output_list = []\n    for el in input_list:\n        if isinstance(el, list):\n            output_list.extend(flatten_list(el))\n        else:\n            output_list.append(el)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 40.7393594372694}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if complex_mode == \"split\":\n        x = np.real(x)\n    elif complex_mode == \"magnitude\":\n        x = np.abs(x)\n    elif complex_mode == \"jax\":\n        x = np.clip(x, 0.0, 1.0)\n    else:\n        raise ValueError(\"Invalid value for complex_mode\")\n    if eps is not None:\n        x = np.clip(x, eps, 1 - eps)\n    return np.log(x / (1 - x))\n\n", "CodeBleu_score": 47.54377805666364}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if complex_mode == \"split\":\n        x = np.asarray(x, dtype=np.complex64)\n        x = np.asarray(x.real, dtype=np.float32)\n        x = np.asarray(x.imag, dtype=np.float32)\n    elif complex_mode == \"magnitude\":\n        x = np.abs(x)\n    elif complex_mode == \"jax\":\n        x = np.asarray(x, dtype=np.complex64)\n        x = np.asarray(x.real, dtype=np.float32)\n        x = np.asarray(x.imag, dtype=np.float32)\n        x = np.asarray(x.real, dtype=np.float32)\n        x = np.asarray(x.imag, dtype=np.float32)\n    if eps is None:\n        return np.log(x / (1 - x))\n    else:\n        return np.log(x / (1 - x))\n\n", "CodeBleu_score": 41.64509138878593}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    return np.logit(x, eps=eps, out=out)\n\n", "CodeBleu_score": 17.1742890076788}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    return np.where(np.abs(x) > lambd, x, 0).astype(x.dtype)\n\n", "CodeBleu_score": 26.578348319387135}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    ret = np.where(x > lambd, x, np.where(x < -lambd, x, 0))\n    if ivy.exists(out):\n        return ivy.inplace_update(out, ret).astype(x.dtype)\n    return ivy.astype(ret, x.dtype)\n\n", "CodeBleu_score": 83.72577475370882}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    ret = np.where(x > lambd, x - lambd, np.where(x < -lambd, x + lambd, 0))\n    if ivy.exists(out):\n        return ivy.inplace_update(out, ret).astype(x.dtype)\n    return ivy.astype(ret, x.dtype)\n\n", "CodeBleu_score": 72.65783218722324}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            np.sin(\n                np.pi\n                * (\n                    np.arange(\n                        window_length,\n                        dtype=np.float64,\n                    )\n                    / window_length\n                )\n            )\n            ** 2\n        ],\n        dtype=dtype,\n    )\n\n", "CodeBleu_score": 25.066166011701203}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            np.sin(\n                np.pi\n                * (\n                    np.arange(window_length, dtype=dtype)\n                    - (window_length - 1) / 2\n                )\n                / (window_length - 1)\n            )\n        ]\n    )\n\n", "CodeBleu_score": 29.819469846353673}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            np.sin(np.pi * i / (window_length - 1)) ** 2\n            for i in range(window_length)\n        ],\n        dtype=dtype,\n    )\n\n", "CodeBleu_score": 37.75514643379071}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    coeffs = np.array(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    x = np.array(x, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    return np.polyval(coeffs, x)\n\n", "CodeBleu_score": 30.92974106707564}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    # Set the precision mode to 2 for float32 and float64\n    np.seterr(all='raise', under='ignore', over='raise')\n\n    # Promote the data types of the coefficients and inputs to a common type\n    coeffs = coeffs.astype(np.promote_types(coeffs.dtype, x.dtype))\n    x = x.astype(np.promote_types(coeffs.dtype, x.dtype))\n\n    # Compute the polynomial values using numpy's polyval\n    y = np.polyval(coeffs, x)\n\n    # Convert the result to an array with the promoted data type\n    y = np.array(y, dtype=coeffs.dtype)\n\n    return y\n\n", "CodeBleu_score": 32.134344831841844}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    return np.polyval(coeffs, x)\n\n", "CodeBleu_score": 13.70552496491311}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 50.26661683142579}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        return np.amax(x)\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 36.8450438499149}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 26.902649231113717}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 32.625925040171246}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 32.625925040171246}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 32.625925040171246}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if dtype is None:\n        dtype = np.int64\n    if axis is None:\n        axis = tuple(range(len(a.shape)))\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n\n", "CodeBleu_score": 64.17258845916528}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        axis = tuple(range(len(a.shape)))\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    return np.asarray(np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype), dtype=dtype)\n\n", "CodeBleu_score": 59.069000225816914}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    ret = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    if np.isscalar(ret):\n        if dtype is None:\n            dtype = ivy.default_int_dtype(as_native=True)\n        ret = np.asarray(ret, dtype=dtype)\n    return ret\n\n", "CodeBleu_score": 75.29896466487159}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if axis is None:\n        axis = tuple(range(len(x.shape)))\n    if isinstance(axis, int):\n        axis = (axis,)\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    if isinstance(spacing, int):\n        spacing = [spacing] * len(axis)\n    if isinstance(spacing, list):\n        spacing = tuple(spacing)\n    if isinstance(spacing, tuple):\n        if len(spacing) != len(axis):\n            raise ValueError(\n                \"The length of the spacing must be equal to the length of the axis.\"\n            )\n        return [np.gradient(x, spacing=spacing[i], axis=axis[i], edge_order=edge_order) for i in range(len(axis))]\n    raise TypeError(\n        f\"The type of the spacing must be int, list, or tuple. Found {type(spacing)}.\"\n    )\n\n", "CodeBleu_score": 42.98748075615994}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    if isinstance(spacing, list):\n        spacing = tuple(spacing)\n    if isinstance(spacing, int):\n        return np.gradient(x, spacing, axis, edge_order)\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 53.772626447320505}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if axis is None:\n        axis = list(range(x.ndim))\n    if isinstance(axis, int):\n        axis = [axis]\n    if isinstance(spacing, int):\n        spacing = [spacing]\n    if isinstance(spacing, list):\n        spacing = tuple(spacing)\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 48.155703959021366}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if not ivy.is_float_dtype(x):\n        x = x.astype(ivy.default_float_dtype(as_native=True))\n        y = y.astype(ivy.default_float_dtype(as_native=True))\n    if np.all(x == 0):\n        return np.array(0.0, dtype=x.dtype)\n    return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 51.35861268153741}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if not ivy.is_float_dtype(x):\n        x = x.astype(ivy.default_float_dtype(as_native=True))\n        y = y.astype(ivy.default_float_dtype(as_native=True))\n    if np.any(x == 0):\n        return np.zeros_like(x)\n    return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 51.42415520404818}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if not ivy.is_float_dtype(x):\n        x = x.astype(ivy.default_float_dtype(as_native=True))\n        y = y.astype(ivy.default_float_dtype(as_native=True))\n    return np.where(x == 0, 0.0, x * np.log(y))\n\n", "CodeBleu_score": 54.672769151804445}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    if data_format == \"channel_last\":\n        if dims == 4:\n            x = ivy.permute_dims(x, (0, 3, 1, 2))\n            kernel = kernel[::-1]\n            strides = strides[::-1]\n            dims = 5\n        elif dims == 5:\n            x = ivy.permute_dims(x, (0, 4, 1, 2, 3))\n            kernel = kernel[::-1]\n            strides = strides[::-1]\n            dims = 6\n    elif data_format == \"channel_first\":\n        if dims == 4:\n            x = ivy.permute_dims(x, (0, 2, 3, 1))\n            kernel = kernel[1::]\n            strides = strides[1::]\n            dims = 5\n        elif dims == 5:\n            x = ivy.permute_dims(x, (0, 2, 3, 4, 1))\n            kernel = kernel[1::]\n            strides = strides[1::]\n            dims = 6\n    depth_pooling_needed = _depth_max_pooling_helper(x, kernel, strides, dims)\n    return x, kernel, strides, depth_pooling_needed\n\n", "CodeBleu_score": 33.9256208582668}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    kernel_size = kernel\n    strides_size = strides\n    depth_pooling = False\n    if data_format == \"channel_last\":\n        if len(x.shape) == 4:\n            if dims == 2:\n                if x.shape[1] == 1:\n                    depth_pooling = True\n                    x = np.transpose(x, (0, 2, 3, 1))\n                    kernel_size = (kernel_size[0], 1, kernel_size[1], 1)\n                    strides_size = (strides_size[0], 1, strides_size[1], 1)\n            else:\n                depth_pooling = True\n                x = np.transpose(x, (0, 3, 1, 2))\n                kernel_size = (1, kernel_size[0], kernel_size[1], 1)\n                strides_size = (1, strides_size[0], strides_size[1], 1)\n    elif data_format == \"channel_first\":\n        if len(x.shape) == 4:\n            if dims == 2:\n                if x.shape[3] == 1:\n                    depth_pooling = True\n                    x = np.transpose(x, (0, 2, 3, 1))\n                    kernel_size = (kernel_size[0], 1, kernel_size[1], 1)\n                    strides_size = (strides_size[0], 1, strides_size[1], 1)\n            else:\n                depth_pooling = True\n                x = np.transpose(x, (0, 1, 3, 2))\n                kernel_size = (1, 1, kernel_size[0], kernel_size[1])\n                strides_size = (1, 1, strides_size[0], strides_size[1])\n    return x, kernel_size, strides_size, depth_pooling\n\n", "CodeBleu_score": 29.367538667660487}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    if data_format == \"channel_last\":\n        if dims == 2:\n            x = x.transpose((0, 2, 3, 1))\n            kernel = (kernel[0], kernel[1], x.shape[1])\n            strides = (strides[0], strides[1], 1)\n        elif dims == 3:\n            x = x.transpose((0, 2, 3, 4, 1))\n            kernel = (kernel[0], kernel[1], kernel[2], x.shape[1])\n            strides = (strides[0], strides[1], strides[2], 1)\n    elif data_format == \"channel_first\":\n        if dims == 2:\n            x = x.transpose((0, 3, 1, 2))\n            kernel = (kernel[0], x.shape[1], kernel[1])\n            strides = (strides[0], 1, strides[1])\n        elif dims == 3:\n            x = x.transpose((0, 4, 1, 2, 3))\n            kernel = (kernel[0], x.shape[1], kernel[1], kernel[2])\n            strides = (strides[0], 1, strides[1], strides[2])\n    return x, kernel, strides\n\n", "CodeBleu_score": 26.128763090687833}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if data_format == \"NWC\":\n        if len(x.shape) == 3:\n            x = np.swapaxes(x, 1, 2)\n    if training:\n        if len(x.shape) == 3:\n            mask = np.random.binomial(1, prob, x.shape[1]).astype(x.dtype)\n            x = x * mask * (1 / (1 - prob))\n        else:\n            mask = np.random.binomial(1, prob, x.shape[2]).astype(x.dtype)\n            x = x * mask * (1 / (1 - prob))\n    if data_format == \"NWC\":\n        if len(x.shape) == 3:\n            x = np.swapaxes(x, 1, 2)\n    return x\n\n", "CodeBleu_score": 43.86902907659997}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if data_format == \"NWC\":\n        x = x\n    elif data_format == \"NCW\":\n        x = np.swapaxes(x, 1, -1)\n    else:\n        raise ivy.utils.exceptions.IvyException(\n            \"data_format must be either 'NWC' or 'NCW'\"\n        )\n    if training:\n        if len(x.shape) == 3:\n            x = np.swapaxes(x, 1, -1)\n            x = np.expand_dims(x, axis=0)\n            x = np.swapaxes(x, 1, -1)\n        mask = np.random.binomial(1, prob, size=x.shape)\n        if out is None:\n            return (x * mask) / (1 - prob)\n        else:\n            np.multiply(x, mask, out=out)\n            np.divide(out, 1 - prob, out=out)\n            return out\n    else:\n        if len(x.shape) == 3:\n            x = np.swapaxes(x, 1, -1)\n            x = np.expand_dims(x, axis=0)\n            x = np.swapaxes(x, 1, -1)\n        return x\n\n", "CodeBleu_score": 38.14477591557884}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NWC\":\n            x = np.swapaxes(x, 1, 2)\n        mask = np.random.binomial(1, prob, x.shape[1])\n        x = x * (1 / (1 - prob)) * mask\n        if data_format == \"NWC\":\n            x = np.swapaxes(x, 1, 2)\n    return x\n\n", "CodeBleu_score": 34.42435524507616}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    top_values, top_indices = np.topk(x, k, axis=axis, largest=largest, sorted=sorted)\n    if isinstance(out, tuple) and len(out) == 2:\n        out = namedtuple(\"TopK\", [\"values\", \"indices\"])\n        out = out(top_values, top_indices)\n    return top_values, top_indices\n\n", "CodeBleu_score": 27.027531956413632}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    if axis is None:\n        x_shape = x.shape\n        x_ravel = x.ravel()\n        axis = len(x_shape) - 1\n    else:\n        x_shape = np.shape(x)\n        x_ravel = np.reshape(x, (-1, x_shape[axis]))\n\n    if largest:\n        # sort in descending order\n        x_sorted = np.sort(x_ravel, axis=axis)[::-1]\n    else:\n        # sort in ascending order\n        x_sorted = np.sort(x_ravel, axis=axis)\n\n    if sorted:\n        # return the top k elements\n        top_k = x_sorted[:, :k]\n        # calculate the indices of the top k elements\n        top_k_indices = np.argpartition(x_ravel, k, axis=axis)[:, :k]\n    else:\n        # return the top k elements\n        top_k = x_sorted[:, :k]\n        # calculate the indices of the top k elements\n        top_k_indices = np.argpartition(x_ravel, k, axis=axis)[:, :k]\n\n    # reshape the top k indices to match the original shape\n    top_k_indices = np.reshape(top_k_indices, x_shape)\n\n    # create a named tuple to store the top k values and indices\n    TopK = namedtuple(\"TopK\", [\"values\", \"indices\"])\n    top_k_tuple = TopK(top_k, top_k_indices)\n\n    return top_k_tuple\n\n", "CodeBleu_score": 43.06607772052077}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    return np.asarray(np.topk(x, k, axis, largest, sorted))\n\n", "CodeBleu_score": 15.587841577434917}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    if axis is None:\n        x = x.flatten()\n        norm = np.sum(np.abs(x))\n        norm = np.maximum(norm, 1e-12)\n        ret = x / norm\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        norm = np.maximum(norm, 1e-12)\n        ret = x / norm\n    if out is None:\n        return ret\n    else:\n        out[...] = ret\n        return out", "CodeBleu_score": 49.60120236693892}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = axis if axis is not None else x.ndim - 1\n    denorm = np.sum(np.abs(x), axis=axis, keepdims=True)\n    denorm = np.where(denorm == 0, np.finfo(x.dtype).eps, denorm)\n    return np.divide(x, denorm, out=out)", "CodeBleu_score": 41.05125348097134}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        denorm = np.sum(np.abs(x.flatten()))\n    else:\n        denorm = np.sum(np.abs(x), axis=axis, keepdims=True)\n    denorm = np.where(denorm > 0, denorm, np.finfo(x.dtype).eps)\n    return np.divide(x, denorm, out=out)\n\n", "CodeBleu_score": 57.897431618316645}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(shape, alpha, beta)\n    dtype = _check_shapes_broadcastable(shape, alpha, beta, dtype)\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.beta(alpha, beta, size=shape, dtype=dtype)\n\n", "CodeBleu_score": 71.27107871640328}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape)\n    dtype = ivy.as_native_dtype(dtype)\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.beta(alpha, beta, shape).astype(dtype)\n\n", "CodeBleu_score": 75.06265804472667}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = shape if shape is not None else []\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.beta(alpha, beta, shape)\n\n", "CodeBleu_score": 66.55503056207378}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    x = np.array(x)\n    inverse = np.zeros(x.shape, dtype=int)\n    inverse[np.argsort(x)] = np.arange(x.size)\n    return inverse\n\n", "CodeBleu_score": 34.335796729774614}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    x = np.asarray(x)\n    inverse = np.zeros_like(np.argsort(x))\n    inverse[np.argsort(x)] = np.arange(x.size)\n    return np.sort(inverse)\n\n", "CodeBleu_score": 35.346987196962765}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    x = np.array(x)\n    inverse = np.zeros_like(np.argsort(x))\n    inverse[np.argsort(x)] = np.arange(len(x))\n    return np.sort(inverse)", "CodeBleu_score": 37.76033491731782}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\t\n    # Set style\n    sns.set_style(config.plot_style)\n\n    # Create figure with subplots\n    fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Plot total vomit per month\n    plot_monthly_vomit(ax[0, 0], misc_data)\n\n    # Plot days between vomit\n    plot_days_between_vomit(ax[0, 1], misc_data)\n\n    # Plot days in daycare\n    plot_daycare_days(ax[1, 0], misc_data)\n\n    # Plot doctor visits per month\n    plot_doctor_visit_monthly(ax[1, 1], misc_data)\n\n    # Adjust layout\n    fig.tight_layout()\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 28.75827203735712}, {"code": "def plot_medical_charts() -> None\n\t\n    # Set plot style\n    sns.set_style(\\\"whitegrid\\\")\n\n    # Create figure with subplots\n    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Plot total vomit per month\n    plot_monthly_vomit(axs[0, 0], misc_data)\n\n    # Plot days between vomit\n    plot_days_between_vomit(axs[0, 1], misc_data)\n\n    # Plot days in daycare\n    plot_daycare_days(axs[1, 0], misc_data)\n\n    # Plot doctor visits\n    plot_doctor_visit_monthly(axs[1, 1], misc_data)\n\n    # Adjust layout\n    fig.tight_layout()\n\n    # Export figure\n    export_figure(fig, config.plot_medical_charts_file_name)", "CodeBleu_score": 27.865358513431442}, {"code": "def plot_medical_charts() -> None\n\t\n\n    # Set the style of the plotting environment\n    sns.set_style(\\\"darkgrid\\\")\n\n    # Create a figure with subplots\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n\n    # Plot total vomit per month\n    plot_monthly_vomit(axes[0, 0], misc_data)\n\n    # Plot days between vomit\n    plot_days_between_vomit(axes[0, 1], misc_data)\n\n    # Plot days in daycare\n    plot_daycare_days(axes[1, 0], misc_data)\n\n    # Plot doctor visits per month\n    plot_doctor_visit_monthly(axes[1, 1], misc_data)\n\n    # Adjust the layout of the subplots\n    plt.tight_layout()\n\n    # Export the figure to a specified file location\n    export_figure(fig, \\\"medical_charts\\\", config.plot_folder)", "CodeBleu_score": 29.106689604894708}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\t\n    # Import and extract feeding data\n    data = parse_raw_data(feeding_bottle_data, [\"Bottle feeding time\"])\n    data = data.append(parse_raw_data(feeding_solid_data, [\"Solid feeding time\"]))\n\n    # Compute offset from birthday\n    offset = (\n        data[\"Solid feeding time\"].iloc[0].date() - data[\"Bottle feeding time\"].iloc[0].date()\n    ).days\n\n    # Plot setup\n    sns.set(style=\"darkgrid\")\n    figure = plt.figure()\n    fig_ax = figure.add_subplot(111)\n\n    # Plot bottle feeding times\n    fig_ax.scatter(\n        data[\"Bottle feeding time\"].dt.hour + data[\"Bottle feeding time\"].dt.minute / 60,\n        data[\"Bottle feeding time\"].dt.day + offset,\n        color=\"red\",\n        label=\"Bottle feeding\",\n    )\n\n    # Plot solid feeding times\n    fig_ax.scatter(\n        data[\"Solid feeding time\"].dt.hour + data[\"Solid feeding time\"].dt.minute / 60,\n        data[\"Solid feeding time\"].dt.day + offset,\n        color=\"blue\",\n        label=\"Solid feeding\",\n    )\n\n    # Add legend\n    fig_ax.legend()\n\n    # End date - one year or full\n    end_date = get_end_date(\n        data[\"Bottle feeding time\"],\n        first_year_only=config[\"output_format\"][\"output_year_one_only\"],\n    )\n\n    # Format plot - vertical or horizontal\n    if config[\"output_format\"][\"output_feeding_viz_orientation\"] == \"vertical\":\n        format_24h_week_plot_vertical(fig_ax, end_date)\n    else:\n        format_24h_week_plot_horizontal(fig_ax, end_date, \"Feeding\")\n\n    # Export figure\n    export_figure(figure, config[\"output_data\"][\"output_feeding_v", "CodeBleu_score": 35.65183531916323}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n    # Import and extract feeding data\n    data = parse_raw_data(feeding_solid_data, [\"Time\"])\n\n    # Convert date to day number\n    data[\"day_number\"] = (data[\"Time\"].dt.date - data[\"Time\"].dt.date[0]).dt.days + 1\n\n    # Compute offset from birthday\n    offset = (data[\"Time\"].dt.date - data[\"Time\"].dt.date[0]).dt.days + 1\n\n    # Plot setup\n    sns.set(style=\"darkgrid\")\n    figure = plt.figure()\n    fig_ax = figure.add_subplot(111)\n\n    # Plot feeding times\n    fig_ax.scatter(offset, data[\"day_number\"], color=\"blue\", label=\"solid\")\n    fig_ax.scatter(offset, data[\"day_number\"], color=\"red\", label=\"bottle\")\n\n    # End date - one year or full\n    end_date = get_end_date(\n        data[\"day_number\"], first_year_only=config[\"output_format\"][\"output_year_one_only\"]\n    )\n\n    # Format plot - vertical or horizontal\n    if config[\"output_format\"][\"output_feeding_viz_orientation\"] == \"vertical\":\n        format_24h_week_plot_vertical(fig_ax, end_date)\n    else:\n        format_24h_week_plot_horizontal(fig_ax, end_date, \"Feeding\")\n\n    # Export figure\n    export_figure(figure, config[\"output_data\"][\"output_feeding_viz\"])\n\n", "CodeBleu_score": 35.2441042384565}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n    # Import and extract feeding data\n    data = parse_raw_data(feeding_bottle_data, [\"Bottle feeding time\"])\n    data = data.append(parse_raw_data(feeding_solid_data, [\"Solid feeding time\"]))\n\n    # Convert time to decimal hour\n    data[\"timestamp_hour\"] = data[\"timestamp_hour\"].dt.hour + data[\"timestamp_hour\"].dt.minute / 60\n\n    # Compute offset from birthday\n    offset = data.iloc[0][\"timestamp_hour\"]\n    data[\"timestamp_hour\"] = data[\"timestamp_hour\"] - offset\n\n    # Plot setup\n    sns.set(style=\"darkgrid\")\n    figure = plt.figure()\n    fig_ax = figure.add_subplot(111)\n\n    # Plot bottle feedings\n    fig_ax.scatter(data[\"day_number\"], data[\"timestamp_hour\"], c=\"red\", label=\"Bottle\")\n\n    # Plot solid feedings\n    fig_ax.scatter(data[\"day_number\"], data[\"timestamp_hour\"], c=\"blue\", label=\"Solid\")\n\n    # Legend\n    fig_ax.legend()\n\n    # End date - one year or full\n    end_date = get_end_date(\n        data[\"day_number\"], first_year_only=config[\"output_format\"][\"output_year_one_only\"]\n    )\n\n    # Format plot - vertical or horizontal\n    if config[\"output_format\"][\"output_feeding_viz_orientation\"] == \"vertical\":\n        format_24h_week_plot_vertical(fig_ax, end_date)\n    else:\n        format_24h_week_plot_horizontal(fig_ax, end_date, \"Feeding\")\n\n    # Export figure\n    export_figure(figure, config[\"output_data\"][\"output_feeding_viz\"])\n\n", "CodeBleu_score": 35.41633055582648}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts, target_names = self._get_target_counts(task=task)\n\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n    ax.set_ylabel('Counts')\n    ax.set_title(f'Target counts for {task.name}')\n\n", "CodeBleu_score": 46.84031185166659}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n\n    ax.set_title(f'Target counts for {task.name}')\n    ax.set_xlabel('Target')\n    ax.set_ylabel('Count')\n\n    return ax\n\n", "CodeBleu_score": 40.534054958007374}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts, target_names = self._get_target_counts(task=task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n    ax.set_title(f'Target counts for {task.name}')\n\n\n", "CodeBleu_score": 47.586357108746235}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n\t\n    import numpy as np\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n    except ModuleNotFoundError:\n        print('please install matplotlib')\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    rgba_buf = canvas.buffer_rgba()\n    img = np.reshape(np.asarray(rgba_buf), (figure.canvas.get_width_height()[::-1] + (4,)))\n    img = img[:, :, :3]\n    return img\n\n", "CodeBleu_score": 37.91881029038411}, {"code": "def render_to_rgb(figure)\n\t\n    import numpy as np\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n    except ModuleNotFoundError:\n        print('please install matplotlib')\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            rgba_buf = canvas.buffer_rgba()\n            image = np.reshape(np.asarray(rgba_buf), (fig.canvas.get_width_height()[::-1] + (4,)))\n            image = image[..., :3]\n            images.append(image)\n        return np.stack(images)\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        rgba_buf = canvas.buffer_rgba()\n        image = np.reshape(np.asarray(rgba_buf), (figure.canvas.get_width_height()[::-1] + (4,)))\n        image = image[..., :3]\n        return image", "CodeBleu_score": 39.57260945303314}, {"code": "def render_to_rgb(figure)\n\t\n    import numpy as np\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n    except ModuleNotFoundError:\n        print('please install matplotlib')\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n    w, h = figure.canvas.get_width_height()\n    image = data.reshape(h, w, 4)\n    image = image[:, :, :3]\n    image = np.moveaxis(image, source=2, destination=0)\n    return image", "CodeBleu_score": 56.54964333222349}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n\t\n    for i in range(1000):\n        tensor_list1 = [tensor_and_name(np.random.rand(2, 3), 'tensor_%d' % i) for i in range(5)]\n        tensor_list2 = [tensor_and_name(np.random.rand(3, 4), 'tensor_%d' % i) for i in range(5, 10)]\n\n        beholder = beholder_lib.Beholder(LOG_DIRECTORY)\n        beholder.update(tensor_list1, frame=i)\n        beholder.update(tensor_list2, frame=i)\n\n        time.sleep(0.1)\n        print(i)\n\n", "CodeBleu_score": 41.39303604940769}, {"code": "def beholder_pytorch()\n\t\n    for i in range(1000):\n        tensor_list1 = [tensor_and_name(np.random.randn(1000, 1000), 'tensor1'), tensor_and_name(np.random.randn(1000, 1000), 'tensor2')]\n        tensor_list2 = [tensor_and_name(np.random.randn(1000, 1000), 'tensor3'), tensor_and_name(np.random.randn(1000, 1000), 'tensor4')]\n\n        beholder = beholder_lib.Beholder(LOG_DIRECTORY)\n        beholder.update(tensor_list1, frame=i)\n        beholder.update(tensor_list2, frame=i)\n\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 34.64730265371661}, {"code": "def beholder_pytorch()\n\t\n    for i in range(1000):\n        tensor_list_1 = [tensor_and_name(np.random.rand(10, 10), 'tensor_1_%d' % i),\n                         tensor_and_name(np.random.rand(10, 10), 'tensor_2_%d' % i)]\n        tensor_list_2 = [tensor_and_name(np.random.rand(10, 10), 'tensor_3_%d' % i),\n                         tensor_and_name(np.random.rand(10, 10), 'tensor_4_%d' % i)]\n\n        beholder = beholder_lib.Beholder(LOG_DIRECTORY)\n        beholder.update(tensor_list_1, frame=i)\n        beholder.update(tensor_list_2, frame=i)\n\n        time.sleep(0.1)\n        print(i)\n\n", "CodeBleu_score": 37.06343103843125}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    tensor_data = summary.tensor_data\n    if len(tensor_data.shape) == 1:\n        tensor_data = tensor_data.reshape((1, -1))\n    if len(tensor_data.shape) == 2:\n        tensor_data = tensor_data.reshape((-1, tensor_data.shape[0]))\n    if len(tensor_data.shape) == 3:\n        tensor_data = tensor_data.reshape((tensor_data.shape[0], -1))\n    thresholds = [float(i) / num_thresholds for i in range(num_thresholds)]\n    TP = tensor_data[0].tolist()\n    FP = tensor_data[1].tolist()\n    TN = tensor_data[2].tolist()\n    FN = tensor_data[3].tolist()\n    precision = tensor_data[4].tolist()\n    recall = tensor_data[5].tolist()\n    pr_data = {}\n        pr_data['tag'] = tag\n        pr_data['TP'] = TP\n        pr_data['FP'] = FP\n        pr_data['TN'] = TN\n        pr_data['FN'] = FN\n        pr_data['precision'] = precision\n        pr_data['recall'] = recall\n        pr_data['thresholds'] = thresholds\n        self.log_asset_data(pr_data, tag, step=step)\n\n    @_requiresComet\n    def log_pr_curve(self, tag, summary, num_thresholds, step=None):\n        \\\"\\\"\\\"Logs a precision-recall curve as an asset.\n\n        Args:\n        tag: Data identifier\n        summary: Summary object\n        num_thresholds: Number of thresholds\n        step: step value to record\n        \\\"\\\"\\\"\n        self.log_pr_data(tag, summary, num_thresholds, step)\n        self.log_asset_data(None, tag, step=step, asset_type='", "CodeBleu_score": 30.53534816721089}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    tensor_data = summary.tensor_data\n        if len(tensor_data) == 0:\n            return\n        tensor_data = tensor_data[0]\n        if len(tensor_data.shape) == 1:\n            tensor_data = tensor_data.reshape(1, tensor_data.shape[0])\n        elif len(tensor_data.shape) == 2:\n            tensor_data = tensor_data.reshape(tensor_data.shape[0], tensor_data.shape[1])\n        elif len(tensor_data.shape) == 3:\n            tensor_data = tensor_data.reshape(tensor_data.shape[0], tensor_data.shape[1], tensor_data.shape[2])\n        thresholds = [i / num_thresholds for i in range(num_thresholds)]\n        tp = tensor_data[:, 0].tolist()\n        fp = tensor_data[:, 1].tolist()\n        tn = tensor_data[:, 2].tolist()\n        fn = tensor_data[:, 3].tolist()\n        precision = tensor_data[:, 4].tolist()\n        recall = tensor_data[:, 5].tolist()\n        pr_data = {\n            \\\"tag\\\": tag,\n            \\\"thresholds\\\": thresholds,\n            \\\"tp\\\": tp,\n            \\\"fp\\\": fp,\n            \\\"tn\\\": tn,\n            \\\"fn\\\": fn,\n            \\\"precision\\\": precision,\n            \\\"recall\\\": recall,\n        }\n        self.log_asset_data(pr_data, tag, step=step)\n\n    @_requiresComet\n    def log_pr_curve(self, tag, summary, num_thresholds, step=None):\n        \\\"\\\"\\\"Logs a precision-recall curve.\n\n        Args:\n        tag: Data identifier\n        summary: Summary object\n        num_thresholds: Number of thresholds to use\n        step: step value to record\n        \\\"\\\"\\\"", "CodeBleu_score": 28.032413230305586}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    tensor_data = summary.tensors.get(tag)\n    if tensor_data is None:\n        return\n    if len(tensor_data.shape) == 0:\n        return\n    if len(tensor_data.shape) == 1:\n        tensor_data = tensor_data.reshape(1, -1)\n    if len(tensor_data.shape) == 2:\n        tensor_data = tensor_data.reshape(-1, tensor_data.shape[-1])\n    thresholds = [i / num_thresholds for i in range(num_thresholds)]\n    tp = tensor_data[:, 0].tolist()\n    fp = tensor_data[:, 1].tolist()\n    tn = tensor_data[:, 2].tolist()\n    fn = tensor_data[:, 3].tolist()\n    precision = tensor_data[:, 4].tolist()\n    recall = tensor_data[:, 5].tolist()\n    pr_data = {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'precision': precision, 'recall': recall, 'thresholds': thresholds}\n    self.log_asset_data(pr_data, tag, step=step)\n\n", "CodeBleu_score": 41.12588456545314}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n\t\n    data = np.zeros(num * dim, dtype=np.uint8)\n    target = np.zeros(num, dtype=np.uint8)\n    with gzip.open(os.path.join(parent, images), 'rb') as fimg,\n            gzip.open(os.path.join(parent, labels), 'rb') as flbl:\n        fimg.read(16)\n        flbl.read(8)\n        for i in six.miterange(num):\n            data[i * dim:(i + 1) * dim]\n                = np.frombuffer(fimg.read(dim), dtype=np.uint8)\n            target[i]\n                = np.frombuffer(flbl.read(1), dtype=np.uint8)\n    return data, target\n\n\n\n\n", "CodeBleu_score": 40.319277060035326}, {"code": "def load_mnist(images, labels, num)\n\t\n    data = np.zeros(num * dim, dtype=np.uint8)\n    target = np.zeros(num, dtype=np.uint8)\n    with gzip.open(os.path.join(parent, labels), 'rb') as flbl:\n        flbl.read(8)\n        for i in range(num):\n            target[i] = np.frombuffer(flbl.read(1), dtype=np.uint8)\n    with gzip.open(os.path.join(parent, images), 'rb') as fimg:\n        fimg.read(16)\n        for i in range(num):\n            data[i * dim:(i + 1) * dim] = np.frombuffer(fimg.read(dim), dtype=np.uint8).reshape(dim)\n    return data, target\n\n", "CodeBleu_score": 43.34572792859207}, {"code": "def load_mnist(images, labels, num)\n\t\n    data = np.zeros(num * dim, dtype=np.uint8)\n    target = np.zeros(num, dtype=np.uint8)\n    with gzip.open(os.path.join(parent, labels), 'rb') as flbl:\n        flbl.read(8)\n        for i in six.moves.range(num):\n            target[i] = ord(flbl.read(1))\n    with gzip.open(os.path.join(parent, images), 'rb') as fimg:\n        fimg.read(16)\n        for i in six.moves.range(num):\n            for j in six.moves.range(dim):\n                data[i * dim + j] = ord(fimg.read(1))\n    return data, target\n\n", "CodeBleu_score": 50.42893689071474}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    ldf._unique_values = {}\n        ldf._min_max = {}\n        ldf._cardinality = {}\n        ldf.len = len(ldf)\n        for attr in ldf.columns:\n            if pd.api.types.is_integer_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_float_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_datetime64_any_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_string_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_bool_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_categorical_dtype(ldf.dtypes[attr]):\n                ldf._unique_values[attr] = ldf[attr].unique()\n                ldf._cardinality[attr] = len(ldf._unique_values[attr])\n            elif pd.api.types.is_object_dtype(ldf.dtypes[attr]", "CodeBleu_score": 35.07894082141444}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    ldf._unique_values = {}\n        ldf._min_max = {}\n        ldf._cardinality = {}\n        ldf._length = len(ldf)\n\n        for attr in list(ldf.columns):\n            if attr in ldf._type_override:\n                ldf._data_type[attr] = ldf._type_override[attr]\n            else:\n                temporal_var_list = [\\\"month\\\", \\\"year\\\", \\\"day\\\", \\\"date\\\", \\\"time\\\", \\\"weekday\\\"]\n\n                if is_timedelta64_series(ldf[attr]):\n                    ldf._data_type[attr] = \\\"quantitative\\\"\n                    ldf._min_max[attr] = (\n                        timedelta64_to_float_seconds(ldf[attr].min()),\n                        timedelta64_to_float_seconds(ldf[attr].max()),\n                    )\n                elif is_datetime(ldf[attr]):\n                    ldf._data_type[attr] = \\\"temporal\\\"\n                elif self._is_datetime_string(ldf[attr]):\n                    ldf._data_type[attr] = \\\"temporal\\\"\n                elif isinstance(attr, pd._libs.tslibs.timestamps.Timestamp):\n                    ldf._data_type[attr] = \\\"temporal\\\"\n                elif str(attr).lower() in temporal_var_list:\n                    ldf._data_type[attr] = \\\"temporal\\\"\n                elif self._is_datetime_number(ldf[attr]):\n                    ldf._data_type[attr] = \\\"temporal\\\"\n                elif self._is_geographical_attribute(ldf[attr]):\n                    ldf._data_type[attr] = \\\"geographical\\\"\n                elif pd.api.types.is_float_dtype(ldf.dtypes[attr]):\n                    if ldf.cardinality[attr] != len(ldf) and (ldf.cardinality[attr] < 20):\n", "CodeBleu_score": 38.29396107401385}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    if ldf.pre_aggregated:\n        return\n    ldf._unique_values = {}\n    ldf._cardinality = {}\n    ldf._min_max = {}\n    ldf._length = len(ldf)\n    for attr in list(ldf.columns):\n        if attr in ldf._type_override:\n            ldf._data_type[attr] = ldf._type_override[attr]\n        else:\n            temporal_var_list = [\"month\", \"year\", \"day\", \"date\", \"time\", \"weekday\"]\n\n            if is_timedelta64_series(ldf[attr]):\n                ldf._data_type[attr] = \"quantitative\"\n                ldf._min_max[attr] = (\n                    timedelta64_to_float_seconds(ldf[attr].min()),\n                    timedelta64_to_float_seconds(ldf[attr].max()),\n                )\n            elif is_datetime(ldf[attr]):\n                ldf._data_type[attr] = \"temporal\"\n            elif self._is_datetime_string(ldf[attr]):\n                ldf._data_type[attr] = \"temporal\"\n            elif isinstance(attr, pd._libs.tslibs.timestamps.Timestamp):\n                ldf._data_type[attr] = \"temporal\"\n            elif str(attr).lower() in temporal_var_list:\n                ldf._data_type[attr] = \"temporal\"\n            elif self._is_datetime_number(ldf[attr]):\n                ldf._data_type[attr] = \"temporal\"\n            elif self._is_geographical_attribute(ldf[attr]):\n                ldf._data_type[attr] = \"geographical\"\n            elif pd.api.types.is_float_dtype(ldf.dtypes[attr]):\n                if ldf.cardinality[attr] != len(ldf) and (ldf.cardinality[attr] < 20):\n                    ldf._data_type[attr] = \"nominal\"\n                else", "CodeBleu_score": 38.17993776988753}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = f\\\"\\\"\\\"\n        SELECT *\n        FROM {tbl.table_name}\n        LIMIT {preview_size}\n        \\\"\\\"\\\"\n    df = pd.read_sql(preview_query, tbl.sql_connection)\n    return df\n\n", "CodeBleu_score": 25.45134609732121}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = f\"\"\"\n    SELECT *\n    FROM {tbl.name}\n    LIMIT {preview_size}\n    \"\"\"\n    df = pd.read_sql(preview_query, tbl.connection)\n    return df\n\n", "CodeBleu_score": 16.948527259373307}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = f\\\"SELECT * FROM {tbl.name} LIMIT {preview_size}\\\"\n    df = pd.read_sql(preview_query, tbl.connection)\n    return df\n\n", "CodeBleu_score": 28.116932868802223}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n    if sample_flag:\n        if sample_start is not None:\n            sample_limit = sample_start\n        elif sample_cap is not None:\n            sample_limit = sample_cap\n        elif sample_frac is not None:\n            table_len = pandas.read_sql(lux.config.query_templates['table_len_query'].format(table_name = tbl.table_name), lux.config.SQLconnection)['count'][0]\n            sample_limit = math.ceil(table_len * sample_frac)\n        else:\n            sample_limit = 100\n        sample_query = lux.config.query_templates['sample_query']\n        tbl._sampled = pandas.read_sql(sample_query.format(table_name = tbl.table_name, sample_limit = sample_limit), lux.config.SQLconnection)\n    else:\n        tbl._sampled = pandas.read_sql(lux.config.query_templates['table_query'].format(table_name = tbl.table_name), lux.config.SQLconnection)\n\n", "CodeBleu_score": 35.90259383482756}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n\n    if sample_flag:\n        if sample_start is None:\n            sample_start = 0\n        if sample_cap is None:\n            sample_cap = sample_start + 10000\n        if sample_frac is None:\n            sample_frac = 0.1\n        sample_limit = math.floor(sample_frac * sample_cap)\n        sample_query = lux.config.query_templates['sample_query']\n        output = pandas.read_sql(sample_query.format(table_name = tbl.table_name, sample_start = sample_start, sample_limit = sample_limit), lux.config.SQLconnection)\n        tbl._sampled = output\n    else:\n        tbl._sampled = tbl.df\n\n", "CodeBleu_score": 33.50036859098289}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n    if sample_flag:\n        # Get length of table\n        length_query = lux.config.query_templates['length_query']\n        length_query = length_query.format(table_name = tbl.table_name)\n        length = pandas.read_sql(length_query, lux.config.SQLconnection).iloc[0,0]\n        # Get limit based on sample fraction\n        if sample_frac:\n            limit = math.ceil(sample_frac * length)\n        elif sample_cap:\n            limit = min(sample_cap, length)\n        elif sample_start:\n            limit = min(length, sample_start + sample_cap)\n        else:\n            limit = length\n        # Sample rows\n        sample_query = lux.config.query_templates['sample_query']\n        sample_query = sample_query.format(table_name = tbl.table_name, limit = limit)\n        tbl._sampled = pandas.read_sql(sample_query, lux.config.SQLconnection)\n    else:\n        tbl._sampled = pandas.read_sql(f'SELECT * FROM {tbl.table_name}', lux.config.SQLconnection)\n\n", "CodeBleu_score": 42.13509004362328}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    # Override text parameters with provided parameters\n    text_params = override_params(params[\"text\"], params)\n    text = text_params.pop(\"text\")\n    x = text_params.pop(\"x\")\n    y = text_params.pop(\"y\")\n\n    # Get background bounds\n    xmin, ymin, xmax, ymax = background.bounds\n\n    # Interpolate x and y values to fit within background bounds\n    x = interpolate(x, xmin, xmax)\n    y = interpolate(y, ymin, ymax)\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **text_params)\n\n", "CodeBleu_score": 18.06200436898077}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    # Override default text settings with provided parameters\n    text = params.pop(\\\"text\\\")\n    x = params.pop(\\\"x\\\")\n    y = params.pop(\\\"y\\\")\n\n    # Retrieve background bounds\n    xmin, ymin, xmax, ymax = background.bounds\n    dx, dy = xmax - xmin, ymax - ymin\n\n    # Interpolate x and y values to fit within background bounds\n    x = np.interp(x, [0, 1], [xmin, xmax])\n    y = np.interp(y, [0, 1], [ymin, ymax])\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 18.182515723453054}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    # Override default text settings with provided parameters\n    text = params.pop(\\\"text\\\", {})\n    text = override_params(TEXT_DEFAULT, text)\n    text_str = text.pop(\\\"text\\\")\n    text_x = text.pop(\\\"x\\\")\n    text_y = text.pop(\\\"y\\\")\n\n    # Get background bounds\n    xmin, ymin, xmax, ymax = background.bounds\n\n    # Interpolate x and y values to fit within background bounds\n    text_x = np.interp(text_x, [0, 1], [xmin, xmax])\n    text_y = np.interp(text_y, [0, 1], [ymin, ymax])\n\n    # Draw text\n    plt.text(text_x, text_y, text_str, **text)\n\n", "CodeBleu_score": 17.01177288714154}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    # TODO: make this function more robust\n    if datetime_format is not None:\n        return datetime_format\n    if len(dates) < 2:\n        return '%Y-%m-%d'\n    diffs = [(dates[i+1] - dates[i]).total_seconds() for i in range(len(dates)-1)]\n    avg_diff = sum(diffs) / len(diffs)\n    if avg_diff < 0.33:\n        if (dates[-1] - dates[0]).days > 1:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if (dates[-1] - dates[0]).days > 365:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n\n", "CodeBleu_score": 43.87579043728023}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format is not None:\n        return datetime_format\n    if len(dates) < 2:\n        return '%Y-%m-%d'\n    diff = np.diff(dates)\n    if np.average(diff) < 0.33:\n        if np.max(dates) - np.min(dates) > datetime.timedelta(days=1):\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if np.max(dates) - np.min(dates) > datetime.timedelta(days=365):\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n", "CodeBleu_score": 31.459071976691206}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format is not None:\n        return datetime_format\n    if len(dates) < 2:\n        return '%Y-%m-%d'\n    if len(dates) < 10:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 100:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 1000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 10000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 100000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 1000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 10000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 100000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 1000000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 10000000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 100000000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 1000000000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 10000000000000:\n        return '%Y-%m-%d %H:%M'\n    if len(dates) < 100000000000000:\n        return '%Y-%m", "CodeBleu_score": 23.08577608816689}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if alpha is None:\n        alpha = 1.0\n    if _is_uint8_rgb_or_rgba(c):\n        if c[0] > 1:\n            c = c[0] / 255.0, c[1] / 255.0, c[2] / 255.0\n        if len(c) == 4:\n            return c[0], c[1], c[2], c[3]\n        else:\n            return c[0], c[1], c[2], alpha\n    else:\n        return mcolors.to_rgba(c, alpha)\n\n", "CodeBleu_score": 29.933540756281545}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if isinstance(c, str):\n        return mcolors.to_rgba(c, alpha=alpha)\n    if _is_uint8_rgb_or_rgba(c):\n        c = np.asarray(c, dtype=float) / 255.0\n        if len(c) == 4:\n            return c\n        return c[0:3]\n    return mcolors.to_rgba(c, alpha=alpha)\n\n", "CodeBleu_score": 29.466144020098284}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if alpha is None:\n        alpha = 1\n    if isinstance(c, str):\n        c = mcolors.to_rgba(c)\n    elif _is_uint8_rgb_or_rgba(c):\n        c = tuple([float(v) / 255 for v in c])\n    if len(c) == 4:\n        return c\n    return c + (alpha,)\n\n", "CodeBleu_score": 32.89266043683236}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n\t\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    result = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(result,mpl_axes.Axes):\n        result.mpfstyle = style\n    elif isinstance(result,np.ndarray):\n        for ax in result:\n            ax.mpfstyle = style\n    else:\n        raise TypeError('Unexpected type of result from subplots')\n\n    return result", "CodeBleu_score": 61.56168753685415}, {"code": "def subplots(self,*args,**kwargs)\n\t\n\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    result = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(result,mpl_axes.Axes):\n        result.mpfstyle = style\n    elif isinstance(result,np.ndarray):\n        for ax in result.flatten():\n            ax.mpfstyle = style\n    else:\n        raise TypeError('subplots() returned unexpected type: {0}'.format(type(result)))\n\n    return result\n\n", "CodeBleu_score": 63.54689724578036}, {"code": "def subplots(self,*args,**kwargs)\n\t\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    plt.figure(self.number)  # make it the current Figure\n\n    axs = plt.subplots(*args,**kwargs)\n\n    if isinstance(axs,mpl_axes.Axes):\n        axs.mpfstyle = style\n    elif isinstance(axs,np.ndarray):\n        for ax in axs.flatten():\n            ax.mpfstyle = style\n    else:\n        raise TypeError('Unexpected result from subplots()')\n\n    return axs", "CodeBleu_score": 61.00749914500043}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n\t\n    if config['figratio'] is not None:\n        # Get the figure ratio from the config\n        figratio = config['figratio']\n        if isinstance(figratio,dict):\n            if 'ratio' in figratio:\n                figratio = figratio['ratio']\n            else:\n                figratio = None\n        if figratio is not None:\n            if isinstance(figratio,str):\n                figratio = tuple(map(float,figratio.split(':')))\n            if len(figratio) == 2:\n                if figratio[0] <= 0 or figratio[1] <= 0:\n                    raise ValueError('figratio must be a tuple of two positive numbers')\n                if figratio[0] < 1 or figratio[1] < 1:\n                    raise ValueError('figratio must be a tuple of two positive numbers greater than 1')\n                if figratio[0] > 10 or figratio[1] > 10:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 10')\n                if figratio[0] > 100 or figratio[1] > 100:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 100')\n                if figratio[0] > 1000 or figratio[1] > 1000:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 1000')\n                if figratio[0] > 10000 or figratio[1] > 10000:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 10000')\n                if figratio[0] > 100000 or figratio[1] > 100000:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 100000')\n                if figratio[0] > 1000000 or figratio[1] > 1000000:\n                    raise ValueError('figratio must be a tuple of two positive numbers less than 10000", "CodeBleu_score": 23.99406538072274}, {"code": "def _adjust_figsize(fig,config)\n\t\n    #print('_adjust_figsize(fig,config)')\n    #print('type(fig)=',type(fig))\n    #print('type(config)=',type(config))\n    #print('config=',config)\n    #print('config[\\'figratio\\']=',config['figratio'])\n    #print('config[\\'figscale\\']=',config['figscale'])\n    #print('config[\\'figsize\\']=',config['figsize'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inches'])\n    #print('config[\\'figsize_inches\\']=',config['figsize_inc", "CodeBleu_score": 0.36231884057971014}, {"code": "def _adjust_figsize(fig,config)\n\t\n    if not config['figratio']:\n        return\n\n    if config['figsize'] is not None:\n        # If user specified figsize, use that\n        fig.set_size_inches(config['figsize'])\n        return\n\n    # If user did not specify figsize, then calculate figsize\n    # based on figratio\n    # If figratio is not a tuple, then it is a float and we\n    # assume that figratio is the aspect ratio of the figure\n    if not isinstance(config['figratio'],tuple):\n        aspect = config['figratio']\n    else:\n        aspect = config['figratio'][0]/config['figratio'][1]\n\n    # If figratio is a tuple, then it is the aspect ratio of the figure\n    # and the width of the figure in inches\n    if isinstance(config['figratio'],tuple):\n        width = config['figratio'][1]\n    else:\n        width = config['figratio']\n\n    # If figratio is a tuple, then it is the aspect ratio of the figure\n    # and the height of the figure in inches\n    if isinstance(config['figratio'],tuple):\n        height = config['figratio'][0]\n    else:\n        height = width/aspect\n\n    # If height is greater than 10, then set height to 10\n    if height > 10:\n        height = 10\n\n    # If height is less than 1, then set height to 1\n    if height < 1:\n        height = 1\n\n    # If width is greater than 10, then set width to 10\n    if width > 10:\n        width = 10\n\n    # If width is less than 1, then set width to 1\n    if width < 1:\n        width = 1\n\n    # Set the figure size in inches\n    fig.set_size_inches(width,height)\n\n", "CodeBleu_score": 20.324919042425982}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tpmav.get('shift', 0)\n        periods = apmav.get('periods', [])\n        apmav = None\n    # If the moving averages are specified as a list, extract the shift values and periods\n    elif isinstance(apmav, list):\n        shift = [apmav.get(k, {}).get('shift', 0) for k in apmav]\n        periods = [apmav.get(k, {}).get('periods', []) for k in apmav]\n        apmav = None\n    # If the moving averages are not specified, use the default values from the config\n    if apmav is None:\n        apmav = config['_mav_config']['mav']\n    # If the moving averages are specified as a dictionary, extract the shift values and periods\n    if isinstance(apmav, dict):\n        shift = apmav.get('shift', 0)\n        periods = apmav.get('periods', [])\n    # If the moving averages are specified as a list, extract the shift values and periods\n    elif isinstance(apmav, list):\n        shift = [apmav.get(k, {}).get('shift', 0) for k in apmav]\n        periods = [apmav.get(k, {}).get('periods', []) for k in apmav]\n    # If the moving averages are not specified, use the default values from the config\n    if apmav is None:\n        apmav = config['_mav_config']['mav']\n    # If the moving averages are specified as a dictionary, extract the shift values and periods\n    if isinstance(apmav, dict):\n        shift = apmav.get('shift', 0)\n        periods = apmav.get('periods', [])\n    # If the moving averages are specified as a list, extract the shift values and periods\n    elif isinstance(apmav, list):\n        shift = [apmav.get", "CodeBleu_score": 26.086978476293236}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t apmav['shift']\n            del apmav['shift']\n        else:\n            shift = 0\n        if 'period' in apmav:\n            period = apmav['period']\n            del apmav['period']\n        else:\n            period = None\n    else:\n        shift = 0\n        period = None\n    if apwidth is None:\n        apwidth = config['_width_config']['mav']\n    if isinstance(apwidth,dict):\n        if 'width' in apwidth:\n            width = apwidth['width']\n            del apwidth['width']\n        else:\n            width = None\n    else:\n        width = None\n    if len(apmav) == 0:\n        return None\n    if len(apmav) > 7:\n        raise ValueError('Too many moving averages')\n    mavprices = []\n    for mav,mavwidth in zip(apmav,apwidth):\n        if period is None:\n            period = mav\n        if width is None:\n            width = mavwidth\n        mavprices.append(prices.rolling(period).mean())\n    for mavp,mav in zip(mavprices,apmav):\n        if mav == 'SMA':\n            ax.plot(xdates,mavp,color=config['_mav_config']['color'],linewidth=width)\n        elif mav == 'EMA':\n            ax.plot(xdates,mavp,color=config['_mav_config']['color'],linewidth=width)\n        elif mav == 'WMA':\n            ax.plot(xdates,mavp,color=config['_mav_config']['color'],linewidth=width)\n        elif mav == 'DEMA':\n            ax.plot(xdates,mavp,color=config['_mav_config']['color'],linewidth=width)\n        elif mav == 'TEMA':\n            ax", "CodeBleu_score": 34.98086715454526}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t   apwidth = config['_mav_config']['width']\n\n    # Check if apmav is a dictionary\n    if isinstance(apmav, dict):\n        # Extract shift values and periods\n        shift = apmav['shift']\n        period = apmav['period']\n    else:\n        # Treat as a list of periods\n        shift = [0] * len(apmav)\n        period = apmav\n\n    # Ensure at most 7 moving averages are plotted\n    if len(period) > 7:\n        raise ValueError('At most 7 moving averages are supported.')\n\n    # Calculate and plot moving averages\n    apmavprices = []\n    for i in range(len(period)):\n        # Calculate rolling mean\n        mav = prices.rolling(period[i]).mean()\n        # Apply shift\n        mav = mav.shift(shift[i])\n        # Plot moving average\n        ax.plot(xdates, mav, linewidth=apwidth, color=config['_mav_config']['color_cycle'][i], label='mav' + str(period[i]))\n        apmavprices.append(mav)\n\n    return apmavprices\n\n", "CodeBleu_score": 31.035067534177387}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    assert len(inputs) == 2\n    assert len(outputs) == 1\n    kernel_h, kernel_w = inputs[1].shape[2:]\n    num_input_channels = inputs[0].shape[1]\n    flops = np.prod(outputs[0].shape) * num_input_channels * kernel_h * kernel_w\n    if opnode.params[\\\"with_bias\\\"]:\n        flops += flops\n    return flops\n\n", "CodeBleu_score": 30.89509820565781}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    assert len(inputs) == 2\n    assert len(outputs) == 1\n    kh, kw = inputs[0].shape[2:]\n    n_input_channel = inputs[0].shape[1]\n    n_output_element = np.prod(outputs[0].shape)\n    flops = n_output_element * n_input_channel * kh * kw\n    if opnode.params[\\\"with_bias\\\"]:\n        flops += n_output_element * n_input_channel\n    return flops\n\n", "CodeBleu_score": 31.767678402134496}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    kernel_h, kernel_w = inputs[0].shape[-2:]\n    in_channels = inputs[0].shape[1]\n    out_channels = outputs[0].shape[1]\n    out_elements = np.prod(outputs[0].shape)\n    flops = out_elements * in_channels * kernel_h * kernel_w\n    if len(inputs) == 3:\n        flops += out_elements * out_channels\n    return flops\n\n", "CodeBleu_score": 26.234304018899007}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 20.149569944707952}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype),\n        \\\"mge_dtype should be a numpy.dtype, but get {}\\\".format(mge_dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 31.714310579040106}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 20.149569944707952}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.astype(dtype.canonicalize(x.dtype))\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n        ir_type = mge_dtype_to_ir_type(x.dtype)\n    if x.dtype == dtype.bfloat16:\n        x = x.astype(np.float32)\n        ir_type = mge_dtype_to_ir_type(x.dtype)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get_from_numpy(x, ir_type)\n    return [ir.ConstantOp(attr)]\n\n", "CodeBleu_score": 47.3675011798491}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.astype(dtype.canonicalize_dtype(x.dtype))\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    if x.dtype == dtype.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get_from_numpy(x)\n    return [ir.ConstantOp(attr)]\n\n", "CodeBleu_score": 38.79565397893892}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.astype(np.float32)\n    if x.dtype == np.bool_:\n        x = x.astype(np.int32)\n    if x.dtype == dtype.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get_from_numpy(x)\n    return [ir.ConstantOp(attr, mge_varinfo_to_ir_type_tuple(x))]\n\n", "CodeBleu_score": 36.67845794774051}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if val.strides[0] == 0 and val.size > 0:\n        zero_stride_axes = []\n        non_zero_stride_axes = []\n        for i, stride in enumerate(val.strides):\n            if stride == 0:\n                zero_stride_axes.append(i)\n            else:\n                non_zero_stride_axes.append(i)\n        if zero_stride_axes:\n            val = np.squeeze(val, axis=tuple(zero_stride_axes))\n        if canonicalize_types:\n            val = np.asarray(val, dtype.canonicalize_dtype(val.dtype))\n        return (\n            hlo.BroadcastOp(\n                val,\n                shape=val.shape,\n                broadcast_dimensions=tuple(non_zero_stride_axes),\n            ).result,\n        )\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 35.27063353264253}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if val.strides[0] == 0 and val.size > 0:\n        # collapse zero-stride axes\n        zero_stride_axes = []\n        non_zero_stride_axes = []\n        for i, stride in enumerate(val.strides):\n            if stride == 0:\n                zero_stride_axes.append(i)\n            else:\n                non_zero_stride_axes.append(i)\n        val = np.squeeze(val, axis=zero_stride_axes)\n        if canonicalize_types:\n            val = np.asarray(val, dtype.canonicalize_dtype(val.dtype))\n        return (hlo.BroadcastOp(val, non_zero_stride_axes).result,)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 30.127071006024984}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if val.strides == (0,) * val.ndim:\n        # No strides, so we can use a constant.\n        return _numpy_array_constant(val, canonicalize_types)\n\n    # Identify the zero-stride and non-zero-stride axes.\n    zero_stride_axes = []\n    non_zero_stride_axes = []\n    for i, stride in enumerate(val.strides):\n        if stride == 0:\n            zero_stride_axes.append(i)\n        else:\n            non_zero_stride_axes.append(i)\n\n    # Collapse the array along the zero-stride axes.\n    val = np.squeeze(val, axis=zero_stride_axes)\n    if canonicalize_types:\n        val = np.asarray(val, dtype.canonicalize_dtype(val.dtype))\n\n    # Create a broadcast operation for the collapsed array.\n    shape = val.shape\n    element_type = mge_dtype_to_ir_type(val.dtype)\n    attr = ir.DenseElementsAttr.get(val, type=element_type, shape=shape)\n    return (\n        hlo.BroadcastInDimOp(\n            hlo.ConstantOp(attr).result,\n            hlo.ConstantOp(non_zero_stride_axes).result,\n        ).result,\n    )\n\n", "CodeBleu_score": 37.2085559558809}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    nelems = len(xs)\n    xs = np.asarray(xs, np.bool_)\n    if nelems == 1:\n        xs = np.array(0 if xs.item() == 0 else 0xFF, np.uint8)\n    xs = np.packbits(xs, bitorder=\"little\")\n    return ir.DenseElementsAttr.get(xs, type=ir.IntegerType.get_signless(1), shape=(nelems,))\n\n", "CodeBleu_score": 56.60215317941929}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    nelems = len(xs)\n    if nelems == 1:\n        xs = np.array(0 if xs[0] else 0xFF, np.uint8)\n    else:\n        xs = np.packbits(xs, bitorder=\"little\")\n    return ir.DenseElementsAttr.get(xs, type=ir.IntegerType.get_signless(1), shape=(nelems,))\n\n", "CodeBleu_score": 47.139625847147194}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    xs = np.asarray(xs, np.bool_)\n    nelems = xs.size\n    xs = np.packbits(xs, bitorder=\\\"little\\\")\n    if nelems == 1:\n        xs = np.array(0 if xs.item() == 0 else 0xFF, np.uint8)\n    return ir.DenseElementsAttr.get(xs, type=ir.IntegerType.get_signless(1), shape=(nelems,))\n\n", "CodeBleu_score": 52.15371215295059}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1, f\"{len(args)}\"\n    assert len(ctx.vars_in) == 1, f\"{len(ctx.vars_in)}\"\n    assert len(ctx.vars_out) == 1, f\"{len(ctx.vars_out)}\"\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    assert in_var.data is not None or out_var.data is not None, \"Only tensor creation from constants is supported\"\n    if in_var.data is not None:\n        ctx.module_context.set_value(in_var, in_var.data)\n    if out_var.data is not None:\n        ctx.module_context.set_value(out_var, out_var.data)\n    assert in_var.shape == out_var.shape, f\"{in_var.shape} != {out_var.shape}\"\n    if in_var.data is not None:\n        data = np.array(in_var.data, out_var.data_type)\n    elif out_var.data is not None:\n        data = np.array(out_var.data, out_var.data_type)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n    return HLOTensor(data)\n\n", "CodeBleu_score": 58.11823466845176}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1, f\"{len(args)}\"\n    assert len(ctx.vars_in) == 1, f\"{len(ctx.vars_in)}\"\n    assert len(ctx.vars_out) == 1, f\"{len(ctx.vars_out)}\"\n\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n\n    if in_var.bound_data is not None:\n        ctx.module_context.set_value(in_var, in_var.bound_data)\n        ctx.module_context.set_value(out_var, in_var.bound_data)\n        assert in_var.shape == out_var.shape, f\"{in_var.shape} != {out_var.shape}\"\n        out_var.data = np.array(in_var.bound_data, out_var.data_type)\n    elif out_var.bound_data is not None:\n        ctx.module_context.set_value(out_var, out_var.bound_data)\n        assert out_var.shape == in_var.shape, f\"{out_var.shape} != {in_var.shape}\"\n        out_var.data = np.array(out_var.bound_data, out_var.data_type)\n    else:\n        assert False, \"only tensor creation from constants is supported\"\n\n    return HLOTensor(out_var.data)\n\n", "CodeBleu_score": 51.65304107529038}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1, f\"{len(args)}\"\n    assert len(ctx.vars_in) == 1, f\"{len(ctx.vars_in)}\"\n    assert len(ctx.vars_out) == 1, f\"{len(ctx.vars_out)}\"\n    assert args[0].data is not None, f\"{args[0].data}\"\n    _check_shape(ctx.vars_in[0].shape, ctx.vars_out[0].shape)\n    if args[0].data is not None:\n        ctx.module_context.set_value(ctx.vars_in[0], args[0].data)\n        ctx.module_context.set_value(ctx.vars_out[0], args[0].data)\n        return HLOTensor(args[0].data)\n    raise AssertionError(\"Only tensor creation from constants is supported\")\n\n", "CodeBleu_score": 34.5597631262881}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n\t\n    x = np.array(x, dtype=_scalar_type_to_dtype(x))\n    return _device_put_nparray(x, device)\n\n", "CodeBleu_score": 19.209332521573074}, {"code": "def _device_put_scalar(x, device)\n\t\n    x = np.array(x, dtype=_scalar_type_to_dtype[type(x)])\n    return _device_put_nparray(x, device)\n\n", "CodeBleu_score": 22.870659342124767}, {"code": "def _device_put_scalar(x, device)\n\t\n    x = np.array(x, dtype=canonicalize_arg(x, _python_scalar_dtypes))\n    return _device_put_nparray(x, device)\n\n", "CodeBleu_score": 17.514358341645874}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert all([isinstance(x, int) for x in lhs_shape])\n    assert all([isinstance(x, int) for x in rhs_shape])\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for (lhs, rhs) in zip(lhs_shape, rhs_shape):\n        if lhs != rhs:\n            return False\n    return True\n\n", "CodeBleu_score": 47.26548867634974}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if not lhs_shape and not rhs_shape:\n        return True\n    assert isinstance(lhs_shape[0], int)\n    assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for (lhs, rhs) in zip(lhs_shape, rhs_shape):\n        if lhs != rhs:\n            return False\n    return True\n\n", "CodeBleu_score": 44.58031096489851}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if not lhs_shape and not rhs_shape:\n        return True\n    assert isinstance(lhs_shape[0], int)\n    assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for (lhs_dim, rhs_dim) in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True\n\n", "CodeBleu_score": 42.13345903806742}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    test = pd.read_csv(test_set_filename, dtype=dtypes)\n    train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n\n    # remove target from meta_dtypes\n    del meta_dtypes[\\\"target\\\"]\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    # restore target\n    meta_dtypes[\\\"target\\\"] = \\\"int32\\\"\n    return train, test, train_meta, test_meta\n\n", "CodeBleu_score": 36.21496171537101}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    test = pd.read_csv(test_set_filename, dtype=dtypes)\n    train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n\n    # remove 'target' from test_meta dtypes\n\n    test_meta_dtypes = test_meta.dtypes.to_dict()\n    del test_meta_dtypes[\\\"target\\\"]\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=test_meta_dtypes)\n\n    return train, test, train_meta, test_meta\n\n", "CodeBleu_score": 35.834113023749985}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    test = pd.read_csv(test_set_filename, dtype=dtypes)\n    train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    # remove target column from test_meta\n    test_meta = test_meta.drop([\\\"target\\\"], axis=1)\n    # restore target column type\n    test_meta[\\\"target\\\"] = train_meta[\\\"target\\\"]\n    return train, test, train_meta, test_meta\n\n", "CodeBleu_score": 34.638137066937944}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n\t\n    from matplotlib import rcmod\n    axes_style = rcmod.axes_style\n    rcmod.axes_style = axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(8, 1))\n    fig.subplots_adjust(top=0.95, bottom=0.05, left=0.01, right=0.99)\n    ax.set_axis_off()\n    ax.imshow(np.array([[0, 1]]), cmap=cmap, aspect=\"auto\", extent=(0, 1, 0, 1))\n    rcmod.axes_style = axes_style\n\n", "CodeBleu_score": 32.73386475499799}, {"code": "def _show_cmap(cmap)\n\t\n    from matplotlib.axes import axes_style\n    with axes_style(\\\"white\\\"):\n        fig, ax = plt.subplots(figsize=(8, 1))\n        fig.subplots_adjust(bottom=0, top=1, left=0, right=1)\n        ax.set_axis_off()\n        ax.imshow(np.arange(256).reshape(1, -1), cmap=cmap, aspect=\\\"auto\\\")\n    return fig\n\n", "CodeBleu_score": 32.05270414812289}, {"code": "def _show_cmap(cmap)\n\t\n    from matplotlib.axes import axes_style\n    with axes_style('white'):\n        fig, ax = plt.subplots(figsize=(12, 1))\n        ax.set_axis_off()\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_yticks([])\n        ax.set_xticks([])\n        ax.pcolormesh(np.linspace(0, 1, 256), np.linspace(0, 1, 1), cmap(np.linspace(0, 1, 256)))\n    return fig, ax\n\n", "CodeBleu_score": 38.06935194764717}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    ax = obj.axes\n    if ax is None:\n        return xdata, ydata\n\n    transform = ax.transData\n    if transform != obj.get_transform():\n        xdata, ydata = transform.inverted().transform_points(obj.get_transform(), np.array([xdata, ydata]).T).T\n\n    return xdata, ydata\n\n", "CodeBleu_score": 50.01293973666964}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    ax = obj.axes\n    if ax is None:\n        return xdata, ydata\n    if ax.transData == ax.transData.transforms[-1]:\n        return xdata, ydata\n    return ax.transData.inverted().transform_points(ax.transData, np.vstack([xdata, ydata]).T).T\n\n", "CodeBleu_score": 32.31773097787977}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is None:\n        return xdata, ydata\n\n    if obj.axes.transData != obj.get_transform():\n        xdata, ydata = obj.axes.transData.inverted().transform_points(obj.get_transform(), np.vstack((xdata, ydata)).T).T\n    return xdata, ydata\n\n", "CodeBleu_score": 50.481544165317885}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None and not pd.api.types.is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 67.69055557411838}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None and not pd.api.types.is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 67.69055557411838}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None and not pd.api.types.is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 67.69055557411838}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n\t\n    # Small fix for programa and programar.\n\n    df.loc[df['lemma_lower'] == \\\"programa\\\", \\\"lemma_lower\\\"] = \\\"programar\\\"\n\n    words = df[df[\\\"is_alphabet\\\"] == True][\\\"text_lower\\\"].count()\n    print(\\\"Words:\\\", words)\n\n    unique_words = df[df[\\\"is_alphabet\\\"] == True][\\\"lemma_lower\\\"].nunique()\n    print(\\\"Unique words:\\\", unique_words)\n\n    # Filtering the dataframe to include only alphabetic tokens that are longer than one character and are not stop words.\n\n    words_df = df[df[\\\"is_alphabet\\\"] == True][df[\\\"text_lower\\\"] != \\\"\\\"]\n\n    words_df = words_df[words_df[\\\"text_lower\\\"].str.len() > 1]\n\n    words_df = words_df[words_df[\\\"is_stop_word\\\"] == False]\n\n\n    # Counting the occurrences of these words.\n\n    words_count = words_df[\\\"lemma_lower\\\"].value_counts()\n\n\n    # Selecting the top 20.\n\n    words_count = words_count.head(20)\n\n\n    # Creating a bar plot showing their frequency.\n\n    sns.barplot(x=words_count.values, y=words_count.index, color=\\\"#FF7000\\\", alpha=0.75)\n\n    plt.xlabel(\\\"Frecuencia\\\")\n\n    plt.ylabel(\\\"Palabra\\\")\n\n    plt.title(\\\"Palabras m\u00e1s usadas\\\")\n\n    plt.savefig(\\\"images/most_used_words.png\\\", dpi=300)\n\n    plt.show()\n\n\n", "CodeBleu_score": 22.329130721921715}, {"code": "def plot_most_used_words(df)\n\t\n    # Small fix for programa and programar.\n\n    df.loc[df['lemma_lower'] == \\\"programa\\\", \\\"lemma_lower\\\"] = \\\"programar\\\"\n\n\n    words = df[df[\\\"is_alphabet\\\"] == True][\\\"text_lower\\\"].count()\n    print(\\\"Words:\\\", words)\n\n    unique_words = df[df[\\\"is_alphabet\\\"] == True][\\\"lemma_lower\\\"].nunique()\n    print(\\\"Unique words:\\\", unique_words)\n\n    # Filter out stop words and words with length less than 2.\n    words = df[df[\\\"is_alphabet\\\"] == True][\\\"lemma_lower\\\"].value_counts()\n    words = words[words.index.str.len() > 1]\n    words = words[~words.index.isin(stopwords.words(\\\"spanish\\\"))]\n\n    # Get the top 20 words.\n    words = words.head(20)\n\n    # Create a bar plot.\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=words.values, y=words.index, palette=\\\"viridis\\\")\n    plt.title(\\\"Most used words\\\")\n    plt.xlabel(\\\"Frequency\\\")\n    plt.ylabel(\\\"Word\\\")\n    plt.tight_layout()\n    plt.savefig(\\\"most_used_words.png\\\")\n\n\n", "CodeBleu_score": 24.837297142754483}, {"code": "def plot_most_used_words(df)\n\t\n    # Small fix for programa and programar.\n    df.loc[df['lemma_lower'] == 'programa', 'lemma_lower'] = 'programar'\n\n    # Filter the dataframe to include only alphabetic tokens that are longer than one character and are not stop words.\n    df = df[(df['is_alphabet'] == True) & (df['text_lower'].str.len() > 1) & (~df['is_stop'])]\n\n    # Count the occurrences of these words.\n    word_counts = df['lemma_lower'].value_counts()\n\n    # Select the top 20 words.\n    top_words = word_counts.head(20)\n\n    # Create a bar plot showing their frequency.\n    plt.figure(figsize=(12, 7))\n    sns.barplot(x=top_words.values, y=top_words.index, palette='deep')\n    plt.title('Most Frequent Words', fontsize=20)\n    plt.xlabel('Frequency', fontsize=16)\n    plt.ylabel('Word', fontsize=16)\n    plt.savefig('most_frequent_words.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n", "CodeBleu_score": 29.58160258593053}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n\n    a = dtypes.promote_to_dtype(a, \"float64\")\n    depth = nputils.rolling_window_depth(window, a.shape[axis])\n    if min_count is None:\n        min_count = window\n    if min_count > window:\n        raise ValueError(\"min_count must be less than or equal to window\")\n    if min_count < 1:\n        raise ValueError(\"min_count must be greater than or equal to 1\")\n    if depth < 1:\n        raise ValueError(\"window is too large for the given array\")\n    ag = da.overlap.overlap(a, depth, boundary=\"reflect\")\n    out = da.map_blocks(\n        moving_func,\n        ag,\n        window,\n        min_count,\n        axis=axis,\n        boundary=\"reflect\",\n        dtype=a.dtype,\n    )\n    return da.overlap.trim_internal(out, depth, boundary=\"reflect\")\n\n", "CodeBleu_score": 32.845003866333016}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from dask.array.overlap import overlap, trim_internal\n    from dask.array.utils import assert_eq\n    from dask.array.core import normalize_axis\n    from", "CodeBleu_score": 9.025463870463279}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n    import numpy as np\n    if min_count is None:\n        min_count = window\n    dtype = dtypes.dtype_to_promote(a.dtype)\n    if dtype == np.dtype(object):\n        dtype = np.dtype(np.float64)\n    depth = nputils.get_depth(a, axis)\n    if depth == 0:\n        return moving_func(a, window, min_count=min_count)\n    if depth == 1:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 2:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 3:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 4:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 5:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 6:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 7:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 8:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 9:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 10:\n        return moving_func(a.squeeze(axis), window, min_count=min_count).squeeze(axis)\n    if depth == 11", "CodeBleu_score": 22.99843625271026}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    import dask.array as da\n    import numpy as np\n\n    lhs = da.asarray(lhs)\n    rhs = da.asarray(rhs)\n\n    if skipna:\n        if rhs.ndim == 1:\n            rhs = rhs.reshape(1, -1)\n        elif rhs.ndim == 2:\n            rhs = rhs.reshape(-1, 1)\n\n        poly = da.polynomial.Polynomial.fit(lhs, rhs, axis=0, deg=1, full=True)\n        coef, res, _, _ = poly\n        coef = coef.reshape(rhs.shape)\n        res = res.reshape(rhs.shape)\n    else:\n        coef = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        res = lhs.dot(coef) - rhs\n\n    return coef, res\n\n", "CodeBleu_score": 34.57737191475928}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    import dask.array as da\n    import numpy as np\n    from xarray.core.nputils import _maybe_promote_dtypes\n\n    if rhs.ndim == 1:\n        rhs = rhs.reshape(rhs.shape + (1,))\n\n    if skipna:\n        # This is a bit of a hack to get around the fact that\n        # np.polynomial.polynomial.polyfit doesn't support\n        # dask arrays.\n        #\n        # We'll fit a polynomial to the non-missing data, then\n        # use the coefficients to predict the missing values.\n        #\n        # This is a bit of a hack, but it seems to work well\n        # enough for now.\n        #\n        # TODO: Find a better way to do this.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO: This is a bit of a hack.  We should probably\n        # do a better job of handling missing values in the\n        # polynomial fit.\n        #\n        # TODO", "CodeBleu_score": 10.159475316682018}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    import dask.array as da\n    import numpy as np\n    from xarray.core.nputils import _maybe_promote_dtypes\n\n    lhs, rhs = _maybe_promote_dtypes(lhs, rhs)\n\n    if skipna:\n        # reshape rhs to 1D\n        rhs_reshaped = rhs.reshape((-1, rhs.shape[-1]))\n        # apply polynomial fit\n        coef, residuals = da.polyfit(lhs, rhs_reshaped, 1, rcond=rcond)\n        # extract coefficients and residuals\n        coef = coef[..., 0]\n        residuals = residuals[..., 0]\n    else:\n        # compute least squares solution\n        coef, residuals, _, _ = da.linalg.lstsq(lhs, rhs)\n\n    return coef, residuals\n\n", "CodeBleu_score": 30.226417610650806}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    from dask.array import from_array\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 63.14171671640242}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self._data):\n        array_func, array_args = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\\\"Method requires self.data to be a dask array.\\\")\n\n", "CodeBleu_score": 64.90554080106148}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self._data):\n        array_func, array_args = self._data.__dask_postpersist__()\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\\\"Method requires self.data to be a dask array.\\\")\n\n", "CodeBleu_score": 61.905851581291756}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self._data):\n        array_func, array_args = self._data.__dask_postpersist__()\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\\\"Method requires self.data to be a dask array.\\\")\n\n", "CodeBleu_score": 61.905851581291756}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n    if not dask_available:\n        dask_module = None\n\n    def f(*args, **kwargs):\n        if dask_module is not None and any(is_duck_dask_array(arg) for arg in args):\n            dask_module = import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 54.91039379370635}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n    dask_module = import_module(dask_module)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args + tuple(kwargs.values())):\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 53.55425369311691}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n    from xarray.core.utils import import_optional_dependency\n\n\n    dask_module = import_optional_dependency(dask_module)\n\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args + tuple(kwargs.values())):\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n\n\n    return f\n\n\n\n\n", "CodeBleu_score": 43.86509806924222}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if hasattr(arr1, \\\"__dask_token__\\\") and hasattr(arr2, \\\"__dask_token__\\\"):\n        return tokenize(arr1) == tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 58.35909336091505}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_available and isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        token1 = dask.base.tokenize(arr1)\n        token2 = dask.base.tokenize(arr2)\n        if token1 == token2:\n            return True\n        else:\n            return None\n    return None\n\n", "CodeBleu_score": 69.25420157077353}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_available and is_dask_array(arr1) and is_dask_array(arr2):\n        arr1 = arr1.dask.tokenize()\n        arr2 = arr2.dask.tokenize()\n        if arr1 == arr2:\n            return True\n        else:\n            return None\n    else:\n        return None\n", "CodeBleu_score": 78.74177510835939}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    if chunks is not None:\n        import dask.array as da\n        from dask.array.utils import assert_chunk\n        assert_chunk(chunks)\n        chunks = chunks\n    else:\n        import numpy as np\n        chunks = None\n    rng = np.random.default_rng(seed)\n    out = rng.standard_normal(shape)\n    if frac_nan is not None:\n        mask = rng.random(shape) < frac_nan\n        out[mask] = np.nan\n    if chunks is not None:\n        out = da.from_array(out, chunks=chunks)\n    return out\n\n", "CodeBleu_score": 47.48540442738678}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    if chunks is not None:\n        import dask.array as da\n\n        rng = np.random.default_rng(seed)\n        a = da.from_array(rng.standard_normal(shape), chunks=chunks)\n        if frac_nan is not None:\n            a[rng.random(shape) < frac_nan] = np.nan\n        return a\n    else:\n        rng = np.random.default_rng(seed)\n        a = rng.standard_normal(shape)\n        if frac_nan is not None:\n            a[rng.random(shape) < frac_nan] = np.nan\n        return a\n\n", "CodeBleu_score": 47.797148362317586}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    if chunks is not None:\n        import dask.array as da\n        rng = np.random.RandomState(seed)\n        arr = rng.randn(*shape)\n        if frac_nan is not None:\n            nan_indices = rng.choice(arr.size, int(frac_nan * arr.size), replace=False)\n            arr.ravel()[nan_indices] = np.nan\n        return da.from_array(arr, chunks=chunks)\n    else:\n        return np.random.RandomState(seed).randn(*shape)\n\n", "CodeBleu_score": 51.60386889321257}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 90.57611571255117}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 90.57611571255117}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 90.57611571255117}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.FINANCIALS_STATEMENTS:\n        raise ValueError(f\"{name} is not a valid financial statement\")\n    if timescale not in const.PERIODS:\n        raise ValueError(f\"{timescale} is not a valid period\")\n    try:\n        return self._create_financials_table(name, timescale)\n    except Exception as e:\n        logger.error(e)\n        return pd.DataFrame()\n\n", "CodeBleu_score": 24.9159318908264}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.FINANCIALS_TABLES:\n        raise ValueError(f'{name} is not a valid financial statement')\n    if timescale not in const.PERIODS:\n        raise ValueError(f'{timescale} is not a valid period')\n    try:\n        table = self._create_financials_table(name, timescale, proxy)\n        return table\n    except Exception as e:\n        utils.log_error(f'Error fetching {name} time series: {e}')\n        return pd.DataFrame()\n", "CodeBleu_score": 33.24053446548142}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.FINANCIALS_STATEMENTS:\n        raise ValueError(f'{name} is not a valid financial statement')\n    if timescale not in const.FINANCIALS_PERIODS:\n        raise ValueError(f'{timescale} is not a valid period')\n    try:\n        table = self._create_financials_table(name, timescale)\n    except Exception as e:\n        self._data.log.error(f'Error fetching {name} time series: {e}')\n        return pd.DataFrame()\n    return table\n\n", "CodeBleu_score": 29.358166217172748}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n\t\n    holders = data[\\\"ownershipList\\\"]\n    for owner in holders:\n        for k, v in owner.items():\n            owner[k] = self._parse_raw_values(v)\n        del owner[\\\"maxAge\\\"]\n    df = pd.DataFrame(holders)\n    if not df.empty:\n        df[\\\"reportDate\\\"] = pd.to_datetime(df[\\\"reportDate\\\"], unit=\\\"s\\\")\n        df.rename(columns={\\\"reportDate\\\": \\\"Date Reported\\\", \\\"organization\\\": \\\"Holder\\\", \\\"position\\\": \\\"Shares\\\", \\\"value\\\": \\\"Value\\\"}, inplace=True)\n    self._mutualfund = df\n\n", "CodeBleu_score": 53.52880406904639}, {"code": "def _parse_fund_ownership(self, data)\n\t\n    holders = data[\\\"ownershipList\\\"]\n    for owner in holders:\n        for k, v in owner.items():\n            owner[k] = self._parse_raw_values(v)\n        del owner[\\\"maxAge\\\"]\n    df = pd.DataFrame(holders)\n    if not df.empty:\n        df[\\\"reportDate\\\"] = pd.to_datetime(df[\\\"reportDate\\\"], unit=\\\"s\\\")\n        df.rename(columns={\\\"reportDate\\\": \\\"Date Reported\\\", \\\"organization\\\": \\\"Holder\\\", \\\"position\\\": \\\"Shares\\\", \\\"value\\\": \\\"Value\\\"}, inplace=True)  # \\\"pctHeld\\\": \\\"% Out\\\"\n    self._mutualfund = df\n\n", "CodeBleu_score": 51.4225325080881}, {"code": "def _parse_fund_ownership(self, data)\n\t\n    holders = data[\"ownershipList\"]\n    for owner in holders:\n        for k, v in owner.items():\n            owner[k] = self._parse_raw_values(v)\n        del owner[\"maxAge\"]\n    df = pd.DataFrame(holders)\n    if not df.empty:\n        df[\"reportDate\"] = pd.to_datetime(df[\"reportDate\"], unit=\"s\")\n        df.rename(\n            columns={\n                \"reportDate\": \"Date Reported\",\n                \"organization\": \"Holder\",\n                \"position\": \"Shares\",\n                \"value\": \"Value\",\n            },\n            inplace=True,\n        )  # \"pctHeld\": \"% Out\"\n    self._mutualfund = df\n\n", "CodeBleu_score": 71.38301606464611}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n\t\n    if self._currency is None:\n        self._currency = self._get_exchange_metadata()[\\\"currency\\\"]\n    return self._currency\n\n", "CodeBleu_score": 35.8772694074942}, {"code": "def currency(self)\n\t\n    if self._currency is None:\n        self._currency = self._get_exchange_metadata()[\\\"currency\\\"]\n    return self._currency\n\n", "CodeBleu_score": 35.8772694074942}, {"code": "def currency(self)\n\t\n    if self._currency is None:\n        self._currency = self._tkr.get_history_metadata(proxy=self.proxy)['currency']\n    return self._currency\n\n", "CodeBleu_score": 48.83764303409867}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        self._fetch_recommendations(self.proxy)\n\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    else:\n        try:\n            self._recommendations = pd.DataFrame(self._recommendations)\n\n\n", "CodeBleu_score": 28.086086505282537}, {"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        try:\n            self._recommendations = self._data.fetch(\n                \"recommendationTrend\",\n                self._symbol,\n                proxy=self.proxy,\n            )\n        except Exception as e:\n            if \"recommendationTrend\" in str(e):\n                self._recommendations = pd.DataFrame()\n            else:\n                raise\n\n    return self._recommendations\n\n", "CodeBleu_score": 41.462228177160966}, {"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        self._fetch_recommendations(self.proxy)\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    else:\n        try:\n            self._recommendations = pd.DataFrame(self._recommendations['trend'])\n        except Exception as e:\n            raise YFDataException(f\"Failed to parse recommendations for {self._symbol}: {e}\")\n    return self._recommendations\n\n", "CodeBleu_score": 43.23023609347434}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError('Classifier must have n_clusters attribute.')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, clf.n_clusters + 1)\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n\n    sse = []\n    cluster_times = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start_time = time.time()\n        clf.fit(X)\n        cluster_times.append(time.time() - start_time)\n        sse.append(clf.inertia_)\n\n    ax.plot(cluster_ranges, sse, 'b*-')\n    ax.set_xlabel('n_clusters', fontsize=text_fontsize)\n    ax.set_ylabel('SSE', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_times, 'r*-')\n        ax2.set_ylabel('Time (s)', fontsize=text_fontsize)\n        ax2.yaxis.label.set_color('r')\n        for tl in ax2.get_yticklabels():\n            tl.set_color('r')\n\n    return ax\n\n", "CodeBleu_score": 49.64396471471553}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError('Classifier does not have attribute n_clusters. '\n                         'Please set n_clusters manually.')\n    if cluster_ranges is None:\n        cluster_ranges = range(2, clf.n_clusters + 1)\n    if n_jobs == 1:\n        scores = [clf.fit(X).score(X) for n_clusters in cluster_ranges]\n    else:\n        scores = Parallel(n_jobs=n_jobs)(delayed(clf.fit)(X)\n                                         for n_clusters in cluster_ranges)\n        scores = [score.score(X) for score in scores]\n    if ax is None:\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_subplot(111)\n    ax.plot(cluster_ranges, scores, 'bo-', linewidth=2, markersize=8)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    if show_cluster_time:\n        clf_time = [clf.fit(X).cluster_time_ for n_clusters in cluster_ranges]\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, clf_time, 'ro-', linewidth=2, markersize=8)\n        ax2.set_ylabel('Clustering time (s)', fontsize=text_fontsize)\n        ax2.yaxis.label.set_color('r')\n        for tl in ax2.get_yticklabels():\n            tl.set_color('r')\n    return ax\n\n", "CodeBleu_score": 53.265065919493146}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError('The classifier does not have the attribute '\n                         'n_clusters. Please pass a classifier that has '\n                         'this attribute.')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, clf.n_clusters + 1)\n\n    if n_jobs == 1:\n        scores = [clf.fit(X, n_clusters=n_clusters).score(X)\n                  for n_clusters in cluster_ranges]\n    else:\n        scores = Parallel(n_jobs=n_jobs)(delayed(clf.fit)(X,\n                                                          n_clusters=n_clusters)\n                                         .score(X) for n_clusters in\n                                         cluster_ranges)\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'bo-')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if show_cluster_time:\n        cluster_times = [clf.fit(X, n_clusters=n_clusters).cluster_time_\n                         for n_clusters in cluster_ranges]\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_times, 'ro-')\n        ax2.set_ylabel('Clustering time (s)', fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 56.882544947081136}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf_ = clone(clf)\n    clf_.n_clusters = n_clusters\n    clf_.fit(X)\n    return clf_.score(X), time.time() - start_time\n\n\n", "CodeBleu_score": 34.249354564528794}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start = time.time()\n    clf_ = clone(clf)\n    clf_.n_clusters = n_clusters\n    clf_.fit(X)\n    return clf_.score(X), time.time() - start\n\n\n", "CodeBleu_score": 37.77899106594836}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    t = time.time()\n    cloned_clf = clone(clf)\n    cloned_clf.n_clusters = n_clusters\n    cloned_clf.fit(X)\n    return cloned_clf, time.time() - t\n\n\n\n\n\n\n", "CodeBleu_score": 32.3456044504769}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit'):\n        raise TypeError('{} does not have a fit method'.format(clf.__class__.__name__))\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError('{} does not have a fit_predict method'.format(clf.__class__.__name__))\n\n    def _plot_silhouette(self, X, cluster_labels, metric='euclidean', title='Silhouette Plot', ax=None):\n        return plot_silhouette(X, cluster_labels, metric=metric, title=title, ax=ax)\n\n    def _plot_elbow_curve(self, X, cluster_range=range(2, 10), metric='euclidean', title='Elbow Curve', ax=None):\n        return plot_elbow_curve(X, cluster_range=cluster_range, metric=metric, title=title, ax=ax)\n\n    for method in [_plot_silhouette, _plot_elbow_curve]:\n        if hasattr(clf, method.__name__):\n            warnings.warn('Overriding existing {} method'.format(method.__name__))\n        setattr(clf, method.__name__, types.MethodType(method, clf))\n\n    return clf", "CodeBleu_score": 35.21103838333486}, {"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit'):\n        raise TypeError('clf must have a fit method')\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError('clf must have a fit_predict method')\n\n    for method in [plot_silhouette, plot_elbow_curve]:\n        if hasattr(clf, method.__name__):\n            warnings.warn(\"The clusterer already has a method named '{}'.\".format(method.__name__))\n        setattr(clf, method.__name__, types.MethodType(method, clf))\n\n    return clf", "CodeBleu_score": 25.498982634059008}, {"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('The object passed as a clusterer is not a valid clusterer instance.')\n    if not hasattr(clf, 'plot_silhouette'):\n        setattr(clf, 'plot_silhouette', plot_silhouette)\n    if not hasattr(clf, 'plot_elbow_curve'):\n        setattr(clf, 'plot_elbow_curve', plot_elbow_curve)\n    return clf", "CodeBleu_score": 15.454498481951207}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    if scoring is None:\n        scoring = 'accuracy'\n\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y,\n                                                            train_sizes=train_sizes,\n                                                            cv=cv,\n                                                            n_jobs=n_jobs,\n                                                            scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Training examples', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color='r')\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color='g')\n    ax.plot(train_sizes, train_scores_mean, 'o-', color='r',\n            label='Training score')\n    ax.plot(train_sizes, test_scores_mean, 'o-', color='g',\n            label='Cross-validation score')\n\n    ax.legend(loc='best', fontsize=text_font", "CodeBleu_score": 68.62475113346518}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    \"\"\"Generates a plot of the learning curve for a classifier.\n\n    Args:\n        clf: Classifier instance that implements ``fit`` and ``predict_proba``\n            methods. The classifier must also have a ``score`` method.\n\n        X (array-like): The training input samples.\n\n        y (array-like): The target values.\n\n        title (string, optional): Title of the generated plot. Defaults to\n            \\\"Learning Curve\\\".\n\n        cv (int, cross-validation generator or an iterable, optional): Determines\n            the cross-validation splitting strategy. Possible inputs for cv are:\n\n\n            - None, to use the default 3-fold cross-validation,\n\n            - integer, to specify the number of folds in a `(Stratified)KFold`,\n\n            - :class:`sklearn.model_selection.BaseCrossValidator`,\n            - :class:`sklearn.model_selection.GridSearchCV`,\n            - :class:`sklearn.model_selection.RandomizedSearchCV`,\n            - an iterable yielding train/test splits.\n\n            For integer/None inputs, if ``y`` is binary or multiclass,\n            :class:`StratifiedKFold` used. If the estimator is not a classifier\n            or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n            Refer :ref:`User Guide <cross_validation>` for the various\n            cross-validators that can be used here.\n\n            Defaults to 3-fold cross-validation.\n\n        train_sizes (array-like, optional): Determines the training set sizes\n            used to generate the learning curve. If None, defaults to\n            np.linspace(0.1, 1.0, 5).\n\n        n_jobs (int, optional): Number of jobs to run in parallel. Defaults to 1.\n\n        scoring (string, callable or None, optional): A string (see model\n            evaluation documentation) or a scorer callable object / function with\n            signature ``scorer(estimator, X, y)``.", "CodeBleu_score": 8.823616261972283}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Training examples')\n    ax.set_ylabel('Score')\n    ax.tick_params(labelsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, train_sizes=train_sizes, n_jobs=n_jobs,\n        scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color='r')\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color='g')\n    ax.plot(train_sizes, train_scores_mean, 'o-', color='r',\n            label='Training score')\n    ax.plot(train_sizes, test_scores_mean, 'o-', color='g',\n            label='Cross-validation score')\n\n    ax.legend(loc='best', fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 71.90185539404544}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError('clf must have an n_clusters attribute')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    classifiers = [clone(clf) for i in cluster_ranges]\n\n    scores = []\n    for clf in classifiers:\n        clf.n_clusters = clf.n_clusters\n        clf.fit(X)\n        scores.append(abs(clf.score(X)))\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'o-')\n    ax.grid(True)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 64.11337171998133}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError('The provided classifier does not have an n_clusters attribute.')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    classifiers = []\n    for n_clusters in cluster_ranges:\n        classifiers.append(clone(clf).set_params(n_clusters=n_clusters))\n\n    scores = [abs(score(classifier, X)) for classifier in classifiers]\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'o-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 59.358785670425895}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError('clf must have an n_clusters attribute')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    classifiers = [clone(clf) for i in cluster_ranges]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'o-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 59.24111397940634}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'explained_variance_ratio_'):\n        raise TypeError('\\\"clf\\\" does not have explained_variance_ratio_ '\\\n                        'attribute. Has the PCA been fitted?')\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n\n    pca = PCA(n_components=2, random_state=0)\n    pca.fit(X)\n    X_2d = pca.transform(X)\n    ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap=cmap, s=20)\n\n    ax.set_xlabel('First principal component', fontsize=text_fontsize)\n    ax.set_ylabel('Second principal component', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 46.702466354458686}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'explained_variance_ratio_'):\n        raise TypeError('\\\"clf\\\" does not have explained_variance_ratio_ '\\\n                        'attribute. Has the PCA been fitted?')\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if hasattr(clf, 'components_'):\n        X_transformed = clf.transform(X)\n    else:\n        X_transformed = clf.fit_transform(X)\n\n    if len(X_transformed.shape) == 1:\n        X_transformed = X_transformed.reshape(-1, 1)\n\n    ax.scatter(X_transformed[:, 0], X_transformed[:, 1],\n               c=y, cmap=cmap, s=50)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 45.699791581640184}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'explained_variance_ratio_'):\n        raise TypeError('\\\"clf\\\" does not have explained_variance_ratio_ '\\\n                        'attribute. Has the PCA been fitted?')\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if hasattr(clf, 'n_components_'):\n        n_components = clf.n_components_\n    else:\n        n_components = 2\n\n    X_transformed = clf.transform(X)[:, :n_components]\n    ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap=cmap)\n    ax.grid(True)\n    ax.set_xlabel('First principal component', fontsize=text_fontsize)\n    ax.set_ylabel('Second principal component', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.legend(loc=\\\"best\\\", fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 45.0790674278087}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n    Y1 = np.random.uniform(0, 1, 16)\n    Y2 = np.random.uniform(0, 1, 16)\n    ax.bar(np.arange(16), Y1, color='gray', zorder=50, clip_on=True, clip_path=clip1)\n    ax.bar(np.arange(16), Y2, color='red', zorder=50, clip_on=True, clip_path=clip2)\n    ax.set_xlim(-1, 16)\n    ax.text(0.0, -0.08, 'Bar plot', color='k', ha='left', size=8, transform=ax.transAxes)\n    ax.text(1.0, -0.08, 'ax.bar()', color='blue', ha='right', size=8, transform=ax.transAxes, family=\"monospace\")\n\n", "CodeBleu_score": 38.96015722967777}, {"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.rand(16)\n    Y2 = np.random.rand(16)\n\n    ax.bar(np.arange(16), Y1, color='gray', edgecolor='k', clip_on=True, clip_path=clip1)\n    ax.bar(np.arange(16), Y2, color='red', edgecolor='k', clip_on=True, clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.0, -0.08, 'Bar plot',\n            color='k',\n            ha='left',\n            size=8,\n            transform=ax.transAxes)\n    ax.text(1.0, -0.08, 'ax.bar()',\n            color='blue',\n            ha='right',\n            size=8,\n            transform=ax.transAxes,\n            family=\"monospace\")\n\n", "CodeBleu_score": 42.93096644166954}, {"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.uniform(0, 10, 16)\n    Y2 = np.random.uniform(0, 10, 16)\n\n    ax.bar(np.arange(16), Y1, color='gray', width=0.5, align='edge', clip_on=True, clip_path=clip1)\n    ax.bar(np.arange(16), Y2, color='red', width=0.5, align='edge', clip_on=True, clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.0, -0.08, 'Bar plot',\n            color='k',\n            ha='left',\n            size=8,\n            transform=ax.transAxes)\n    ax.text(1.0, -0.08, 'ax.bar()',\n            color='blue',\n            ha='right',\n            size=8,\n            transform=ax.transAxes,\n            family=\"monospace\")\n\n", "CodeBleu_score": 44.166692185741894}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n\t\n    width = 11 / n_segment\n    padding = width / 4\n    X0 = np.arange(0, 11, width) + padding\n    X1 = X0 + width - padding\n    return X0, X1\n\n", "CodeBleu_score": 26.267924135842367}, {"code": "def split(n_segment)\n\t\n    n = n_segment\n    width = 11/n\n    padding = (11 - (n * width)) / (n + 1)\n    X0 = np.arange(n) * (width + padding)\n    X1 = X0 + width\n    return X0, X1\n\n", "CodeBleu_score": 36.948175239317884}, {"code": "def split(n_segment)\n\t\n    width = 11 / n_segment\n    padding = (11 - width * n_segment) / 2\n    X0 = np.arange(0, 11, width) + padding\n    X1 = X0 + width\n    return X0, X1\n\n", "CodeBleu_score": 29.338661027301544}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n\t\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, 'a', size=75, ha='center', va='center')\n    ax.set_axis_off()\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8).reshape(\n        canvas.get_width_height()[::-1] + (3,))\n    data = np.roll(data, 3, axis=-1)\n    return data\n\n", "CodeBleu_score": 48.41979357687423}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.axis('off')\n    ax.text(0.5, 0.5, 'a', fontsize=75, horizontalalignment='center', verticalalignment='center')\n    canvas.draw()\n    img = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8).reshape(dpi, dpi, 4)\n    img = np.roll(img, 3, axis=2)\n    return img\n\n", "CodeBleu_score": 51.98105666922955}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.axis('off')\n    ax.text(0.5, 0.5, 'a', size=75, ha='center', va='center')\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    data = data.reshape(dpi, dpi, 4)\n    data = np.roll(data, 3, axis=2)\n    return data\n\n", "CodeBleu_score": 53.8372221608109}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n\t\n    n = np.random.randint(1, 10)\n    widths = np.random.uniform(0.1, 1.0, n)\n    widths /= np.sum(widths)\n    scales = np.random.uniform(0.1, 1.0, n)\n    scales /= np.sum(scales)\n    centers = np.random.uniform(0.0, 1.0, n)\n    centers.sort()\n    curve = np.zeros(1000)\n    for width, scale, center in zip(widths, scales, centers):\n        curve += scale * np.exp(-(np.linspace(0.0, 1.0, 1000) - center)**2 / (2 * width**2))\n    return curve\n\n", "CodeBleu_score": 47.50814294618631}, {"code": "def curve()\n\t\n    N = 10000\n    x = np.linspace(0, 10, N)\n    y = np.zeros(N)\n    n = np.random.randint(1, 10)\n    for i in range(n):\n        c = np.random.uniform(0, 10)\n        w = np.random.uniform(0, 1)\n        s = np.random.uniform(0, 10)\n        y += s * np.exp(-((x - c) ** 2) / (2 * w ** 2))\n    return y\n\n", "CodeBleu_score": 29.017256173009574}, {"code": "def curve()\n\t\n    n = np.random.randint(1, 10)\n    x = np.linspace(0, 1, 1000)\n    y = np.zeros(1000)\n    for i in range(n):\n        c = np.random.uniform(0, 1)\n        w = np.random.uniform(0, 1)\n        s = np.random.uniform(0, 1)\n        y += c * np.exp(-((x - w) ** 2) / (2 * s ** 2))\n    return y\n\n", "CodeBleu_score": 27.37888304154228}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n\t\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.arange(-radius, radius + 1)\n    vandermonde = np.vander(points, increasing=True)\n    inverse = np.linalg.inv(vandermonde)\n    coefficients = inverse[0]\n    return coefficients / factorial(order - 1), points\n\n", "CodeBleu_score": 46.1006532168414}, {"code": "def difference(derivative, accuracy=1)\n\t\n    derivative += 1\n    radius = accuracy * (derivative - 1)\n    points = np.arange(-radius, radius + 1)\n    coefficients = np.linalg.inv(np.vander(points, increasing=True))[derivative - 1]\n    return coefficients / factorial(derivative - 1), points\n\n", "CodeBleu_score": 57.710360660296914}, {"code": "def difference(derivative, accuracy=1)\n\t\n    points = np.arange(-accuracy * derivative, accuracy * derivative + 1)\n    points = points[points != 0]\n    vandermonde = np.vander(points, increasing=True)\n    coeffs = np.linalg.inv(vandermonde)[0]\n    coeffs = coeffs / np.math.factorial(derivative - 1)\n    return coeffs, points\n\n", "CodeBleu_score": 40.34063398655454}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n\t\n    # Compute distances\n    D = np.diff(X)\n    D = np.sqrt(D**2 + np.diff(Y)**2)\n    D = np.hstack((D, 0))\n    D = np.cumsum(D)\n    # Interpolate\n    Xi = np.interp(T, D, X)\n    Yi = np.interp(T, D, Y)\n    return Xi, Yi, D[-1]\n\n", "CodeBleu_score": 32.53722842514822}, {"code": "def interpolate(X, Y, T)\n\t\n    D = np.sqrt((np.diff(X)**2 + np.diff(Y)**2))\n    D = np.hstack((0, np.cumsum(D)))\n    D_max = D[-1]\n    X_interpolated = np.interp(T, D, X)\n    Y_interpolated = np.interp(T, D, Y)\n    return X_interpolated, Y_interpolated, D_max\n\n", "CodeBleu_score": 31.138146275340183}, {"code": "def interpolate(X, Y, T)\n\t\n    D = np.sqrt((X[1:] - X[:-1])**2 + (Y[1:] - Y[:-1])**2)\n    D = np.concatenate(([0], np.cumsum(D)))\n    return (X[0] + T * (X[1:] - X[0]), Y[0] + T * (Y[1:] - Y[0]), D[-1])\n\n", "CodeBleu_score": 26.680980935807558}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    c = x + y[:, None] * 1j\n    z = np.zeros(c.shape, dtype=np.complex64)\n    divtime = maxiter + np.zeros(z.shape, dtype=int)\n    for i in range(maxiter):\n        z = z**2 + c\n        diverge = np.greater(abs(z), horizon)\n        div_now = np.greater(divtime, 0) * diverge\n        divtime[div_now] = i\n        z[diverge] = np.nan\n    return z, divtime\n\n", "CodeBleu_score": 45.85471765271683}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None]*1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 71.73444197840678}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter - 1] = 0\n    return Z, N\n\n", "CodeBleu_score": 76.36034174606606}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = max(int(sigma * 3), 3) * 2 + 1\n    A1 = smooth1d(A, window_len)\n    A2 = smooth1d(A1.T, window_len).T\n    return A2\n\n", "CodeBleu_score": 25.4878860950151}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = int(4 * sigma + 1)\n    window_len = window_len + 1 if window_len % 2 == 0 else window_len\n    A = np.array([smooth1d(row, window_len) for row in A])\n    A = A.T\n    A = np.array([smooth1d(row, window_len) for row in A])\n    A = A.T\n    return A\n\n", "CodeBleu_score": 33.47875712096284}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = max(int(sigma * 3), 3)\n    window_len = window_len + 1 - window_len % 2\n    A = [smooth1d(a, window_len) for a in A]\n    A = np.transpose(A)\n    A = [smooth1d(a, window_len) for a in A]\n    A = np.transpose(A)\n    return A\n\n", "CodeBleu_score": 37.34760615836204}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.offsets\n    return np.roll(np.roll(padded_src, int(oy / 72.0 * dpi), axis=0), int(ox / 72.0 * dpi), axis=1)\n\n", "CodeBleu_score": 47.4396670070814}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.offsets\n    tgt_image = np.roll(padded_src, int(ox / 72.0 * dpi), axis=0)\n    tgt_image = np.roll(tgt_image, int(oy / 72.0 * dpi), axis=1)\n    return tgt_image\n\n", "CodeBleu_score": 42.53771057304791}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.offsets\n    ny, nx, depth = padded_src.shape\n    ox = int(ox / 72.0 * dpi)\n        oy = int(oy / 72.0 * dpi)\n        tgt_image = np.roll(padded_src, (oy, ox), (0, 1))\n        return tgt_image\n\n", "CodeBleu_score": 27.394165798466325}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    if np.isscalar(shape):\n        shape = (shape, shape)\n    if np.isscalar(width):\n        width = (width, width)\n    if np.isscalar(center):\n        center = (center, center)\n    grid = np.mgrid[-shape[0] // 2:shape[0] // 2, -shape[1] // 2:shape[1] // 2]\n    sigma2 = np.multiply(width, width)\n    return np.exp(-np.sum(np.square(grid), 0) / (2 * sigma2))\n\n", "CodeBleu_score": 24.74527529505352}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    x, y = np.mgrid[0:shape[0], 0:shape[1]]\n    x = x - center[0]\n    y = y - center[1]\n    sigma2 = width ** 2\n    return np.exp(-(x ** 2 + y ** 2) / (2 * sigma2))\n\n", "CodeBleu_score": 10.14360791231739}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    shape = np.array(shape)\n    width = np.array(width)\n    center = np.array(center)\n    x, y = np.mgrid[0:shape[0], 0:shape[1]]\n    x = x - center[0]\n    y = y - center[1]\n    x = x ** 2 / width[0] ** 2\n    y = y ** 2 / width[1] ** 2\n    return np.exp(-(x + y))\n\n", "CodeBleu_score": 16.024304471336407}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n\t\n    x, y = position\n    x_mesh, y_mesh = np.meshgrid(np.linspace(-1, 1, 1024), np.linspace(-1, 1, 1024))\n    R = (x_mesh - x) ** 2 + (y_mesh - y) ** 2\n    return np.exp(-R / (2 * size ** 2)) * intensity\n\n", "CodeBleu_score": 17.212155277897104}, {"code": "def stimulus(position, size, intensity)\n\t\n    x, y = position\n    x = x / (size / 2) - 1\n    y = y / (size / 2) - 1\n    x = np.linspace(-1, 1, size)\n    y = np.linspace(-1, 1, size)\n    x, y = np.meshgrid(x, y)\n    R = np.sqrt(x ** 2 + y ** 2)\n    return np.exp(-R ** 2 / 2) * intensity\n\n", "CodeBleu_score": 24.548491133525918}, {"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0]\n    y = position[1]\n    # Normalize the coordinates\n    x_norm = x / (WIDTH - 1)\n    y_norm = y / (HEIGHT - 1)\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    X, Y = np.meshgrid(np.linspace(0, 1, WIDTH), np.linspace(0, 1, HEIGHT))\n    X = X - x_norm\n    Y = Y - y_norm\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = X ** 2 + Y ** 2\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / (2 * size ** 2)) * intensity\n", "CodeBleu_score": 17.965175894458923}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n\t\n    ax.set(xlim=(0, 1), ylim=(0, 1), xticks=np.linspace(0, 1, 5), yticks=np.linspace(0, 1, 5), xlabel='x', ylabel='y')\n    ax.text(0.5, 0.5, text, ha='center', va='center', size=20, bbox=dict(boxstyle='round', fc='w', ec='k', lw=1))\n    ax.set_title('title', fontfamily='serif', fontweight='bold')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 36.44226088836165}, {"code": "def plot(ax, text)\n\t\n    ax.set(xlim=(0, 1), xticks=np.arange(0, 1, 0.1), ylim=(0, 1), yticks=np.arange(0, 1, 0.1), xlabel='X', ylabel='Y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontfamily='Roboto Condensed', fontweight='light', fontsize=14)\n    ax.set_title('Title', fontfamily='Roboto Condensed', fontweight='light', fontsize=16)\n\n", "CodeBleu_score": 29.923278843608585}, {"code": "def plot(ax, text)\n\t\n    ax.set(xlim=(0, 1), ylim=(0, 1), xticks=np.linspace(0, 1, 5), yticks=np.linspace(0, 1, 5))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, transform=ax.transAxes, ha='center', va='center', fontsize=18, fontfamily='Roboto Condensed', fontweight='light')\n    ax.set_title('Title', fontfamily='Roboto Condensed', fontweight='light')\n\n", "CodeBleu_score": 41.10090459669264}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, step in enumerate(self.steps[:-1]):\n        Xt = step[1].transform(Xt)\n    return self.steps[-1][1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 45.759886338663726}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return self.steps[-1][1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 55.93117157420651}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, step in self.steps[:-1]:\n        Xt = step.transform(Xt)\n    return self.steps[-1][1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 50.67258443402216}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, _, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 80.08786605449052}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, _, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 80.08786605449052}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, _, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 80.08786605449052}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n\t\n    if results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_OPTIMAL:\n        return\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_PINF:\n        raise RuntimeError(\\\"Primal infeasibility detected.\\\")\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_DINF:\n        raise RuntimeError(\\\"Dual infeasibility detected.\\\")\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_MAXIT:\n        warnings.warn(\n            (f\\\"ECOS solver did not converge: {results[\\\"info\\\"][\\\"status\\\"]}\\\"),\n            category=ConvergenceWarning,\n            stacklevel=2,\n        )\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_NUMERICS:\n        raise RuntimeError(\\\"Numerical problems (unreliable search direction).\\\")\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_OUTCONE:\n        raise RuntimeError(\\\"Numerical problems (slacks or multipliers outside cone).\\\")\n    elif results[\\\"info\\\"][\\\"status\\\"] == self.EXIT_INACC_OFFSET:\n        warnings.warn(\n            (f\\\"ECOS solver did not converge: {results[\\\"info\\\"][\\\"status\\\"]}\\\"),\n            category=ConvergenceWarning,\n            stacklevel=2,\n        )\n    else:\n        raise RuntimeError(f\\\"ECOS solver failed: {results[\\\"info\\\"][\\\"status\\\"]}\\\")\n\n    def _decompose(self, P):\n        \\\"\\\"\\\"Decomposes P into a block diagonal matrix of positive definite matrices L and\n        a maximum eigenvalue.\\\"\\\"\\\"\n        L, max_eigval = linalg.schur(P, output=\\\"complex\\\")\n        if self.cond is not None:\n            max_eigval = max(max_eigval.real, self.cond)\n        return L, max_eigval\n\n", "CodeBleu_score": 35.93230615448144}, {"code": "def _check_success(self, results)\n\t\n    exit_flag = results[\\\"info\\\"][\\\"status\\\"]\n    if exit_flag == self.EXIT_OPTIMAL or exit_flag == self.EXIT_PINF or exit_flag == self.EXIT_DINF:\n        return\n    elif exit_flag == self.EXIT_MAXIT:\n        warnings.warn(\n                (f\\\"ECOS solver did not converge: {results.info.status}\\\"),\n                category=ConvergenceWarning,\n                stacklevel=2,\n            )\n    elif exit_flag == self.EXIT_NUMERICS or exit_flag == self.EXIT_OUTCONE:\n        raise RuntimeError(f\\\"ECOS solver failed: {results.info.status}\\\")\n    elif exit_flag >= self.EXIT_INACC_OFFSET:\n        raise RuntimeError(f\\\"ECOS solver failed: {results.info.status}\\\")\n\n\n    def _decompose(self, P):\n        \\\"\\\"\\\"Returns the decomposition of a symmetric matrix into a positive definite\n        matrix and its largest eigenvalue.\\\"\\\"\\\"\n        if self.cond is None:\n            return linalg.cholesky(P, lower=True), np.max(np.abs(np.linalg.eigvals(P)))\n        else:\n            return linalg.cholesky(P, lower=True, cond=self.cond), np.max(np.abs(np.linalg.eigvals(P)))\n\n\n    def _check_success(self, results):\n        \\\"\\\"\\\"The function _check_success evaluates the exit flag from solver results to determine the outcome of an optimization problem. If the exit flag indicates an optimal solution or an acceptable level of inaccuracy, the function completes without action. If the maximum iterations are reached, it issues a convergence warning. For exit flags indicating primal or dual infeasibility, it raises a RuntimeError. For any other unknown exit flags, it also raises a RuntimeError with the exit status.\\\"\\\"\\\"\n        exit_flag = results[\\\"info\\\"][\\\"status\\\"]\n        if exit_flag == self.EXIT_OPTIMAL or exit_flag ==", "CodeBleu_score": 36.336988426406336}, {"code": "def _check_success(self, results)\n\t\n    exit_flag = results[\\\"info\\\"][\\\"status\\\"]\n    if exit_flag == self.EXIT_OPTIMAL:\n        return\n    elif exit_flag == self.EXIT_PINF:\n        raise RuntimeError(\\\"Primal infeasibility detected.\\\")\n    elif exit_flag == self.EXIT_DINF:\n        raise RuntimeError(\\\"Dual infeasibility detected.\\\")\n    elif exit_flag == self.EXIT_MAXIT:\n        warnings.warn(\\\"ECOS solver did not converge: maximum number of iterations reached\\\", category=ConvergenceWarning, stacklevel=2)\n        return\n    elif exit_flag == self.EXIT_NUMERICS:\n        raise RuntimeError(\\\"ECOS solver failed: numerical problems (unreliable search direction)\\\")\n    elif exit_flag == self.EXIT_OUTCONE:\n        raise RuntimeError(\\\"ECOS solver failed: numerical problems (slacks or multipliers outside cone)\\\")\n    elif exit_flag >= self.EXIT_INACC_OFFSET:\n        raise RuntimeError(\\\"ECOS solver failed: unknown exit flag: {exit_flag}\\\")\n\n", "CodeBleu_score": 43.329236407775305}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n\t\n    eigvals, eigvecs = linalg.eigh(P)\n    largest_eigval = np.max(eigvals)\n    cond = self.cond or 100 * largest_eigval * P.shape[0] * np.finfo(P.dtype).eps\n    pos_cutoff = -np.finfo(P.dtype).eps * cond\n    neg_cutoff = -np.finfo(P.dtype).eps * cond * largest_eigval\n    eigvals = eigvals[eigvals > neg_cutoff]\n    eigvals = eigvals[eigvals < pos_cutoff]\n    eigvals /= largest_eigval\n    return eigvecs.T, largest_eigval\n\n", "CodeBleu_score": 25.135647660001908}, {"code": "def _decompose(self, P)\n\t\n    eigvals, eigvecs = linalg.eigh(P)\n    max_eigval = eigvals.max()\n    cond = max_eigval * P.shape[0] * np.finfo(np.float64).eps\n    cutoff_pos = -cond\n    cutoff_neg = -10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "CodeBleu_score": 11.592233086870468}, {"code": "def _decompose(self, P)\n\t\n    eigvals, eigvecs = linalg.eigh(P, eigvals=(0, P.shape[0] - 1))\n    # find largest eigenvalue\n    max_eigval = np.max(np.abs(eigvals))\n    # compute condition number\n    cond = max_eigval * P.shape[0] * np.finfo(P.dtype).eps\n    # filter out eigenvalues below cutoff\n    eigvals = eigvals[eigvals >= -self.cond]\n    # normalize eigenvalues\n    eigvals /= max_eigval\n    # compute decomposition\n    L = eigvecs.T @ np.diag(eigvals)\n    return L, max_eigval\n    return eigvals, eigvecs\n\n", "CodeBleu_score": 24.181284149793502}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n\t\n    # TODO: This is a hack to avoid a bug in scikit-learn 1.1.3. Remove this when the bug is fixed.\n    coef = coef.copy()\n    coef[np.abs(coef) < self.threshold] = 0\n    coef *= D\n    self.coef_ = coef\n\n    return coef\n\n", "CodeBleu_score": 17.076248387390415}, {"code": "def _update_coef(self, coef, D)\n\t\n    # TODO: This is a copy of the original method from the MinlipSurvivalAnalysis class.\n    #       It is not clear why the original method was copied.\n    #       It should be removed and the original method should be used.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       MinlipSurvivalAnalysis class.\n    #       The original method is not compatible with the current version of the\n    #       Minl", "CodeBleu_score": 4.388888888888888}, {"code": "def _update_coef(self, coef, D)\n\t\n    # TODO: This is a workaround for a bug in scikit-learn 1.2.0\n    # which is fixed in 1.2.1.\n    # Remove when scikit-learn 1.2.1 is released.\n    if np.all(coef == 0):\n        return coef\n\n    # TODO: This is a workaround for a bug in scikit-learn 1.2.0\n    # which is fixed in 1.2.1.\n    # Remove when scikit-learn 1.2.1 is released.\n    if np.all(D == 0):\n        return coef\n\n    # select coefficients that are greater than a small threshold\n    # and scale them by corresponding elements in D\n    coef = coef * D\n\n    # set small coefficients to 0\n    coef[np.abs(coef) < self.threshold] = 0\n\n    # store the updated coefficients in the model's coefficient attribute\n    self.coef_ = coef\n\n    return coef\n\n\n\n", "CodeBleu_score": 17.440612709480142}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    # TODO add support for sample weights\n    return coxph_negative_gradient(y_true[\"event\"].astype(np.uint8), y_true[\"time\"], raw_prediction.ravel())\n\n", "CodeBleu_score": 19.23455684722917}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    # TODO add support for sample weights\n    return coxph_negative_gradient(y_true[\"event\"].astype(np.uint8), y_true[\"time\"], raw_prediction.ravel())\n\n", "CodeBleu_score": 19.23455684722917}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    # TODO add support for sample weights\n    return coxph_negative_gradient(y_true[\"event\"].astype(np.uint8), y_true[\"time\"], raw_prediction.ravel())\n\n", "CodeBleu_score": 19.23455684722917}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values(X):\n        if np.all(np.isfinite(X)):\n            return None\n        else:\n            raise ValueError(\\\"%s does not support missing values\\\" % estimator_name)\n\n    if not issparse(X):\n        # check for overflow in sum of X\n        # see https://github.com/scipy/scipy/issues/13186\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037841979\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037843976\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037845389\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037846246\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037847483\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037849130\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037850630\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037851655\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037852972\n        # and https://github.com/scipy/scipy/issues/13186#issuecomment-1037853668\n       ", "CodeBleu_score": 16.783785434601565}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values(X):\n        if np.all(np.isfinite(X)):\n            return None\n        else:\n            raise ValueError(\n                \\\"%s does not support missing values in data X\\\" % estimator_name\n            )\n\n    # Check for NaN values in X\n    if np.any(np.isnan(X)):\n        # We need to check if the sum of X is finite or not\n        # If the sum is finite, then there are no missing values\n        # If the sum is infinite, then there are missing values\n        # If the sum is NaN, then there are missing values\n        X_sum = np.sum(X, axis=0, dtype=np.float64, keepdims=True)\n        with np.errstate(over=\\\"ignore\\\"):\n            X_sum_finite = np.isfinite(X_sum)\n\n        if np.all(X_sum_finite):\n            return None\n        else:\n            # We have a NaN in X_sum\n            # Check if all the elements in X_sum are NaN\n            if np.all(np.isnan(X_sum)):\n                # If all elements are NaN, then there are missing values\n                # in all features\n                return np.ones(X.shape[1], dtype=bool)\n            else:\n                # If there are some finite elements in X_sum, then there are\n                # missing values in some features\n                return ~X_sum_finite\n    else:\n        return None\n\n\n\n", "CodeBleu_score": 40.95937155962832}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values(X):\n        if np.all(np.isfinite(X)):\n            return None\n        else:\n            raise ValueError(\n                \\\"%s does not support missing values in data X\\\"\n                % estimator_name\n            )\n\n    # Compute the sum of X while ignoring overflow errors.\n    # The sum of X is finite if and only if X does not contain\n    # infinite values.\n    with np.errstate(over=\\\"ignore\\\"):\n        X_sum = X.sum(axis=0)\n\n    if np.all(np.isfinite(X_sum)):\n        return None\n\n    # If the sum is finite, but not NaN, then there are no missing\n    # values.\n    if not np.any(np.isnan(X_sum)):\n        return None\n\n    # Otherwise, there are missing values.\n    return np.isnan(X_sum)\n\n", "CodeBleu_score": 32.58590949005427}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n    if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        return pred[..., 1]\n\n    chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n    return _cum_hazard_to_surv(chf, self.unique_times_)\n\n", "CodeBleu_score": 67.98852275354658}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n    if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        arr = pred[..., 1]\n        if return_array:\n            return arr\n        return _array_to_step_function(self.unique_times_, arr)\n    else:\n        raise NotImplementedError(\"predict_survival_function is not implemented in low memory mode. Run fit with low_memory=False to disable low memory mode.\")\n\n", "CodeBleu_score": 85.36728403913824}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n    self._check_low_memory(\"predict_survival_function\")\n    check_is_fitted(self, \"tree_\")\n    X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n\n    pred = self.tree_.predict(X)\n    arr = pred[..., 1]\n    if return_array:\n        return arr\n    return _array_to_step_function(self.unique_times_, arr)\n\n", "CodeBleu_score": 94.90018669592166}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n\t\n    X, y = check_array_survival(X, y)\n        n_samples, n_features = X.shape\n        if n_samples < 2:\n            raise ValueError(\\\"Number of samples must be at least 2; got %d\\\" % n_samples)\n\n        # sort the data\n        order = y.argsort()\n        X = np.asfortranarray(X[order, :])\n        y = np.asfortranarray(y[order])\n        event = np.asfortranarray(event[order])\n\n        # center and scale the data\n        if self.normalize:\n            self.feature_means_ = X.mean(axis=0)\n            X -= self.feature_means_\n            self.scales_ = np.sqrt(np.sum(X ** 2, axis=0))\n            self.scales_[self.scales_ == 0] = 1.0\n            X /= self.scales_\n        else:\n            self.feature_means_ = np.zeros(n_features, dtype=X.dtype)\n            self.scales_ = np.ones(n_features, dtype=X.dtype)\n\n        return X, y, event, self.feature_means_, self.scales_\n\n    def _fit_baseline_models(self, X, y, event):\n        \"\"\"Fit baseline survival models for each alpha value.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values (survival times).\n\n        event : array-like of shape (n_samples,)\n            The censorship indicator.\n\n        Returns\n        -------\n        baseline_models : list of CoxPHFitter objects\n            The baseline survival models.\n        \"\"\"\n        baseline_models = []\n        for alpha in self.alphas_:\n            baseline_model", "CodeBleu_score": 19.253294803059152}, {"code": "def _pre_fit(self, X, y)\n\t\n    X, y = check_array_survival(X, y, ensure_min_samples=2, ensure_min_features=1)\n        y = y.astype(np.float64)\n        event = y[:, 0]\n        time = y[:, 1]\n\n        if self.normalize:\n            X_mean = X.mean(axis=0)\n            X_std = np.std(X, axis=0, ddof=1)\n            X_std[X_std == 0] = 1\n            X = (X - X_mean) / X_std\n        else:\n            X_mean = X.mean(axis=0)\n            X_std = np.ones(X.shape[1])\n\n        X = np.asfortranarray(X)\n        event = np.asfortranarray(event)\n        time = np.asfortranarray(time)\n        X_mean = np.asfortranarray(X_mean)\n        X_std = np.asfortranarray(X_std)\n\n        # sort by time in descending order\n        desc_order = np.argsort(time, kind=\\\"stable\\\")\n        event = event[desc_order]\n        time = time[desc_order]\n\n        return X, event, time, X_mean, X_std\n\n    def _fit(self, X, y):\n        \"\"\"The function _fit fits the Cox's proportional hazards model to the input data X and survival events y. It first preprocesses the data using the _pre_fit function. It then computes the regularization path using the call_fit_coxnet function, which returns the optimal alphas, the regularization path, and the number of features used. The function then fits the model to the regularization path, using the _fit_coxnet function. It returns the fitted model.", "CodeBleu_score": 20.739388470158175}, {"code": "def _pre_fit(self, X, y)\n\t\n    # Check that X and y have correct shape\n        X, y = check_X_y(X, y, accept_sparse=True)\n        # Convert to float64\n        X = X.astype(np.float64)\n        y = y.astype(np.float64)\n\n        # Check that X and y have the same number of samples\n        check_consistent_length(X, y)\n\n        # Check that X is 2D\n        if X.ndim != 2:\n            raise ValueError(\n                \\\"X must be 2D, got X.shape = %s\\\" % str(X.shape)\n            )\n\n        # Check that X has at least 2 samples\n        if X.shape[0] < 2:\n            raise ValueError(\n                \\\"X must have at least 2 samples, got X.shape = %s\\\" % str(X.shape)\n            )\n\n        # Check that y has at least 2 samples\n        if y.shape[0] < 2:\n            raise ValueError(\n                \\\"y must have at least 2 samples, got y.shape = %s\\\" % str(y.shape)\n            )\n\n        # Check that y is 1D\n        if y.ndim != 1:\n            raise ValueError(\n                \\\"y must be 1D, got y.shape = %s\\\" % str(y.shape)\n            )\n\n        # Check that y has the same number of samples as X\n        if y.shape[0] != X.shape[0]:\n            raise ValueError(\n                \\\"y must have the same number of samples as X, got X.shape = %s and y.shape = %s\\\"\n                % (str(X.shape), str(y.shape))\n            )\n\n        # Check that y is not all zero\n        if np.all(y == 0):\n            raise ValueError(\n                \\\"y must not be all zero, got y = %s\\\" % str(y)\n            )\n\n        # Check that y is not all one\n        if", "CodeBleu_score": 15.564035137885766}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        self.alphas = np.linspace(0, 1, self.n_alphas)\n        self.n_alphas = len(self.alphas)\n        self.alphas_ = self.alphas\n    else:\n        self.alphas = column_or_1d(self.alphas, warn=True)\n        if self.alphas.ndim != 1:\n            raise ValueError(\n                \\\"alphas must be a 1D array, but got \\\"\n                + str(self.alphas.shape)\n            )\n        assert_all_finite(self.alphas, input_name=\\\"alphas\\\")\n        check_non_negative(self.alphas, \\\"alphas\\\")\n        self.alphas_ = self.alphas\n        self.n_alphas = len(self.alphas_)\n    return self.alphas_, False\n\n", "CodeBleu_score": 36.36214483724018}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        self.alphas = np.linspace(0, self.alpha_max, self.n_alphas)\n        new_path = True\n    else:\n        self.alphas = np.array(self.alphas, copy=True).ravel()\n        new_path = False\n    if self.alphas.ndim != 1:\n        raise ValueError(\\\"alphas must be a 1D array.\\\")\n    if not np.all(np.isfinite(self.alphas)):\n        raise ValueError(\\\"alphas must be finite.\\\")\n    if not np.all(self.alphas >= 0):\n        raise ValueError(\\\"alphas must be non-negative.\\\")\n    return self.alphas, new_path\n\n\n", "CodeBleu_score": 37.56947613176536}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        if self.n_alphas <= 0:\n            raise ValueError(\\\"n_alphas must be a positive integer, got {self.n_alphas}\\\")\n        self.alphas = np.logspace(0, self.n_alphas - 1, num=self.n_alphas, base=10.0)\n        self.alphas = np.unique(self.alphas)\n        self.alphas = np.sort(self.alphas)[::-1]\n        self._alphas_changed = True\n    else:\n        self.alphas = column_or_1d(self.alphas, warn=True)\n        if self.alphas.ndim != 1:\n            raise ValueError(\\\"alphas must be a 1D array\\\")\n        if self.alphas.size == 0:\n            raise ValueError(\\\"alphas must be a non-empty array\\\")\n        assert_all_finite(self.alphas, input_name=\\\"alphas\\\")\n        check_non_negative(self.alphas, \\\"alphas\\\")\n        if self.alphas.size != self.n_alphas:\n            self._alphas_changed = True\n        else:\n            self._alphas_changed = False\n    return self.alphas, self._alphas_changed\n\n\n\n", "CodeBleu_score": 41.3515219601733}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time_enter, time_exit = check_y_survival(event, time_enter=None, time_exit=time, allow_all_censored=True)\n    check_consistent_length(event, time_enter, time_exit)\n    uniq_times, n_events, n_at_risk = _compute_counts(event, time_exit)\n    cum_hazard = np.cumsum(n_events / n_at_risk)\n    return uniq_times, cum_hazard\n\n\n", "CodeBleu_score": 34.103934693828755}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time_enter, time_exit = check_y_survival(event, time_enter=None, time_exit=time, allow_all_censored=True)\n    check_consistent_length(event, time_enter, time_exit)\n    uniq_times, n_events, n_at_risk = _compute_counts(event, time_exit)\n    cum_hazard = np.cumsum(n_events / n_at_risk)\n    return uniq_times, cum_hazard\n\n", "CodeBleu_score": 34.103934693828755}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time_enter, time_exit = check_y_survival(event, time_enter, time_exit, allow_all_censored=True)\n    check_consistent_length(event, time_enter, time_exit)\n    uniq_times, n_events, n_at_risk, n_censored = _compute_counts(event, time_exit)\n    ratio = np.divide(n_events, n_at_risk, out=np.zeros(uniq_times.shape[0], dtype=float), where=n_events != 0)\n    values = np.cumsum(ratio)\n    return uniq_times, values\n\n\n", "CodeBleu_score": 29.460614084896818}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n\t\n    event, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n    if np.all(event):\n        return np.ones(len(event))\n    _, prob_survival = kaplan_meier_estimator(event, time, reverse=True)\n    ind = np.searchsorted(prob_survival, time)\n    weights = 1 / prob_survival[ind]\n    weights[weights < 0] = 0\n    return weights\n\n\n", "CodeBleu_score": 40.42700761069318}, {"code": "def ipc_weights(event, time)\n\t\n    event, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n    if np.all(event):\n        return np.ones(len(event))\n    time_unique, km_survival = kaplan_meier_estimator(event, time, reverse=True)\n    event_time_index = np.searchsorted(time_unique, time)\n    survival_prob = km_survival[event_time_index]\n    survival_prob = np.maximum(survival_prob, 1e-10)\n    weights = 1 / survival_prob\n    return weights\n\n\n", "CodeBleu_score": 40.2841655353317}, {"code": "def ipc_weights(event, time)\n\t\n    event, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n    if np.all(event):\n        return np.ones(len(event))\n    km_survival = kaplan_meier_estimator(event, time, reverse=True)\n    time_values, survival_probabilities = km_survival[0], km_survival[1]\n    event_indices = np.searchsorted(time_values, time)\n    event_survival_probabilities = survival_probabilities[event_indices - 1]\n    event_survival_probabilities = np.maximum(event_survival_probabilities, 1e-15)\n    weights = 1 / event_survival_probabilities\n    return weights\n\n\n\n", "CodeBleu_score": 34.34723652677152}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        for i, df in enumerate(dfs):\n            if isinstance(df, pd.DataFrame):\n                dfs[i] = PandasConnector(df)\n            elif isinstance(df, BaseConnector):\n                pass\n            else:\n                raise ValueError(\n                    f\"Invalid data type: {type(df)}. Please provide a pandas dataframe, modin dataframe, polars dataframe, or connector object.\"\n                )\n    else:\n        raise ValueError(\n            f\"Invalid data type: {type(dfs)}. Please provide a pandas dataframe, modin dataframe, polars dataframe, or connector object.\"\n        )\n    return dfs\n\n", "CodeBleu_score": 29.136529213910116}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            df = PandasConnector(df)\n        elif isinstance(df, BaseConnector):\n            pass\n        else:\n            raise ValueError(\n                \"Invalid type of dataframe. Please provide a pandas or modin dataframe or a connector.\"\n            )\n\n    return dfs\n\n", "CodeBleu_score": 25.255656025163542}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        if not all(isinstance(df, (pd.DataFrame, BaseConnector)) for df in dfs):\n            raise ValueError(\n                \"Invalid data input. Please provide a single dataframe, a connector, or a list of dataframes or connectors.\"\n            )\n    else:\n        raise ValueError(\n            \"Invalid data input. Please provide a single dataframe, a connector, or a list of dataframes or connectors.\"\n        )\n\n    return [\n        PandasConnector(df) if isinstance(df, pd.DataFrame) else df for df in dfs\n    ]\n\n", "CodeBleu_score": 29.93390873029019}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n\t\n    if isinstance(self._instance, pd.DataFrame):\n        return self._instance\n\n    cached_path = self._cached(include_additional_filters=self._additional_filters is not None and len(self._additional_filters) > 0)\n    if cached_path is not None:\n        self._instance = pd.read_parquet(cached_path)\n        return self._instance\n\n    config = self.config.dict()\n    url = f\\\"{self._root_url}{config['base_id']}/{config['table']}\\\"\n    headers = {\\\"Authorization\\\": f\\\"Bearer {config['token']}\\\"}\n        if self._additional_filters:\n            url += \\\"?filterByFormula=\\\" + \\\"&\\\".join(self._additional_filters)\n\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            data = response.json()\n            records = data['records']\n            fields = [field['name'] for field in data['fields']]\n            self._instance = pd.DataFrame(records, columns=fields)\n            self._save_cache(self._instance)\n            return self._instance\n    else:\n            raise InvalidRequestError(\\\"Failed to fetch data from Airtable. Status code: {response.status_code}, message: {response.text}\\\")\n\n    @cached_property\n    def columns(self):\n        \\\"\\\"\\\"\n        Returns the columns of the data.\n\n        Returns :\n            list : The columns of the data.\n        \\\"\\\"\\\"\n        if self._instance is None:\n            self.execute()\n\n        return list(self._instance.columns)\n\n    @cached_property\n    def rows(self):\n        \\\"\\\"\\\"\n        Returns the rows of the data.\n\n        Returns :\n            list : The rows of the data.\n        \\\"\\\"\\\"\n        if self._instance is None:\n            self.execute()\n\n        return list(self._instance.index)", "CodeBleu_score": 44.633263814652025}, {"code": "def execute(self) -> pd.DataFrame\n\t\n    if self._instance is not None:\n            return self._instance\n\n        cached_data_path = self._cached()\n        if cached_data_path is not None:\n            return pd.read_parquet(cached_data_path)\n\n        if self._additional_filters is not None and len(self._additional_filters) > 0:\n            cached_data_path = self._cached(include_additional_filters=True)\n            if cached_data_path is not None:\n                return pd.read_parquet(cached_data_path)\n\n        # Fetch data from Airtable\n        url = f\\\"{self._root_url}{self.config.base_id}/{self.config.table}\\\"\n        headers = {\n            \\\"Authorization\\\": f\\\"Bearer {self.config.token}\\\",\n            \\\"Content-Type\\\": \\\"application/json\\\",\n        }\n        params = {\n            \\\"maxRecords\\\": 1000,\n            \\\"view\\\": \\\"Grid view\\\",\n            \\\"fields\\\": self.config.fields,\n        }\n\n        if self._additional_filters is not None and len(self._additional_filters) > 0:\n            params[\\\"filterByFormula\\\"] = \\\"AND(\\\"\n                + \\\",\\\".join(self._additional_filters)\n                + \\\")\\\"\n\n        response = requests.get(url, headers=headers, params=params)\n        if response.status_code == 200:\n            data = response.json()[\\\"records\\\"]\n            df = pd.DataFrame(data)\n            df = df.drop(columns=[\\\"id\\\", \\\"createdTime\\\"])\n            self._instance = df\n            self._save_cache(df)\n            return df\n        else:\n            raise InvalidRequestError(\n                f\\\"Failed to fetch data from Airtable.\n                Status code: {response.status_code},\n                message: {response.text}\\\"\n            )\n\n    def _get_columns(", "CodeBleu_score": 40.2281561509134}, {"code": "def execute(self) -> pd.DataFrame\n\t\n    if self._cached() is not None:\n            return pd.read_parquet(self._cached())\n        elif self._cached(include_additional_filters=True) is not None:\n            return pd.read_parquet(self._cached(include_additional_filters=True))\n        elif isinstance(self._instance, pd.DataFrame):\n            return self._instance\n        else:\n            self._instance = self._fetch_data()\n            self._save_cache(self._instance)\n            return self._instance\n\n", "CodeBleu_score": 60.32237862723102}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n\t\n    if not isinstance(self._instance, pd.DataFrame):\n        self._instance = self.execute()\n    return hashlib.sha256(self._instance.columns.astype(str).str.cat(sep=\\\"|\\\").encode()).hexdigest()\n\n", "CodeBleu_score": 49.925002913644015}, {"code": "def column_hash(self)\n\t\n    if isinstance(self._instance, pd.DataFrame):\n        df = self._instance\n    else:\n        df = self.execute()\n    columns = \\\"|\\\".join(df.columns)\n    formula = self.config.formula\n    return hashlib.sha256(f\\\"{columns}|{formula}\\\".encode()).hexdigest()\n\n", "CodeBleu_score": 42.23744204458825}, {"code": "def column_hash(self)\n\t\n    if not isinstance(self._instance, pd.DataFrame):\n        self._instance = self.execute()\n    column_names = self._instance.columns.tolist()\n    column_names.sort()\n    column_names_str = '|'.join(column_names)\n    formula = self.config.formula\n    if formula is not None:\n        column_names_str += f'|{formula}'\n    return hashlib.sha256(column_names_str.encode('utf-8')).hexdigest()\n\n", "CodeBleu_score": 58.6000255112032}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.DataFrame):\n        self.pandas_df = df\n    elif isinstance(df, pd.Series):\n        self.pandas_df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self.pandas_df = pd.read_csv(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        self.pandas_df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n\n", "CodeBleu_score": 44.26599834400486}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.DataFrame):\n        self.pandas_df = df\n    elif isinstance(df, pd.Series):\n        self.pandas_df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self.pandas_df = pd.read_csv(df)\n    elif isinstance(df, list):\n        self.pandas_df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.pandas_df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n\n", "CodeBleu_score": 44.617260436922145}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.DataFrame):\n        self.pandas_df = df\n    elif isinstance(df, pd.Series):\n        self.pandas_df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self.pandas_df = pd.read_csv(df)\n    elif isinstance(df, list):\n        self.pandas_df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.pandas_df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n\n", "CodeBleu_score": 44.617260436922145}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if n >= len(self.df):\n        return self.df.sample(frac=1).head(n)\n    else:\n        sample_df = pd.DataFrame()\n        for col in self.df.columns:\n            if self.df[col].dtype == object:\n                sample_df[col] = self.df[col].apply(lambda x: x[:n])\n            elif self.df[col].dtype == int:\n                sample_df[col] = self.df[col].sample(n, random_state=42)\n            elif self.df[col].dtype == float:\n                sample_df[col] = self.df[col].sample(n, random_state=42)\n            elif self.df[col].dtype == bool:\n                sample_df[col] = self.df[col].sample(n, random_state=42)\n            elif self.df[col].dtype == np.dtype('datetime64[ns]'):\n                sample_df[col] = self.df[col].sample(n, random_state=42)\n            else:\n                sample_df[col] = self.df[col].sample(n, random_state=42)\n        return Anonymizer(sample_df).anonymize().head(n)\n\n", "CodeBleu_score": 32.24706745968934}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if n > len(self.df):\n        return self.df.sample(frac=1).head(n)\n\n    # Sample each column individually using a custom sampling method\n    samples = {}\n    for col in self.df.columns:\n        if self.df[col].dtype == object:\n            samples[col] = self.df[col].sample(n, random_state=42).tolist()\n        elif self.df[col].dtype == np.dtype('int64'):\n            samples[col] = self.df[col].sample(n, random_state=42).tolist()\n        elif self.df[col].dtype == np.dtype('float64'):\n            samples[col] = self.df[col].sample(n, random_state=42).tolist()\n        elif self.df[col].dtype == np.dtype('datetime64[ns]'):\n            samples[col] = self.df[col].sample(n, random_state=42).tolist()\n        else:\n            samples[col] = self.df[col].sample(n, random_state=42).tolist()\n\n    # Combine the samples into a new DataFrame\n    sampled_df = pd.DataFrame(samples)\n\n    # Anonymize the first few rows of the sampled DataFrame\n    anonymizer = Anonymizer()\n    anonymized_df = anonymizer.anonymize(sampled_df.head(n))\n\n    return anonymized_df\n\n", "CodeBleu_score": 33.28973332398068}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if len(self.df) < n:\n        return self.df.sample(frac=1).head(n)\n    else:\n        # Sample each column individually\n        samples = {col: self.sample_column(col, n) for col in self.df.columns}\n        # Combine the samples into a new DataFrame\n        sample_df = pd.DataFrame(samples)\n        # Anonymize the first few rows of the sampled DataFrame\n        sample_df = Anonymizer.anonymize(sample_df.head(n))\n        return sample_df\n\n", "CodeBleu_score": 47.11072752451254}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    return f\"\"\"<dataframe name=\"{df.name}\" description=\"{df.description}\">", "CodeBleu_score": 17.37225372579445}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    if extras is None:\n        extras = {}\n    name = extras.get(\"name\", df.name)\n    description = extras.get(\"description\", \"\")\n    if name is None:\n        name = \"\"\n    if description is None:\n        description = \"\"\n    return f\"\"\"<DataFrame name=\"{name}\" description=\"{description}\">", "CodeBleu_score": 33.15849879838218}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    if extras is None:\n        extras = {}\n    name = extras.get(\"name\", \"\")\n    description = extras.get(\"description\", \"\")\n    df_name = df.name if df.name else \"DataFrame\"\n    df_description = df.description if df.description else \"\"\n    df_dimensions = f\"{df.shape[0]} rows x {df.shape[1]} columns\"\n    df_content = df.to_csv(index=False, header=True)\n    df_content = df_content.replace('\"', '\"\"')\n    df_content = df_content.replace(\"", "CodeBleu_score": 34.11823666464782}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n\t\n\n    \"\"\"\n    Convert df to sql connector like format where sql connector is wrapped inside <dataframe></dataframe>\n    Args:\n        df (pd.DataFrame): PandasAI dataframe or dataframe\n        extras (dict, optional): expect index to exists\n\n    Returns:\n        str: dataframe stringify\n    \"\"\"\n    dataframe_info = \"<dataframe\"\n\n    # Add name attribute if available\n    if df.name is not None:\n        dataframe_info += f' name=\"{df.name}\"'\n\n    # Add description attribute if available\n    if df.description is not None:\n        dataframe_info += f' description=\"{df.description}\"'\n\n    dataframe_info += \">\"\n\n    # Add dataframe details\n    dataframe_info += f\"", "CodeBleu_score": 26.31216114150159}, {"code": "def convert_df_sql_connector_to_str\n\t\n    dataframe_info = \"<dataframe\"\n\n    # Add name attribute if available\n    if df.name is not None:\n        dataframe_info += f' name=\"{df.name}\"'\n\n    # Add description attribute if available\n    if df.description is not None:\n        dataframe_info += f' description=\"{df.description}\"'\n\n    dataframe_info += \">\"\n\n    # Add dataframe details\n    dataframe_info += f\"", "CodeBleu_score": 26.20763938454761}, {"code": "def convert_df_sql_connector_to_str\n\t\n    \"\"\"\n    Convert df to csv like format where csv is wrapped inside <dataframe></dataframe>\n    Args:\n        df (pd.DataFrame): PandasAI dataframe or dataframe\n        extras (dict, optional): expect index to exists\n\n    Returns:\n        str: dataframe stringify\n    \"\"\"\n    dataframe_info = \"<dataframe\"\n\n    # Add name attribute if available\n    if df.name is not None:\n        dataframe_info += f' name=\"{df.name}\"'\n\n    # Add description attribute if available\n    if df.description is not None:\n        dataframe_info += f' description=\"{df.description}\"'\n\n    dataframe_info += \">\"\n\n    # Add dataframe details\n    dataframe_info += f\"", "CodeBleu_score": 26.31216114150159}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_str = self.convert_df_to_json_str(df, extras)\n    yaml_str = yaml.dump(json.loads(json_str), allow_unicode=True)\n    if extras.get(\"is_direct_sql\") and extras[\"is_direct_sql\"]:\n        return f\"<table>{yaml_str}</table>\"\n    return yaml_str\n\n", "CodeBleu_score": 57.052546847975904}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_data = self.convert_df_to_json(df, extras)\n    yaml_str = yaml.dump(json_data)\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yaml_str}</table>\"\n    return yaml_str\n\n", "CodeBleu_score": 51.54379233331764}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_str = self.convert_df_to_json(df, extras)\n    yml_str = yaml.dump(json_str)\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yml_str}</table>\"\n    return yml_str\n\n", "CodeBleu_score": 56.384007909431986}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n    if expected_type == \"number\":\n        return isinstance(self, (int, float))\n    if expected_type == \"string\":\n        return isinstance(self, str)\n    if expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    if expected_type == \"plot\":\n        return isinstance(self, str) and re.match(r\"^(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\)?(?:.*\\\\", "CodeBleu_score": 54.67321894441564}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n    if expected_type == \"number\":\n        return isinstance(self, (int, float))\n    if expected_type == \"string\":\n        return isinstance(self, str)\n    if expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    if expected_type == \"plot\":\n        return isinstance(self, (str, dict))\n    return False\n\n", "CodeBleu_score": 61.26681325064149}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n    if expected_type == \"number\":\n        return isinstance(self, (int, float))\n    if expected_type == \"string\":\n        return isinstance(self, str)\n    if expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    if expected_type == \"plot\":\n        if isinstance(self, str):\n            return re.match(r\"^.+\\.(png|jpg|jpeg)$\", self) is not None\n        if isinstance(self, dict):\n            return all(isinstance(key, str) and isinstance(value, str) for key, value in self.items())\n    raise InvalidOutputValueMismatch(f\"Unsupported expected type: {expected_type}\")\n\n", "CodeBleu_score": 69.2804142181328}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n            self.distance_params = {}\n        if isinstance(self.distance, str):\n            return pairwise_distance(X, X2, metric=self.distance, metric_params=self.distance_params)\n        else:\n            if X2 is None:\n                if len(signature(self.distance.pairwise).parameters) == 2:\n                    return self.distance.pairwise(X, **self.distance_params)\n                else:\n                    return self.distance(X, **self.distance_params)\n            else:\n                return self.distance.pairwise(X, X2, **self.distance_params)\n\n\n\n\n def _one_element_distance_npdist(self, x, y):\n    \"\"\"The function _one_element_distance_npdist calculates the distance between two 1D arrays x and y using a specified distance metric and its parameters. If the distance metric is a string, it uses the one_element_distance function from sktime.distances. Otherwise, it directly calls the distance function with the given arrays and parameters. The function returns the computed distance.", "CodeBleu_score": 29.669893624864763}, {"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n            self.distance_params = {}\n        if isinstance(self.distance, str):\n            return pairwise_distance(X, X2, metric=self.distance, metric_params=self.distance_params)\n        else:\n            if X2 is None:\n                sig = signature(self.distance.distance)\n                if len(sig.parameters) == 2:\n                    return self.distance.distance(X)\n                else:\n                    return self.distance.distance(X, X2, self.distance_params)\n            else:\n                return self.distance.distance(X, X2, self.distance_params)\n\n\n\n\n def _one_element_distance_npdist(self, x, y):\n    \"\"\"The function _one_element_distance_npdist calculates the distance between two 1D numpy arrays using a specified distance metric and its parameters. It first converts the input arrays to numpy arrays using the convert function from sktime.datatypes. It then checks if the distance metric is a string and if so, uses the numba_distance function from sktime.distances. If the distance metric is not a string, it directly calls the distance function with the given datasets and parameters. The function returns the computed distance.", "CodeBleu_score": 33.06776805622717}, {"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n            self.distance_params = {}\n        if isinstance(self.distance, str):\n            return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n        else:\n            if X2 is None:\n                sig = signature(self.distance.pairwise)\n                if len(sig.parameters) == 1:\n                    return self.distance.pairwise(X, **self.distance_params)\n                else:\n                    return self.distance.pairwise(X)\n            else:\n                return self.distance.pairwise(X, X2, **self.distance_params)\n\n\n\n\n def _convert_X_to_sklearn(self, X):\n    \"\"\"The function _convert_X_to_sklearn converts the input data X into a format that can be used by a scikit-learn-compatible estimator. It first converts the input data X to a numpy array using the convert function from sktime.datatypes. It then checks if the input data is of equal length and whether it has more than one variable. If the input data is of equal length and has more than one variable, the function concatenates the data along the first axis to form a 2D array. If the input data is not of equal length or has only one variable, the function reshapes the data to a 2D array with the first dimension corresponding to the length of the time series. The function then returns the converted data.", "CodeBleu_score": 30.504989409467285}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.is_equal_length:\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy()\n        elif isinstance(X, np.ndarray):\n            if X.ndim == 2:\n                pass\n            elif X.ndim == 3:\n                X = X.reshape((X.shape[0], -1))\n            else:\n                raise ValueError(f\"X must be 2D or 3D, but has shape {X.shape}\")\n        else:\n            raise ValueError(f\"X must be a pandas DataFrame, numpy array, or list of numpy arrays, but is of type {type(X)}\")\n    else:\n        if isinstance(X, pd.DataFrame):\n            X = X.reset_index(drop=True)\n            X = X.pivot(index=\"index\", columns=\"level_1\", values=\"values\")\n            X = X.fillna(0)\n            X = X.to_numpy()\n        elif isinstance(X, np.ndarray):\n            if X.ndim == 2:\n                pass\n            elif X.ndim == 3:\n                X = X.reshape((X.shape[0], -1))\n            else:\n                raise ValueError(f\"X must be 2D or 3D, but has shape {X.shape}\")\n        else:\n            raise ValueError(f\"X must be a pandas DataFrame, numpy array, or list of numpy arrays, but is of type {type(X)}\")\n        X = np.concatenate([np.reshape(x, (1, -1)) for x in X], axis=0)\n        X = np.concatenate([np.reshape(np.array(len(x)), (1, 1)) for x in X], axis=0)\n    return X\n\n", "CodeBleu_score": 34.35206164667719}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, pd.Series):\n        X = X.values.reshape(-1, 1)\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        if X.ndim == 2 and self.is_equal_length:\n            return X.reshape(-1, self.n_vars)\n        if X.ndim == 2 and not self.is_equal_length:\n            X = pd.DataFrame(X)\n    if isinstance(X, pd.DataFrame):\n        X = X.reset_index(drop=True)\n        X = X.pivot(index=\"index\", columns=\"variable\", values=\"value\")\n        X = X.fillna(0)\n        X = X.values\n    if self.is_equal_length:\n        X = X.reshape(-1, self.n_vars)\n    else:\n        X_len = np.apply_along_axis(len, 1, X)\n        X = np.concatenate((X_len.reshape(-1, 1), X), axis=1)\n    return X\n\n", "CodeBleu_score": 37.935523148595415}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.is_equal_length:\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n        elif isinstance(X, np.ndarray):\n            if X.ndim == 1:\n                X = X.reshape(-1, 1)\n        else:\n            raise TypeError(f\"X must be a pandas DataFrame, numpy array, or 1D numpy array, but got {type(X)}\")\n        return X\n    else:\n        if isinstance(X, pd.DataFrame):\n            X = X.reset_index()\n            X = X.pivot(index='index', columns='level_1', values='level_2')\n            X = X.fillna(0)\n            X = X.values\n        elif isinstance(X, np.ndarray):\n            if X.ndim == 1:\n                X = X.reshape(-1, 1)\n        else:\n            raise TypeError(f\"X must be a pandas DataFrame, numpy array, or 1D numpy array, but got {type(X)}\")\n        X_len = np.array([len(x) for x in X])\n        X_len = np.reshape(X_len, (-1, 1))\n        X = np.concatenate((X_len, X), axis=1)\n        return X\n", "CodeBleu_score": 35.497395820554445}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 14.940245748602448}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 14.940245748602448}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 14.940245748602448}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(self.regressor, [*self.transformers, other])\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 17.885890750903897}, {"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        new_transformers = self._transformers + [(None, other)]\n        new_transformers = TransformerPipeline(new_transformers)\n        return SklearnRegressorPipeline(self.regressor_, new_transformers)\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 37.54094656307505}, {"code": "def __rmul__(self, other)\n\t\n    from sktime.transformations.compose import TransformerPipeline\n\n    if isinstance(other, BaseTransformer):\n        # create a TransformerPipeline by multiplying the other object with the current object's transformers\n        transformer_pipeline = TransformerPipeline(self._transformers) * other\n        # create a new SklearnRegressorPipeline with the existing regressor and the expanded transformer pipeline\n        return SklearnRegressorPipeline(self.regressor, transformer_pipeline._steps)\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 37.78845701279396}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 40.667516976634914}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(c) for c in df.columns]\n    return df\n\n", "CodeBleu_score": 40.05438232405342}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 40.667516976634914}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n\t\n    if not self._is_fitted:\n            raise NotFittedError(\n                \\\"This instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\\\"\n            )\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n            X = X.to_numpy()\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \\\"RotationForest is not a time series classifier. \\\"\n                \\\"A valid sklearn input such as a 2d numpy array is required.\\\"\n                \\\"Sparse input formats are currently not supported.\\\"\n            )\n        X = self._validate_data(X=X, reset=False)\n\n        # remove useless attributes\n        X = X[:, self._useful_atts]\n\n        # normalise attributes\n        X = (X - self._min) / self._ptp\n\n        # get probabilities\n        y_pred_prob = np.zeros((X.shape[0], self.n_classes_))\n\n        for i in range(self._n_estimators):\n            y_pred_prob += self.estimators_[i].predict_proba(X)\n\n        y_pred_prob /= self._n_estimators\n\n        # get predictions\n        y_pred = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            max_indices = np.argwhere(y_pred_prob[i] == y_pred_prob[i].max()).flatten()\n            y_pred[i] = self.classes_[np.random.choice(max_indices)]\n\n        return y_pred\n\n\n\n", "CodeBleu_score": 34.20913667214662}, {"code": "def predict(self, X)\n\t\n    check_is_fitted(self)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required. \"\n            \"Sparse input formats are currently not supported.\"\n        )\n\n    X = self._validate_data(X=X, reset=False)\n\n    if self.n_atts_ != X.shape[1]:\n        raise ValueError(\n                \\\"The number of attributes in X must be equal to the number of attributes in the training data.\\\"\n            )\n\n        if self.n_atts_ != X.shape[1]:\n            raise ValueError(\n                \\\"The number of attributes in X must be equal to the number of attributes in the training data.\\\"\n            )\n\n        # normalise attributes\n        X = (X - self._min) / self._ptp\n\n        # transform data\n        X_transformed = Parallel(n_jobs=self._n_jobs)(delayed(self._transform)(X, pca, group) for pca, group in zip(self._pcas, self._groups))\n\n        # predict\n        y_pred = Parallel(n_jobs=self._n_jobs)(delayed(self._predict)(estimator, X_transformed) for estimator in self.estimators_)\n\n        return self._map_predictions(y_pred)\n\n    def _map_predictions(self, y_pred):\n        \\\"\\\"\\\"Maps predictions to class labels.\\\"\\\"\\\"\n        if self.n_classes_ == 1:\n", "CodeBleu_score": 35.77362233730794}, {"code": "def predict(self, X)\n\t\n    check_is_fitted(self)\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\"RotationForest is not a time series classifier. A valid sklearn input such as a 2d numpy array is required. Sparse input formats are currently not supported.\")\n    X = check_array(X, ensure_min_samples=2)\n    n_instances, n_atts = X.shape\n    if n_atts != self._n_atts:\n        raise ValueError(\\\"Number of attributes in X must be equal to the number of attributes used to fit the model.\\\")\n    if self.save_transformed_data:\n        X = np.concatenate(self.transformed_data_)\n    if self.save_transformed_data:\n        X = np.concatenate(self.transformed_data_)\n    else:\n        X = X[:, self._useful_atts]\n        X = (X - self._min) / self._ptp\n    y_pred = np.zeros(n_instances, dtype=np.int32)\n    for i, estimator in enumerate(self.estimators_):\n        y_pred += estimator.predict(X) * self._pcas[i].transform(X)\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = self.classes_[y_pred]\n    return y_pred\n\n", "CodeBleu_score": 36.385708440035955}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X_t = np.concatenate([pcas[i].transform(X[:, group]) for i, group in enumerate(groups)], axis=1)\n    X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n    y_proba = clf.predict_proba(X_t)\n        # adjust the probabilities to match the expected class structure\n        if y_proba.shape[1] != self.n_classes_:\n            y_proba = np.zeros((len(X), self.n_classes_))\n            for i in range(len(X)):\n                y_proba[i, np.argmax(y_proba[i])] = 1\n        return y_proba\n\n", "CodeBleu_score": 36.23329596237751}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X_t = np.concatenate([pcas[i].transform(X[:, group]) for i, group in enumerate(groups)], axis=1)\n    X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n    y_proba = clf.predict_proba(X_t)\n        if y_proba.shape[1] != self.n_classes_:\n            y_proba = np.zeros((X.shape[0], self.n_classes_))\n            y_proba[:, 0] = 1.0\n\n        return y_proba\n\n    def _train_probas_for_estimator(self, y, idx):\n        clf = self.estimators_[idx]\n        pcas = self.pcas_[idx]\n        groups = self.groups_[idx]\n\n        # merge all the pca_transformed data into one instance and build a classifier\n        # on it.\n        X_t = np.concatenate(\n            [pcas[i].transform(self.X_train_[:, group]) for i, group in enumerate(groups)], axis=1\n        )\n        X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n\n        # get the training probabilities\n        y_proba = clf.predict_proba(X_t)\n\n        # if the number of classes does not match the expected number of classes, adjust the\n        # probabilities to match the expected class structure.\n        if y_proba.shape[1] != self.n_classes_:\n            y_proba = np.zeros((self.n_instances_, self.n_classes_))\n            y_proba[:, 0] = 1.0\n\n        # compute the oob indices\n        oob = np.zeros(self.n_instances_, dtype=bool)\n        for i in range(self.n_instances_):\n            if y[i] in y_", "CodeBleu_score": 52.45266029950493}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    # concatenate all the transformed data into one instance\n        X_t = np.concatenate(\n            [pcas[i].transform(X[:, group]) for i, group in enumerate(groups)], axis=1\n        )\n        X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n        y_probas = clf.predict_proba(X_t)\n\n        # if the number of predicted classes does not match the expected number of classes,\n        # adjust the probabilities to match the expected class structure.\n        if y_probas.shape[1] != self.n_classes_:\n            y_probas = np.zeros((y_probas.shape[0], self.n_classes_))\n            for i in range(y_probas.shape[0]):\n                y_probas[i, :] = np.sum(y_probas[i, :])\n\n        return y_probas\n\n    def _train_probas_for_estimator(self, y, idx):\n        clf = self.estimators_[idx]\n        pcas = self.pcas_[idx]\n        groups = self.groups_[idx]\n\n        # merge all the pca_transformed data into one instance and build a classifier\n        # on it.\n        X_t = np.concatenate(\n            [pcas[i].transform(self.X_[i, :, groups[i]]) for i in range(len(groups))], axis=1\n        )\n        X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n        y_probas = clf.predict_proba(X_t)\n\n        # if the number of predicted classes does not match the expected number of classes,\n        # adjust the probabilities to match the expected class structure.\n        if y_probas.shape[1] != self.n_classes_:\n            y_probas = np.zeros((y_probas.shape[0], self.n", "CodeBleu_score": 52.716915949275766}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n            raise NotFittedError(\n                \\\"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\\\"\n                % {\\\"name\\\": type(self).__name__}\n            )\n\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \\\"ContinuousIntervalTree is not a time series classifier. \\\"\n                \\\"A valid sklearn input such as a 2d numpy array is required.\\\"\n                \\\"Sparse input formats are currently not supported.\\\"\n            )\n        X = self._validate_data(X=X, force_all_finite=\\\"allow-nan\\\")\n\n        if self.n_classes_ == 1:\n            return np.ones((X.shape[0], 1))\n\n        return np.array(\n            [self._root.predict_proba(x) for x in X]\n        )\n\n\n\n", "CodeBleu_score": 57.933195682382376}, {"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n            raise NotFittedError(\\\"This instance is not fitted yet. Call \\\\\\\"fit\\\\\\\" with appropriate arguments before using this method.\\\")\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \\\"ContinuousIntervalTree is not a time series classifier. \\\"\n                \\\"A valid sklearn input such as a 2d numpy array is required.\\\"\n                \\\"Sparse input formats are currently not supported.\\\"\n            )\n        X = self._validate_data(X=X, ensure_min_samples=2, force_all_finite=\\\"allow-nan\\\")\n\n        return np.array([self._root.predict_proba(x) for x in X])\n\n    def predict(self, X):\n        \\\"\\\"\\\"Predict for all cases in X. Built on top of predict_proba.\n\n        Parameters\n        ----------\n        X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n            The data to make predictions for.\n\n        Returns\n        -------\n        y : array-like, shape = [n_instances]\n            Predicted class labels.\\\"\\\"\\\"\n        rng = check_random_state(self.random_state)\n        return np.array(\n            [\n                self.classes_[int(rng.choice(np.flatnonzero(prob == prob.max())))]\n                for prob in self.predict_proba(X)\n            ]\n        )\n\n\n\n", "CodeBleu_score": 45.93642533393568}, {"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n            raise NotFittedError(\n                \\\"This instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\\\"\n            )\n\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \\\"ContinuousIntervalTree is not a time series classifier. \\\"\n                \\\"A valid sklearn input such as a 2d numpy array is required.\\\"\n                \\\"Sparse input formats are currently not supported.\\\"\n            )\n        X = self._validate_data(X, force_all_finite=\\\"allow-nan\\\")\n\n        return np.array(\n            [\n                self._root.predict_proba(X[i, :])\n                for i in range(len(X))\n            ]\n        )\n\n\n", "CodeBleu_score": 55.530771186579756}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    from sktime.datatypes import convert\n    from sktime.datatypes._panel._convert import _panel_to_2d_numpy\n    from sktime.datatypes._panel._convert import _table_to_2d_numpy\n\n    # convert X to a 2D numpy array\n    X_sklearn = convert(X, from_type=self.transformers_.scitype, to_type=\"numpy3D\")\n\n    # determine the scitype of the transformed output from the transformers_ attribute\n    if self.transformers_.scitype == \"Primitives\":\n        X_sklearn = _table_to_2d_numpy(X_sklearn)\n    elif self.transformers_.scitype == \"Series\":\n        X_sklearn = _panel_to_2d_numpy(X_sklearn)\n    else:\n        raise TypeError(\n            f\"Unexpected output type from transformers: {self.transformers_.scitype}\"\n        )\n\n    return X_sklearn\n\n", "CodeBleu_score": 35.96801758923165}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    from sktime.datatypes import convert\n    from sktime.datatypes._panel._convert import _convert_to_nested_dataframe\n    from sktime.datatypes._panel._convert import _convert_to_nested_numpy3D\n    from sktime.datatypes._panel._convert import _convert_to_numpy3D\n\n    # check if X is already a 2D numpy array\n        if isinstance(X, np.ndarray) and X.ndim == 2:\n            return X\n\n        # check if X is a list of pandas dataframes\n        if isinstance(X, list) and all(isinstance(x, pd.DataFrame) for x in X):\n            # convert list of pandas dataframes to a nested dataframe\n            X = _convert_to_nested_dataframe(X)\n\n        # check if X is a list of numpy arrays\n        if isinstance(X, list) and all(isinstance(x, np.ndarray) for x in X):\n            # convert list of numpy arrays to a numpy 3D array\n            X = _convert_to_numpy3D(X)\n\n        # check if X is a nested dataframe\n        if isinstance(X, pd.DataFrame):\n            # convert nested dataframe to a numpy 3D array\n            X = _convert_to_nested_numpy3D(X)\n\n        # check if X is a numpy 3D array\n        if isinstance(X, np.ndarray) and X.ndim == 3:\n            # convert numpy 3D array to a nested dataframe\n            X = _convert_to_nested_dataframe(X)\n\n        # check if X is a pandas dataframe\n        if isinstance(X, pd.DataFrame):\n            # convert pandas dataframe to a numpy 3D array\n            X = _convert_to_numpy3D(X)\n\n        # check if X is a numpy array\n        if isinstance(X, np", "CodeBleu_score": 22.821851799383218}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    # check if X is a pandas dataframe\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n\n        # check if X is a numpy array\n        if isinstance(X, np.ndarray):\n            # check if X is a 2D numpy array\n            if X.ndim == 2:\n                # check if X is a 2D numpy array with Table scitype\n                if isinstance(self.transformers_.scitype, str) and self.transformers_.scitype == \\\"table\\\":\n                    # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex\n                    if isinstance(X.index, pd.MultiIndex) and len(X.index.levels) == 2:\n                        # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex and the first level is a string\n                        if isinstance(X.index.levels[0][0], str):\n                            # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex and the first level is a string and the second level is a string\n                            if isinstance(X.index.levels[1][0], str):\n                                # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex and the first level is a string and the second level is a string and the first level is a string\n                                if isinstance(X.index.levels[0][0], str):\n                                    # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex and the first level is a string and the second level is a string and the first level is a string and the second level is a string\n                                    if isinstance(X.index.levels[1][0], str):\n                                        # check if X is a 2D numpy array with Table scitype and has a 2-level multiindex and the first level is a string and the second level is a string and the first level is", "CodeBleu_score": 17.371078444565523}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params_to_update = {}\n    for param_name, param_value in estimator.get_params().items():\n        if param_name.endswith(\\\"__random_state\\\") and param_value is None:\n            params_to_update[param_name] = random_state.randint(0, 1000000000)\n    if len(params_to_update) > 0:\n        estimator.set_params(**params_to_update)\n    return estimator\n\n", "CodeBleu_score": 32.14772513631887}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    if random_state is not None:\n        random_state = check_random_state(random_state)\n    else:\n        random_state = np.random\n    random_state_dict = dict()\n    for param_name, param_value in estimator.get_params().items():\n        if (param_name.endswith('random_state') and param_value is None):\n            random_state_dict[param_name] = random_state.randint(1, 100000)\n    if (len(random_state_dict) > 0):\n        estimator.set_params(**random_state_dict)\n    return estimator", "CodeBleu_score": 35.174407741342115}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    # First ensure random_state is a valid random state object\n    random_state = check_random_state(random_state)\n    # Initialize an empty dictionary to hold parameters that need updating\n    params_to_update = {}\n    # Iterate over the estimator's parameters\n    for param_name, param_value in estimator.get_params().items():\n        # Check if the parameter is related to random state\n        if param_name.endswith('random_state') or param_name == 'random_state':\n            # Assign a random integer value to the parameter\n            params_to_update[param_name] = random_state.randint(0, 1000000)\n    # Update the estimator with the new random state values\n    if params_to_update:\n        estimator.set_params(**params_to_update)\n    return estimator\n\n\n\n\n", "CodeBleu_score": 35.04962658218872}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n\t\n    n_instances, _ = X.shape\n    n_intervals = intervals.shape[0]\n    transformed = np.zeros((n_intervals, n_instances))\n    for i, interval in enumerate(intervals):\n        start, end = interval[0], interval[1]\n        slice_ = X[:, start:end]\n        transformed[i, :] = np.mean(slice_, axis=1)\n    return transformed.T\n\n", "CodeBleu_score": 24.63058055592507}, {"code": "def _transform(X, intervals)\n\t\n    n_instances, n_intervals = intervals.shape[0], intervals.shape[1] // 2\n    transformed = np.zeros((n_intervals, n_instances))\n    for interval_index in range(n_intervals):\n        interval_start, interval_end = intervals[interval_index, 0], intervals[interval_index, 1]\n        interval_slice = X[:, interval_start:interval_end]\n        transformed[interval_index, :] = np.concatenate(\n            [\n                np.mean(interval_slice, axis=1),\n                np.std(interval_slice, axis=1),\n                np.mean(np.diff(interval_slice, axis=1), axis=1),\n            ]\n        )\n    return transformed.T\n\n", "CodeBleu_score": 30.127511265737866}, {"code": "def _transform(X, intervals)\n\t\n    n_instances, n_intervals = intervals.shape[0], intervals.shape[1] // 2\n    Xt = np.zeros((n_instances, n_intervals, 3))\n    for i in range(n_intervals):\n        start, end = intervals[0, i * 2], intervals[0, i * 2 + 1]\n        Xt[:, i, 0] = X[:, start:end].mean(axis=1)\n        Xt[:, i, 1] = X[:, start:end].std(axis=1)\n        Xt[:, i, 2] = np.diff(X[:, start:end], axis=1).mean(axis=1)\n    return Xt.transpose((0, 2, 1))\n\n", "CodeBleu_score": 27.04005542516459}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    self.plot = plot\n    self.plot_params = plot_params\n    self.configuration = configuration\n    self.annotation_func = annotation_func\n    self.annotation_params = annotation_params\n    self.ax_op_before = ax_op_before\n    self.ax_op_after = ax_op_after\n    self.annotate_params = annotate_params\n    self.kwargs = kwargs\n    self._init_annotation_and_operation_parameters()\n    self._init_plot_and_configure()\n    self._annotate_plot()\n    self._apply_post_plot_ops()\n    return self.plot_output\n\n", "CodeBleu_score": 39.44877868786552}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if plot_params is None:\n        plot_params = {}\n    if configuration is None:\n        configuration = {}\n\n    plot_params = self._get_plot_params(plot_params)\n    configuration = self._get_configuration(configuration)\n\n    plot_params.update(plot_params)\n    configuration.update(configuration)\n\n    self._maybe_warn_about_configuration()\n\n    self._set_plot_params(plot_params)\n    self._set_configuration(configuration)\n\n    self._set_annotate_params(annotation_params)\n    self._set_annotate_func(annotation_func)\n    self._set_annotate_params(annotate_params)\n\n    self._set_ax_op_before(ax_op_before)\n    self._set_ax_op_after(ax_op_after)\n\n    self._set_plot_params(plot_params)\n    self._set_configuration(configuration)\n\n    self._set_annotate_params(annotation_params)\n    self._set_annotate_func(annotation_func)\n    self._set_annotate_params(annotate_params)\n\n    self._set_ax_op_before(ax_op_before)\n    self._set_ax_op_after(ax_op_after)\n\n    self._set_plot_params(plot_params)\n    self._set_configuration(configuration)\n\n    self._set_annotate_params(annotation_params)\n    self._set_annotate_func(annotation_func)\n    self._set_annotate_params(annotate_params)\n\n    self._set_ax_op_before(ax_op_before)\n    self._set_ax_op_after(ax_op_after)\n\n    self._set", "CodeBleu_score": 39.80711790125315}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    self._reset_default_values()\n    self._set_attributes(configuration)\n    self._set_attributes(annotation_params)\n    self._set_attributes(annotate_params)\n    self._set_attributes(plot_params)\n    self._set_attributes(kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_attributes(self._annotation_params)\n    self._set_attributes(self._configuration)\n    self._set_attributes(self._kwargs)\n    self._set_attributes(self._plot_params)\n    self._set_attributes(self._annotate_params)\n    self._set_", "CodeBleu_score": 30.36592888096607}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, '_program'):\n        raise NotFittedError(\\\"This SymbolicClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\\\")\n    X = check_array(X, accept_sparse=True)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\\\"Number of features of the model must match the input.\\\")\n    scores = self._program.execute(X)\n    probs = self.transformer_.transform(scores)\n    return np.hstack([1 - probs, probs])\n\n    def predict(self, X):\n    \\\"\\\"\\\"The function predict predicts the class labels for the input data X. It first checks if the model has been fitted by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature count is checked against the model's expected feature count, raising an error if they do not match. The function then executes the program to obtain scores, transforms these scores into probabilities, and selects the class label with the highest probability for each sample. It returns this array of class labels.\\\"\\\"\\\"\n    if not hasattr(self, '_program'):\n        raise NotFittedError(\\\"This SymbolicClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\\\")\n    X = check_array(X, accept_sparse=True)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\\\"Number of features of the model must match the input.\\\")\n    scores = self._program.execute(X)\n    probs = self.transformer_.transform(scores)\n    return np.argmax(probs, axis=1)\n\n    def fit(self, X, y, sample_weight=None):\n    \\\"\\\"\\\"The function fit trains the SymbolicClassifier model on the input data X and target labels y. It first validates the input data and target labels, ensuring they have the same number of samples and that the target labels are binary. It then initializes the population of programs by generating random programs. The function then iterates through", "CodeBleu_score": 43.29510709334831}, {"code": "def predict_proba(self, X)\n\t\n\n    if not hasattr(self, '_program'):\n        raise NotFittedError(\\\"SymbolicClassifier has not been fitted yet. Call 'fit' with appropriate arguments before using this estimator.\\\")\n    X = check_array(X, accept_sparse=True)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\\\"X has %d features per sample; expecting %d\\\" % (X.shape[1], self.n_features_in_))\n    scores = self._program.execute(X)\n    probabilities = self._transformer.transform(scores)\n    return probabilities\n\n    def _predict(self, X):\n        \\\"\\\"\\\"The function _predict is used internally by the predict method to make predictions on the input data X. It first checks if the model has been fitted by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature count is checked against the model's expected feature count, raising an error if they do not match. The function then executes the program to obtain scores, transforms these scores into probabilities, and uses these probabilities to make class predictions. It returns the predicted classes.\\\"\\\"\\\"\n\n        if not hasattr(self, '_program'):\n            raise NotFittedError(\\\"SymbolicClassifier has not been fitted yet. Call 'fit' with appropriate arguments before using this estimator.\\\")\n        X = check_array(X, accept_sparse=True)\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(\\\"X has %d features per sample; expecting %d\\\" % (X.shape[1], self.n_features_in_))\n        scores = self._program.execute(X)\n        probabilities = self._transformer.transform(scores)\n        return probabilities.argmax(axis=1)\n\n    def _predict_proba_lr(self, X):\n        \\\"\\\"\\\"The function _predict_proba_lr is used internally by the predict_proba method to make predictions on the input data X using a logistic regression model. It first checks if the model has been fitted by verifying the presence of", "CodeBleu_score": 39.44349199562124}, {"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, '_program'):\n        raise NotFittedError('The model has not been fitted yet. Call `fit` before `predict_proba`.')\n        return self._program.predict_proba(X)\n\n    if not isinstance(X, (np.ndarray, csr_matrix, csc_matrix)):\n        raise TypeError('The input data must be a numpy array, sparse matrix, or pandas dataframe.')\n\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError('The number of features in the input data does not match the number of features in the model.')\n\n    return self._program.predict_proba(X)\n\n\n    def _more_tags(self):\n        return {\\'requires_y\\': True}\n\n\n\n", "CodeBleu_score": 33.47674420669781}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n\t\n    # Create a copy of the program\n    program = copy(self.program)\n    # Initialize an empty list to store the modified node indices\n    modified = []\n    # Iterate over each node in the program\n    for i, node in enumerate(program):\n        # Check if the node should be mutated based on the probability threshold\n        if random_state.uniform() < self.point_mutation_rate:\n            # If the node is a function, find a valid replacement function with the same arity\n            if isinstance(node, _Function):\n                # Get a list of all function nodes with the same arity\n                candidates = [n for n in self.functions if n.arity == node.arity]\n                # If there are no candidates, continue to the next node\n                if not candidates:\n                    continue\n                # Select a random candidate function\n                node = random_state.choice(candidates)\n            # If the node is a terminal, replace it with a constant or a variable\n            else:\n                # Get a list of all terminal nodes\n                candidates = self.terminals\n                # If there are no candidates, continue to the next node\n                if not candidates:\n                    continue\n                # Select a random candidate terminal\n                node = random_state.choice(candidates)\n            # Add the index of the modified node to the list\n            modified.append(i)\n    # Return the mutated program and the list of modified node indices\n    return program, modified\n\n", "CodeBleu_score": 18.99532723753784}, {"code": "def point_mutation(self, random_state)\n\t\n    # Create a copy of the program\n    program = self.reproduce()\n\n    # Identify nodes to modify\n    nodes_to_modify = random_state.uniform(size=len(program)) < self.point_mutation_rate\n    nodes_to_modify = np.where(nodes_to_modify)[0]\n\n    # Perform the mutation\n    for node_index in nodes_to_modify:\n        node = program[node_index]\n        if isinstance(node, _Function):\n            # Find a valid replacement function with the same arity\n            replacement_function = self.get_random_function(random_state, node.arity)\n            program[node_index] = replacement_function\n        else:\n            # Replace with either a constant or a variable\n            if random_state.uniform() < 0.5:\n                program[node_index] = self.get_random_constant(random_state)\n            else:\n                program[node_index] = self.get_random_variable(random_state)\n\n    return program, nodes_to_modify\n\n", "CodeBleu_score": 24.13604243578539}, {"code": "def point_mutation(self, random_state)\n\t\n    program = self.program.copy()\n    modified_nodes = []\n    for node_index in random_state.choice(range(len(program)), size=int(len(program) * self.point_mutation_rate), replace=False):\n        node = program[node_index]\n        if isinstance(node, _Function):\n            replacement_function = random_state.choice(self.functions)\n            while replacement_function.arity != node.arity:\n                replacement_function = random_state.choice(self.functions)\n            program[node_index] = replacement_function\n        elif isinstance(node, _Constant):\n            replacement_value = random_state.uniform(self.constant_range[0], self.constant_range[1])\n            program[node_index] = _Constant(replacement_value)\n        elif isinstance(node, _Variable):\n            replacement_value = random_state.choice(range(self.n_features))\n            program[node_index] = _Variable(replacement_value)\n        modified_nodes.append(node_index)\n    return program, modified_nodes\n\n", "CodeBleu_score": 26.757852941288572}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate arguments\n    length = int(length) if length and length > 0 else 20\n    mamode = mamode.lower() if isinstance(mamode, str) else 'sma'\n    min_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\n    offset = get_offset(offset)\n\n    # Calculate Result\n    close = verify_series(close)\n    if close is None: return\n    ma_ = ma(close=close, length=length, mamode=mamode, **kwargs)\n    if ma_ is None: return\n    result = close.sub(ma_).div(ma_).mul(100)\n\n    # Offset\n    if offset != 0:\n        result = result.shift(offset)\n\n    # Name and Categorize it\n    result.name = f\"BIAS_{length}\"\n    result.category = 'momentum'\n\n    return result\n\n", "CodeBleu_score": 48.389836396615316}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate arguments\n    length = int(length) if length and length > 0 else 20\n    mamode = mamode.lower() if isinstance(mamode, str) else \"sma\"\n    min_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\n    offset = get_offset(offset)\n\n    # Calculate Result\n    close = verify_series(close)\n    ma_ = ma(close=close, length=length, mamode=mamode, **kwargs)\n    result = close.sub(ma_)\n    result *= 100\n\n    # Offset\n    if offset != 0:\n        result = result.shift(offset)\n\n    # Name & Category\n    result.name = f\"BIAS_{length}\"\n    result.category = \"momentum\"\n\n    return result\n\n", "CodeBleu_score": 38.69809362351085}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate arguments\n    mamode = mamode.lower() if isinstance(mamode, str) else \"sma\"\n    length = int(length) if length and length > 0 else 20\n    min_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\n    offset = get_offset(offset)\n\n    # Calculate Result\n    close = verify_series(close)\n    if close is None: return\n\n    # Calculate the MA\n    ma_ = ma(close, length=length, mamode=mamode, **kwargs)\n    if ma_ is None: return\n\n    # Calculate BIAS\n    result = close.sub(ma_).div(ma_)\n    result.name = f\"BIAS_{length}_{mamode}\"\n\n    # Offset\n    if offset != 0:\n        result = result.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        result.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        result.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    result.name = f\"BIAS_{length}_{mamode}\"\n    result.category = \"momentum\"\n\n    return result", "CodeBleu_score": 55.290434767980955}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        url = URL[broker]['kline'] % (code, KTYPES[ktype][broker], size)\n        data = _get_data(url)\n        if broker == 'hb':\n            data = data['ticker']\n        elif broker == 'ok':\n            data = data['kline']\n        elif broker == 'chbtc':\n            data = data['data']\n        df = pd.DataFrame(data)\n        if broker == 'hb':\n            df.columns = ['date', 'open', 'close', 'low', 'high', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n        elif broker == 'ok':\n            df.columns = ['date', 'open', 'close', 'low', 'high', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n        elif broker == 'chbtc':\n            df.columns = ['date', 'open', 'close', 'low', 'high', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n        df['date'] = pd.to_datetime(df['date'])\n        return df\n    except Exception as e:\n        print(traceback.format_exc())\n\n", "CodeBleu_score": 43.91728383056168}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    url = URL[broker]['kline'] % (code, KTYPES[ktype][broker], size)\n    data = _get_data(url)\n    try:\n        data = json.loads(data)\n        if broker == 'hb':\n            df = pd.DataFrame(data['ticker']['data'])\n            df.columns = ['date', 'open', 'close', 'high', 'low', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n        elif broker == 'ok':\n            df = pd.DataFrame(data['data'])\n            df.columns = ['date', 'open', 'close', 'high', 'low', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n        elif broker == 'chbtc':\n            df = pd.DataFrame(data['data'])\n            df.columns = ['date', 'open', 'close', 'high', 'low', 'vol']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\n    except Exception as e:\n        print(traceback.format_exc())\n    return df\n\n", "CodeBleu_score": 36.978150210785806}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    url = URL[broker]['kline'] % (code, KTYPES[ktype][broker], size)\n    data = _get_data(url)\n    try:\n        data = json.loads(data)\n    except:\n        pass\n    try:\n        data = data[code.upper()]\n    except:\n        pass\n    try:\n        data = data['data']\n    except:\n        pass\n    try:\n        data = data[ktype]\n    except:\n        pass\n    try:\n        data = pd.DataFrame(data)\n    except:\n        pass\n    try:\n        data.columns = ['open', 'high', 'low', 'close', 'vol']\n    except:\n        pass\n    try:\n        data.index = [pd.to_datetime(x, unit='ms') for x in data.index]\n    except:\n        pass\n    return data\n\n", "CodeBleu_score": 28.979815167457918}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        js = _get_data(URL[broker]['snapshot'] % (code, size))\n        if js is None:\n            return js\n        if broker == 'hb':\n            ask = pd.DataFrame(js['asks'], columns=['ASK', 'ASK_VOL'])\n            bid = pd.DataFrame(js['bids'], columns=['BID', 'BID_VOL'])\n            ask['TIME'] = js['date']\n            bid['TIME'] = js['date']\n        else:\n            ask = pd.DataFrame(js['asks'], columns=['ASK', 'ASK_VOL'])\n            bid = pd.DataFrame(js['bids'], columns=['BID', 'BID_VOL'])\n            ask['TIME'] = int2time(js['date'] / 1000)\n            bid['TIME'] = int2time(js['date'] / 1000)\n        ask['TIME'] = pd.to_datetime(ask['TIME'])\n        bid['TIME'] = pd.to_datetime(bid['TIME'])\n        return pd.Panel({'ASK': ask, 'BID': bid})\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 51.89750445383914}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        js = _get_data(URL[broker]['snapshot'] % (code, size, broker))\n        if js is None:\n            return js\n        if broker == 'chbtc':\n            js = js['data']\n        df_ask = pd.DataFrame(js['asks'], columns=['PRICE', 'VOL'])\n        df_ask['TIME'] = js['date']\n        df_ask['TIME'] = df_ask['TIME'].apply(lambda x: int2time(x / 1000))\n        df_ask['TIME'] = pd.to_datetime(df_ask['TIME'])\n        df_ask['PRICE'] = df_ask['PRICE'].apply(lambda x: float(x))\n        df_ask['VOL'] = df_ask['VOL'].apply(lambda x: float(x))\n        df_bid = pd.DataFrame(js['bids'], columns=['PRICE', 'VOL'])\n        df_bid['TIME'] = js['date']\n        df_bid['TIME'] = df_bid['TIME'].apply(lambda x: int2time(x / 1000))\n        df_bid['TIME'] = pd.to_datetime(df_bid['TIME'])\n        df_bid['PRICE'] = df_bid['PRICE'].apply(lambda x: float(x))\n        df_bid['VOL'] = df_bid['VOL'].apply(lambda x: float(x))\n        return pd.Panel({'ask': df_ask, 'bid': df_bid})\n    except Exception:\n        print(traceback.print_exc())\n\n\n\n\n", "CodeBleu_score": 49.416715713795845}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        js = _get_data(URL[broker]['snapshot'] % (code, size))\n        if js is None:\n            return js\n        if broker == 'hb':\n            js = js['data']\n        ask = pd.DataFrame(js['asks'], columns=['PRICE', 'AMOUNT'])\n        bid = pd.DataFrame(js['bids'], columns=['PRICE', 'AMOUNT'])\n        ask['DATE'] = pd.to_datetime(int2time(js['date'] / 1000))\n        bid['DATE'] = pd.to_datetime(int2time(js['date'] / 1000))\n        return pd.Panel({'ask': ask, 'bid': bid})\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 47.463979112263374}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n\t\n    try:\n        js = _get_data(URL[broker]['trade'] % (code))\n        if js is None:\n            return js\n        if broker == 'hb':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x / 1000))\n        elif broker == 'ok':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x / 1000))\n        elif broker == 'chbtc':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x))\n        df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n        return df\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 60.7774063966571}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    try:\n        js = _get_data(URL[broker]['trade'] % (code))\n        if js is None:\n            return js\n        if broker == 'hb':\n            df = pd.DataFrame(js['data'], columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x / 1000))\n            df['datetime'] = pd.to_datetime(df['datetime'])\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n        elif broker == 'ok':\n            df = pd.DataFrame(js['data'], columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n            df['datetime'] = pd.to_datetime(df['datetime'])\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n        else:\n            df = pd.DataFrame(js['data'], columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x))\n            df['datetime'] = pd.to_datetime(df['datetime'])\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n        return df\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 56.41675586132959}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    try:\n        js = _get_data(URL[broker]['trade'] % (code))\n        if js is None:\n            return js\n        if broker == 'hb':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x / 1000))\n        elif broker == 'ok':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x / 1000))\n        elif broker == 'chbtc':\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n            df['datetime'] = df['datetime'].apply(lambda x: int2time(x))\n        else:\n            df = pd.DataFrame(js, columns=['tid', 'datetime', 'price', 'amount', 'type'])\n        df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n        return df\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 58.70966954451278}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    ct._write_head()\n    try:\n        text = urlopen(url, timeout=10).read()\n    except Exception as e:\n        print(e)\n        return pd.DataFrame()\n    text = text.decode('gbk')\n    text = text.replace('var data_netvalue=', '').replace(';', '')\n    try:\n        org_js = json.loads(text)\n    except Exception as e:\n        print(e)\n        return pd.DataFrame()\n    data = org_js['data']\n    if fund_type == 'open':\n        fund_df = pd.DataFrame(data, columns=ct.FUND_OPEN_COLS)\n    elif fund_type == 'close':\n        fund_df = pd.DataFrame(data, columns=ct.FUND_CLOSE_COLS)\n    elif fund_type == 'grading':\n        fund_df = pd.DataFrame(data, columns=ct.FUND_GRADING_COLS)\n    else:\n        return pd.DataFrame()\n    fund_df = fund_df.fillna(0)\n    return fund_df\n\n", "CodeBleu_score": 37.84409681620536}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    try:\n        ct._write_head()\n        text = urlopen(url, timeout=10).read()\n        text = text.decode('gbk')\n        org_js = json.loads(text)\n    except Exception as e:\n        print(e)\n        return pd.DataFrame()\n\n    status_code = int(org_js['result']['status']['code'])\n    if status_code != 0:\n        status = str(org_js['result']['status']['msg'])\n        raise ValueError(status)\n    data = org_js['result']['data']\n    fund_df = pd.DataFrame(data, columns=ct.FUND_DATA_COLS[fund_type])\n    fund_df = fund_df.fillna(0)\n    return fund_df\n\n", "CodeBleu_score": 33.65866957729334}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    try:\n        response = urlopen(url, timeout=10)\n        text = response.read().decode('gbk')\n        text = text.replace('var data=', '').replace(';', '')\n        org_js = json.loads(text)\n        fund_df = pd.DataFrame(org_js, columns=ct.FUND_COLS[fund_type])\n        fund_df.fillna(0, inplace=True)\n        return fund_df\n    except Exception as e:\n        print(e)\n\n", "CodeBleu_score": 29.107610893095533}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if market == 'CFFEX':\n        start = ct.convert_date(start) if start is not None else datetime.date.today()\n        end = ct.convert_date(end) if end is not None else datetime.date.today()\n        data_list = list()\n        for day in (start + datetime.timedelta(days=x) for x in range((end - start).days + 1)):\n            data = get_cffex_daily(day)\n            if data is not None:\n                data_list.append(data)\n        if len(data_list) > 0:\n            return pd.concat(data_list)\n        else:\n            return None\n    elif market == 'SHFE':\n        start = ct.convert_date(start) if start is not None else datetime.date.today()\n        end = ct.convert_date(end) if end is not None else datetime.date.today()\n        data_list = list()\n        for day in (start + datetime.timedelta(days=x) for x in range((end - start).days + 1)):\n            data = get_shfe_daily(day)\n            if data is not None:\n                data_list.append(data)\n        if len(data_list) > 0:\n            return pd.concat(data_list)\n        else:\n            return None\n    elif market == 'DCE':\n        start = ct.convert_date(start) if start is not None else datetime.date.today()\n        end = ct.convert_date(end) if end is not None else datetime.date.today()\n        data_list = list()\n        for day in (start + datetime.timedelta(days=x) for x in range((end - start).days + 1)):\n            data = get_dce_daily(day, type='future')\n            if data is not None:\n                data_list.append(data)\n        if len(data_list) > 0:\n            return pd.concat(data_list)\n        else:\n            return None\n    elif market == 'CZCE':", "CodeBleu_score": 47.58048729087682}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if market == 'CFFEX':\n        func = get_cffex_daily\n    elif market == 'SHFE':\n        func = get_shfe_daily\n    elif market == 'DCE':\n        func = get_dce_daily\n    else:\n        print('invalid market: ' + market + ', should be one of \\\"CFFEX\\\", \\\"SHFE\\\", \\\"DCE\\\"')\n        return\n    start_day = ct.convert_date(start) if start is not None else datetime.date.today()\n    end_day = ct.convert_date(end) if end is not None else datetime.date.today()\n    df_list = list()\n    for day in (start_day + datetime.timedelta(n) for n in range((end_day - start_day).days + 1)):\n        df = func(day)\n        if df is not None:\n            df_list.append(df)\n    return pd.concat(df_list, ignore_index=True)\n\n", "CodeBleu_score": 57.48536676886555}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    start = ct.convert_date(start) if start is not None else datetime.date.today()\n    end = ct.convert_date(end) if end is not None else datetime.date.today()\n    if start > end:\n        raise ValueError('start date must be before end date')\n    if market == 'CFFEX':\n        df_list = []\n        for day in date_range(start, end):\n            df = get_cffex_daily(day)\n            if df is not None:\n                df_list.append(df)\n        if len(df_list) > 0:\n            return pd.concat(df_list)\n    elif market == 'SHFE':\n        df_list = []\n        for day in date_range(start, end):\n            df = get_shfe_daily(day)\n            if df is not None:\n                df_list.append(df)\n        if len(df_list) > 0:\n            return pd.concat(df_list)\n    elif market == 'DCE':\n        df_list = []\n        for day in date_range(start, end):\n            df = get_dce_daily(day, type='future')\n            if df is not None:\n                df_list.append(df)\n        if len(df_list) > 0:\n            return pd.concat(df_list)\n    elif market == 'CZCE':\n        df_list = []\n        for day in date_range(start, end):\n            df = get_czce_daily(day)\n            if df is not None:\n                df_list.append(df)\n        if len(df_list) > 0:\n            return pd.concat(df_list)\n    elif market == 'INE':\n        df_list = []\n        for day in date_range(start, end):\n            df = get_ine_daily(day)\n            if df is not None:\n                df_list.append(df)\n        if len(df_list) > 0:\n            return pd.concat(df_list)\n    elif market == 'SSE", "CodeBleu_score": 42.577698204576976}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    url = 'http://api.boxoffice.cn/boxoffice/day?d=%s' % du.today()\n    req = Request(url)\n    req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36')\n    attempt = 0\n    while attempt < retry_count:\n        try:\n            response = urlopen(req)\n            json_str = response.read()\n            data = json.loads(json_str)\n            if data['code'] == '0':\n                df = pd.DataFrame(data['data2'])\n                df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n                df['timestamp'] = time.time()\n                return df\n            else:\n                print(data['msg'])\n                return None\n        except Exception as e:\n            print(e)\n            attempt += 1\n            time.sleep(pause)\n    print('Maximum attempts reached')\n    return None\n\n", "CodeBleu_score": 37.96463321111243}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    url = 'http://api.boxoffice.cn/boxoffice/day?d=%s' % du.today()\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}\n    request = Request(url, None, headers)\n    while retry_count > 0:\n        try:\n            response = urlopen(request)\n            content = response.read()\n            if content:\n                data = json.loads(content)\n                if data.get(\\'data2\\'):\n                    df = pd.DataFrame(data[\\'data2\\'])\n                    df = df.drop([\\'MovieImg\\', \\'mId\\'])\n                    df[\\'timestamp\\'] = time.time()\n                    return df\n        except Exception as e:\n            print(e)\n        retry_count -= 1\n        time.sleep(pause)\n    return None\n\n\n\n", "CodeBleu_score": 36.63747146527113}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    url = 'http://api.boxoffice.cn/boxoffice/day?d=%s' % du.today()\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}\n    req = Request(url, None, headers)\n    for i in range(retry_count):\n        time.sleep(pause)\n        try:\n            response = urlopen(req)\n            data = response.read().decode('utf-8')\n            if data:\n                data = json.loads(data)\n                df = pd.DataFrame(data['data2'])\n                df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n                df['timestamp'] = time.time()\n                return df\n        except Exception as e:\n            print(e)\n    return None\n\n", "CodeBleu_score": 42.05799908646796}]}
