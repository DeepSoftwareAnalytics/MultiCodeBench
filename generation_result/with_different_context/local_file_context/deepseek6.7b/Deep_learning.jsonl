{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    if cached_ldsr_model is not None:\n        self.model = cached_ldsr_model\n        return {'model': self.model}\n\n    # Load model from config\n    config = OmegaConf.load(self.config_path)\n    config.model.params.params.sample_fn = 'ddim'\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.model.params.params.ddim_sampler)\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.model.params.params.ddim_sampler)\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.model.params.params.ddim_sampler)\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.model.params.params.ddim_sampler)\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.model.params.params.ddim_sampler)\n    config.model.params.params.ddim_eta = 0.0\n    config.model.params.params.ddim_steps = 100\n    config.model.params.params.ddim_sampler = DDIMSampler(**config.", "CodeBleu_score": 23.479023790222257}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if os.path.isfile(self.model_path):\n        if os.path.splitext(self.model_path)[1] == '.safetensors':\n            self.model = safetensors.torch.load_file(self.model_path, device=self.device)\n        else:\n            self.model = torch.load(self.model_path, map_location=self.device)\n    else:\n        raise FileNotFoundError(f'Model file not found at {self.model_path}')\n    if self.half_attention:\n        self.model.half()\n    if self.memory_format_optimization:\n        self.model.to(memory_format=torch.channels_last)\n    self.model.eval()\n    self.model.to(self.device)\n    if self.model_cache_path is not None:\n        torch.save(self.model, self.model_cache_path)\n    return self.model\n\n", "CodeBleu_score": 25.377731243292867}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    if cached_ldsr_model is not None and os.path.isfile(cached_ldsr_model):\n        model = safetensors.torch.load_file(cached_ldsr_model, device=\"cpu\")\n        model = model[\"model\"]\n        model = model.to(devices.device)\n        model.eval()\n        return {\"model\": model}\n\n    # Load model from path\n    if shared.opts.ldsr_model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load_file(shared.opts.ldsr_model_path, device=\"cpu\")\n    else:\n        model = torch.load(shared.opts.ldsr_model_path, map_location=\"cpu\")\n    model = model[\"model\"]\n    model = model.to(devices.device)\n    model.eval()\n\n    # Configure model\n    config = OmegaConf.load(shared.opts.ldsr_model_config)\n    model = instantiate_from_config(config.model)\n\n    # Move model to device\n    model = model.to(devices.device)\n\n    # Set half-precision attention\n    if half_attention:\n        model.half()\n\n    # Set memory format optimization\n    if shared.opts.ldsr_memory_format_optimization:\n        model.mem_format_optimization()\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Cache model for future use\n    if shared.opts.ldsr_cache_model:\n        os.makedirs(os.path.dirname(cached_ldsr_model), exist_ok=True)\n        safetensors.torch.save_file(model, cached_ldsr_model)\n\n    return {\"model\": model}\n\n", "CodeBleu_score": 39.982431568742136}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f'sampling steps: {steps}')\n    print(f'eta: {eta}')\n    if callback is not None and not isinstance(callback, Callback):\n        raise TypeError('`callback` should be a subclass of `Callback`')\n\n    # Create a DDIM sampler\n    sampler = DDIMSampler(model)\n\n    # Sample the images\n    samples, intermediates = sampler.sample(steps, batch_size, shape, cond, eta, callback=callback,\n                                            normals_sequence=normals_sequence,\n                                            mask=mask, x0=x0, quantize_x0=quantize_x0,\n                                            temperature=temperature,\n                                            score_corrector=score_corrector,\n                                            corrector_kwargs=corrector_kwargs, x_T=x_t)\n\n    # Return the generated samples and intermediate values\n    return samples, intermediates\n\n", "CodeBleu_score": 52.33088419913393}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    print(f'sampling steps {steps} eta {eta}')\n    if not hasattr(model, 'split_input_params'):\n        model.split_input_params = None\n    sampler = DDIMSampler(model)\n    return sampler.sample(steps, batch_size, shape, cond, eta, callback, normals_sequence, mask, x0,\n                          quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_T=x_t)\n\n", "CodeBleu_score": 36.231438350567245}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    device = next(model.parameters()).device\n    if isinstance(shape, torch.Tensor):\n        shape = shape.tolist()\n    if isinstance(steps, torch.Tensor):\n        steps = steps.tolist()\n    if isinstance(eta, torch.Tensor):\n        eta = eta.tolist()\n    if isinstance(x_t, torch.Tensor):\n        x_t = x_t.to(device)\n    if isinstance(cond, torch.Tensor):\n        cond = cond.to(device)\n\n    batch_size = shape[0]\n\n    print(f'eta: {eta}, steps: {steps}')\n\n    sampler = DDIMSampler(model)\n    return sampler.sample(batch_size, shape, cond, steps, eta, callback, normals_sequence,\n                          mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs,\n                          x_t)\n\n", "CodeBleu_score": 41.63843022691172}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and context is not None:\n        print(f\"{context}: Using EMA weights\")\n    if self.use_ema:\n        self.model_ema.ema_step()\n        yield\n        self.model_ema.ema_restore()\n    else:\n        yield\n    if self.use_ema and context is not None:\n        print(f\"{context}: Restored original weights\")\n\n", "CodeBleu_score": 55.751963130559744}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            print(f\"{context}: Switching to EMA weights\")\n        self.model_ema.use()\n        yield\n        if context is not None:\n            print(f\"{context}: Restoring original weights\")\n        self.model_ema.restore()\n    else:\n        yield\n\n", "CodeBleu_score": 52.93304671936725}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            print(f\"{context}: EMA weights in use.\")\n        ema_weights = self.model_ema.ema_weights()\n        try:\n            yield\n        finally:\n            self.load_state_dict(ema_weights)\n            if context is not None:\n                print(f\"{context}: Restored original weights.\")\n    else:\n        yield\n\n", "CodeBleu_score": 58.18353724406031}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n    self.load_state_dict(sd, strict=False)\n\n", "CodeBleu_score": 15.600805850016735}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    keys = list(sd.keys())\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    missing, unexpected = [], []\n    for k in keys:\n        if not hasattr(self, k):\n            missing.append(k)\n        elif hasattr(self, k) and not hasattr(self, k) and k not in sd:\n            unexpected.append(k)\n    if len(missing) > 0:\n        print(f\"Missing keys in state_dict: {missing}\")\n    if len(unexpected) > 0:\n        print(f\"Unexpected keys in state_dict: {unexpected}\")\n\n", "CodeBleu_score": 39.48133245703007}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys = []\n    unexpected_keys = []\n    for k, v in self.state_dict().items():\n        if k not in state_dict:\n            missing_keys.append(k)\n        elif v.shape != state_dict[k].shape:\n            print(f\"Shape mismatch for {k}: {v.shape} vs {state_dict[k].shape}\")\n    for k in state_dict.keys():\n        if k not in self.state_dict():\n            unexpected_keys.append(k)\n    if len(missing_keys) > 0:\n        print(f\"Missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 40.904952221149024}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.batch_resize_range is not None:\n        input = self.batch_resize(input)\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, self.quantize.get_codebook_indices(quant)\n    return dec, diff\n\n", "CodeBleu_score": 53.519437782586124}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.batch_resize_range is not None:\n            assert input.shape[0] == 1\n            input = torch.randint(0, self.batch_resize_range, (1, 3, 32, 32), device=input.device)\n\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n\n        if return_pred_indices:\n            pred_indices = self.quantize.get_codebook_indices(quant)\n            return dec, diff, pred_indices\n        else:\n            return dec, diff\n\n    def forward_loss(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        loss = self.loss(dec, input)\n        return loss, diff\n\n    def forward_loss_and_colorize(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        if hasattr(self, \\\"colorize\\\"):\n            dec = dec * self.colorize.to(dec.dtype).to(dec.device)\n        loss = self.loss(dec, input)\n        return loss, diff\n\n    def forward_loss_and_colorize_and_quantize(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        if hasattr(self, \\\"colorize\\\"):\n            dec = dec * self.colorize.to(dec.dtype).to(dec.device)\n        quant, diff, _ = self.encode(dec)\n        dec = self.decode(quant)\n        loss = self.loss(dec, input)\n        return loss, diff\n\n    def forward_loss_and_colorize_and_quantize_and_ema(self, input):\n        with self.ema_scope(\\\"forward_loss_and_colorize_and_quantize_and_ema\\\"):\n            quant, diff, _ = self.encode(input)\n            dec = self", "CodeBleu_score": 40.81104257414868}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n            pred_indices = self.quantize.get_codebook_indices(quant)\n            return dec, diff, pred_indices\n        else:\n            return dec, diff\n\n", "CodeBleu_score": 43.50261250229284}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n        if x.ndim == 2:\n            x = x.unsqueeze(1)\n        if x.ndim == 3:\n            x = x.unsqueeze(1)\n        if x.ndim == 4:\n            x = x.permute(0, 3, 1, 2)\n        if self.batch_resize_range is not None:\n            x = F.interpolate(x, size=(self.batch_resize_range[0] + self.global_step % (self.batch_resize_range[1] - self.batch_resize_range[0]), x.shape[2], x.shape[3]), mode=\\\"bicubic\\\", align_corners=False)\n        x = x.to(torch.float32)\n        x = x.detach()\n        return x\n\n\n\n", "CodeBleu_score": 50.78243757032867}, {"code": "def get_input(self, batch, k)\n\t\n    if k == \\\"image\\\":\n        x = batch[k]\n        if x.ndim == 2:\n            x = x.unsqueeze(0)\n        if x.ndim == 3:\n            x = x.unsqueeze(0)\n        if x.ndim == 4 and x.shape[1] == 1:\n            x = torch.cat([x, x, x], 1)\n        if x.ndim == 4 and x.shape[1] == 3:\n            x = x.permute(0, 3, 1, 2)\n        if x.ndim == 4 and x.shape[1] == 4:\n            x = x.permute(0, 2, 3, 1)\n        if self.batch_resize_range is not None:\n            x = F.interpolate(x, size=(self.batch_resize_range[0] + self.global_step % (self.batch_resize_range[1] - self.batch_resize_range[0]), self.batch_resize_range[0] + self.global_step % (self.batch_resize_range[1] - self.batch_resize_range[0])), mode=\\\"bicubic\\\", align_corners=False)\n        x = x.contiguous().float()\n        x = x.detach()\n        return x\n    else:\n        return batch[k]\n\n\n\n", "CodeBleu_score": 49.31731844839885}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x[:, None, None, :]\n    if len(x.shape) == 3:\n        x = x[:, None, :, :]\n    if len(x.shape) == 4 and x.shape[1] == 3:\n        x = x.permute(0, 3, 1, 2)\n    x = torch.from_numpy(x).to(self.device).float()\n    if self.batch_resize_range is not None:\n        if self.global_step < self.batch_resize_range[0]:\n            return x\n        if self.global_step > self.batch_resize_range[1]:\n            return x\n        x = F.interpolate(x, size=(self.global_step, self.global_step), mode=\\\"bicubic\\\", align_corners=False)\n    return x.detach()\n\n", "CodeBleu_score": 50.491491378936956}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # Retrieve the input data and predictions\n    x = self.get_input(batch, self.image_key)\n\n    x_rec, diff = self(x)\n\n    if self.global_step < self.start_disc_step:\n\n        # Compute autoencoding loss\n        loss = self.loss(x_rec, x, diff)\n\n        self.log(\\\"loss\\\", loss, prog_bar=True, logger=True)\n\n        return loss\n\n    else:\n\n        # Compute discriminator loss\n        loss = self.discriminator_loss(x, x_rec, diff)\n\n        self.log(\\\"disc_loss\\\", loss, prog_bar=True, logger=True)\n\n        return loss\n\n\n", "CodeBleu_score": 21.040727591763265}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n\n    if self.global_step < 25 or random.random() < 0.0001:\n\n        ldm.models.autoencoder.training_step(self, batch, batch_idx, optimizer_idx)\n\n        return\n\n    if optimizer_idx == 0:\n\n        # autoencode\n\n        pred, loss = self(x)\n\n        log_dict = {\\\"autoencode_loss\\\": loss}\n\n        self.log_dict(log_dict, prog_bar=True)\n\n        return loss\n\n    elif optimizer_idx == 1:\n\n        # discriminator\n\n        if self.global_step % self.disc_steps == 0:\n\n            pred, loss = self(x, return_pred_indices=True)\n\n            log_dict = {\\\"discriminator_loss\\\": loss}\n\n            self.log_dict(log_dict, prog_bar=True)\n\n            return loss\n\n\n\n", "CodeBleu_score": 37.09038800844423}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n        if self.global_step % self.ema_update_interval == 0:\n            self.ema_step()\n        if optimizer_idx == 0:\n            # autoencode\n            pred, loss = self(x)\n            log = {\\\"loss\\\": loss}\n            self.log_dict(log, prog_bar=True)\n            return loss\n        elif optimizer_idx == 1:\n            # discriminator\n            fake = self(x, return_pred_indices=True)\n            real = self(x, return_pred_indices=True)\n            loss = self.loss(fake, real)\n            log = {\\\"loss_disc\\\": loss}\n            self.log_dict(log, prog_bar=True)\n            return loss\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()) +\n                                  list(self.quant_conv.parameters()) + list(self.post_quant_conv.parameters()), lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n        if self.scheduler_config is not None:\n            scheduler_ae = instantiate_from_config(self.scheduler_config)\n            scheduler_disc = instantiate_from_config(self.scheduler_config)\n        else:\n            scheduler_ae = None\n            scheduler_disc = None\n        if self.lr_g_factor != 1.0:\n            scheduler_ae = LambdaLR(opt_ae, lr_lambda=lambda step: self.lr_g_factor)\n        return [opt_ae, opt_disc],\n               [scheduler_ae, scheduler_disc]\n\n    def ema_step(self):", "CodeBleu_score": 38.376611392710046}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = batch[self.image_key]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = upper_size\n        else:\n            new_resize = np.random.choice(np.arange(lower_size, upper_size + 16, 16))\n        if new_resize != x.shape[2]:\n            x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n        x = x.detach()\n    if self.use_ema:\n        with self.ema_scope():\n            xrec, _, _ = self(x, return_pred_indices=True)\n    else:\n        xrec, _, _ = self(x, return_pred_indices=True)\n    if self.use_ema:\n        xrec_ema, _, _ = self(x, return_pred_indices=True)\n    if self.hparams.channels == 1:\n        x = x.squeeze(1)\n        xrec = xrec.squeeze(1)\n        if plot_ema:\n            xrec_ema = xrec_ema.squeeze(1)\n    elif self.hparams.channels == 3:\n        x = x.permute(0, 2, 3, 1)\n        xrec = xrec.permute(0, 2, 3, 1)\n        if plot_ema:\n            xrec_ema = xrec_ema.permute(0, 2, 3, 1)\n    else:\n        raise ValueError(\\\"channels should be 1 or 3\\", "CodeBleu_score": 26.05176492878988}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = batch[self.image_key]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = upper_size\n        else:\n            new_resize = np.random.choice(np.arange(lower_size, upper_size + 16, 16))\n        if new_resize != x.shape[2]:\n            x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n        x = x.detach()\n    if self.use_ema:\n        with self.ema_scope():\n            xrec, _, _ = self(x, return_pred_indices=True)\n    else:\n        xrec, _, _ = self(x, return_pred_indices=True)\n    if only_inputs:\n        self.logger.experiment.add_images(f\"{self.log_prefix}/inputs\", x, self.global_step)\n        return\n    if x.shape[1] == 1:\n        x = torch.cat([x, x, x], dim=1)\n    if xrec.shape[1] == 1:\n        xrec = torch.cat([xrec, xrec, xrec], dim=1)\n    self.logger.experiment.add_images(f\"{self.log_prefix}/inputs\", x, self.global_step, dataformats=\"NCHW\")\n    self.logger.experiment.add_images(f\"{self.log_prefix}/reconstructions\", xrec, self.global_step, dataformats=\"NCHW\")\n", "CodeBleu_score": 27.123888650838534}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = batch[self.image_key]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = upper_size\n        else:\n            new_resize = np.random.choice(np.arange(lower_size, upper_size + 16, 16))\n        if new_resize != x.shape[2]:\n            x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n        x = x.detach()\n    if self.do_colorize:\n        x = self.colorize(x)\n    if not only_inputs:\n        if plot_ema:\n            with self.ema_scope():\n                xrec, _, _ = self(x)\n        else:\n            xrec, _, _ = self(x)\n        if self.do_colorize:\n            xrec = self.colorize(xrec)\n        x = torch.cat([x, xrec], dim=0)\n    self.logger.experiment.add_images(f\"{self.image_key}\", x, self.global_step, dataformats=\"NCHW\", **kwargs)\n\n", "CodeBleu_score": 35.33655297933062}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        if mean:\n            return F.l1_loss(pred, target, reduction=\\\"mean\\\")\n        else:\n            return F.l1_loss(pred, target, reduction=\\\"none\\\")\n    elif self.loss_type == \\\"l2\\\":\n        if mean:\n            return F.mse_loss(pred, target, reduction=\\\"mean\\\")\n        else:\n            return F.mse_loss(pred, target, reduction=\\\"none\\\")\n    else:\n        raise ValueError(\\\"Unknown loss type: {}\\\".format(self.loss_type))\n\n", "CodeBleu_score": 28.56082424767909}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean(dim=1).mean(dim=1).mean(dim=1)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean(dim=1).mean(dim=1).mean(dim=1)\n        return loss\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n\n", "CodeBleu_score": 52.699750811552285}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return F.l1_loss(pred, target, reduction='mean')\n        else:\n            return F.l1_loss(pred, target, reduction='none')\n    elif self.loss_type == 'l2':\n        if mean:\n            return F.mse_loss(pred, target, reduction='mean')\n        else:\n            return F.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError('Unknown loss type: {}'.format(self.loss_type))\n\n", "CodeBleu_score": 36.69859710676767}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # get time steps\n    if self.time_embedding_type == \\\"positional\\\":\n        if self.time_embedding_freq_shift == \\\"auto\\\":\n            self.time_embedding_freq_shift = (self.num_timesteps - 1) / 2\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    elif self.time_embedding_type == \\\"learned\\\":\n        t = torch.arange(0, x.shape[0], device=x.device)\n    elif self.time_embedding_type == \\\"none\\\":\n        t = None\n    else:\n        raise ValueError(\\\"time_embedding_type must be one of 'positional', 'learned', or 'none'.\\\")\n\n    # apply conditioning\n    if c is not None:\n        x = self.apply_cond(x, c, t)\n\n    # process inputs\n    x = self.process_input(x, t)\n\n    # compute loss\n    loss = self.compute_loss(x, t)\n\n    return loss\n\n", "CodeBleu_score": 36.238475107423795}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        # Select a random time step for each sample in the batch.\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n    else:\n        # Use the last time step for inference.\n        t = torch.full((x.shape[0],), self.num_timesteps - 1, device=x.device).long()\n\n    # Get the corresponding time step embedding for the selected time steps.\n    emb = self.get_time_embedding(t)\n\n    # Apply conditioning if applicable.\n    if c is not None:\n        emb = emb + self.cond_stage_model.get_cond_embedding(c)\n\n    # Process the inputs according to the model's conditioning requirements.\n    if self.cond_stage_model.conditioning_key is not None:\n        x = {**x, self.cond_stage_model.conditioning_key: emb}\n    else:\n        x = x + emb\n\n    # Perform the forward pass.\n    x = self.forward_stage(x)\n\n    # Compute the loss.\n    loss = self.loss_fn(x, c)\n\n    return loss\n\n", "CodeBleu_score": 52.46493591122843}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.first_stage_in_channels == 1:\n        x = x.repeat(1, 3, 1, 1)\n    elif self.first_stage_in_channels == 3:\n        x = x\n    else:\n        raise ValueError(f'Unsupported in_channels {self.first_stage_in_channels}')\n\n    if self.cond_stage_in_channels == 1:\n        c = c.repeat(1, 3, 1, 1)\n    elif self.cond_stage_in_channels == 3:\n        c = c\n    else:\n        raise ValueError(f'Unsupported in_channels {self.cond_stage_in_channels}')\n\n    if hasattr(self, \\\"split_input_params\\\"):\n        if self.split_input_params[\\\"patch_distributed_vq\\\"]:\n            ks = self.split_input_params[\\\"ks\\\"]  # eg. (128, 128)\n                stride = self.split_input_params[\\\"stride\\\"]  # eg. (64, 64)\n                df = self.split_input_params[\\\"vqf\\\"]\n                uf = self.split_input_params[\\\"vqf\\\"]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\\\"reducing Kernel\\\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\\\"reducing stride\\\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df, uf=uf)\n                x = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n               ", "CodeBleu_score": 25.294529045682463}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape specifying (batch, height, width, channel)\n    # TODO: support for more dim tensors\n    if self.remap is not None:\n        indices = indices.reshape(shape[0], -1)\n        used = self.used.to(indices)\n        match = (indices[:, :, None] == used[None, None, ...]).long()\n        new = match.argmax(-1)\n        unknown = match.sum(2) < 1\n        if self.unknown_index == \"random\":\n            new[unknown] = torch.randint(0, self.re_embed, size=new[unknown].shape).to(device=new.device)\n        else:\n            new[unknown] = self.unknown_index\n        indices = new.reshape(shape)\n    indices = indices.reshape(-1)\n    indices = self.embedding(indices).view(shape)\n    return indices\n\n", "CodeBleu_score": 56.6732498631217}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    z_q = self.embedding(indices)\n    if self.sane_index_shape:\n        z_q = z_q.reshape(indices.shape[0], indices.shape[2], indices.shape[3], -1)\n    if shape is not None:\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 67.99784909925249}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    z_q = self.embedding(indices)\n    if self.sane_index_shape:\n        z_q = z_q.reshape(shape)\n    return z_q\n\n", "CodeBleu_score": 34.59159583952212}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n    if factor < 0:\n        m, n = dimension, 1\n        while m > n:\n            if dimension % m == 0:\n                n = dimension // m\n                if m <= n:\n                    return m, n\n            m -= 1\n    raise ValueError(f\"No factorization found for dimension {dimension} with factor {factor}\")\n\n", "CodeBleu_score": 37.75692659416517}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        raise ValueError(f\"The specified factor {factor} does not divide the dimension {dimension}.\")\n    elif factor < 0:\n        m, n = 1, dimension\n        while True:\n            if m * n == dimension:\n                return m, n\n            elif m * n > dimension:\n                m += 1\n            else:\n                n += 1\n    else:\n        raise ValueError(\"The factor must be either positive or negative.\")\n\n", "CodeBleu_score": 39.9822836690648}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        return factorization(dimension, factor - 1)\n    else:\n        m, n = 1, dimension\n        while m < n:\n            m += 1\n            if dimension % m == 0:\n                n = dimension // m\n        return m, n\n\n", "CodeBleu_score": 38.01140504703271}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n    combined_weight = updown.to(device, dtype) + orig_weight.to(device, dtype)\n    combined_weight = combined_weight / torch.norm(combined_weight)\n    combined_weight = combined_weight * self.scale\n    adjusted_weight = combined_weight - orig_weight\n    return adjusted_weight\n\n", "CodeBleu_score": 10.331331208808402}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown:\n        if self.dora_scale is not None:\n            if self.dora_scale.device != orig_weight.device:\n                self.dora_scale = self.dora_scale.to(orig_weight.device)\n            if self.dora_scale.dtype != orig_weight.dtype:\n                self.dora_scale = self.dora_scale.to(orig_weight.dtype)\n            return orig_weight - self.dora_scale * (self.calc_scale() * self.sd_module.weight.data)\n        else:\n            return orig_weight - self.calc_scale() * self.sd_module.weight.data\n    else:\n        if self.dora_scale is not None:\n            if self.dora_scale.device != orig_weight.device:\n                self.dora_scale = self.dora_scale.to(orig_weight.device)\n            if self.dora_scale.dtype != orig_weight.dtype:\n                self.dora_scale = self.dora_scale.to(orig_weight.dtype)\n            return orig_weight + self.dora_scale * (self.calc_scale() * self.sd_module.weight.data)\n        else:\n            return orig_weight + self.calc_scale() * self.sd_module.weight.data\n\n", "CodeBleu_score": 21.175121116557566}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    orig_weight = orig_weight.to(updown.weight.device)\n    orig_weight = orig_weight.to(updown.weight.dtype)\n    combined_weight = updown.weight + orig_weight\n    combined_weight = combined_weight / torch.norm(combined_weight)\n    combined_weight = combined_weight * self.scale\n    adjusted_weight = combined_weight - orig_weight\n    return adjusted_weight\n\n", "CodeBleu_score": 19.30551432178003}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    orig_weight = orig_weight.to(updown.dtype)\n    if ex_bias is not None:\n            ex_bias = ex_bias.to(updown.dtype)\n        else:\n            ex_bias = None\n\n        if ex_bias is not None:\n            updown = updown + ex_bias\n\n        if orig_weight.numel() == updown.numel():\n            updown = updown.reshape(orig_weight.shape)\n        else:\n            updown = updown.reshape(output_shape)\n\n        if self.scale is not None:\n            updown = updown * self.scale\n\n        if self.dora_scale is not None:\n            updown = self.apply_weight_decompose(updown, orig_weight)\n\n        if ex_bias is not None:\n            ex_bias = ex_bias * self.scale\n\n        return updown, ex_bias\n\n", "CodeBleu_score": 57.879942450425204}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n            ex_bias = ex_bias.to(updown.dtype)\n\n        if self.bias is not None:\n            updown = updown + self.bias\n\n        if ex_bias is not None:\n            updown = updown + ex_bias\n\n        if self.scale is not None:\n            updown = updown * self.scale\n\n        if self.dim is not None and self.alpha is not None:\n            updown = updown * self.alpha / self.dim\n\n        if self.dora_scale is not None:\n            updown = self.apply_weight_decompose(updown, orig_weight)\n\n        if updown.shape != output_shape:\n            updown = updown.reshape(output_shape)\n        elif updown.numel() == orig_weight.numel():\n            updown = updown.reshape(orig_weight.shape)\n\n        return updown, ex_bias\n\n", "CodeBleu_score": 56.993979351150074}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Adjust the updown tensor with the bias and reshape it according to the output shape\n    if ex_bias is not None:\n            updown = updown + ex_bias\n        updown = updown.reshape(output_shape)\n\n        # If the original and updown weights have the same number of elements,\n        # reshape updown to match the original weight's shape\n        if orig_weight.numel() == updown.numel():\n            updown = updown.reshape(orig_weight.shape)\n\n        # If an additional bias is provided, apply a multiplier to it\n        if self.bias is not None:\n            updown = updown * self.bias\n\n        # If a scaling factor is defined, apply a weight decomposition to updown\n        if self.scale is not None:\n            updown = updown * self.scale\n\n        return updown, self.bias\n\n    def get_extra_kwargs(self):\n        return self.extra_kwargs\n\n    def get_ops(self):\n        return self.ops\n\n    def get_weight_shape(self):\n        return self.shape\n\n    def get_weight_name(self):\n        return self.sd_key\n\n    def get_weight_sd_key(self):\n        return self.sd_key\n\n    def get_weight_sd_module(self):\n        return self.sd_module\n\n    def get_network(self):\n        return self.network\n\n    def get_network_key(self):\n        return self.network_key\n\n    def get_network_name(self):\n        return self.network.name\n\n    def get_network_on_disk(self):\n        return self.network.network_on_disk\n\n    def get_network_on_disk_name(self):\n        return self.network.network_on_disk.name\n\n    def get_network_on_disk_filename(self):\n", "CodeBleu_score": 43.18963308086746}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n        self.ex_bias = self.ex_bias.to(orig_weight.device)\n\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 38.861746779921816}, {"code": "def calc_updown(self, orig_weight)\n\t\n    orig_weight = orig_weight.to(self.weight.device)\n    self.weight = self.weight.to(orig_weight.device)\n    if self.ex_bias is not None:\n        self.ex_bias = self.ex_bias.to(orig_weight.device)\n    return self.calc_updown_final(orig_weight)\n\n", "CodeBleu_score": 44.076625567035705}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = orig_weight.to(self.weight.device)\n    if self.ex_bias is not None:\n        bias = self.ex_bias.to(weight.device)\n    else:\n        bias = None\n    return self.calc_updown_finalize(weight, bias)\n\n", "CodeBleu_score": 45.56285776207991}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global loaded_networks\n    if len(loaded_networks) <= shared.opts.max_networks_in_memory:\n        return\n\n    logging.info(f\\\"Purging networks from memory because we have {len(loaded_networks)} loaded networks and the maximum is {shared.opts.max_networks_in_memory}\\\")\n\n    loaded_networks = [net for net in loaded_networks if net.is_loaded()]\n\n    if len(loaded_networks) <= shared.opts.max_networks_in_memory:\n        return\n\n    # remove networks until we're within the limit\n    loaded_networks.sort(key=lambda x: x.mtime)\n    while len(loaded_networks) > shared.opts.max_networks_in_memory:\n        loaded_networks.pop(0)\n\n    # trigger garbage collection to free up memory\n    gc.collect()\n\n\n", "CodeBleu_score": 32.36325736616574}, {"code": "def purge_networks_from_memory()\n\t\n    global loaded_networks\n    if len(loaded_networks) <= shared.opts.max_loaded_networks:\n        return\n\n    logging.info(f\\\"Purging networks from memory, {len(loaded_networks)} > {shared.opts.max_loaded_networks}\\\")\n\n    while len(loaded_networks) > shared.opts.max_loaded_networks:\n        if shared.opts.max_loaded_networks == 0:\n            break\n\n        # remove the oldest network\n        oldest_network = min(loaded_networks.values(), key=lambda x: x.mtime)\n        loaded_networks.pop(oldest_network.filename)\n\n    # trigger garbage collection to free up memory\n    gc.collect()\n\n\n", "CodeBleu_score": 33.24729942193338}, {"code": "def purge_networks_from_memory()\n\t\n    if len(shared.loaded_networks) > shared.max_loaded_networks:\n        while len(shared.loaded_networks) > shared.max_loaded_networks:\n            for key, net in shared.loaded_networks.items():\n                del net\n                break\n\n        gc.collect()\n\n", "CodeBleu_score": 29.03610375091294}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    backup_weights = getattr(self, \\\"backup_weights\\\", None)\n    backup_bias = getattr(self, \\\"backup_bias\\\", None)\n\n    if backup_weights is not None:\n        self.weight.data.copy_(backup_weights)\n    if backup_bias is not None:\n        self.bias.data.copy_(backup_bias)\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        backup_projection_weights = getattr(self, \\\"backup_projection_weights\\\", None)\n        backup_projection_bias = getattr(self, \\\"backup_projection_bias\\\", None)\n\n        if backup_projection_weights is not None:\n            self.q_proj.weight.data.copy_(backup_projection_weights)\n        if backup_projection_bias is not None:\n            self.q_proj.bias.data.copy_(backup_projection_bias)\n\n        if backup_projection_weights is not None:\n            self.k_proj.weight.data.copy_(backup_projection_weights)\n        if backup_projection_bias is not None:\n            self.k_proj.bias.data.copy_(backup_projection_bias)\n\n        if backup_projection_weights is not None:\n            self.v_proj.weight.data.copy_(backup_projection_weights)\n        if backup_projection_bias is not None:\n            self.v_proj.bias.data.copy_(backup_projection_bias)\n\n        if backup_projection_weights is not None:\n            self.out_proj.weight.data.copy_(backup_projection_weights)\n        if backup_projection_bias is not None:\n            self.out_proj.bias.data.copy_(backup_projection_bias)\n\n    # If the layer has no bias, set it to None.\n    if not hasattr(self, \\\"bias\\\") or self.bias is None:\n", "CodeBleu_score": 42.98652009271335}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    backup_weights_key = f'{self.name}_backup_weights'\n    backup_biases_key = f'{self.name}_backup_biases'\n    if backup_weights_key in self.backup_weights:\n        self.weight.data = self.backup_weights[backup_weights_key]\n    if backup_biases_key in self.backup_weights:\n        if self.bias is not None:\n            self.bias.data = self.backup_weights[backup_biases_key]\n        else:\n            self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.q_proj.weight.data = self.backup_weights[f'{self.name}_q_proj_backup_weights']\n        self.k_proj.weight.data = self.backup_weights[f'{self.name}_k_proj_backup_weights']\n        self.v_proj.weight.data = self.backup_weights[f'{self.name}_v_proj_backup_weights']\n        self.o_proj.weight.data = self.backup_weights[f'{self.name}_o_proj_backup_weights']\n        if self.bias is not None:\n            self.q_proj.bias.data = self.backup_weights[f'{self.name}_q_proj_backup_biases']\n            self.k_proj.bias.data = self.backup_weights[f'{self.name}_k_proj_backup_biases']\n            self.v_proj.bias.data = self.backup_weights[f'{self.name}_v_proj_backup_biases']\n            self.o_proj.bias.data = self.backup_weights[f'{self.name}_o_proj_backup_biases']\n        else:\n            self.q_proj.bias = None\n            self.k_proj.bias = None\n            self.v_proj.bias = None\n            self.o_proj.bias = None\n\n\n", "CodeBleu_score": 34.64816034825965}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_w is not None:\n        self.weight.data = self.backup_w\n        self.backup_w = None\n    if self.backup_b is not None:\n        self.bias.data = self.backup_b\n        self.backup_b = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_proj_w is not None:\n            self.in_proj_weight.data = self.backup_proj_w\n            self.backup_proj_w = None\n        if self.backup_proj_b is not None:\n            self.in_proj_bias.data = self.backup_proj_b\n            self.backup_proj_b = None\n        if self.backup_out_proj_w is not None:\n            self.out_proj.weight.data = self.backup_out_proj_w\n            self.backup_out_proj_w = None\n        if self.backup_out_proj_b is not None:\n            self.out_proj.bias.data = self.backup_out_proj_b\n            self.backup_out_proj_b = None\n    if self.backup_proj_w is not None:\n        self.proj_weight.data = self.backup_proj_w\n        self.backup_proj_w = None\n    if self.backup_proj_b is not None:\n        self.proj_bias.data = self.backup_proj_b\n        self.backup_proj_b = None\n\n", "CodeBleu_score": 33.082345377281264}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    network_layer_name = getattr(org_module, 'network_layer_name', None)\n    if network_layer_name is None:\n        return original_forward(input)\n\n    input = input.to(devices.cpu)\n    network_restore_weights_from_backup(org_module)\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(network_layer_name, None)\n        if module is not None:\n            try:\n                output = module.forward(output)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n            continue\n\n        module_q = net.modules.get(network_layer_name + \"_q_proj\", None)\n        module_k = net.modules.get(network_layer_name + \"_k_proj\", None)\n        module_v = net.modules.get(network_layer_name + \"_v_proj\", None)\n        module_out = net.modules.get(network_layer_name + \"_out_proj\", None)\n\n        if isinstance(org_module, torch.nn.MultiheadAttention) and module_q and module_k and module_v and module_out:\n            try:\n                qw, kw, vw = org_module.in_proj_weight.chunk(3, 0)\n                qw = module_q.forward(qw)\n                kw = module_k.forward(kw)\n                vw = module_v.forward(vw)\n                del qw, kw, vw\n                org_module.in_proj_weight = torch.cat([qw, kw, vw], dim=0)\n                output = module_out.forward(output)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n               ", "CodeBleu_score": 35.5669138017222}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    global loaded_networks\n    if not loaded_networks:\n        return original_forward(org_module, input)\n\n    input = input.to(devices.device, copy=False)\n    network_restore_weights_from_backup(org_module)\n    output = original_forward(org_module, input)\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad():\n                    if getattr(org_module, 'fp16_weight', None) is None:\n                        weight = org_module.weight\n                        bias = org_module.bias\n                    else:\n                        weight = org_module.fp16_weight.clone().to(org_module.weight.device)\n                        bias = getattr(org_module, 'fp16_bias', None)\n                        if bias is not None:\n                            bias = bias.clone().to(org_module.bias.device)\n                    updown, ex_bias = module.calc_updown(weight)\n\n                    if len(weight.shape) == 4 and weight.shape[1] == 9:\n                        # inpainting model. zero pad updown to make channel[1]  4 to 9\n                        updown = torch.nn.functional.pad(updown, (0, 0, 0, 0, 0, 5))\n\n                    org_module.weight.copy_((weight.to(dtype=updown.dtype) + updown).to(dtype=org_module.weight.dtype))\n                    if ex_bias is not None and hasattr(org_module, 'bias'):\n                        if org_module.bias is None:\n                            org_module.bias = torch.nn.Parameter(ex_bias).to(org_module.weight.dtype)\n                        else:\n                            org_module.bias.copy_((bias + ex_bias).to(dtype=org_module.bias.dtype))\n            except RuntimeError as e:\n               ", "CodeBleu_score": 38.170509019478914}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not isinstance(org_module, torch.nn.Module):\n        return original_forward(input)\n\n    if not hasattr(org_module, 'network_current_names'):\n        return original_forward(input)\n\n    input = input.to(devices.device)\n\n    network_restore_weights_from_backup(org_module)\n\n    output = original_forward(input)\n\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad():\n                    if getattr(org_module, 'fp16_weight', None) is None:\n                        weight = org_module.weight\n                        bias = org_module.bias\n                    else:\n                        weight = org_module.fp16_weight.clone().to(org_module.weight.device)\n                        bias = getattr(org_module, 'fp16_bias', None)\n                        if bias is not None:\n                            bias = bias.clone().to(org_module.bias.device)\n                    output = module.apply_forward(output, weight, bias)\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n            continue\n\n        module_q = net.modules.get(org_module.network_layer_name + \"_q_proj\", None)\n        module_k = net.modules.get(org_module.network_layer_name + \"_k_proj\", None)\n        module_v = net.modules.get(org_module.network_layer_name + \"_v_proj\", None)\n        module_out = net.modules.get(org_module.network_layer_name + \"_out_proj\", None)\n\n        if isinstance(org_module, torch.nn.MultiheadAttention) and module_q and module_", "CodeBleu_score": 30.71752961358697}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainable_params.append(layer.weight)\n            trainable_params.append(layer.bias)\n    return trainable_params\n\n", "CodeBleu_score": 55.957087935122054}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    shared.hypernetworks.clear()\n    for i, name in enumerate(names):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.multiplier = multipliers[i]\n            shared.hypernetworks.append(hypernetwork)\n\n\n\n\n\n", "CodeBleu_score": 34.57177298872204}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    # Check for already loaded hypernetworks\n    if shared.hypernetworks:\n        clear_hypernetworks()\n\n    # Load or retrieve each hypernetwork by name, setting its multiplier if provided\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is None:\n            return None\n        hypernetwork.multiplier = multiplier\n        shared.hypernetworks.append(hypernetwork)\n\n    # Append each loaded hypernetwork to the shared list\n    return shared.hypernetworks\n", "CodeBleu_score": 27.64216791820874}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if shared.hypernetworks is None:\n        shared.hypernetworks = []\n    else:\n        shared.hypernetworks.clear()\n\n    for name in names:\n        if multipliers is not None:\n            multiplier = multipliers[name]\n        else:\n            multiplier = 1\n\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            hypernetwork.multiplier = multiplier\n            shared.hypernetworks.append(hypernetwork)\n", "CodeBleu_score": 36.08202556082455}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 78.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context[0]\n    context_v = context[1]\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 54.55620260953854}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 68.4402416707376}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    ## x: (batch, channels, height, width)\n    ## context: (batch, channels, height, width)\n    ## mask: (batch, 1, height, width)\n    ## kwargs: {}\n    ## returns: (batch, channels, height, width)\n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n    ## \n   ", "CodeBleu_score": 1.9834764933446987}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    if self.hypernetworks is not None:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context, self)\n    else:\n        context_k = context\n        context_v = context\n\n    if self.use_scale_t:\n        scale_t = self.scale_t\n    else:\n        scale_t = None\n\n    query = self.query(x)\n    key = self.key(context_k)\n    value = self.value(context_v)\n\n    query = self.reshape_heads_to_batch_dim(query)\n    key = self.reshape_heads_to_batch_dim(key)\n    value = self.reshape_heads_to_batch_dim(value)\n\n    if mask is not None:\n        mask = self.reshape_heads_to_batch_dim(mask)\n\n    if self.is_cross_attention:\n        attn = self.attn(query, key, value, mask=mask, scale_t=scale_t)\n    else:\n        attn = self.attn(query, key, value, mask=mask)\n\n    attn = self.reshape_batch_dim_to_heads(attn)\n\n    attn = self.out(attn)\n    return attn\n\n", "CodeBleu_score": 34.16666947195333}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[0](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[1](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[2](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[3](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[4](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[5](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[6](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[7](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[8](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[9](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[10](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[11](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[12](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[13](context)\n\n    if self.use_hypernetworks:\n        if context is not None:\n            context = self.hypernetworks[14](", "CodeBleu_score": 17.87249166238353}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_state = hypernetwork.__getstate__()\n    original_checkpoint_state = checkpoint.__getstate__()\n    original_hypernetwork_name = hypernetwork.name\n    original_checkpoint_name = checkpoint.name\n    try:\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.checkpoint_name = checkpoint.name\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.__setstate__(original_hypernetwork_state)\n        checkpoint.__setstate__(original_checkpoint_state)\n        hypernetwork.name = original_hypernetwork_name\n        checkpoint.name = original_checkpoint_name\n        raise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.02922985477216}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        original_attributes = {attr: getattr(hypernetwork, attr) for attr in hypernetwork.__dict__.keys()}\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except Exception as e:\n        for attr, value in original_attributes.items():\n            setattr(hypernetwork, attr, value)\n        raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 31.435806802452106}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        # Backup original attributes.\n        original_attributes = {key: getattr(hypernetwork, key) for key in hypernetwork.__dict__.keys()}\n        hypernetwork.checkpoint_name = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore original attributes.\n        for key, value in original_attributes.items():\n            setattr(hypernetwork, key, value)\n        raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 31.56437814675065}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and context is not None:\n        with torch.no_grad():\n            ema_params = {\n                key: value.clone()\n                for (key, value) in self.model_ema.state_dict().items()\n            }\n            self.model_ema.load_state_dict(self.model.state_dict())\n            if self.monitor is not None:\n                self.monitor.log_ema_weights(context)\n        yield\n        with torch.no_grad():\n            self.model.load_state_dict(ema_params)\n            if self.monitor is not None:\n                self.monitor.log_restore_weights(context)\n    else:\n        yield\n\n", "CodeBleu_score": 44.642820985672756}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n            yield\n            return\n        if context is None:\n            context = {}\n        original_params = self.model.parameters()\n        original_buffers = self.model.buffers()\n        original_state_dict = self.model.state_dict()\n        self.model_ema.load_state_dict(original_state_dict)\n        self.model.load_state_dict(self.model_ema.state_dict())\n        if self.monitor is not None:\n            self.monitor.log_ema_switch(context)\n        try:\n            yield\n        finally:\n            self.model.load_state_dict(original_state_dict)\n            if self.monitor is not None:\n                self.monitor.log_ema_restore(context)\n\n\n    def _get_loss(self, batch, batch_idx, mode=\\\"train\\\", context=None):\n        if self.use_ema and mode == \\\"train\\\":\n            with ema_scope(self, context):\n                return self._get_loss(batch, batch_idx, mode=mode, context=context)\n\n        if self.parameterization == \\\"eps\\\":\n            return self._get_loss_eps(batch, batch_idx, mode=mode, context=context)\n        elif self.parameterization == \\\"x0\\\":\n            return self._get_loss_x0(batch, batch_idx, mode=mode, context=context)\n        else:\n            raise NotImplementedError(\\\"mu not supported\\\")\n\n    def _get_loss_eps(self, batch, batch_idx, mode=\\\"train\\\", context=None):\n        if self.use_ema and mode == \\\"train\\\":\n            with ema_scope(self, context):\n                return self._get_loss_eps(batch, batch_idx, mode=mode, context=context)\n\n        if self.use_ema and mode == \\\"val\\\":\n            with ema_scope(self, context):\n                return self._get_loss_eps(batch, batch_idx", "CodeBleu_score": 31.390663071655634}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n            context = {}\n        if self.use_ema:\n            # Store current model parameters\n            original_params = {k: v.clone() for k, v in self.model.named_parameters()}\n            # Copy EMA weights to the model\n            self.model_ema.load_state_dict(self.model.state_dict())\n            # Log the EMA weights switch\n            if self.monitor is not None:\n                self.monitor.log_ema_switch(self.model_ema)\n\n            # Execute the code block\n            yield\n\n            # Restore original model parameters\n            self.model.load_state_dict(original_params)\n            # Log the EMA weights restoration\n            if self.monitor is not None:\n                self.monitor.log_ema_restoration(self.model_ema)\n        else:\n            yield\n\n    def update_ema(self, model_params, ema_params, ema_step):\n        \"\"\"This function updates the EMA weights of the model using the given model parameters and EMA step.", "CodeBleu_score": 36.001110342520725}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert x.shape == t.shape\n\n    model_mean, _, model_log_variance = self.q_mean_variance(x, t)\n    if clip_denoised:\n        x_recon = self.predict_start_from_noise(x, t=t, noise=model_log_variance.div(2).exp() * torch.randn_like(x))\n        model_mean = self.predict_start_from_noise(x_recon, t=t, noise=torch.zeros_like(model_log_variance))\n\n    return model_mean, model_log_variance\n\n\n\n", "CodeBleu_score": 37.68375061151442}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.parameterization == \"eps\":\n        model_var = torch.Tensor([self.model.var_eps]).to(x.device)\n        model_mean, _, model_log_variance = self.q_posterior(x_start=x, x_t=model_output, t=t)\n    elif self.parameterization == \"x0\":\n        model_var = torch.Tensor([self.model.var_x0]).to(x.device)\n        model_mean, _, model_log_variance = self.q_posterior(x_start=model_output, x_t=x, t=t)\n    else:\n        raise NotImplementedError(\"mu not supported\")\n\n    if clip_denoised:\n        x_recon = model_mean.clamp(-1, 1)\n    else:\n        x_recon = model_mean\n    return model_mean, model_var, model_log_variance, x_recon\n\n", "CodeBleu_score": 49.70504564449946}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.parameterization == \\\"eps\\\":\n        model_mean, _, model_log_variance = self.q_posterior(x, x, t)\n        model_variance = torch.exp(model_log_variance)\n    elif self.parameterization == \\\"x0\\\":\n        model_mean, model_variance, model_log_variance = self.q_posterior(x, x, t)\n    else:\n        raise NotImplementedError(\\\"Unknown parameterization\\\")\n\n    if clip_denoised:\n        x_recon = torch.clamp(model_mean, -1, 1)\n    else:\n        x_recon = model_mean\n\n    return model_mean, model_variance, model_log_variance\n\n\n\n", "CodeBleu_score": 41.38154445229715}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        loss = F.l1_loss(pred, target, reduction=\\\"none\\\")\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == \\\"l2\\\":\n        if mean:\n            return F.mse_loss(pred, target)\n        else:\n            return F.mse_loss(pred, target, reduction=\\\"none\\\")\n    else:\n        raise NotImplementedError(\\\"Unknown loss type: {}\\\".format(self.loss_type))\n\n\n\n", "CodeBleu_score": 35.83576922697685}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        return torch.abs(pred - target).mean() if mean else torch.abs(pred - target)\n    elif self.loss_type == \\\"l2\\\":\n        return F.mse_loss(pred, target, reduction=\\\"mean\\\" if mean else \\\"none\\\")\n    else:\n        raise ValueError(f\\\"Unknown loss type: {self.loss_type}\\\")\n\n", "CodeBleu_score": 25.688673303016387}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        loss = F.l1_loss(pred, target, reduction=\\\"none\\\")\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == \\\"l2\\\":\n        return F.mse_loss(pred, target, reduction=\\\"none\\\")\n    else:\n        raise NotImplementedError(f\\\"{self.loss_type} is not a valid loss type.\\\")\n\n\n\n", "CodeBleu_score": 33.40643230153483}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=x_start.device, dtype=torch.long)\n    model_output = self.model(x_start, t)\n    mean, log_var = model_output[:2]\n    kl = gaussian_kl(mean, log_var)\n    kl = kl.sum(dim=1)\n    bpd = -1 * kl / np.log(2)\n    return bpd\n\n", "CodeBleu_score": 27.167506808790904}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=self.device, dtype=torch.long)\n    mean, log_variance = self.q_posterior_mean_variance(x_start, t)\n    return -0.5 * torch.sum(1.0 + log_variance - mean ** 2 - log_variance.exp()) / np.log(2.0)\n\n", "CodeBleu_score": 29.162838671236525}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b, *_, device, dtype = *x_start.shape, x_start.device, x_start.dtype\n    t = torch.ones(b, device=device, dtype=dtype)\n    model_output = self.model(x_start, t)\n    if isinstance(model_output, tuple):\n        model_output = model_output[0]\n    mean, log_var = model_output.chunk(2, dim=-1)\n    mean_of_mean = mean.mean(dim=0)\n    mean_of_log_var = log_var.mean(dim=0)\n    mean_of_log_var = 0.5 * mean_of_log_var\n    kl = mean_of_log_var - mean_of_mean.pow(2) - mean_of_log_var.exp()\n    kl = kl.sum(dim=-1)\n    bpd = kl.div(-np.log(2)).mean()\n    return bpd\n\n", "CodeBleu_score": 32.917885450847486}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    if subsection not in caches:\n        caches[subsection] = diskcache.Cache(cache_dir, size_limit=4e9, disk_min_file_size=256e3, eviction_policy=\\\"least-recently-used\\\", eviction_ttl=3600, eviction_trigger=1000, stale_index_ttl=3600, stale_if_error=True, timeout=60, shards=1, subdirs=True, disk_strict=True, disk_pickle=True, disk_compress=True, disk_compress_level=9, disk_compress_max=100000000, disk_compress_threshold=1000000, disk_compress_min=1000000, disk_compress_max_age=3600, disk_compress_max_size=100000000, disk_compress_max_chunk=100000000, disk_compress_max_queue=1000000, disk_compress_max_workers=100, disk_compress_max_threads=100, disk_compress_max_processes=100, disk_compress_max_processes_per_worker=100, disk_compress_max_threads_per_worker=100, disk_compress_max_processes_per_worker_per_worker=100, disk_compress_max_threads_per_worker_per_worker=100, disk_compress_max_processes_per_worker_per_worker_per_worker=100, disk_compress_max_threads_per_worker_per_worker_per_worker=100, disk_compress_max_processes_per_worker_per_worker_per_worker_per_worker=100, disk_compress_max_threads_per_worker_per_worker_per_worker_per_worker=100, disk_compress_max_process", "CodeBleu_score": 35.63076545284488}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    if subsection not in caches:\n        caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4e9, disk_min_file_size=256e3)\n    return caches[subsection]\n\n", "CodeBleu_score": 51.61372067925145}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(cache_dir, subsection)\n    os.makedirs(cache_path, exist_ok=True)\n    return diskcache.Cache(cache_path, size_limit=4e9, eviction_policy=\\\"lfu\\\", disk_min_file_size=256e3)\n\n", "CodeBleu_score": 47.30808810900524}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj", "CodeBleu_score": 67.25921240957332}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.mkdir(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 67.39628028529373}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 67.25921240957332}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if cache_obj.get(title) and cache_obj[title][\"mtime\"] >= mtime:\n        return cache_obj[title][\"data\"]\n    data = func()\n    cache_obj[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 23.4376732866895}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache_obj:\n        if cache_obj[title][\"mtime\"] < mtime:\n            del cache_obj[title]\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = {\"data\": data, \"mtime\": mtime}\n        dump_cache()\n    return cache_obj[title][\"data\"]", "CodeBleu_score": 24.689156376782893}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache_obj:\n        if cache_obj[title][\"mtime\"] < mtime:\n            del cache_obj[title]\n\n    if title not in cache_obj:\n        cache_obj[title] = {\"mtime\": mtime, \"data\": func()}\n        dump_cache()\n\n    return cache_obj[title][\"data\"]\n\n", "CodeBleu_score": 22.247785538982782}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in textwrap.wrap(message, width=70):\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for line in get_exceptions():\n            print(f\"--- {line}\", file=sys.stderr)\n\n", "CodeBleu_score": 43.9701142261332}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for line in traceback.format_exc().splitlines():\n            print(f\"    {line}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 56.18109894329668}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in textwrap.wrap(message, width=70):\n        sys.stderr.write(f\"*** {line}", "CodeBleu_score": 22.56628797537189}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(\n        style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(\n        style.negative_prompt, negative_prompt)\n\n    if positive_extracted and negative_extracted:\n        return True, positive_prompt, negative_prompt\n\n    return False, prompt, negative_prompt\n\n", "CodeBleu_score": 54.304388313404594}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    if style.prompt:\n        prompt_success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n\n    if style.negative_prompt:\n        negative_prompt_success, negative_prompt = extract_style_text_from_prompt(\n            style.negative_prompt, negative_prompt\n        )\n\n    if not prompt_success or not negative_prompt_success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 67.8683403649249}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    extracted_prompt, extracted_negative_prompt = False, False\n    if style.prompt is not None:\n        extracted_prompt, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    if style.negative_prompt is not None:\n        extracted_negative_prompt, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    return extracted_prompt and extracted_negative_prompt, prompt, negative_prompt\n\n", "CodeBleu_score": 55.293081416282774}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 64.07063310277653}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be True.\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 86.43951469229563}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be True.\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 86.43951469229563}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set. Please choose only one.\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 86.0442806522079}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        _device = device\n\n    _model = SpeakerEncoder(_device)\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint['model_state'])\n    _model.eval()\n    print('Loaded encoder \"%s\" trained to step %d' % (weights_fpath.name, checkpoint['step']))\n\n", "CodeBleu_score": 60.66105968889691}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded encoder \\\"{}\\\" trained to step {}\".format(weights_fpath.name, _model._step))\n    _device = device\n\n", "CodeBleu_score": 49.17743764257001}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    global _device\n    if device is None:\n        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    elif device == 'cpu':\n        _device = torch.device('cpu')\n    else:\n        raise Exception('Specified device is not supported: ' + device)\n    _model = SpeakerEncoder(_device)\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint['model_state'])\n    _model.eval()\n    print('Loaded encoder \\\"{}\\\" trained to step {}'.format(weights_fpath.name, checkpoint['step']))\n\n", "CodeBleu_score": 58.07754794528315}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if not is_loaded():\n        raise Exception(\\\"The speaker encoder module has not been loaded yet. Call load_model() first.\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 60.829542628377695}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\\\"Model has not been loaded. Call load_model() first.\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed\n\n\n\n\n", "CodeBleu_score": 69.1980825972212}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\\\"You must load a model before you can embed frames.\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames_batch).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 66.74732163872005}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be a float within the range [0.0, 1.0]\"\n    assert 0 < overlap < 1.0, \"overlap must be a float within the range (0.0, 1.0)\"\n\n    # Compute how many frames will result in exactly partial_utterance_n_frames if we include\n    # overlap. This also serves as a lower bound on the number of frames needed in the wav\n    # file.\n    min_required_n_frames = int(np.ceil(\n        (1 + overlap) * partial_utterance_n_frames / frame_step\n    ))\n\n    # Compute how many frames there are in the wav file (this code assumes the wav file was\n    # correctly read by scipy.io.wavefile.read).\n    assert n_samples % sample_rate == 0, \"n_samples must be a multiple of sample_rate\"\n    n_frames = n_samples // sample_rate\n\n    # The minimum number of frames needed is min_required_n_frames. We now figure out how many\n    # frames we can include without going over this.\n    n_frames_available = n_frames\n    n_frames_take = min_required_n_frames\n    while n_frames_take <= n_frames_available:\n        n_frames_available -= n_frames_take\n        n_frames_take = int(np.ceil(n_frames_take * overlap))\n\n    # Now we know how many frames we want to have in our final utterance. Compute how many\n    # frames we should get from this and how many we should discard from the start.\n    n_pad_start = (n_frames_available + n_frames_take - min_required_n_frames) // 2\n    n_pad_end = n_frames_available - n_pad_start\n\n    # If there aren't enough frames of wav left for two partial utterances plus padding, we\n    # are done here.\n    if n_pad_start + min_required_n_frames + n_pad_end > n", "CodeBleu_score": 22.102782109803133}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1.0\n    assert 0 <= overlap <= 1.0\n    assert partial_utterance_n_frames > 0\n\n    # Compute how many frames the partial utterance should cover\n    total_frames = int(round(n_samples / params.hop_length))\n    total_frames = max(0, total_frames - params.utterance_embedding_duration)\n    total_frames = min(total_frames, params.max_partial_utterance_length)\n    frames_per_step = int(round(params.hop_length * (1 - overlap)))\n\n    # Compute how many sub-utterances (i.e. partial utterances) can fit in the utterance\n    n_steps = int(math.ceil(total_frames / float(partial_utterance_n_frames)))\n    if n_steps > 1:\n        # Compute how many frames will be used in each partial utterance\n        frames_per_utternace = int(math.ceil(total_frames / float(n_steps)))\n        # Compute how many frames will be covered by the first partial utterance\n        frames_coverage = frames_per_utternace * overlap\n        # Compute how many frames will be needed for the last partial utterance\n        frames_pad = frames_per_utternace + partial_utterance_n_frames - frames_coverage\n        # Re-adjust the number of frames per utterance to cover at least min_pad_coverage\n        # of the last partial utterance\n        frames_per_utternace = int(\n            math.ceil(frames_per_utternace * min_pad_coverage))\n        if frames_pad < frames_per_utternace:\n            frames_per_utternace = int(frames_per_utternace - frames_pad)\n        if frames_per_utternace < 1:\n            frames_per_utternace = 1\n    else:\n        frames_per_utternace = total_frames\n\n    # Compute start and end sample indexes\n    start_sample = 0\n    wav_slices = []\n    mel_slices = []\n", "CodeBleu_score": 31.00236048745637}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1.0, 'min_pad_coverage must be a float within the range [0.0, 1.0]'\n    assert 0 <= overlap <= 1.0, 'overlap must be a float within the range [0.0, 1.0]'\n\n    # Compute how many frames will result in the desired number of partial utterances\n    n_frames_step = int(frame_length * (1 - overlap))\n    assert partial_utterance_n_frames % n_frames_step == 0, \\\n        'partial_utterance_n_frames must be a multiple of (frame_length * (1 - overlap))'\n    n_partial_utterances = partial_utterance_n_frames // n_frames_step\n\n    # Compute how many audio samples will result in the desired number of partial utterances\n    samples_per_partial = int(n_samples / n_partial_utterances)\n\n    # Compute how many audio samples will result in the desired number of partial utterances with a padding of min_pad_coverage\n    min_pad_coverage_samples = int(samples_per_partial * min_pad_coverage)\n\n    # Compute how many audio samples will result in the desired number of partial utterances with a padding of max_pad_coverage\n    max_pad_coverage_samples = int(samples_per_partial * (1 + max_pad_coverage))\n\n    # Compute how many audio samples will result in the desired number of partial utterances with a padding of min_pad_coverage\n    min_pad_coverage_n_frames = int(min_pad_coverage_samples / frame_length)\n\n    # Compute how many audio samples will result in the desired number of partial utterances with a padding of max_pad_coverage\n    max_pad_coverage_n_frames = int(max_pad_coverage_samples / frame_length)\n\n    # Compute the number of frames of padding required to reach min_pad_coverage\n    n_pad_frames_start = min_pad_coverage_n_frames - n_partial_utterances\n", "CodeBleu_score": 21.914272714714812}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(shape)\n\n    cmap = plt.get_cmap(\\\"jet\\\")\n    cmap.set_under(color='white')\n    cmap.set_bad(color='black')\n    im = ax.imshow(embed, cmap=cmap, vmin=color_range[0], vmax=color_range[1])\n    plt.colorbar(im, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n\n", "CodeBleu_score": 61.01722344748464}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n        if shape[1] == -1:\n            shape = (shape[0], shape[0])\n    assert len(embed) == shape[0] * shape[1]\n    ax.pcolormesh(embed.reshape(shape), cmap=\"inferno\", vmin=color_range[0], vmax=color_range[1])\n    ax.axis(\"off\")\n    ax.set_title(title)\n    ax.set_adjustable(\\\"box-forced\\\")\n\n    # Remove axis ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Add a color bar\n    cax = ax.inset_axes([1.04, 0.1, 0.05, 0.8])\n    plt.colorbar(cax=cax)\n\n    return ax\n\n", "CodeBleu_score": 48.02025749109872}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n        if shape[1] == -1:\n            shape = (shape[0], shape[0])\n    assert len(embed) == shape[0] * shape[1]\n    ax.imshow(embed.reshape(shape), cmap=plt.cm.get_cmap('hot', 100),\n              interpolation='nearest', vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.axis('off')\n    ax.set_adjustable('box-forced')\n    return ax\n", "CodeBleu_score": 42.95482962527552}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the batch size and the number of utterances per speaker\n    num_speakers = embeds.size(0)\n    num_utterances = embeds.size(1)\n\n    # Compute the centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-5)\n    centroids_excl = torch.zeros((num_speakers, embeds.size(2)), device=self.loss_device)\n    for i in range(num_speakers):\n        centroids_excl[i, :] = torch.mean(torch.cat((embeds[:i, :, :], embeds[i + 1:, :, :])), dim=0)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-5)\n\n    # Initialize the similarity matrix and the mask matrix\n    sim_mat = torch.zeros((num_speakers, num_speakers), device=self.loss_device)\n    mask = torch.ones((num_speakers, num_speakers), device=self.loss_device)\n    mask = mask - torch.eye(num_speakers, device=self.loss_device)\n\n    # Compute the similarity scores and update the similarity matrix\n    for i in range(num_speakers):\n        sim_mat[i, :] = torch.mm(embeds[i, :, :], centroids_incl[i, :].t())\n        sim_mat[:, i] = torch.mm(embeds[i, :, :], centroids_excl[i, :].t())\n\n    # Scale and bias the similarity matrix\n    sim_mat = sim_mat * self.similarity_weight + self.similarity_bias\n\n    # Mask out self-similarities\\", "CodeBleu_score": 41.799641131601796}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n\n    centroids_incl = torch.zeros(n_speakers, model_embedding_size, device=self.loss_device)\n    centroids_excl = torch.zeros(n_speakers, model_embedding_size, device=self.loss_device)\n    for i in range(n_speakers):\n        centroids_incl[i] = embeds[i].mean(dim=0)\n        centroids_incl[i] /= torch.norm(centroids_incl[i])\n        centroids_excl[i] = (embeds[i].sum(dim=0) - embeds[i][0]) / (n_utts - 1)\n        centroids_excl[i] /= torch.norm(centroids_excl[i])\n\n    similarity_matrix = torch.zeros(n_speakers, n_speakers, device=self.loss_device)\n    mask = torch.ones(n_speakers, n_speakers, device=self.loss_device)\n    mask = torch.tril(mask, diagonal=-1)\n\n    for i in range(n_speakers):\n        similarity_matrix[i] = self.similarity_weight * (embeds[i] * centroids_incl[i]).sum(dim=1) + self.similarity_bias\n        similarity_matrix[i] = similarity_matrix[i] * mask[i]\n\n    similarity_matrix = similarity_matrix / (torch.norm(similarity_matrix, dim=1, keepdim=True) + 1e-5)\n\n    return similarity_matrix\n\n", "CodeBleu_score": 40.03690989278699}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers and utterances from the shape of the embeddings\n        num_spk = embeds.size(0)\n        num_utts = embeds.size(1)\n\n        # Compute centroids\n        centroids_incl = torch.zeros(num_spk, model_embedding_size, device=self.loss_device)\n        centroids_excl = torch.zeros(num_spk, model_embedding_size, device=self.loss_device)\n        mask = torch.ones(num_spk, num_utts, device=self.loss_device)\n        for i in range(num_spk):\n            centroids_incl[i] = embeds[i].mean(dim=0, keepdim=True)\n            centroids_incl[i] = centroids_incl[i] / (torch.norm(centroids_incl[i], dim=1, keepdim=True) + 1e-5)\n            mask[i, i] = 0\n\n        # Compute similarity matrix\n        sim_matrix = torch.zeros(num_spk, num_spk, device=self.loss_device)\n        for i in range(num_spk):\n            centroid = centroids_incl[i].repeat(num_utts, 1)\n            sim_matrix[i] = (embeds * centroid).sum(dim=1)\n        sim_matrix = sim_matrix * mask\n\n        centroids_excl = (embeds.sum(dim=1) - embeds) / (num_utts - 1)\n        for i in range(num_spk):\n            centroids_excl[i] = centroids_excl[i] / (torch.norm(centroids_excl[i], dim=1, keepdim=True) + 1e-5)\n        for i in range(num_spk):\n            sim_matrix[i] = (embeds * centroids_excl[i]).sum(dim=1)\n\n        # S", "CodeBleu_score": 33.543578989626525}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Flatten the similarity matrix\n    sim_matrix = sim_matrix.view(-1)\n\n    # Generate ground truth labels\n    gt_labels = torch.zeros(sim_matrix.shape[0]).long().to(self.loss_device)\n    gt_labels[::utterances_per_speaker] = 1\n\n    # Compute loss\n    loss = self.loss_fn(sim_matrix, gt_labels)\n\n    # Compute EER\n    with torch.no_grad():\n        # Compute the false acceptance and false rejection rates\n        fpr, tpr, thresholds = roc_curve(gt_labels.cpu().numpy(), sim_matrix.cpu().numpy(), pos_label=1)\n\n        # Compute the EER as the mean of the false acceptance and false rejection rates\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 46.24449182335616}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch, -1))\n\n    # Generate labels\n    labels = torch.zeros(speakers_per_batch, 1 + utterances_per_speaker * (speakers_per_batch - 1)).long().to(self.loss_device)\n    c = 1\n    for i in range(speakers_per_batch):\n        labels[i, 0] = c\n        for j in range(1 + utterances_per_speaker * (i - 1), 1 + utterances_per_speaker * i):\n            labels[i, j] = c\n            c += 1\n\n    # Calculate loss\n    loss = self.loss_fn(sim_matrix, labels.reshape(-1))\n\n    # Calculate EER\n    with torch.no_grad():\n        embeds = embeds.reshape((speakers_per_batch, -1))\n        sim_matrix = sim_matrix.reshape((speakers_per_batch, -1))\n\n        # Calculate cosine similarity\n        cos_sim_matrix = torch.nn.CosineSimilarity(dim=1, eps=1e-6)(embeds, embeds)\n\n        # Calculate equal error rate\n        fpr, tpr, thresholds = roc_curve(labels.reshape(-1).cpu().numpy(),\n                                         cos_sim_matrix.reshape(-1).cpu().numpy(),\n                                         pos_label=1)\n        fnr = 1 - tpr\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        thresh = interp1d(fpr, thresholds)(eer)\n\n    return loss, eer, thresh\n\n", "CodeBleu_score": 52.687333505838104}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    # Flatten the similarity matrix\n    sim_matrix = sim_matrix.view(-1)\n\n    # Generate ground truth labels\n    gt_labels = torch.zeros(speakers_per_batch * utterances_per_speaker).long()\n    for i in range(speakers_per_batch):\n        for j in range(1, utterances_per_speaker):\n            gt_labels[i * utterances_per_speaker + j] = i\n    # Compute loss\n    loss = self.loss_fn(sim_matrix, gt_labels)\n\n    # Compute EER\n    with torch.no_grad():\n        embeds_np = embeds.cpu().numpy()\n        embeds_np = embeds_np.reshape(speakers_per_batch * utterances_per_speaker, -1)\n        sim_matrix_np = sim_matrix.cpu().numpy()\n        sim_matrix_np = sim_matrix_np.reshape(speakers_per_batch * utterances_per_speaker, -1)\n        fpr, tpr, thresholds = roc_curve(gt_labels.cpu().numpy(), sim_matrix_np, pos_label=0)\n\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 54.41266513665206}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Get the model and data parameters\n    model_params = vars(self.model_params)\n    data_params = vars(self.data_params)\n\n    # Create a string with the parameters\n    params_string = \\\"\\\"\n    for key, value in model_params.items():\n        params_string += \\\"%s: %s\\", "CodeBleu_score": 8.62148721693253}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    params = []\n    params.append(\\\"Model parameters\\\")\n    for name, param in self.model.named_parameters():\n        params.append(\\\"%s: %s\\\" % (name, str(param.size())))\n    params.append(\\\"\\\")\n    params.append(\\\"Data parameters\\\")\n    for name, param in self.data.named_parameters():\n        params.append(\\\"%s: %s\\\" % (name, str(param.size())))\n    params.append(\\\"\\\")\n    params.append(\\\"\\\")\n    params.append(\\\"\\\")\n    params.append(\\\"\\\")\n    params.append(\\\"\\\")\n    self.vis.text(\\\"\\", "CodeBleu_score": 18.091649174687525}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    params = \\\"\\\"\\\"\n    Model parameters\n    ----------------\n    \\\"\\\"\\\"\n    for name, value in self.model.named_parameters():\n        params += \\\"{:<30} {}", "CodeBleu_score": 5.965524718125079}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = np.vstack(embeds)\n    n_speakers = len(embeds)\n    colors = colormap[np.linspace(0, 1, n_speakers, endpoint=False, dtype=int)]\n    speaker_ids = np.repeat(np.arange(n_speakers), utterances_per_speaker)\n    projections = umap.UMAP(n_neighbors=3, min_dist=0.0, metric=\\\"cosine\\\").fit_transform(embeds)\n    projections = np.hstack([projections, np.reshape(colors, (-1, 3))])\n    self.projection_win = self.vis.scatter(projections, opts=dict(markersize=5, markercolor=projections[:, 3:],\n                                                       title=\\\"Speaker projections (step %d)\\\" % step),\n                                                       win=self.projection_win)\n    if out_fpath is not None:\n        self.vis.save([self.projection_win])\n        print(\\\"Saved speaker projections to %s\\\" % out_fpath)\n\n", "CodeBleu_score": 45.555461459103675}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    if len(embeds) > max_speakers:\n        print(\\\"Warning: more speakers than the maximum number of speakers (%d) specified.\\\" % max_speakers)\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = np.array([e for speaker_embeds in embeds for e in speaker_embeds])\n    embeds_flattened = embeds.reshape((embeds.shape[0], -1))\n    labels = np.repeat(np.arange(len(embeds)), utterances_per_speaker)\n    if embeds_flattened.shape[1] > 2:\n        projections = umap.UMAP().fit_transform(embeds_flattened)\n    else:\n        projections = embeds_flattened\n    if out_fpath is not None:\n        self.vis.save([self.vis.scatter(projections, opts=dict(colors=colormap[labels], markersize=5, title=\\\"Step %d\\\" % step), env=self.env_name, fname=out_fpath)])\n    else:\n        self.projection_win = self.vis.scatter(projections, opts=dict(colors=colormap[labels], markersize=5, title=\\\"Step %d\\\" % step), win=self.projection_win)\n\n", "CodeBleu_score": 42.98998441230819}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape((embeds.shape[0] * utterances_per_speaker, -1))\n    speaker_ids = np.repeat(np.arange(embeds.shape[0] // utterances_per_speaker), utterances_per_speaker)\n    projections = umap.UMAP(n_neighbors=30, min_dist=0.001, metric=\\\"cosine\\\").fit_transform(embeds)\n    self.projection_win = self.vis.scatter(projections, colormap=colormap[speaker_ids], opts=dict(markersize=10, title=\\\"Step %d\\\" % step))\n    if out_fpath is not None:\n        self.vis.save([self.projection_win])\n    return projections\n\n", "CodeBleu_score": 41.73673598788348}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    (batch_size, seq_len, dim) = x.size()\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, seq_len, 1)\n    speaker_embedding = speaker_embedding.view(batch_size, seq_len, dim)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 18.720545329811433}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    (batch_size, text_len, channels) = x.size()\n    speaker_embedding = speaker_embedding.view(batch_size, 1, channels)\n    speaker_embedding = speaker_embedding.repeat(1, text_len, 1)\n    speaker_embedding = speaker_embedding.view(batch_size, text_len, channels)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 20.414018378633152}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    batch_size, seq_len, _ = x.size()\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, seq_len, 1)\n    speaker_embedding = speaker_embedding.reshape(batch_size, seq_len, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 18.82696061435531}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.size()[:2]\n    self.cumulative = torch.zeros((batch_size, seq_len), device=device)\n    self.attention = torch.zeros((batch_size, seq_len), device=device)\n\n", "CodeBleu_score": 34.40455335489542}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    # Get the device from the model's parameters\n    device = next(self.parameters()).device\n\n    # Get the batch size and sequence length from the input\n    batch_size, seq_len = encoder_seq_proj.size()[:2]\n\n    # Create and zero cumulative and attention tensors\n    self.cumulative = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 39.60157976417912}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size = encoder_seq_proj.size(0)\n    seq_len = encoder_seq_proj.size(1)\n    self.cumulative = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 24.44058941306469}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    self.to(next(self.parameters()).device)\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    self.load_state_dict(checkpoint['state_dict'])\n    if optimizer is not None and 'optimizer' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n", "CodeBleu_score": 42.20526158633462}, {"code": "def load(self, path, optimizer=None)\n\t\n    self.set_device(next(self.parameters()).device)\n        checkpoint = torch.load(path, map_location=self.device)\n        self.load_state_dict(checkpoint['model_state'])\n        if optimizer is not None and checkpoint.get(\\\"optimizer_state\\\"):\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n        return checkpoint\n\n    def save(self, path, optimizer=None):\n        \"\"\"The function saves a model checkpoint to a given path.", "CodeBleu_score": 33.19261812670115}, {"code": "def load(self, path, optimizer=None)\n\t\n    self.to(next(self.parameters()).device)\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n", "CodeBleu_score": 42.42467109326514}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    torch.save({\n        \\\"model_state\\\": self.state_dict(),\n        \\\"optimizer_state\\\": optimizer.state_dict() if optimizer is not None else None\n    }, str(path))\n\n", "CodeBleu_score": 52.573091337676615}, {"code": "def save(self, path, optimizer=None)\n\t\n    torch.save({\n        'model_state': self.state_dict(),\n        'optimizer_state': optimizer.state_dict() if optimizer else None\n    }, path)\n\n", "CodeBleu_score": 46.91121853417354}, {"code": "def save(self, path, optimizer=None)\n\t\n    checkpoint = {\n            \\\"model_state\\\": self.state_dict(),\n        }\n\n        if optimizer is not None:\n            checkpoint[\\\"optimizer_state\\\"] = optimizer.state_dict()\n\n        torch.save(checkpoint, path)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \\\"a\\\") as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\\\"model_state\\\"])\n\n        if \\\"optimizer_state\\\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\\\"optimizer_state\\\"])\n\n", "CodeBleu_score": 37.04399137260948}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n\n\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    clean_text = _clean_text(text, cleaner_names)\n    sequence += _arpabet_to_sequence(clean_text)\n    return sequence\n\n", "CodeBleu_score": 7.445918649412642}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    if cleaner_names is not None:\n        for name in cleaner_names:\n            cleaner = getattr(cleaners, name)\n            if not cleaner:\n                raise Exception(f'Unknown cleaner: {name}')\n            text = cleaner(text)\n    sequence += _symbols_to_sequence(text)\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 28.299197367336227}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    for cleaner_name in cleaner_names:\n        cleaner = getattr(cleaners, cleaner_name)\n        text = cleaner(text)\n    sequence += _symbols_to_sequence(text)\n    return sequence\n\n", "CodeBleu_score": 16.384209045937478}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n        if len(s) > 1 and s[0] == \\\"{\\\":\n            s = \\\"{\\\" + s[1:-1] + \\\"}\\\"\n        result.append(s)\n\n    # Replace curly braces around ARPAbet with a space:\n    result = \\\" \\\".join(result)\n    result = re.sub(_curly_re, \\\"{\\\\\\\\\\\\\\\\\\g<1>{\\\\\\\\\\\\\\\\\\g<2>\\\\\\\\\\\\\\\\g<3>}\\\", result)\n    return result\n\n\n\n\n", "CodeBleu_score": 48.744246532067095}, {"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n\n            if len(s) > 1 and s[0] == \\\"{\\\":\n                s = \\\"{\\\" + s[1:-1] + \\\"}\\\"\n\n            result.append(s)\n        else:\n            result.append(\\\"\\\")\n    text = \\\" \\\".join(result)\n\n    # Remove a lone space at the end:\n    return text.strip()\n\n\n\n\n", "CodeBleu_score": 50.104930212053986}, {"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \\\"-\\\" and s[1:]\n            s = \\\"{\\\" + s[1:] + \\\"}\\\"\n            result.append(s)\n        else:\n            result.append(\\\"\\\")\n    result = \\\"\\\".join(result)\n\n    # Remove a lone space at the end\n    result = result.rstrip(\\\" \\\")\n\n    # Replace all instances of \\' \\' with \\'|\\'\n    result = result.replace(\\\" \\\", \\\"|\\\")\n\n    return result\n\n", "CodeBleu_score": 51.16258338782899}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav = encoder.preprocess_wav(fpath)\n    embed = encoder.embed_utterance(wav)\n\n    # If the mean vector length is 0, then the utterance is too short to\n    # encode. Suppress this warning and return None instead of the embedding.\n    if np.linalg.norm(embed) == 0:\n        warnings.warn(\\\"utterance has zero vector for speaker embedding\\\")\n        return None\n\n    # Load the mean vector for the speaker\n    spk_id = fpath.split(\\\"/\\\")[-2]\n    spk_embed_fpath = Path(fpath).parent.joinpath(\\\"spk_embed\\\", \\\"spk-%s.npy\\\" % spk_id)\n    spk_embed = np.load(spk_embed_fpath)\n\n    # Compute the cosine similarity between the embeddings\n    return np.dot(embed, spk_embed) / (np.linalg.norm(embed) * np.linalg.norm(spk_embed))\n\n\n\n\n", "CodeBleu_score": 49.773507038767136}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not isinstance(fpaths, list):\n        fpaths = [fpaths]\n\n    # Load the encoder\n    if embed.speaker_embedding_model is None:\n        _logger.info(\\\"Loading the encoder model...\\\")\n        embed.load_model(encoder_model_fpath)\n        _logger.info(\\\"Encoder model loaded.\\\")\n\n    # Load the wav file\n    wav = audio.load_wav(fpaths[0], sr=hparams.sample_rate)\n\n    # Compute the speaker embedding\n    embed = embed.embed_utterance(wav)\n\n    # Truncate the embedding to the first 128 values\n    embed = embed[:128]\n\n    # Save it on the disk\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n    # Return the embedding (just the first 128 values)\n    return embed\n\n\n\n\n", "CodeBleu_score": 43.99921860621157}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav = encoder.preprocess_wav(fpath)\n    embed = encoder.embed_utterance(wav)\n    return embed\n\n", "CodeBleu_score": 45.66311869275792}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder architecture.\n    supported_models = encoder.supported_models()\n    if encoder_model_fpath not in supported_models:\n        raise ValueError(\\\"Encoder model {0} not supported. Supported models are: {1}\\\".format(encoder_model_fpath, supported_models))\n\n    # Configure multiprocessing\n    pool = multiprocessing.Pool(processes=n_processes)\n\n    # Verify output directories\n    if not synthesizer_root.exists():\n        raise Exception(\\\"Synthesizer root directory {0} does not exist.\\\".format(synthesizer_root))\n    audio_dir = synthesizer_root.joinpath(\\\"audio\\\")\n    if not audio_dir.exists():\n        raise Exception(\\\"Audio directory {0} does not exist.\\\".format(audio_dir))\n    embed_dir = synthesizer_root.joinpath(\\\"embeds\\\")\n    if not embed_dir.exists():\n        _ = embed_dir.mkdir(parents=True, exist_ok=True)\n\n    # Gather the input wave filepath and the corresponding embed filepath\n    wav_fpaths = list(audio_dir.glob(\\\"**/*.npy\\\"))\n    embed_fpaths = [str(wav_fpath).replace(str(audio_dir), str(embed_dir)).replace(\\\".npy\\\", \\\".npy\\\") for wav_fpath in wav_fpaths]\n    assert len(wav_fpaths) == len(embed_fpaths)\n\n    # Embed the utterances\n    list(tqdm(pool.imap(embed_utterance, zip(wav_fpaths, embed_fpaths)), total=len(wav_fpaths), desc=\\\"Embedding\\\"))\n    # for wav_fpath, embed_fpath in zip(wav_fpaths, embed_fpaths):\n    #     _ = embed_utterance((wav_fpath, embed_fpath))\n\n\n\n\n", "CodeBleu_score": 34.38279046452938}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder model path exists\n    if not encoder_model_fpath.exists():\n        raise Exception(\\\"Encoder model file does not exist at path: %s\\\" % encoder_model_fpath)\n\n    # Get dataset\n    metadata_fpath = synthesizer_root.joinpath(\\\"train.txt\\\")\n    assert metadata_fpath.exists(), \\\"train.txt file does not exist at path: %s\\\" % synthesizer_root\n    metadata = _read_metadata(metadata_fpath)\n\n    # Create output directories for embeddings\n    wav_dir = synthesizer_root.joinpath(\\\"audio\\\")\n    mel_dir = synthesizer_root.joinpath(\\\"mels\\\")\n    embed_dir = synthesizer_root.joinpath(\\\"embeds\\\")\n    os.makedirs(mel_dir, exist_ok=True)\n    os.makedirs(embed_dir, exist_ok=True)\n\n    # Create a pool of processes. By default, data is loaded using one core. Since we're\n    # running multiple processes, we need to increase that number.\n    pool = multiprocessing.Pool(processes=n_processes)\n\n    # Iterate over all the metadata\n    metadata = list(metadata)\n    random.seed(1234)\n    random.shuffle(metadata)\n    metadata = [m for m in metadata if m[2] > 0]\n    metadata = [m for m in metadata if m[3] > hparams.utterance_min_duration * hparams.sample_rate]\n    metadata = [m for m in metadata if m[3] < hparams.max_wav_value]\n    metadata = [m for m in metadata if m[4] < hparams.max_mel_frames]\n\n    # Process the utterances in the multiprocessing pool\n    futures = []\n    with tqdm(total=len(metadata), unit=\\\"utterance\\\") as pbar:\n        for m in metadata:\n            wav_fpath, mel_fpath, embed_fpath, wav_files", "CodeBleu_score": 36.01409831063813}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder existence\n    if not encoder_model_fpath.exists():\n        raise Exception(\\\"Trained encoder model not found at {}\\\".format(encoder_model_fpath))\n\n    # Create a output directory for embeddings\n    output_dir = synthesizer_root.joinpath(\\\"embeds\\\")\n    output_dir.mkdir(exist_ok=True)\n\n    # Gather the input wave filepath and the corresponding metadata\n    metadata_fpaths = list(synthesizer_root.glob(\\\"**/metadata.csv\\\"))\n    metadata_fpaths = list(synthesizer_root.glob(\\\"**/metadata.csv\\\"))\n    metadata_fpaths = [f for f in metadata_fpaths if f.is_file()]\n    metadata_fpaths = sorted(metadata_fpaths)\n    wav_dirs = [f.parent for f in metadata_fpaths]\n    assert len(wav_dirs) == len(metadata_fpaths)\n\n    # Gather the input wave filepath and the corresponding metadata\n    in_fpaths_and_text = []\n    for wav_dir, metadata_fpath in zip(wav_dirs, metadata_fpaths):\n        with metadata_fpath.open(\\\"r\\\", encoding=\\\"utf-8\\\")\n                as metadata_file:\n            for line in metadata_file:\n                wav_fname, _, text = line.strip().split(\\\"|\\\")\n                wav_fpath = wav_dir.joinpath(wav_fname)\n                assert wav_fpath.exists()\n                in_fpaths_and_text.append((wav_fpath, text))\n\n    # Create a multiprocessing pool and process the utterances\n    pool = multiprocessing.Pool(n_processes)\n    list(tqdm(pool.imap(partial(embed_utterance, encoder_model_fpath=encoder_model_fpath), in_fpaths_and_text), \\\"Embedding\\\", len(in_fpaths_and_text), unit=\\\"utterances\\\"", "CodeBleu_score": 33.313046325292916}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    if attention is not None:\n        fig = plot_attention(attention, input_seq, hparams)\n        fig.savefig(plot_dir.joinpath(f\\\"attention_{step:06d}_{sample_num:02d}.png\\\"))\n\n    # Save mel spectrogram\n    fig = plot_mel_spectrogram(mel_prediction, hparams)\n    fig.savefig(mel_output_dir.joinpath(f\\\"mel_{step:06d}_{sample_num:02d}.png\\\"))\n\n    # Save waveform\n    wav = spectrogram2wav(mel_prediction, hparams)\n    wav_fpath = wav_dir.joinpath(f\\\"wav_{step:06d}_{sample_num:02d}.wav\\\")\n    wav.save(str(wav_fpath))\n\n    # Save comparison plot\n    fig = plot_comparison(target_spectrogram, mel_prediction, hparams)\n    fig.suptitle(f\\\"Step: {step} Loss: {loss.item():.5f}\\\")\n    fig.savefig(plot_dir.joinpath(f\\\"comparison_{step:06d}_{sample_num:02d}.png\\\"))\n\n    # Print input sequence\n    print(\\\"Input sequence: \\\", input_seq)\n\n\n\n", "CodeBleu_score": 35.35126458023352}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_plot_fpath = plot_dir.joinpath(\\\"attention_plot_%d_%d.png\\\" % (step, sample_num))\n    plot_attention(attention, attention_plot_fpath)\n\n    # Save predicted mel spectrogram\n    mel_prediction_fpath = mel_output_dir.joinpath(\\\"mel_%d_%d.npy\\\" % (step, sample_num))\n    np.save(mel_prediction_fpath, mel_prediction)\n\n    # Save target mel spectrogram\n    target_mel_fpath = mel_output_dir.joinpath(\\\"mel_target_%d_%d.npy\\\" % (step, sample_num))\n    np.save(target_mel_fpath, target_spectrogram)\n\n    # Reconstruct waveform from the predicted mel spectrogram\n    wav_fpath = wav_dir.joinpath(\\\"wav_%d_%d.wav\\\" % (step, sample_num))\n    audio = inv_mel_spectrogram(mel_prediction.T, hparams)\n    wavfile.write(wav_fpath, hparams.sample_rate, audio)\n\n    # Save the plot\n    plot_fpath = plot_dir.joinpath(\\\"plot_%d_%d.png\\\" % (step, sample_num))\n    plot_title = \\\"%s (%s)\\\" % (hparams.tts_model_type, datetime.now().strftime(\\\"%Y-%m-%d_%H-%M-%S\\\"))\n    plot_spectrogram(mel_prediction.T, target_spectrogram.T, plot_title, plot_fpath)\n\n    # Print the input sequence\n    print(\\\"Input sequence:", "CodeBleu_score": 29.823959066172634}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n\n    if hparams.tts_plot_attention:\n        plot_attention(attention, os.path.join(plot_dir, \\\"attention_{}.png\\\".format(step)))\n\n    # Save predicted mel spectrogram\n    if hparams.tts_plot_mel_prediction:\n        plot_mel_spectrogram(mel_prediction, os.path.join(mel_output_dir, \\\"mel_prediction_{}.png\\\".format(step)))\n\n    # Save target mel spectrogram\n    if hparams.tts_plot_mel_target:\n        plot_mel_spectrogram(target_spectrogram, os.path.join(mel_output_dir, \\\"mel_target_{}.png\\\".format(step)))\n\n    # Save reconstructed waveform\n    if hparams.tts_plot_reconstructed_waveform:\n        if hparams.tts_use_lws:\n            waveform = lws_isft(mel_prediction)\n        else:\n            waveform = griffin_lim(mel_prediction)\n\n        plot_waveform(waveform, os.path.join(wav_dir, \\\"waveform_{}.png\\\".format(step)))\n\n    # Save comparison plot\n    plot_comparison(target_spectrogram, mel_prediction, os.path.join(mel_output_dir, \\\"comparison_{}.png\\\".format(step)))\n\n    # Save comparison plot\n    plot_comparison(target_spectrogram, mel_prediction, os.path.join(mel_output_dir, \\\"comparison_{}.png\\\".format(step)))\n\n    # Save comparison plot\n    plot_comparison(target_spectrogram, mel_prediction, os.path.join(mel_output_dir, \\\"comparison_{}.png\\\".format(step)))\n\n    # Save comparison plot\n    plot_comparison(target_spectrogram, mel_prediction, os.path.join(mel_output_dir, \\\"comparison_{}.png\\\".format(step)))\\", "CodeBleu_score": 28.734944969422095}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    total_params = 0\n    for param in self.parameters():\n        if param.requires_grad:\n            total_params += param.numel()\n    print(f\"Total Trainable Params: {round(total_params/1000000, 3)}M\")\n\n", "CodeBleu_score": 24.520913741863936}, {"code": "def num_params(self)\n\t\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1000000\n\n", "CodeBleu_score": 6.079994776171634}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    stream('Number of trainable parameters: %.3fM', num_params)\n\n", "CodeBleu_score": 17.845797536138974}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        # Calculate the total size of the tensor after padding\n        total_size = x.size(1) + pad\n        # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n        padded_tensor = torch.zeros(x.size(0), total_size, x.size(2))\n        if torch.cuda.is_available():\n            padded_tensor = padded_tensor.cuda()\n        # Place the original tensor into the new padded tensor\n        padded_tensor[:, pad // 2:pad // 2 + x.size(1), :] = x\n    elif side == 'before':\n        # Calculate the total size of the tensor after padding\n        total_size = x.size(1) + pad\n        # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n        padded_tensor = torch.zeros(x.size(0), total_size, x.size(2))\n        if torch.cuda.is_available():\n            padded_tensor = padded_tensor.cuda()\n        # Place the original tensor into the new padded tensor\n        padded_tensor[:, pad:pad + x.size(1), :] = x\n    elif side == 'after':\n        # Calculate the total size of the tensor after padding\n        total_size = x.size(1) + pad\n        # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n        padded_tensor = torch.zeros(x.size(0), total_size, x.size(2))\n        if torch.cuda.is_available():\n            padded_tensor = padded_tensor.cuda()\n        # Place the original tensor into the new padded tensor\n        padded_tensor[:, :x.size(1), :] = x\n    else:\n        raise ValueError('Invalid side value. Must be one of \"before\", \"after\", or \"both\".')\n    return padded_tensor\n\n", "CodeBleu_score": 35.114151272370286}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = torch.cat([x[:, -pad:, :], x, x[:, :pad, :]], dim=1)\n    elif side == 'after':\n        x = torch.cat([x, torch.zeros(x.size(0), pad, x.size(2)).type_as(x)], dim=1)\n    elif side == 'before':\n        x = torch.cat([torch.zeros(x.size(0), pad, x.size(2)).type_as(x), x], dim=1)\n    else:\n        raise ValueError('Unknown padding side')\n    return x\n\n", "CodeBleu_score": 35.13021859273364}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # x is a tensor of shape (batch_size, seq_len, num_features)\n    if side == 'both':\n        # Pad the tensor on both sides\n        pad_left = pad\n        pad_right = pad\n    elif side == 'before':\n        # Pad the tensor on the left side\n        pad_left = pad\n        pad_right = 0\n    elif side == 'after':\n        # Pad the tensor on the right side\n        pad_left = 0\n        pad_right = pad\n    else:\n        raise ValueError('Invalid side value. Must be one of: \"before\", \"after\", or \"both\".')\n\n    # Calculate the new size of the tensor after padding\n    new_size = (x.size(0), x.size(1) + pad_left + pad_right, x.size(2))\n\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n    if torch.cuda.is_available():\n        padded_tensor = torch.zeros(new_size, dtype=x.dtype, device=x.device)\n    else:\n        padded_tensor = torch.zeros(new_size, dtype=x.dtype)\n\n    # Place the original tensor into the padded tensor according to the specified side\n    if side == 'both':\n        padded_tensor[:, pad_left:x.size(1) + pad_left, :] = x\n    elif side == 'before':\n        padded_tensor[:, pad_left:x.size(1) + pad_left, :] = x\n    elif side == 'after':\n        padded_tensor[:, :x.size(1), :] = x\n\n    return padded_tensor\n\n", "CodeBleu_score": 32.350203374595296}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    from matplotlib import pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + [chr(c) for c in range(97, 97 + 26)])\n    ax.set_yticklabels([''] + [chr(c) for c in range(97, 97 + 26)])\n    plt.savefig(path, dpi=300, bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 46.787352893510324}, {"code": "def save_attention(attn, path)\n\t\n    from matplotlib import pyplot as plt\n    import matplotlib\n    matplotlib.use('Agg')\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    fig.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 45.781156735666414}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as ticker\n    import numpy as np\n    import os\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    # Set up axes\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in source_lang.lang_dict] + ['<eos>'])\n    ax.set_yticklabels([''] + [t.lower() for t in target_lang.lang_dict] + ['<eos>'])\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    plt.savefig(path, format='png')\n    plt.close(fig)\n\n", "CodeBleu_score": 44.28689729127113}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as ticker\n\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.tick_params(axis='x', colors='#777777', labelsize='10')\n    ax.tick_params(axis='y', colors='#777777', labelsize='10')\n    ax.xaxis.label.set_color('#777777')\n    ax.yaxis.label.set_color('#777777')\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 59.49779346012802}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.plot(array)\n    ax.set_xlabel(\"Time\", color=\"black\", fontsize=14)\n    ax.set_ylabel(\"Amplitude\", color=\"black\", fontsize=14)\n    ax.tick_params(axis='x', colors=\"black\", labelsize=12)\n    ax.tick_params(axis='y', colors=\"black\", labelsize=12)\n    plt.show()\n\n", "CodeBleu_score": 35.58033153785107}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.plot(array)\n    plt.xlabel('Time', fontsize=16, color='#000000')\n    plt.ylabel('Amplitude', fontsize=16, color='#000000')\n    plt.tick_params(axis='both', which='major', labelsize=16, color='#000000')\n    plt.show()\n\n", "CodeBleu_score": 27.42338643282362}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open(csv_path, 'a', newline='') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if os.stat(csv_path).st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n    # Print the prediction data\n    print(f\\\"Prediction: {prediction}\\\")\n    print(f\\\"Confidence: {confidence}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path\n    print(f\\\"CSV file saved at: {csv_path}\\\")\n\n    # Print the CSV file path", "CodeBleu_score": 41.99048379421876}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open(csv_path, 'a', newline='') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 50.183045603804324}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'Image Name': image_name, 'Prediction': prediction, 'Confidence': confidence}\n    with open(csv_path, 'a', newline='') as csvfile:\n        fieldnames = ['Image Name', 'Prediction', 'Confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 50.5816864912239}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} start export with optimize={optimize}\")\n    with torch.no_grad():\n        model = model.fuse()\n        for k, m in model.named_modules():\n            m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n            if isinstance(m, models.yolo.Detect):\n                m.inplace = False  # assign detect() inplace=False to convert\n        model.model[-1].export = True  # set Detect() layer export=True\n        y = model(im)  # dry run\n        if optimize:\n            try:\n                from torch2torchscript.optimize_for_mobile import optimize_for_mobile\n\n                model = optimize_for_mobile(model)\n            except Exception as e:\n                LOGGER.info(f\"{prefix} optimize=True failed: {e}\")\n        with torch.no_grad():\n            model_trace = torch.jit.trace(model, im)\n            model_trace.save(file)\n            shutil.copy(file, file.replace(\".torchscript\", \".engine\"))  # torchscript engine\n            shutil.copy(file, file.replace(\".torchscript\", \"_debug.engine\"))  # torchscript engine\n            torch.jit.save(model_trace, file.replace(\".torchscript\", \"_jit.pt\"))  # torch jit\n            model_trace._c._save_for_mobile(file.replace(\".torchscript\", \"_mobile\"))  # mobile\n            shutil.copy(file.replace(\".torchscript\", \"_jit.pt\"), file.replace(\".torchscript\", \"_jit-mobile.pt\"))  # jit mobile\n        LOGGER.info(f\"{prefix} export success \u201a\ufffd\u00d6 {time.time() - t:.1f}s, saved as {file} ({file_size(file):.1f} MB)\")\n        return file, None\n\n", "CodeBleu_score": 31.50086927576668}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} exporting TorchScript\")\n    with torch.no_grad():\n        if MACOS:\n            model = torch.jit.script(model)\n        else:\n            model = torch.jit.trace(model, im)\n    if optimize:\n        model = optimize_for_mobile(model)\n    file = Path(file).with_suffix(\".torchscript.pt\")\n    torch.jit.save(model, file)\n    LOGGER.info(f\"{prefix} optimized for mobile\" if optimize else f\"{prefix} export success\")\n    return file, None\n\n", "CodeBleu_score": 36.352418033380516}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} exporting {file}\")\n    with torch.no_grad():\n        model.eval()\n        model.model[-1].export = False  # set Detect() layer export=False\n        y = model(im)  # dry run\n        if hasattr(y, \"ai_trace\"):\n            torch.onnx.export(\n                model,\n                im,\n                file,\n                format=\"ONNX\",\n                opset_version=12,\n                training=torch.onnx.TrainingMode.EVAL,\n                export_params=True,\n                verbose=False,\n                input_names=[\"images\"],\n                output_names=[\"output\"],\n                dynamic_axes={\"images\": {0: \"batch\", 2: \"height\", 3: \"width\"}, \"output\": {0: \"batch\", 1: \"anchors\"}},\n            )\n        else:\n            torch.onnx.export(\n                model,\n                im,\n                file,\n                format=\"ONNX\",\n                opset_version=12,\n                training=torch.onnx.TrainingMode.EVAL,\n                export_params=True,\n                verbose=False,\n                input_names=[\"images\"],\n                output_names=[\"output\"],\n                dynamic_axes={\"images\": {0: \"batch\", 2: \"height\", 3: \"width\"}, \"output\": {0: \"batch\", 1: \"anchors\"}},\n            )\n    if optimize:\n        try:\n            from torch2trt import torch2trt\n        except ImportError:\n            LOGGER.warning(\"Install torch2trt for YOLOv5 \ud83d\ude80 (pip install torch2trt)\")\n        else:\n            LOGGER.info(f\"{prefix} optimizing {file} with torch2trt...\")\n            model_trt = torch2trt(model, [im])\n            torch.save(model_trt.state_dict(), file.replace(\".pt\", \".trt\"))\n    return file, None\n\n\n", "CodeBleu_score": 28.543199996732028}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    LOGGER.info(f\"", "CodeBleu_score": 8.707165598582774}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")  # requires coremltools: https://pypi.org/project/coremltools/\n    import coremltools as ct  # noqa\n\n    LOGGER.info(f\"", "CodeBleu_score": 10.683976376451392}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\\\"coremltools\\\")  # requires coremltools: https://pypi.org/project/coremltools/\n    import coremltools\n    from ultralytics.yolo.utils.coreml import iOSModel\n    from ultralytics.yolo.utils.coreml_utils import apply_nms\n    from ultralytics.yolo.utils.torch_utils import TracedModel\n\n    LOGGER.info(f\\\"", "CodeBleu_score": 11.36585889232648}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        lower_bound, upper_bound = random.choice(input_ranges)\n        individual.append(random.uniform(lower_bound, upper_bound))\n    return individual\n\n", "CodeBleu_score": 64.60156087334741}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        # Get a random float within the specified bounds\n        individual.append(random.uniform(input_ranges[0][0], input_ranges[0][1]))\n        input_ranges = input_ranges[1:]\n    return individual\n\n", "CodeBleu_score": 41.538504841323935}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        lower_bound, upper_bound = random.choice(input_ranges)\n        individual.append(random.uniform(lower_bound, upper_bound))\n    return individual\n\n", "CodeBleu_score": 64.60156087334741}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        return self.log_alpha_array[0, t]\n    elif self.schedule == 'linear':\n        return (self.beta_1 - self.beta_0) * t / self.T + self.beta_0\n    elif self.schedule == 'cosine':\n        return self.cosine_log_alpha_0 - self.cosine_s * math.cos(math.pi * t / self.T)\n\n", "CodeBleu_score": 33.06822759005129}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        t = torch.tensor(t)\n        t = t.reshape((1, -1))\n        if t.shape != self.t_array.shape:\n            raise ValueError(\\\"The shape of t must be {}\\\".format(self.t_array.shape))\n        log_alpha = torch.interp(t, self.t_array, self.log_alpha_array, left=self.log_alpha_array[0], right=self.log_alpha_array[-1]).reshape((-1,))\n    elif self.schedule == 'linear':\n        log_alpha = -0.5 * (self.beta_1 - self.beta_0) * t + self.beta_0 * t * t\n    elif self.schedule == 'cosine':\n        log_alpha = torch.log(torch.cos(self.cosine_s / (1. + self.cosine_s) * torch.pi / 2.)) - torch.log(torch.cos(torch.pi * (t - self.cosine_t_max) / (2. * (1. + self.cosine_s))))\n    else:\n        raise ValueError(\\\"Unsupported schedule {}\\\".format(self.schedule))\n    return log_alpha\n\n", "CodeBleu_score": 51.483923130022816}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        return self.log_alpha_array[:, t]\n    elif self.schedule == 'linear':\n        return (self.beta_1 - self.beta_0) * t + self.beta_0\n    elif self.schedule == 'cosine':\n        t_max = self.cosine_t_max\n        log_alpha_0 = self.cosine_log_alpha_0\n        s = self.cosine_s\n        if t <= t_max:\n            return log_alpha_0 - (math.log(math.cos(s / (1. + s) * math.pi / 2.)) - math.log(math.cos(s / (1. + s) * math.pi * (t - t_max) / t_max))) / s\n        else:\n            return log_alpha_0 - math.log(math.cos(s / (1. + s) * math.pi / 2.))\n\n", "CodeBleu_score": 46.60392444704633}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        log_alpha = interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1,))\n        t = torch.exp(log_alpha)\n    elif self.schedule == 'linear':\n        beta_0 = self.beta_0\n        beta_1 = self.beta_1\n        t = torch.sqrt(1. - torch.exp(-2. * lamb))\n        t = (t - beta_0) / (beta_1 - beta_0)\n    elif self.schedule == 'cosine':\n        log_alpha = lamb + self.cosine_log_alpha_0\n        t = torch.arccos(torch.exp(log_alpha)) / (1. + self.cosine_s) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n    else:\n        raise ValueError('Unsupported schedule {}. The schedule needs to be \\'discrete\\', \\'linear\\' or \\'cosine\\''.format(self.schedule))\n    return t\n\n", "CodeBleu_score": 43.757115804288794}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        log_alpha_t = interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        beta_0 = self.beta_0\n        beta_1 = self.beta_1\n        t = (torch.exp(2. * lamb) - 1.) / (2. * beta_1 * beta_0)\n        t = torch.clamp(t, 0., 1.)\n    elif self.schedule == 'cosine':\n        log_alpha_t = lamb + self.cosine_log_alpha_0\n        t = torch.acos(torch.exp(log_alpha_t)) * (1. + self.cosine_s) / math.pi * 2. - self.cosine_s\n    else:\n        raise NotImplementedError\n    return t\n\n", "CodeBleu_score": 37.75674215998708}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        log_alpha_array = self.log_alpha_array\n        t_array = self.t_array\n        return interpolate_fn(lamb.reshape((-1, 1)), log_alpha_array.to(lamb.device), t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        beta_0 = self.beta_0\n        beta_1 = self.beta_1\n        T = self.T\n        lamb = lamb.reshape((-1,))\n        beta_0_1 = beta_1 - beta_0\n        beta_0_1_sqrt = torch.sqrt(beta_0_1)\n        lamb_sqrt = torch.sqrt(lamb)\n        lamb_sqrt_beta_0 = lamb_sqrt * beta_0\n        beta_0_1_sqrt_lamb_sqrt_beta_0 = beta_0_1_sqrt * lamb_sqrt_beta_0\n        t = beta_0_1_sqrt_lamb_sqrt_beta_0 / beta_0_1 + T\n        return t\n    elif self.schedule == 'cosine':\n        cosine_s = self.cosine_s\n        cosine_beta_max = self.cosine_beta_max\n        cosine_t_max = self.cosine_t_max\n        cosine_log_alpha_0 = self.cosine_log_alpha_0\n        lamb = lamb.reshape((-1,))\n        log_alpha = lamb + cosine_log_alpha_0\n        s = torch.acos(torch.exp(log_alpha)) * 2. * (1. + cosine_s) / math.pi - cosine_s\n        t = s * cosine_t_max / cosine_beta_max\n        return t\n\n", "CodeBleu_score": 34.68608565579523}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == 'noise':\n        return noise_pred(model, x, t_input, **model_kwargs)\n    elif model_type == 'x_start':\n        return x_start_from_noise(model, x, t_input, **model_kwargs)\n    elif model_type == 'v':\n        return v_from_noise(model, x, t_input, **model_kwargs)\n    elif model_type == 'score':\n        return score_from_noise(model, x, t_input, **model_kwargs)\n\n", "CodeBleu_score": 11.306058164157784}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == \"noise\":\n        return noise_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"x_start\":\n        return x_start_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"v\":\n        return v_pred(model, x, t_input, **model_kwargs)\n    elif model_type == \"score\":\n        return score_pred(model, x, t_input, **model_kwargs)\n    else:\n        raise NotImplementedError(f\"model_type {model_type} not implemented\")\n\n", "CodeBleu_score": 16.099775140701436}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    t_input = t_input.expand(x.shape[0], *((1,) * (x.ndim - 1)))\n    if model_type == 'noise':\n        return noise_pred(model, x, t_input, **model_kwargs)\n    elif model_type == 'x_start':\n        return noise_pred_x_start(model, x, t_input, **model_kwargs)\n    elif model_type == 'v':\n        return noise_pred_v(model, x, t_input, **model_kwargs)\n    elif model_type == 'score':\n        return noise_pred_score(model, x, t_input, **model_kwargs)\n    else:\n        raise ValueError(f'Unknown model_type: {model_type}')\n\n", "CodeBleu_score": 17.443317342262237}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.noise_schedule.get_t(x0.shape[0])\n    t = t.reshape((-1,))\n    if t.shape[0] == 1:\n        t = t.expand((x0.shape[0],))\n    abs_x0 = torch.abs(x0)\n    if self.variant == 'bh1':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh2':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh3':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh4':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh5':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh6':\n        q_t = torch.quantile(abs_x0.reshape((-1,)), t, dim=0)\n        scaling_factor = torch.maximum(q_t, torch.tensor(1.0))\n    elif self.variant == 'bh7':\n        q_t = torch.quantile(abs", "CodeBleu_score": 29.449854543919436}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = x0.reshape((-1,))\n    abs_x0 = torch.abs(x0)\n    q = torch.quantile(abs_x0, 0.999)\n    scale = q / 0.999\n    x0 = x0 / scale\n    x0 = x0.clamp(-self.max_val, self.max_val)\n    x0 = x0 * scale\n    if t is not None:\n        x0 = x0.reshape(t.shape)\n    return x0\n\n", "CodeBleu_score": 26.036212477643172}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = torch.tensor(0.)\n    if self.variant == 'bh1':\n        return self.dynamic_thresholding_bh1(x0, t)\n    elif self.variant == 'bh2':\n        return self.dynamic_thresholding_bh2(x0, t)\n    elif self.variant == 'bh3':\n        return self.dynamic_thresholding_bh3(x0, t)\n    elif self.variant == 'bh4':\n        return self.dynamic_thresholding_bh4(x0, t)\n    elif self.variant == 'bh5':\n        return self.dynamic_thresholding_bh5(x0, t)\n    elif self.variant == 'bh6':\n        return self.dynamic_thresholding_bh6(x0, t)\n    elif self.variant == 'bh7':\n        return self.dynamic_thresholding_bh7(x0, t)\n    elif self.variant == 'bh8':\n        return self.dynamic_thresholding_bh8(x0, t)\n    elif self.variant == 'bh9':\n        return self.dynamic_thresholding_bh9(x0, t)\n    elif self.variant == 'bh10':\n        return self.dynamic_thresholding_bh10(x0, t)\n    elif self.variant == 'bh11':\n        return self.dynamic_thresholding_bh11(x0, t)\n    elif self.variant == 'bh12':\n        return self.dynamic_thresholding_bh12(x0, t)\n    elif self.variant == 'bh13':\n        return self.dynamic_thresholding_bh13(x0, t)\n    elif self.variant == 'bh14':\n        return self.dynamic_thresholding_bh14(x0, t)\n    elif self.variant == 'bh15':\n        return self.dynamic_thresholding_bh15(x0, t)\n   ", "CodeBleu_score": 11.585113633829385}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(-1, *((1,) * (input.ndim - 1)))\n    input_scaled = input / sigma\n    model_output = model(input_scaled, **kwargs)\n    eps = model_output * sigma\n    return eps\n\n", "CodeBleu_score": 32.098643061820006}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(-1, 1, 1, 1)\n    sigma_in = sigma_in.expand(input.shape)\n    input = input / sigma_in\n    output = model(input, **kwargs)\n    return output * sigma_in\n\n", "CodeBleu_score": 33.05790898431665}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in\n    if sigma is not None:\n        if sigma.ndim == 1:\n            sigma = sigma.reshape(-1, *((1,) * (input.ndim - 1)))\n        sigma = sigma.to(input.device)\n    eps = model(input, sigma=sigma, **kwargs)\n    if sigma is not None:\n        eps = eps * sigma\n    return eps\n\n", "CodeBleu_score": 38.591174407965546}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus.clone().detach())\n    alpha.requires_grad_()\n    log_alpha = alpha.log()\n    log_alpha.backward(torch.ones_like(log_alpha))\n    integrand = alpha * taus.grad\n    return integrand\n", "CodeBleu_score": 17.08052551060697}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = torch.exp(-0.5 * taus ** 2 * (beta_1 - beta_0) - taus * beta_0)\n    log_alpha = torch.log(alpha)\n    alpha.backward(retain_graph=True)\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    return grad_log_alpha * alpha\n\n", "CodeBleu_score": 18.982072101898744}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha_t = t2alpha_fn(beta_0, beta_1, taus_clone)\n    alpha_t_log = torch.log(alpha_t)\n    alpha_t_log.backward(torch.ones_like(alpha_t_log))\n    integrand = alpha_t * taus_clone.grad\n    return integrand\n\n", "CodeBleu_score": 24.639067194893933}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'denoised': denoised})\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        x = x + d * (sigma_down - sigmas[i])\n        x = x + noise_sampler(sigma_down, sigma_up) * s_noise\n    return x\n\n", "CodeBleu_score": 68.81709313568832}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'denoised': denoised})\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        x = x + d * (sigma_down - sigmas[i])\n        if sigma_up > 0:\n            x = x + noise_sampler(sigma_up, sigmas[i + 1]) * (sigma_up ** 2 - sigmas[i] ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 65.33274754765668}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'denoised': denoised})\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        dt = sigma_up - sigma_down\n        x = x + d * dt\n        x = x + noise_sampler(sigma_up, sigmas[i + 1]) * s_noise\n    return x\n\n", "CodeBleu_score": 63.589832617967915}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > 1 and i < order - 1:\n        raise ValueError(\"Order must be at least 1.\")\n    if order == 1:\n        return 1.0 if i == j else 0.0\n    if order == 2:\n        return 2.0 * t if i == j else 1.0 if i == j + 1 else 0.0\n    if order == 3:\n        return 3.0 * t ** 2 if i == j else 4.0 * t if i == j + 1 else 1.0 if i == j + 2 else 0.0\n    if order == 4:\n        return 4.0 * t ** 3 if i == j else 5.0 * t ** 2 if i == j + 1 else 6.0 * t if i == j + 2 else 1.0 if i == j + 3 else 0.0\n    if order == 5:\n        return 5.0 * t ** 4 if i == j else 7.0 * t ** 3 if i == j + 1 else 9.0 * t ** 2 if i == j + 2 else 6.0 * t if i == j + 3 else 1.0 if i == j + 4 else 0.0\n    if order == 6:\n        return 6.0 * t ** 5 if i == j else 10.0 * t ** 4 if i == j + 1 else 15.0 * t ** 3 if i == j + 2 else 12.0 * t ** 2 if i == j + 3 else 6.0 * t if i == j + 4 else 1.0 if i == j + 5 else 0.0\n    if order == 7:\n        return 7.0 * t ** 6 if i == j else 15.0 * t ** 5 if i == j + 1 else 21.0 * t ** 4 if i == j + 2 else 20.0 * t ** 3 if i == j + 3 else 12.0 * t ** 2 if i == j + 4 else 6.0 * t if i == j + 5 else 1.0 if i == j + 6 else 0", "CodeBleu_score": 12.35109909701273}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        if i == j:\n            return 1\n        else:\n            return 0\n    elif order == 2:\n        if i == j:\n            return 1\n        elif i == j + 1:\n            return t[i] - t[j]\n        else:\n            return 0\n    elif order == 3:\n        if i == j:\n            return 1\n        elif i == j + 1:\n            return t[i] - t[j]\n        elif i == j + 2:\n            return (t[i] - t[j]) * (t[i] - t[j + 1]) / 2\n        else:\n            return 0\n    elif order == 4:\n        if i == j:\n            return 1\n        elif i == j + 1:\n            return t[i] - t[j]\n        elif i == j + 2:\n            return (t[i] - t[j]) * (t[i] - t[j + 1]) / 2\n        elif i == j + 3:\n            return (t[i] - t[j]) * (t[i] - t[j + 1]) * (t[i] - t[j + 2]) / 6\n        else:\n            return 0\n    else:\n        raise ValueError(\"Order must be 1, 2, 3, or 4.\")\n\n", "CodeBleu_score": 18.075254336833698}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if j >= order:\n        raise ValueError(\"Order too high for step index.\")\n    def integrand(t, i, j):\n        return (t / (i + 1) - j / (i + 1)) ** order\n    return quad(integrand, t[i], t[i + 1], args=(i, j))[0]\n\n", "CodeBleu_score": 20.96601614034859}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().ex", "CodeBleu_score": 62.216280074663565}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (1 + 1)\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n            if sigmas[i + 1] != 0:\n                x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2.0)\n    return x\n\n", "CodeBleu_score": 69.78117571723665}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = DDPMSampler_step(x / torch.sqrt(1.0 + sigmas[i] ** 2.0), sigmas[i], sigmas[i + 1], (x - denoised) / sigmas[i], noise_sampler)\n        if sigmas[i + 1] != 0:\n            x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2.0)\n    return x\n\n", "CodeBleu_score": 72.76539267089915}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if x.ndim > target_dims:\n        raise ValueError(f\"Target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({x.ndim})\")\n    if x.device.type == 'mps':\n        return x.expand(*(target_dims - x.ndim) * [1]).detach()\n    else:\n        return x.expand(*(target_dims - x.ndim) * [1])\n\n", "CodeBleu_score": 24.902995437929178}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.ndim == target_dims:\n        return x\n    if x.ndim > target_dims:\n        raise ValueError(f\"Cannot reduce dimensions from {x.ndim} to {target_dims}\")\n    return x.expand(*(target_dims - x.ndim) * [1]).detach() if x.device.type == 'mps' else x.expand(*(target_dims - x.ndim) * [1])\n\n", "CodeBleu_score": 31.087838504203187}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.ndim == target_dims:\n        return x\n    if x.ndim > target_dims:\n        raise ValueError(f\"Cannot reduce dimensions from {x.ndim} to {target_dims}\")\n    num_dims_to_append = target_dims - x.ndim\n    x = x.unsqueeze(-1)\n    for _ in range(num_dims_to_append):\n        x = x.unsqueeze(-1)\n    if x.device.type == \"mps\":\n        return x.detach()\n    return x\n\n", "CodeBleu_score": 40.26827416686732}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is None:\n            return path\n        with path.open('rb') as f:\n            file_digest = hashlib.sha256(f.read()).hexdigest()\n        if file_digest == digest:\n            return path\n        warnings.warn(f'File {path} exists but the SHA-256 hash does not match. Will download again.')\n    with urllib.request.urlopen(url) as source, path.open('wb') as output:\n        shutil.copyfileobj(source, output)\n    with path.open('rb') as f:\n        file_digest = hashlib.sha256(f.read()).hexdigest()\n    if file_digest != digest:\n        raise RuntimeError(f'File {path} exists, but the SHA-256 hash does not match.')\n    return path\n\n", "CodeBleu_score": 46.99909583631916}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            with path.open('rb') as f:\n                if hashlib.sha256(f.read()).hexdigest() == digest:\n                    return path\n                else:\n                    raise RuntimeError(f'File {path} exists but digest does not match.')\n        else:\n            return path\n    with urllib.request.urlopen(url) as source:\n        with path.open('wb') as output:\n            shutil.copyfileobj(source, output)\n    if digest is not None:\n        with path.open('rb') as f:\n            if hashlib.sha256(f.read()).hexdigest() != digest:\n                raise RuntimeError(f'File {path} downloaded but digest does not match.')\n    return path\n\n", "CodeBleu_score": 43.84537405126453}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n                return path\n            else:\n                raise RuntimeError(f'{path} exists, but the SHA-256 hash does not match {digest}')\n        else:\n            return path\n    with urllib.request.urlopen(url) as source, path.open('wb') as output:\n        shutil.copyfileobj(source, output)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n            return path\n        else:\n            raise RuntimeError(f'The SHA-256 hash of {path} does not match {digest}')\n    return path\n\n", "CodeBleu_score": 42.65192916299836}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    modes = []\n    for module in model.modules():\n        if hasattr(module, 'training'):\n            modes.append(module.training)\n            module.train(mode)\n    try:\n        yield model\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            if hasattr(module, 'training'):\n                module.train(mode)\n\n", "CodeBleu_score": 35.24563916795456}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_modes = {m: m.training for m in model.modules()}\n    if mode:\n        for m in model.modules():\n            m.train()\n    else:\n        for m, mode in prev_modes.items():\n            m.train(mode)\n    try:\n        yield\n    finally:\n        for m, mode in prev_modes.items():\n            m.train(mode)\n\n", "CodeBleu_score": 30.345979646004405}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = {m: m.training for m in model.modules()}\n    try:\n        for m in model.modules():\n            m.train(mode)\n        yield model\n    finally:\n        for m, mode in modes.items():\n            m.train(mode)\n\n", "CodeBleu_score": 35.56894457939228}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    averaged_model.load_state_dict(averaged_model.state_dict())\n    for param_name, averaged_param in averaged_model.named_parameters():\n        averaged_param.data.copy_(decay * averaged_param.data + (1 - decay) * model.state_dict()[param_name].data)\n    for buffer_name, averaged_buffer in averaged_model.named_buffers():\n        averaged_buffer.data.copy_(model.state_dict()[buffer_name].data)\n    return averaged_model\n\n", "CodeBleu_score": 34.143921472985696}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 29.081729232085518}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 31.798444875826036}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    l = torch.logit((min_value / max_value).clamp(0, 1))\n    u = u * (torch.logit((max_value / min_value).clamp(0, 1)) - l) + l\n    return torch.log1p(torch.exp(u * scale + loc)).to(dtype)\n\n", "CodeBleu_score": 38.54067070461597}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf = min_value.log1p().neg().log1p().neg().log1p().neg().add(1).mul(0.5)\n    max_cdf = max_value.log1p().neg().log1p().neg().log1p().neg().add(1).mul(0.5)\n    uniform = torch.rand(shape, device=device, dtype=dtype)\n    uniform = uniform * (max_cdf - min_cdf) + min_cdf\n    logit = uniform.log1p().neg().log1p().neg().log1p().neg().add(1).mul(0.5)\n    logit = logit.mul(scale).add(loc)\n    return logit.exp()\n\n", "CodeBleu_score": 38.849576570389864}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_value_cdf = torch.distributions.log_normal.LogNormal(loc, scale).cdf(min_value)\n    max_value_cdf = torch.distributions.log_normal.LogNormal(loc, scale).cdf(max_value)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    uniform_samples = min_value_cdf + uniform_samples * (max_value_cdf - min_value_cdf)\n    return torch.distributions.log_normal.LogNormal(loc, scale).icdf(uniform_samples).to(dtype)\n\n", "CodeBleu_score": 29.747375336481145}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ops.Activation1d(nn.ELU(inplace=True)) if antialias else nn.ELU(inplace=True)\n    if activation == \"snake\":\n        return ops.Activation1d(SnakeBeta(channels)) if antialias else SnakeBeta(channels)\n    if activation == \"none\":\n        return ops.Activation1d(nn.Identity()) if antialias else nn.Identity()\n    raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 43.15818941285699}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ops.Activation1d(nn.ELU(inplace=True)) if antialias else nn.ELU(inplace=True)\n    if activation == \"snake\":\n        return ops.Activation1d(SnakeBeta(channels)) if antialias else SnakeBeta(channels)\n    if activation == \"none\":\n        return ops.Activation1d(nn.Identity()) if antialias else nn.Identity()\n", "CodeBleu_score": 41.415776640292975}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ops.Activation1d(nn.ELU(inplace=True)) if antialias else nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return ops.Activation1d(SnakeBeta(channels)) if antialias else SnakeBeta(channels)\n    elif activation == \"none\":\n        return ops.Activation1d(nn.Identity()) if antialias else nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n", "CodeBleu_score": 47.858167038426885}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    # device = freqs.device\n    device = t.device\n    dtype = t.dtype\n    # t = t.to(torch.float32)\n\n    t = t / self.interpolation_factor\n\n    freqs = torch.einsum('i , j -> i j', t, self.inv_freq.to(dtype=dtype, device=device))\n    freqs = torch.cat((freqs, freqs), dim = -1)\n\n    if self.scale is None:\n        return freqs, 1.\n\n    power = (torch.arange(seq_len, device = device) - (seq_len // 2)) / self.scale_base\n    scale = self.scale.to(dtype=dtype, device=device) ** rearrange(power, 'n -> n 1')\n    scale = torch.cat((scale, scale), dim = -1)\n\n    return freqs, scale\n\n\n    # freqs = freqs * scale\n    # freqs = freqs.to(dtype=dtype, device=device)\n\n    dim = t.shape[-1]\n    t2 = rotate_half(t)\n    t2 = rearrange(t2, '... (j d) -> ... j d', j = 2)\n    t2_real, t2_imag = t2.unbind(dim = -2)\n    freqs_real, freqs_imag = rearrange(freqs, '... (j d) -> ... j d', j = 2).unbind(dim = -2)\n\n    t_real = t2_real * freqs_real - t2_imag * freqs_imag\n    t_imag = t2_real * freqs_imag + t2_imag * freqs_real\n\n    t = torch.stack((t_real, t_imag), dim = -1)\n    t = rearrange(t, '... j d -> ... (j d)')", "CodeBleu_score": 35.1110895046609}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if scale != 1:\n        t = t * scale\n    t = t.to(torch.float32)\n    sin, cos = (freqs[..., 0::2], freqs[..., 1::2])\n    return torch.cat((cos * t[..., None, :], sin * t[..., None, :]), dim = -1)\n\n", "CodeBleu_score": 14.760714760370803}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    dtype = t.dtype\n    if dtype == torch.float16:\n        t = t.float()\n        freqs = freqs.float()\n        scale = scale.float()\n    elif dtype == torch.bfloat16:\n        t = t.float()\n        freqs = freqs.float()\n        scale = scale.float()\n\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    freqs = rearrange(freqs, '... (j d) -> ... j d', j = 2)\n\n    t1, t2 = t.unbind(dim = -2)\n    freq1, freq2 = freqs.unbind(dim = -2)\n\n    t1 = t1 * scale\n    t2 = t2 * scale\n\n    phase_1 = t1 * freq1\n    phase_2 = t2 * freq2\n\n    t1 = torch.cos(phase_1) * t1 - torch.sin(phase_1) * t2\n    t2 = torch.sin(phase_2) * t1 + torch.cos(phase_2) * t2\n\n    return torch.cat((t1, t2), dim = -1)\n", "CodeBleu_score": 34.751831111394694}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    codebook = self.codebook.weight\n    if dim != -1:\n        idx = idx.movedim(-1, dim)\n    return codebook.gather(0, idx)\n\n", "CodeBleu_score": 52.72662759399624}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embeddings = self.codebook(idx)\n    if dim != -1:\n        embeddings = embeddings.permute(0, dim, *range(1, embeddings.ndim))\n    return embeddings\n\n", "CodeBleu_score": 38.58271552650276}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    codebook = self.codebook.weight\n    codebook = codebook.to(idx.device)\n    codebook = codebook.index_select(dim=0, index=idx)\n    if dim != -1:\n        codebook = codebook.permute(dim, *range(dim), len(codebook.shape) - 1)\n    return codebook\n\n", "CodeBleu_score": 31.625512344564555}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x)\n        return x, (vq_loss, commit_loss), indices\n    return x\n\n", "CodeBleu_score": 38.76577746669878}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x)\n        return x, (vq_loss, commit_loss), indices\n    else:\n        return x\n\n", "CodeBleu_score": 41.035094857052314}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    encodings = []\n    for block in self.down_blocks:\n        x = block(x)\n        encodings.append(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vquantizer(x)\n        return x, encodings, indices, (vq_loss, commit_loss)\n    return x, encodings\n", "CodeBleu_score": 31.862190744596358}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        self.model_ema.log_switch(context)\n    if self.use_ema:\n        self.model_ema.apply_ema()\n    yield\n    if self.use_ema:\n        self.model_ema.restore()\n\n", "CodeBleu_score": 31.657898787922022}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is not None:\n        self.model_ema.log_switch(context)\n    with self.model_ema.ema_scope():\n        yield\n    if context is not None:\n        self.model_ema.log_restore(context)\n\n", "CodeBleu_score": 40.84761453568496}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        with self.model_ema.ema_scope(context):\n            yield\n    else:\n        yield\n\n", "CodeBleu_score": 13.257888236425707}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn\n    x = modulate(x, shift_mlp, scale_mlp)\n    if not self.pre_only:\n        x = self.mlp(x)\n        x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 38.82830266618114}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    if not self.pre_only:\n        x = self.mlp(x)\n        x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 42.26471657052895}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn\n    if not self.pre_only:\n        x = self.norm2(x)\n        x = modulate(x, shift_mlp, scale_mlp)\n        if not self.scale_mod_only:\n            x = x * gate_mlp\n        x = self.mlp(x)\n        if not self.scale_mod_only:\n            x = x * gate_msa\n    return x\n\n", "CodeBleu_score": 41.564605665085395}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat((qkv_context, qkv_x), dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context, x = torch.split(attn, [context.shape[1], x.shape[1]], dim=1)\n    context = context_block.post_attention(context, *intermediates_context)\n    x = x_block.post_attention(x, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 23.990245001212813}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context_attn, x_attn = attn.chunk(2, dim=1)\n    context = context_block.post_attention(context_attn, *intermediates_context)\n    x = x_block.post_attention(x_attn, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 23.318463590811216}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat((qkv_context, qkv_x), dim=0)\n    attn = optimized_attention(\n        qkv,\n        num_heads=context_block.attn.num_heads,\n    )\n    context_part, x_part = torch.split(attn, [context.shape[1], x.shape[1]], dim=1)\n    context = context_block.post_attention(context_part, *intermediates_context)\n    x = x_block.post_attention(x_part, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 27.088137852331045}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'Timesteps must be a one-dimensional tensor'\n    timesteps = timesteps.type(torch.long)\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n    return emb\n\n", "CodeBleu_score": 75.98336166532332}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps must be a one-dimensional tensor\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), mode=\"constant\", value=0)\n    return emb\n\n", "CodeBleu_score": 75.2568754801216}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'Timesteps must be a one-dimensional tensor'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), 'constant', 0)\n    return emb\n\n", "CodeBleu_score": 63.91562484745252}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b, c, h, w = q.shape\n    q = q.reshape(b, c, h * w)\n    q = q.permute(0, 2, 1)  # b,hw,c\n    k = k.reshape(b, c, h * w)  # b,c,hw\n    v = v.reshape(b, c, h * w)\n    try:\n        r1 = torch.nn.functional.softmax(torch.bmm(q, k) / math.sqrt(k.shape[1]), dim=2)\n        r1 = torch.bmm(v, r1)\n        h_ = r1.reshape(b, c, h, w)\n        del r1\n        return h_\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 23.936497861768576}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_bias=None)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n", "CodeBleu_score": 57.309222081665254}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b, c, h, w = q.shape\n\n    q = q.reshape(b, c, h * w)\n    q = q.permute(0, 2, 1)  # b,hw,c\n    k = k.reshape(b, c, h * w)  # b,c,hw\n    v = v.reshape(b, c, h * w)\n\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b, c, h, w)\n    del r1\n    return h_\n\n", "CodeBleu_score": 12.116218986696556}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        control_value = control.pop(name)\n        try:\n            h = h + control_value\n        except Exception:\n            warnings.warn(f'Failed to apply control {name} to {h}.')\n    return h\n\n", "CodeBleu_score": 34.50258546843931}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control and control[name] != \\\"\\\":\n        try:\n            h = h + control[name]\n        except:\n            logger.warning(f\\\"Failed to apply control {name} to h.\\\")\n    return h\n\n", "CodeBleu_score": 41.02980485790636}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        control = control[name]\n        if control:\n            try:\n                h = h + control\n            except Exception as e:\n                print(f'Warning: Could not add control {name} to the timestep embedding.')\n                print(e)\n    return h\n\n", "CodeBleu_score": 37.73574642133107}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n        # shape input: (batch, height, width, channel)\n       ", "CodeBleu_score": 2.0359848484848486}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape input: torch.Size([2048, 512])\n\n        # add a small epsilon to the indices to ensure they are differentiable\n        z_q = torch.cat([self.embedding(indices + eps) for eps in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]], dim=0)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n        return z_q\n\n", "CodeBleu_score": 30.378822364560193}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # get quantized latent vectors\n    min_encoding_indices = torch.argmin(self.codebook_dist[indices], dim=1)\n    min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(self.device)\n    min_encodings.scatter_(1, min_encoding_indices.unsqueeze(1), 1)\n    z_q = torch.matmul(min_encodings, self.embedding.weight)\n    # reshape back to match original input shape\n    if shape is not None:\n        z_q = z_q.view(shape)\n    return z_q\n\n", "CodeBleu_score": 40.24319243213485}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    device = self.constant_input.weight.device\n\n    def make_noise_():\n        return torch.randn(1, 1, 4, 4, device=device)\n\n    noises = [make_noise_()]\n\n    if self.log_size == 3:\n        return noises\n    for i in range(3, self.log_size + 1):\n        for j in range(2):\n            noises.append(make_noise_())\n    return noises\n\n", "CodeBleu_score": 45.386080954664216}, {"code": "def make_noise(self)\n\t\n    device = self.constant_input.weight.device\n    noise = []\n    for i in range(self.num_layers):\n        size = 2**((i + 5) // 2)\n        shape = [1, 1, size, size]\n        tensor = torch.randn(*shape, device=device)\n        noise.append(tensor)\n    return noise\n\n", "CodeBleu_score": 36.32804456267616}, {"code": "def make_noise(self)\n\t\n    device = self.constant_input.weight.device\n    make_noise_fn = lambda res: torch.randn(1, self.num_style_feat, res, res, device=device)\n    return [make_noise_fn(4)] + [make_noise_fn(res) for res in self.channels.keys()][::-1]\n\n", "CodeBleu_score": 25.72506735263918}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    shape = [1, self.num_style_feat, 1, 1]\n    x = torch.randn(shape, device=self.constant_input.weight.device)\n    x = self.style_mlp(x)\n    x = x[:, :, 0, 0]\n    x = x.repeat(num_latent, 1)\n    return x\n\n", "CodeBleu_score": 30.299774957770985}, {"code": "def mean_latent(self, num_latent)\n\t\n    shape = [1, num_latent, 1, 1]\n    mean_latent = torch.randn(shape, device=self.device)\n    with torch.no_grad():\n        mean_latent = self.style_mlp(mean_latent)\n    return mean_latent[0]\n\n", "CodeBleu_score": 30.23025941063773}, {"code": "def mean_latent(self, num_latent)\n\t\n    shape = [num_latent, self.num_style_feat]\n    mean_latent = torch.randn(shape, device=self.constant_input.weight.device)\n    with torch.no_grad():\n        mean_latent = self.style_mlp(mean_latent)\n    return mean_latent.mean(0, keepdim=True)\n\n", "CodeBleu_score": 38.653650145711495}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) * shift * 2 - shift\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n", "CodeBleu_score": 44.02047147437378}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) * shift\n    jitter = torch.cat([jitter, 1 - jitter.sum()]).view(3, 1, 1)\n    jitter = jitter.expand(3, img.shape[1], img.shape[2])\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 47.40563076434135}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3).to(img.device) * shift * 2 - shift\n    img = torch.clamp(img + jitter, 0, 1)\n    return img\n\n", "CodeBleu_score": 32.08209027651656}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye_loc' in data:\n        self.left_eye_loc = data['left_eye_loc'].to(self.device)\n    if 'right_eye_loc' in data:\n        self.right_eye_loc = data['right_eye_loc'].to(self.device)\n    if 'mouth_loc' in data:\n        self.mouth_loc = data['mouth_loc'].to(self.device)\n\n", "CodeBleu_score": 55.44895567080691}, {"code": "def feed_data(self, data)\n\t\n    if self.use_facial_disc:\n        # facial component locations\n        self.lq_left_eye = data['lq_left_eye'].to(self.device)\n        self.lq_right_eye = data['lq_right_eye'].to(self.device)\n        self.lq_mouth = data['lq_mouth'].to(self.device)\n        self.gt_left_eye = data['gt_left_eye'].to(self.device)\n        self.gt_right_eye = data['gt_right_eye'].to(self.device)\n        self.gt_mouth = data['gt_mouth'].to(self.device)\n\n        # facial component masks\n        self.lq_left_eye_mask = data['lq_left_eye_mask'].to(self.device)\n        self.lq_right_eye_mask = data['lq_right_eye_mask'].to(self.device)\n        self.lq_mouth_mask = data['lq_mouth_mask'].to(self.device)\n\n        # facial component masks for the ground truth\n        self.gt_left_eye_mask = data['gt_left_eye_mask'].to(self.device)\n        self.gt_right_eye_mask = data['gt_right_eye_mask'].to(self.device)\n        self.gt_mouth_mask = data['gt_mouth_mask'].to(self.device)\n\n        # facial component masks for the ground truth\n        self.gt_left_eye_mask = data['gt_left_eye_mask'].to(self.device)\n        self.gt_right_eye_mask = data['gt_right_eye_mask'].to(self.device)\n        self.gt_mouth_mask = data['gt_mouth_mask'].to(self.device)\n\n        # facial component masks for the ground truth\n        self.gt_left_eye_mask = data['gt_left_eye_mask'].to(self.device)\n        self", "CodeBleu_score": 29.046910556239887}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    # construct img pyramid\n\n    :return: img pyramid as a list\n    \"\"\"\n    img_pyramid = [self.gt]\n    for _ in range(self.opt['train']['loss_pl_pyramid_level']):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 38.72005950049311}, {"code": "def construct_img_pyramid(self)\n\t\n    # generate image pyramid\n        gt_pyramid = [self.gt]\n        for i in range(self.opt['train']['pyramid_loss_n_levels']):\n            gt_pyramid.insert(0, F.interpolate(gt_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n\n        return gt_pyramid\n\n", "CodeBleu_score": 24.397507766086083}, {"code": "def construct_img_pyramid(self)\n\t\n    self.img_pyramid = [self.gt]\n        h, w = self.gt.shape[2:]\n    for i in range(self.num_pyramid_levels):\n        h = h // 2\n        w = w // 2\n        if h < self.min_size or w < self.min_size:\n            break\n        self.img_pyramid.insert(0, F.interpolate(self.img_pyramid[0], size=(h, w), mode='bilinear', align_corners=False))\n\n", "CodeBleu_score": 23.009594950259125}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['rank'] == 0:\n        # save the network weights\n        if self.opt['save_by_epoch']:\n            save_path = self.opt['path']['models']\n            save_path = osp.join(save_path, '{}_epoch_{}.pth'.format(self.opt['name'], epoch))\n        else:\n            save_path = self.opt['path']['models']\n            save_path = osp.join(save_path, '{}_iter_{}.pth'.format(self.opt['name'], current_iter))\n        if hasattr(self, 'net_g_ema'):\n            self.save_network(self.net_g_ema, 'net_g_ema', save_path)\n        self.save_network(self.net_g, 'net_g', save_path)\n        self.save_network(self.net_d, 'net_d', save_path)\n\n        # save facial discriminators\n        if self.use_facial_disc:\n            self.save_network(self.net_d_left_eye, 'net_d_left_eye', save_path)\n            self.save_network(self.net_d_right_eye, 'net_d_right_eye', save_path)\n            self.save_network(self.net_d_mouth, 'net_d_mouth', save_path)\n\n        # save the training state\n        self.save_training_state(epoch, current_iter, save_path)\n\n", "CodeBleu_score": 45.92183952717205}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.net_g, 'net_g', current_iter)\n    self.save_network(self.net_g_ema, 'net_g_ema', current_iter)\n    self.save_network(self.net_d, 'net_d', current_iter)\n    if self.use_facial_disc:\n        self.save_network(self.net_d_left_eye, 'net_d_left_eye', current_iter)\n        self.save_network(self.net_d_right_eye, 'net_d_right_eye', current_iter)\n        self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 81.75887291560552}, {"code": "def save(self, epoch, current_iter)\n\t\n    save_checkpoint(self.net_g, self.optimizer_g, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_g')\n    if hasattr(self, 'net_g_ema'):\n        save_checkpoint(self.net_g_ema, self.optimizer_g_ema, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_g_ema')\n    save_checkpoint(self.net_d, self.optimizer_d, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_d')\n    if self.use_facial_disc:\n        save_checkpoint(self.net_d_left_eye, self.optimizer_d_left_eye, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_d_left_eye')\n        save_checkpoint(self.net_d_right_eye, self.optimizer_d_right_eye, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_d_right_eye')\n        save_checkpoint(self.net_d_mouth, self.optimizer_d_mouth, epoch, current_iter, self.opt, self.opt['path']['models'], 'net_d_mouth')\n    save_checkpoint(self, epoch, current_iter, self.opt, self.opt['path']['models'], 'train_state')\n\n", "CodeBleu_score": 42.05155543971585}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n", "CodeBleu_score": 54.3909785745615}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n", "CodeBleu_score": 54.3909785745615}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n", "CodeBleu_score": 54.3909785745615}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n    sampled_batch = random.sample(self.exps, sample_size)\n    self.exps = [e for e in self.exps if e not in sampled_batch]\n    return sampled_batch\n\n", "CodeBleu_score": 61.38636540397561}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    sampled_batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in sampled_batch]\n    return sampled_batch\n\n", "CodeBleu_score": 73.48601133467406}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in batch]\n    return batch\n\n", "CodeBleu_score": 65.89215820118145}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = [self.all_configs[i] for i in self.trials]\n        configs_flatten = []\n        for config in all_configs:\n            configs_flatten.append(flatten_dict(config))\n        configs_flatten = np.array(configs_flatten)\n        estimated_perf = self.cost_model.predict(configs_flatten)\n        sorted_indices = np.argsort(estimated_perf)\n        return [self.trials[i] for i in sorted_indices]\n\n    def evaluate_configs(self, configs):\n        \"\"\"Evaluate the performance of the given configurations.", "CodeBleu_score": 24.137867878077845}, {"code": "def find_estimated_top_configs(self)\n\t\n    configs = [flatten_dict(config) for config in self.all_configs]\n    configs = np.array([[config[key] for key in self.dims] for config in configs])\n    estimated_costs = self.cost_model.predict(configs)\n    sorted_indices = np.argsort(estimated_costs)\n    top_indices = sorted_indices[-self.num_all_configs:]\n    return top_indices\n\n", "CodeBleu_score": 17.980972640384458}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_configs.append(flatten_dict(config))\n\n    flattened_configs = np.array(flattened_configs)\n\n    # filter out numerical values\n    numerical_values = flattened_configs[:, self.dims.numerical_dims]\n\n    # predict cost\n    cost = self.cost_model.predict(numerical_values)\n\n    # sort by cost\n    sorted_indices = np.argsort(cost)\n\n    # return the top 20% of the sorted indices\n    return sorted_indices[:int(len(sorted_indices) * 0.2)]\n\n", "CodeBleu_score": 26.312418236259816}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 73.88944015415018}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t       try:\n            p = subprocess.Popen(self.optimal_cmd)\n            p.wait()\n            logger.info(\\\"Successfully ran the optimal command found by autotuning.\\\")\n        except Exception as e:\n            logger.error(f\\\"Failed to run the optimal command found by autotuning: {e}\\\")\n    else:\n        logger.info(\\\"Autotuning did not find an optimal configuration.\\\")\n\n    if self.args.run_after_tuning:\n        self.run_after_tuning()\n\n    logger.info(\\\"Finished autotuning.\\\")\n\n    return\n\n    def get_best_space_records(self):\n        \\\"\\\"\\\"Get the best space records for each space.\n        Returns:\n            [dict]: a dictionary with the best space records for each space.\n        \\\"\\\"\\\"\n        best_space_records = {}\n        for space in self.space_records:\n            best_space_records[space] = self.get_best_space_record(space)\n        return best_space_records\n\n    def get_best_space_record(self, space):\n        \\\"\\\"\\\"Get the best space record for the given space.\n        Args:\n            space (str): the space to get the best space record for.\n\n        Returns:\n            [tuple]: a tuple with the best space record, the best metric value, and the best config.\n        \\\"\\\"\\\"\n        best_space_record = None\n        best_metric_val = None\n        best_config = None\n        for space_record in self.space_records[space]:\n            if best_metric_val is None or space_record[1] < best_metric_val:\n                best_space_record = space_record\n                best_metric_val = space_record[1]\n                best_config = space_record[0]\n        return best_space_record, best_", "CodeBleu_score": 42.4586373192263}, {"code": "def run_after_tuning(self)\n\t    subprocess.Popen(self.optimal_cmd).wait()\n            logger.info(\\\"The optimal command has been successfully run.\\\")\n        except Exception as e:\n            logger.info(\\\"Failed to run the optimal command found by autotuning.\\\")\n            logger.info(e)\n        self.optimal_cmd = None\n    else:\n        logger.info(\\\"Autotuning did not find an optimal configuration.\\\")\n\n", "CodeBleu_score": 37.74231291953407}, {"code": "def run_after_tuning(self)\n\t     subprocess.Popen(self.optimal_cmd).wait()\n        logger.info(f\\\"Successfully ran the optimal command found by autotuning: {self.optimal_cmd}\\\")\n    else:\n        logger.info(f\\\"Autotuning did not find an optimal configuration.\\\")\n\n", "CodeBleu_score": 38.9100551804704}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\\\"exp_id\\\"]\n    exp_name = exp[\\\"name\\\"]\n    exp_args = copy.deepcopy(self.args)\n    exp_args.exp_id = exp_id\n    exp_args.name = exp_name\n    exp_args.result_dir = exp[\\\"result_dir\\\"]\n    exp_args.ds_config = exp[\\\"ds_config\\\"]\n    exp_args.hostfile = exp[\\\"hostfile\\\"]\n    exp_args.num_gpus = exp[\\\"num_gpus\\\"]\n    exp_args.master_port = exp[\\\"master_port\\\"]\n    exp_args.num_nodes = exp[\\\"num_nodes\\\"]\n    exp_args.num_gpus_per_node = exp[\\\"num_gpus_per_node\\\"]\n    exp_args.master_addr = exp[\\\"master_addr\\\"]\n    exp_args.master_port = exp[\\\"master_port\\\"]\n    exp_args.rank = exp[\\\"rank\\\"]\n    exp_args.node_rank = exp[\\\"node_rank\\\"]\n    exp_args.world_size = exp[\\\"world_size\\\"]\n    exp_args.node_list = exp[\\\"node_list\\\"]\n    exp_args.gpu_ids = exp[\\\"gpu_ids\\\"]\n    exp_args.master_addr = exp[\\\"master_addr\\\"]\n    exp_args.master_port = exp[\\\"master_port\\\"]\n    exp_args.rank = exp[\\\"rank\\\"]\n    exp_args.node_rank = exp[\\\"node_rank\\\"]\n    exp_args.world_size = exp[\\\"world_size\\\"]\n    exp_args.node_list = exp[\\\"node_list\\\"]\n    exp_args.gpu_ids = exp[\\\"gpu_ids\\\"]\n    exp_args.node_list = exp[\\\"node_list\\\"]\n    exp_args.gpu_ids = exp[\\\"gpu_ids\\\"]\n    exp_args.node_list = exp[\\\"node_list\\\"]\n    exp_args.gpu_ids = exp[\\\"", "CodeBleu_score": 11.90045515673649}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\\\"exp_id\\\"]\n    exp[\\\"master_port\\\"] = self.get_master_port()\n    exp[\\\"result_dir\\\"] = os.path.join(self.results_dir, exp[\\\"name\\\"])\n    if AUTOTUNING in exp[\\\"ds_config\\\"]:\n        metric_file = os.path.join(exp[\\\"result_dir\\\"], \\\"metrics.json\\\")\n        exp[\\\"ds_config\\\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n    stderr_file = os.path.join(exp[\\\"result_dir\\\"], \\\"stderr.log\\\")\n    model_info_file = os.path.join(exp[\\\"result_dir\\\"], \\\"model_info.json\\\")\n    metric_file = os.path.join(exp[\\\"result_dir\\\"], \\\"metrics.json\\\")\n    if os.path.exists(stderr_file) and not was_interruptted(stderr_file):\n        err = search_error(stderr_file)\n        self.finished_experiments[exp_id] = (exp, err)\n        if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n            logger.info(f\\\"Skipping exp {exp['name']} whose result already exists\\\")\n            return\n\n    # update user args\n    for k, v in self.arg_mappings.items():\n        if k in exp[\\\"args\\\"]:\n            exp[\\\"args\\\"][k] = v\n\n    # create experiment directory\n    os.makedirs(exp[\\\"result_dir\\\"], exist_ok=True)\n\n    # create experiment file\n    exp_file = os.path.join(self.exps_dir, exp[\\\"name\\\"] + \\\".json\\\")\n    with open(exp_file, \\\"w\\\") as fd:\n        hjson.dump(exp, fd)\n\n    # start experiment\n    self.running_experiments[", "CodeBleu_score": 29.686880996461483}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\\\"exp_id\\\"]\n    exp_name = exp[\\\"name\\\"]\n    exp_path = exp[\\\"exp_path\\\"]\n    exp_script = exp[\\\"exp_script\\\"]\n    exp_args = exp[\\\"exp_args\\\"]\n    exp_args = exp_args.split(\\\" \\\")\n    exp_args = [arg.strip() for arg in exp_args]\n\n        # update user arguments with argument mappings\n        for k, v in self.arg_mappings.items():\n            for i, arg in enumerate(exp_args):\n                if arg == k:\n                    exp_args[i] = v\n\n        exp_args = \\\" \\\".join(exp_args)\n        logger.info(f\\\"Running exp {exp_name} with args {exp_args}\\\")\n        exp_args = exp_args.split(\\\" \\\")\n    exp_args = [arg.strip() for arg in exp_args]\n\n        # update user arguments with argument mappings\n        for k, v in self.arg_mappings.items():\n            for i, arg in enumerate(exp_args):\n                if arg == k:\n                    exp_args[i] = v\n\n        exp_args = \\\" \\\".join(exp_args)\n        logger.info(f\\\"Running exp {exp_name} with args {exp_args}\\\")\n        exp_args = exp_args.split(\\\" \\\")\n    exp_args = [arg.strip() for arg in exp_args]\n\n        # update user arguments with argument mappings\n        for k, v in self.arg_mappings.items():\n            for i, arg in enumerate(exp_args):\n                if arg == k:\n                    exp_args[i] = v\n\n        exp_args = \\\" \\\".join(exp_args)\n        logger.info(f\\\"Running exp {exp_name} with args {exp_args}\\\")\n        exp_args = exp_args.split(\\\" \\\")\n    exp_args = [arg.strip() for arg in exp_args]\\", "CodeBleu_score": 24.046803848253962}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.is_distributed_init:\n        if ranks is None:\n            ranks = [0]\n        for rank in ranks:\n            message[f\\\"rank_{rank}\\\"] = message\n\n    if self.logging_permitted:\n        if path is None:\n            path = self.log_file\n        with open(path, \\\"a\\\") as fd:\n            json.dump(message, fd)\n            fd.write(\\\"\\", "CodeBleu_score": 21.34662178486591}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.rank == 0 and self.distributed_init:\n        if ranks is None:\n            ranks = list(range(self.world_size))\n        for rank in ranks:\n            message[rank] = self.get_rank_logs(rank)\n        if path is None:\n            path = self.log_file\n        with open(path, \\\"a\\\") as fd:\n            json.dump(message, fd)\n            fd.write(\\\"\\", "CodeBleu_score": 24.492394792945944}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.rank == 0 and self.world_size == 1:\n        if self.args.save_exp_results_to_database:\n            if not path:\n                path = os.path.join(self.args.result_dir, \\\"exp_results.json\\\")\n            with open(path, \\\"a+\\\") as fd:\n                fd.write(json.dumps(message) + \\\"\\", "CodeBleu_score": 20.59196458741565}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n            raise Exception(f'no layer files found for {self.checkpoint_dir}')\n        if layer_index >= len(self.layer_files):\n            raise Exception(f'layer index {layer_index} is out of range')\n        layer_files = self.layer_files[layer_index]\n        layer_files.sort()\n\n        if len(layer_files) % self.tp_degree != 0:\n            raise Exception(f'layer file count {len(layer_files)} is not divisible by tp degree {self.tp_degree}')\n\n        tp_files_map = {}\n        for i in range(self.tp_degree):\n            tp_files_map[i] = layer_files[i::self.tp_degree]\n\n        return tp_files_map\n", "CodeBleu_score": 33.915683805779516}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n            raise ValueError(f'No layer files found. Please make sure to set the layer_files property before calling this method.')\n        if layer_index >= len(self.layer_files):\n            raise ValueError(f'layer_index {layer_index} is out of range. Please make sure to set the layer_files property before calling this method.')\n        layer_file_list = self.layer_files[layer_index]\n        if not layer_file_list:\n            raise ValueError(f'No files found for layer index {layer_index}. Please make sure to set the layer_files property before calling this method.')\n        layer_file_list.sort()\n        tp_other_layer_map = defaultdict(list)\n        for file_name in layer_file_list:\n            tp_index = int(file_name.split(\\'.\\')[-1])\n            tp_other_layer_map[tp_index].append(file_name)\n        return tp_other_layer_map\n\n", "CodeBleu_score": 30.51944137618774}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    assert layer_index < len(self.layer_keys),\\\n        f'layer_index: {layer_index} should be less than len(self.layer_keys): {len(self.layer_keys)}'\n    layer_key = self.layer_keys[layer_index]\n    layer_files = self.zero_checkpoint.get_files_for_layer(layer_key=layer_key)\n    if len(layer_files) == 0:\n        return {}\n    tp_other_layer_map = {}\n    for fname in layer_files:\n        tp_index = self._get_tp_index(fname)\n        if tp_index not in tp_other_layer_map:\n            tp_other_layer_map[tp_index] = []\n        tp_other_layer_map[tp_index].append(fname)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 27.1727934328146}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    info = {}\n    info[\\\"system\\\"] = platform.system()\n    info[\\\"architecture\\\"] = platform.architecture()\n    info[\\\"version\\\"] = platform.version()\n    info[\\\"processor\\\"] = platform.processor()\n    info[\\\"python_version\\\"] = platform.python_version()\n    return info\n\n\n", "CodeBleu_score": 38.84242252775398}, {"code": "def system_info()\n\t\n    info = {}\n    info[\\\"OS\\\"] = platform.system()\n    info[\\\"Architecture\\\"] = platform.architecture()[0]\n    info[\\\"Version\\\"] = platform.version()\n    info[\\\"Processor\\\"] = platform.processor()\n    info[\\\"Python\\\"] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 38.788159840827205}, {"code": "def system_info()\n\t\n    info = {\\\"os\\\": platform.system(), \\\"architecture\\\": platform.architecture()[0], \\\"version\\\": platform.version(), \\\"processor\\\": platform.processor(), \\\"python\\\": platform.python_version()}\n    return info\n\n", "CodeBleu_score": 41.15470259176944}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    return {\n        \\\"CUDA_available\\\": torch.cuda.is_available(),\n        \\\"CUDA_version\\\": torch.version.cuda,\n        \\\"devices\\\": torch.cuda.device_count(),\n        \\\"device_names\\\": torch.cuda.get_device_name(0),\n    }\n\n\n\n\n", "CodeBleu_score": 36.17257356582337}, {"code": "def cuda_info()\n\t\n    return {\n        \\\"available\\\": torch.cuda.is_available(),\n        \\\"devices\\\": torch.cuda.device_count(),\n        \\\"version\\\": torch.version.cuda,\n    }\n\n\n\n\n", "CodeBleu_score": 35.63834318294377}, {"code": "def cuda_info()\n\t\n    return {\n        \\\"gpu\\\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"GPU not found\\\",\n        \\\"cuda\\\": torch.cuda.is_available(),\n        \\\"version\\\": torch.version.cuda,\n    }\n\n", "CodeBleu_score": 33.98568611483126}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"speaker_name\")\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n    class_name_to_class_id = {v: k for k, v in class_id_to_class_name.items()}\n    class_accuracy = {class_name: 0 for class_name in class_id_to_class_name.values()}\n    total_accuracy = 0\n    total_count = 0\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n        class_id = class_name_to_class_id[class_name]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        if embedding is not None:\n            prediction = encoder_manager.predict_class(embedding)\n            if prediction == class_id:\n                class_accuracy[class_name] += 1\n                total_accuracy += 1\n            total_count += 1\n    for class_name, count in class_accuracy.items():\n        class_accuracy[class_name] = count / len(dataset_items)\n        print(f\"Class {class_name}: {class_accuracy[class_name] * 100:.2f}%\")\n    print(f\"Average accuracy: {total_accuracy / total_count * 100:.2f}%\")\n\n", "CodeBleu_score": 33.44557007656862}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name_mapping = encoder_manager.config.class_id_to_class_name_mapping\n    class_name_to_class_id_mapping = {v: k for k, v in class_id_to_class_name_mapping.items()}\n    class_accuracies = {}\n    for item in tqdm(dataset_items):\n        audio_file = item['audio_file']\n        class_name = item[class_name_key]\n        if class_name not in class_name_to_class_id_mapping:\n            continue\n        class_id = class_name_to_class_id_mapping[class_name]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class_id(embedding)\n        if predicted_class_id == class_id:\n            if class_name not in class_accuracies:\n                class_accuracies[class_name] = 0\n            class_accuracies[class_name] += 1\n    for class_name, class_accuracy in class_accuracies.items():\n        print(f\"{class_name}: {class_accuracy / len(dataset_items) * 100:.2f}%\")\n    print(f\"Average accuracy: {sum(class_accuracies.values()) / len(dataset_items) * 100:.2f}%\")\n\n", "CodeBleu_score": 30.673408742189057}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_key = encoder_manager.config.get(\"class_id_key\", \"class_id\")\n    class_name_to_id = encoder_manager.class_name_to_id\n    class_id_to_name = encoder_manager.class_id_to_name\n    class_names = list(class_id_to_name.values())\n    class_accuracy = {class_name: 0 for class_name in class_names}\n    total_accuracy = 0\n    num_items = 0\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n        class_id = class_name_to_id[class_name]\n        if class_id_key in item:\n            class_id = item[class_id_key]\n        if class_name not in class_id_to_name:\n            continue\n        class_name = class_id_to_name[class_id]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        if embedding is None:\n            continue\n        class_prediction = encoder_manager.predict_class(embedding)\n        if class_prediction is None:\n            continue\n        class_accuracy[class_name] += 1 if class_prediction == class_id else 0\n        total_accuracy += 1 if class_prediction == class_id else 0\n        num_items += 1\n    for class_name in class_names:\n        class_accuracy[class_name] /= num_items\n        print(f\"{class_name}: {class_accuracy[class_name]:.2f}\")\n    print(f\"Average accuracy: {total_accuracy / num_items:.2f}\")\n\n", "CodeBleu_score": 35.954397678299124}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    if not args.force and os.path.exists(output_path):\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    speech_detected = remove_silence(audio_path, output_path, args.threshold, args.min_silence_len, args.silence_len, args.aggressive, args.vad_model_path)\n    return output_path, speech_detected\n\n", "CodeBleu_score": 36.89952502229639}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    model, utils = get_vad_model_and_utils(\n        model_path=args.model_path,\n        device=args.device,\n        sample_rate=args.sample_rate,\n        min_silence_duration_ms=args.min_silence_duration_ms,\n        min_speech_duration_ms=args.min_speech_duration_ms,\n        silence_threshold=args.silence_threshold,\n        speech_threshold=args.speech_threshold,\n        step_size=args.step_size,\n        vad_model=args.vad_model,\n    )\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.parent / args.output_dir / audio_path.parent.name / audio_path.name\n    if output_path.exists() and not args.force:\n        return str(output_path), False\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    audio, sr = utils.load_audio(audio_path, sr=args.sample_rate)\n    audio, speech_detected = remove_silence(\n        audio,\n        model,\n        utils,\n        min_silence_duration_ms=args.min_silence_duration_ms,\n        min_speech_duration_ms=args.min_speech_duration_ms,\n        silence_threshold=args.silence_threshold,\n        speech_threshold=args.speech_threshold,\n        step_size=args.step_size,\n    )\n    utils.save_audio(str(output_path), audio, sr)\n    return str(output_path), speech_detected\n\n", "CodeBleu_score": 36.703745428326044}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    if not args.force and os.path.exists(output_path):\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    audio = torch.load(audio_path)\n    audio = remove_silence(audio, vad_model, vad_utils, args.vad_min_silence_length, args.vad_silence_threshold, args.vad_speech_threshold, args.vad_window_length)\n    if audio.shape[0] > 0:\n        torch.save(audio, output_path)\n        return output_path, True\n    else:\n        return output_path, False\n\n", "CodeBleu_score": 41.43185688313839}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    elif output_dir != input_dir:\n        copytree(input_dir, output_dir)\n    file_list = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    func_args = [(file_name, output_sr) for file_name in file_list]\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, func_args), total=len(func_args)))\n    print(f\"Resampling completed for {len(file_list)} files in {input_dir}\")\n\n", "CodeBleu_score": 29.757541714393014}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        copytree(input_dir, output_dir)\n    filenames = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, zip(filenames, [output_sr] * len(filenames))), total=len(filenames)))\n    print(\"Completed resampling files in {} to {}\".format(input_dir, output_dir))\n\n", "CodeBleu_score": 33.03453491079386}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    elif output_dir != input_dir:\n        copytree(input_dir, output_dir)\n    input_files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, zip(input_files, [output_sr] * len(input_files))), total=len(input_files)))\n    print(f\"Completed resampling {len(input_files)} files in {input_dir} to {output_sr} Hz at {output_dir}\")\n\n", "CodeBleu_score": 28.76401363408747}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            inputs = inputs.reshape(-1, c.model_params.get(\"input_dim\", 1))\n            labels = labels.reshape(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            if i == len(data_loader) - 1:\n                plot_embeddings(\n                    outputs.cpu().detach().numpy(),\n                    labels.cpu().detach().numpy(),\n                    classes,\n                    class_map,\n                    global_step,\n                    c.output_path,\n                )\n    avg_loss = total_loss / len(data_loader)\n    print(\n        f\"Evaluation loss at step {global_step} is {avg_loss:.4f}\",\n        flush=True,\n    )\n    return avg_loss\n\n", "CodeBleu_score": 36.485430469985694}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    avg_loss = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.reshape(-1, inputs.shape[-1])\n            labels = labels.reshape(-1)\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            avg_loss += loss.item()\n            if i == len(data_loader) - 1:\n                # Save last batch embeddings\n                plot_embeddings(\n                    outputs.detach().cpu().numpy(),\n                    labels.detach().cpu().numpy(),\n                    global_step,\n                    c.output_path,\n                    \"last_batch\",\n                )\n    avg_loss /= len(data_loader)\n    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n    return avg_loss\n\n", "CodeBleu_score": 32.551470463189816}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    with torch.no_grad():\n        avg_loss = 0.0\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            if use_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            inputs = inputs.reshape(-1, inputs.size(-1))\n            labels = labels.reshape(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            avg_loss += loss.item()\n            if batch_idx == 0:\n                if use_cuda:\n                    inputs = inputs.cpu()\n                    labels = labels.cpu()\n                    outputs = outputs.cpu()\n                plot_embeddings(\n                    inputs,\n                    labels,\n                    outputs,\n                    global_step,\n                    c.exp_dir,\n                    \"eval_embeddings.png\",\n                )\n        avg_loss /= len(data_loader)\n        print(\n            f\"Evaluation loss at step {global_step}: {avg_loss:.4f}\",\n            flush=True,\n        )\n    return avg_loss\n\n", "CodeBleu_score": 33.255300056048355}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # construct the config class name\n    config_class_name = model_name + \\\"Config\\\"\n    # xtts specific config\n    if model_name == \\\"xtts\\\":\n        from TTS.config.xtts_configs import XTTSConfig\n        return XTTSConfig\n\n    # search through predefined paths\n    for path in CONFIG_PATHS:\n        try:\n            config_class = find_module(path, config_class_name)\n            return config_class\n        except ModuleNotFoundError:\n            pass\n\n    raise ModuleNotFoundError(f\\\"Config class for model {model_name} not found.\\\")", "CodeBleu_score": 31.388277208795024}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    if model_name == \\\"xtts\\\":\n        from TTS.config.xtts_config import XTTSConfig\n\n        return XTTSConfig\n    config_class_name = \\\"{model_name.capitalize()}Config\\\"\n    config_class = find_module(config_class_name, [\\\"TTS.config.{model_name}\\\", \\\"TTS.config.shared_configs\\\"])\n    if config_class is None:\n        raise ModuleNotFoundError(f\\\"Config class for model {model_name} not found.\\\")\n    return config_class\n\n", "CodeBleu_score": 24.266866906061313}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.capitalize() + \\\"Config\\\"\n    try:\n        config_class = find_module(\\\"TTS.config.\\\" + model_name + \\\".\\\" + config_class_name)\n    except ModuleNotFoundError as e:\n        if model_name == \\\"xtts\\\":\n            config_class = find_module(\\\"TTS.config.xtts.XTTSConfig\\\")\n        else:\n            raise e\n    return config_class\n\n", "CodeBleu_score": 24.918373280042566}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    ext = os.path.splitext(config_path)[1]\n    if ext == \\\".yaml\\\" or ext == \\\".yml\\\":\n        with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \\\".json\\\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\"Unknown config file type: {ext}\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config", "CodeBleu_score": 41.724366557962}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path is None:\n        raise ValueError(\" [!] Config path is None.\")\n    if not os.path.exists(config_path):\n        raise ValueError(f\" [!] Config path {config_path} does not exist.\")\n    ext = config_path.split(\\\".\\\")[-1]\n    if ext == \\\"yaml\\\" or ext == \\\"yml\\\":\n        with open(config_path, \\\"r\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \\\"json\\\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\" [!] Config path {config_path} is not supported. Only yaml and json are supported.\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class(**config_dict)\n    return config\n", "CodeBleu_score": 50.672090686177285}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    ext = os.path.splitext(config_path)[1].lower()\n    if ext == \\\".yaml\\\" or ext == \\\".yml\\\":\n        with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \\\".json\\\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\" [!] Unknown config extension: {ext}\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n", "CodeBleu_score": 40.440454719914435}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                # check to see if the contains string is not inside the\n                # filename\n                if contains is not None and contains not in imagePath:\n                    continue\n                yield imagePath\n\n", "CodeBleu_score": 66.32091251909752}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint:\n        return \"Please provide a valid XTTS checkpoint path\"\n    if not xtts_config:\n        return \"Please provide a valid XTTS config path\"\n    if not xtts_vocab:\n        return \"Please provide a valid XTTS vocab path\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, vocoder=None)\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Loaded XTTS model\")\n    return \"Loaded XTTS model\"\n\n", "CodeBleu_score": 51.18171307601494}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        return \"Please provide a path to the XTTS checkpoint.\"\n    if xtts_config is None:\n        return \"Please provide a path to the XTTS config.\"\n    if xtts_vocab is None:\n        return \"Please provide a path to the XTTS vocab.\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, strict=False)\n    model.eval()\n    model.load_vocoder_model()\n    model.load_vocoder_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    print(\"Model loaded successfully.\")\n    return \"Model loaded successfully.\"\n\n", "CodeBleu_score": 48.412592637111985}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        return \"Please provide a checkpoint path.\"\n    if xtts_config is None:\n        return \"Please provide a config path.\"\n    if xtts_vocab is None:\n        return \"Please provide a vocab path.\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, strict=False)\n    model.load_vocoder(xtts_vocab)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    return \"Successfully loaded XTTS model.\"\n\n", "CodeBleu_score": 44.07177417694953}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(x, str):\n        x = load_fsspec(x)\n    if x.shape[0] < num_frames:\n        raise ValueError(\\\"Input audio is too short. Please provide an audio of at least {} samples.\\\".format(num_frames))\n    num_frames = min(num_frames, x.shape[0])\n    num_frames_step = num_frames // num_eval\n    offsets = [num_frames_step * i for i in range(num_eval)]\n    x_frames = torch.cat([x[offset:offset + num_frames] for offset in offsets], dim=0)\n    x_frames = x_frames.unsqueeze(0)\n    x_frames = x_frames.to(self.device)\n    emb = self.inference(x_frames, l2_norm=l2_norm)\n    if return_mean:\n        emb = emb.mean(dim=0)\n    return emb\n\n", "CodeBleu_score": 39.24291607980119}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    x = x.unsqueeze(0)\n    if self.config.use_torch_stft:\n        num_frames = int(np.ceil(x.shape[-1] / self.config.hop_length))\n    else:\n        num_frames = min(num_frames, x.shape[-1] // self.config.hop_length)\n    offsets = np.round(np.linspace(0, x.shape[-1] - self.config.hop_length * num_frames, num_eval)).astype(int)\n    offsets = np.concatenate((offsets, np.array([0, x.shape[-1] - self.config.hop_length * num_frames])))\n    offsets = np.unique(offsets)\n    x_eval = [x[:, :, i:i + self.config.hop_length * num_frames] for i in offsets]\n    x_eval = torch.cat(x_eval, dim=0)\n    emb = self.forward(x_eval, l2_norm=False)\n    if return_mean:\n        emb = emb.mean(dim=0)\n        if l2_norm:\n            emb = emb / torch.norm(emb)\n    return emb\n\n", "CodeBleu_score": 38.193446819436105}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust the number of frames based on the hop length if using torch specifications\n    if self.use_torch_specifications:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[-1] - self.hop_length)\n\n    # Compute the offset positions for evaluation\n    offsets = np.arange(0, num_frames, dtype=np.int64) * self.hop_length\n\n    # Extract frames at these offsets\n    frames = x[:, offsets]\n\n    # Concatenate them into a batch\n    frames = torch.stack(frames, dim=0)\n\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(frames, l2_norm)\n\n    # Compute the mean of the embeddings if specified\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n\n    return embeddings\n\n", "CodeBleu_score": 33.576386551563}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        if self.use_torch_spec:\n            x = self.torch_spec(x)\n        else:\n            x = x.squeeze(1)\n            x = x.transpose(1, 2)\n\n        x = self.instancenorm(x)\n\n        x = self.attention(x)\n        x = x.transpose(1, 2)\n\n        if self.encoder_type == \\\"SAP\\\":\n            x = x.max(dim=2)[0]\n        elif self.encoder_type == \\\"ASP\\\":\n            x1 = x.max(dim=2)[0]\n            x2 = x.mean(dim=2)\n            x = torch.cat([x1, x2], dim=1)\n        else:\n            raise ValueError(\\\"Undefined encoder\\\")\n\n        x = self.fc(x)\n\n        return x\n\n", "CodeBleu_score": 74.1195787773748}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.47011482327022}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 87.88668147341942}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    reducer = umap.UMAP(random_state=42)\n    embedding = reducer.fit_transform(embeddings)\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embedding[:, 0], embedding[:, 1], c=colormap[0:num_classes_in_batch])\n    plt.gca().set_aspect(\\\"equal\\\", \\\"datalim\\\")\n    plt.title(\\\"UMAP projection\\\")\n    plt.savefig(\\\"umap\\\")\n    return fig", "CodeBleu_score": 30.69221280449029}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Get the number of utterances per class\n    num_utterances = np.bincount(num_classes_in_batch)\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances) > 10:\n        num_utterances = num_utterances[:10]\n        num_classes_in_batch = num_classes_in_batch[:10]\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    embedding_transformed = reducer.fit_transform(embeddings)\n    # Create a scatter plot with colors representing different classes\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(\n        embedding_transformed[:, 0],\n        embedding_transformed[:, 1],\n        c=num_classes_in_batch,\n        cmap=matplotlib.colors.ListedColormap(colormap),\n    )\n    plt.gca().set_aspect(\\\"equal\\\", \\'datalim\\')\n    plt.title(\\\"UMAP projection\\\")\n    plt.savefig(\\\"umap\\\")\n    return fig", "CodeBleu_score": 35.57915226578683}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = num_classes_in_batch\n    if num_classes > 10:\n        num_classes = 10\n    fig, ax = plt.subplots()\n    ax.set_aspect(\\\"equal\\\", \\\"datalim\\\")\n    ax.set_title(\\\"UMAP projection\\\")\n    ax.scatter(embeddings[:, 0], embeddings[:, 1], s=0.1, c=colormap[0:num_classes, :])\n    plt.savefig(\\\"umap\\\")\n    return fig\n\n\n", "CodeBleu_score": 18.28400350317442}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for spkr, utts in enumerate(dvecs):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim.append(F.cosine_similarity(utts, new_centroids, dim=1))\n    cos_sim = torch.clamp(torch.stack(cos_sim), min=1e-6)\n    return cos_sim\n\n", "CodeBleu_score": 22.664817345770505}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sims = []\n    for spkr, utts in enumerate(dvecs):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim = F.cosine_similarity(utts, new_centroids, dim=1)\n            cos_sim = cos_sim.clamp(min=1e-6)\n            cos_sims.append(cos_sim)\n        return torch.stack(cos_sims).transpose(0, 1)\n\n    def calc_loss(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the loss given the d-vectors and centroids\n        \\\"\\\"\\\"\n        centroids = self.calc_centroids(dvecs)\n        cos_sims = self.calc_cosine_sim(dvecs, centroids)\n        return self.embed_loss(cos_sims)\n\n    def calc_centroids(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the centroids for each speaker\n        \\\"\\\"\\\"\n        centroids = []\n        for spkr in dvecs:\n            centroids.append(torch.mean(spkr, 0))\n        return torch.stack(centroids)\n\n    def embed_loss_softmax(self, cos_sims):\n        \\\"\\\"\\\"\n        Calculates the softmax loss given the cosine similarity matrix\n        \\\"\\\"\\\"\n        # pylint: disable=E1102\n        cos_sims = cos_sims * self.w\n        # pylint: disable=E1102\n        cos_sims = cos_sims + self.b\n        return F.cross_entropy(cos_sims, torch.arange(cos_sims.size(0)).to(dvecs.device))\n\n    def embed_loss_contrast(self, cos_sims):\n        \\\"\\\"\\\"\n        Calcul", "CodeBleu_score": 40.87851371231384}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_mat = []\n    for spkr, utts in enumerate(dvecs):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim = F.cosine_similarity(utts, new_centroids, dim=1)\n        cos_sim = cos_sim.clamp(min=1e-6)\n        cos_sim_mat.append(cos_sim)\n    return torch.stack(cos_sim_mat)\n\n    def embed_loss_softmax(self, cos_sim_mat):\n        \\\"\\\"\\\"\n        Calculates the softmax loss using the cosine similarity matrix.\n        \\\"\\\"\\\"\n        # pylint: disable=E1102\n        return F.cross_entropy(cos_sim_mat, torch.arange(cos_sim_mat.shape[0]))\n\n    def embed_loss_contrast(self, cos_sim_mat):\n        \\\"\\\"\\\"\n        Calculates the contrastive loss using the cosine similarity matrix.\n        \\\"\\\"\\\"\n        # pylint: disable=E1102\n        return torch.mean(torch.sum(cos_sim_mat, dim=1))\n\n    def forward(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the loss using the d-vectors.\n        \\\"\\\"\\\"\n        centroids = torch.mean(dvecs, dim=1)\n        cos_sim_mat = self.calc_cosine_sim(dvecs, centroids)\n        return self.embed_loss(cos_sim_mat)\n\n    def get_loss(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the loss using the d-vectors.\n        \\\"\\\"\\\"\n        return self.forward(dvecs)\n\n    def get_embedding_loss(self, dvecs):\n        \\\"\\\"\\\"\n        Calculates the loss using the d-vectors.\n        \\\"\\\"\\\"\\", "CodeBleu_score": 38.30975057937782}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # pylint: disable=E1101\n        loss_stack = []\n        for row in cos_sim_matrix:\n            loss_stack.append(torch.mean(-torch.log(F.softmax(row * self.w + self.b, dim=0))))\n        return torch.stack(loss_stack)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 24.58399046769911}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # pylint: disable=E1101\n    N, M, _ = dvecs.size()\n    loss_row = []\n    for spkr_idx in range(N):\n        loss_row.append(\n            F.cross_entropy(\n                cos_sim_matrix[spkr_idx].transpose(0, 1),\n                torch.arange(0, M).long().to(dvecs.device),\n                reduction=\\\"none\\\",\n            )\n        )\n    return torch.stack(loss_row, dim=0)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \\\"\\\"\\\"\n    The function calculates the embedding loss using contrastive loss.\n    It iterates over the embeddings, computing the cosine similarity between the\n    embedding and the new centroids, and the cosine similarity between the\n    embedding and the same speaker's other utterances.\n    The losses for each row are stacked into tensors, and the function returns\n    the final stacked tensor of losses.\n    \\\"\\\"\\\"\n    # pylint: disable=E1101\n    N, M, _ = dvecs.size()\n    loss_row = []\n    for spkr_idx in range(N):\n        loss_row.append(\n            torch.clamp(\n                self.w\n                * (cos_sim_matrix[spkr_idx, spkr_idx, spkr_idx] - cos_sim_matrix[spkr_idx, spkr_idx, :])\\\n                + self.b,\n                min=0,\n            )\n        )\n    return torch.stack(loss_row, dim=0)\n\n\n def forward(self, dvecs):\n    \\\"\\\"\\\"\n    Computes the GE2E loss for a batch of speaker embeddings.\n    Args:\n        dvecs: A tensor of shape (N, M, D) where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector (", "CodeBleu_score": 32.427693213301566}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss_list = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        loss_row = []\n        for utt_idx, utterance in enumerate(speaker):\n            # pylint: disable=E1101\n            loss_row.append(\n                -torch.logsumexp(\n                    cos_sim_matrix[spkr_idx, utt_idx, :] * self.w + self.b, dim=0\n                )\n            )\n        loss_list.append(torch.stack(loss_row))\n    return torch.stack(loss_list)\n\n", "CodeBleu_score": 34.95028821680863}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            # exclude the current centroid\n            new_centroids = self.calc_new_centroids(dvecs, centroids, j, i)\n            # vector based cosine similarity for speed\n            contrast_loss = torch.mean(-torch.log(1 / (1 + torch.exp(-cos_sim_matrix[j, i, j]))))\n            L_row.append(contrast_loss)\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 53.206855957856014}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(torch.sigmoid(cos_sim_matrix[j, i, j]) - torch.sigmoid(cos_sim_matrix[j, i, torch.argmax(cos_sim_matrix[j, i])]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 48.95547542160956}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            excl = torch.cat((cos_sim_matrix[j, :i], cos_sim_matrix[j, i + 1:]))\n            excl = torch.mean(excl, 0)\n            L_row.append(F.relu(1 - excl[j]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 50.8747904534695}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1102\n    # pylint: disable=E1130\n    if x.shape[1] < 2:\n        raise ValueError(\\\"GE2E loss requires at least two utterances per speaker\\\")\n    centroids = torch.mean(x, 1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    return torch.mean(self.embed_loss(x, cos_sim_matrix))", "CodeBleu_score": 31.890781778645017}, {"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1101\n        N, M, D = x.shape\n        assert N > 1, \\\"Batch size must be greater than one\\\"\n        assert M > 1, \\\"Each speaker must have at least two utterances\\\"\n        centroids = torch.mean(x, 1)\n        cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n        L = self.embed_loss(x, cos_sim_matrix)\n        L = L * self.w + self.b\n        return torch.mean(L)\n\n    def __call__(self, x, label=None):\n        return self.forward(x, label)", "CodeBleu_score": 46.51792570878868}, {"code": "def forward(self, x, _label=None)\n\t\n    assert x.dim() == 3\n    assert x.size(1) > 1\n    x = x.transpose(1, 2)\n    N, M, D = x.size()\n    centroids = torch.mean(x, 1)\n    x = x.transpose(1, 2)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    loss = self.embed_loss(x, cos_sim_matrix)\n    loss = loss.view(N, M)\n        loss = torch.mean(loss)\n        return loss\n\n", "CodeBleu_score": 52.40579829798513}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = []\n    for voice_dir in extra_voice_dirs:\n        if not os.path.isdir(voice_dir):\n            logger.warning(f\"{voice_dir} is not a valid directory\")\n            continue\n        for root, _, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices.append(os.path.join(root, file))\n                elif file.endswith((\".wav\", \".mp3\")):\n                    voices.append(os.path.join(root, file))\n    if not voices:\n        logger.warning(\n            \"No voice files found. Please make sure you have .npz, .wav, or .mp3 files in the specified directories.\"\n        )\n    return voices\n\n", "CodeBleu_score": 26.8674009154531}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = []\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices.append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voices.append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 23.604851089689156}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            for f in files:\n                if f.endswith('.npz'):\n                    voices[root] = [os.path.join(root, f)]\n                    break\n                elif f.endswith('.wav') or f.endswith('.mp3'):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, f))\n\n    if not voices:\n        for root, _, files in os.walk('TTS/tts/layers/bark/voices'):\n            for f in files:\n                if f.endswith('.npz'):\n                    voices[root] = [os.path.join(root, f)]\n                    break\n                elif f.endswith('.wav') or f.endswith('.mp3'):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, f))\n\n    return voices\n\n", "CodeBleu_score": 24.180105351035756}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \\\"random\\\":\n        return None, None, None\n    voices = get_voices(extra_voice_dirs)\n    voice_paths = voices.get(voice, [])\n    if len(voice_paths) == 0:\n        raise ValueError(f\\\"Voice {voice} not found. Available voices: {', '.join(voices.keys())}\\\")\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice}. Please specify a single voice.\\\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\\\".npz\\\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        npz_path = voice_path.replace(\\\".wav\\\", \\\".npz\\\").replace(\\\".mp3\\\", \\\".npz\\\")\n        if os.path.exists(npz_path):\n            semantic, coarse, fine = load_npz(npz_path)\n        else:\n            semantic, coarse, fine = generate_npz(voice_path, model)\n            semantic, coarse, fine = load_npz(npz_path)\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 42.67502127018802}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voices = get_voices(extra_voice_dirs)\n    if voice == \\\"random\\\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found in {voices.keys()}\\\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice}: {voice_paths}\\\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\\\".npz\\\"):\n        semantic, coarse, fine = load_npz(voice_path)\n        return semantic, coarse, fine\n    if voice_path.endswith(\\\".wav\\\") or voice_path.endswith(\\\".mp3\\\"):\n        # generate npz file and recursively load the voice\n        npz_path = voice_path.replace(\\\".wav\\\", \\\".npz\\\").replace(\\\".mp3\\\", \\\".npz\\\")\n        if not os.path.exists(npz_path):\n            logger.info(f\\\"Generating npz file for {voice_path}\\\")\n            semantic, coarse, fine = generate_npz(model, voice_path, npz_path)\n        else:\n            semantic, coarse, fine = load_npz(npz_path)\n        return semantic, coarse, fine\n    raise ValueError(f\\\"Unsupported voice file: {voice_path}\\\")\n\n", "CodeBleu_score": 38.7187050875331}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voices = get_voices(extra_voice_dirs)\n    voice_paths = voices.get(voice, None)\n    if voice_paths is None:\n        return None, None, None\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice} in {voice_paths}\\\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\\\".npz\\\"):\n        semantic, coarse, fine = load_npz(voice_path)\n    else:\n        semantic, coarse, fine = generate_voice(voice_path, model)\n    return semantic, coarse, fine\n\n", "CodeBleu_score": 29.5321194037641}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros_shape = (batch_size, num_heads, seq_length1, 1)\n    zeros = torch.zeros(zeros_shape, device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.50118718668325}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.08735369887231}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros_shape = batch_size, num_heads, seq_length1, 1\n    zeros = torch.zeros(zeros_shape, device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.186950553633295}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(0)\n    max_len = context.size(2)\n    out = []\n    for i in range(max_len):\n        if i > 0:\n            context = context[:, :, i:]\n        conv_out = self.convolutions(context)\n        conv_out = self.dropout(conv_out)\n        if i == 0:\n            out = conv_out[:, :, :lens]\n        else:\n            out = torch.cat((out, conv_out[:, :, :lens]), dim=2)\n    return out\n\n", "CodeBleu_score": 28.72105070709514}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size, n_channels, n_frames = context.shape\n    context_list = []\n    for i in range(batch_size):\n        context_list.append(context[i, :, :lens[i]])\n    context_padded = nn.utils.rnn.pad_sequence(context_list, batch_first=True)\n    for conv in self.convolutions:\n        context_padded = F.relu(conv(context_padded))\n        context_padded = self.dropout(context_padded)\n    return context_padded\n\n", "CodeBleu_score": 38.75426091076631}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.shape[0]\n    max_len = context.shape[2]\n    processed_contexts = []\n    for i in range(max_len):\n        if i == 0:\n            mask = torch.arange(max_len).unsqueeze(0) < lens.unsqueeze(1)\n        else:\n            mask = torch.cat((mask[:, 1:], mask[:, -1].unsqueeze(1)), dim=1)\n        x = context[:, :, : i + 1]\n        for conv in self.convolutions:\n            x = conv(x, mask)\n            x = F.relu(x)\n            x = self.dropout(x)\n        processed_contexts.append(x)\n    processed_contexts = torch.stack(processed_contexts, dim=2)\n    padded_contexts = torch.zeros((batch_size, self.out_dim, max_len), device=context.device)\n    padded_contexts[:, :, :max_len] = processed_contexts\n    return padded_contexts\n\n", "CodeBleu_score": 24.33097082424902}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True, enforce_sorted=False)\n    context = fn(context)\n    context, _ = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 20.37412704451823}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, sorted_indices = torch.sort(lens, descending=True)\n    context = context[sorted_indices]\n    context = self.run_padded_sequence(context, lens)\n    context = context[torch.argsort(sorted_indices)]\n    return context\n\n", "CodeBleu_score": 10.700640476447527}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = self.run_padded_sequence(context, lens)\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 9.789318547094098}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for layer in self.convt_pre:\n        if isinstance(layer, nn.utils.parametrizations.WeightNorm):\n            layer.remove_weight_norm()\n    for block in self.conv_blocks:\n        for layer in block:\n            if isinstance(layer, nn.utils.parametrizations.WeightNorm):\n                layer.remove_weight_norm()\n\n", "CodeBleu_score": 39.63531028616163}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.parametrizations.remove_weight_norm(self.convt_pre)\n    for i in range(len(self.conv_blocks)):\n        nn.utils.parametrizations.remove_weight_norm(self.conv_blocks[i])\n\n", "CodeBleu_score": 45.0035412859849}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.parametrizations.remove_weight_norm(self.convt_pre[0])\n    for conv in self.conv_blocks:\n        nn.utils.parametrizations.remove_weight_norm(conv[0])\n\n", "CodeBleu_score": 40.773145010177544}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name[3:]\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name[6:]\n    if mx_name.startswith('inceptionv3'):\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 31.890158849065987}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # Convert the input name to lowercase\n    mx_name = mx_name.lower()\n\n    # Replace 'se_' prefix with 'se_resnext_'\n    mx_name = mx_name.replace('se_', 'se_resnext_')\n\n    # Replace 'senet_' prefix with 'senet_'\n    mx_name = mx_name.replace('senet_', 'senet_')\n\n    # Replace 'inceptionv3' prefix with 'inception_v3'\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n\n    # Prefix the name with 'gluon_'\n    mx_name = 'gluon_' + mx_name\n\n    return mx_name\n\n", "CodeBleu_score": 35.95570396098635}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', '')\n    mx_name = mx_name.replace('senet_', '')\n    mx_name = mx_name.replace('inceptionv3', 'inception')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 30.322632159108153}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.batch_size > 1 and self.is_training:\n        num_samples = math.ceil(self.num_samples / self.batch_size) * self.batch_size\n    else:\n        num_samples = self.num_samples\n    if self.repeats > 0:\n        num_samples *= self.repeats\n    return num_samples\n\n", "CodeBleu_score": 28.584809405154775}, {"code": "def _num_samples_per_worker(self)\n\t\n    # Calculate the number of samples per worker\n    if self.is_training:\n        num_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    else:\n        num_samples_per_worker = self.num_samples // self.global_num_workers\n\n    # Adjust the number of samples to be a multiple of the batch size\n    if self.batch_size > 1:\n        num_samples_per_worker = (num_samples_per_worker + self.batch_size - 1) // self.batch_size * self.batch_size\n\n    # Adjust the number of samples to be a multiple of the number of repeats\n    num_samples_per_worker = (num_samples_per_worker + self.repeats - 1) // self.repeats * self.repeats\n\n    return num_samples_per_worker\n\n", "CodeBleu_score": 35.66906930126761}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = self.num_samples // self.global_num_workers\n    if self.repeats > 1:\n        num_samples_per_worker = num_samples_per_worker * self.repeats\n    if self.is_training and self.batch_size > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 40.52475699823046}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        folders = os.listdir(folder)\n        folders.sort()\n        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(folders)}\n    elif class_to_idx is not None and not isinstance(class_to_idx, dict):\n        raise RuntimeError(\n            'class_to_idx must be a dictionary or None, but got {}'.format(type(class_to_idx)))\n    labels = set()\n    folder_len = len(folder)\n    images = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname)\n                item = path[folder_len + 1:] if leaf_name_only else path\n                images.append(item)\n                if len(images) >= 1:\n                    label = os.path.basename(os.path.dirname(item))\n                    if label not in class_to_idx:\n                        class_to_idx[label] = len(class_to_idx)\n                    labels.add(label)\n    if sort:\n        images.sort(key=natural_key)\n        labels.sort(key=natural_key)\n    return images, [class_to_idx[os.path.basename(os.path.dirname(path))] for path in images], class_to_idx\n\n\n", "CodeBleu_score": 40.8005342434273}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    # Get list of all files ending with a valid image extension under `folder`\n    extensions = get_img_extensions()\n    is_valid_file = lambda x: x.lower().endswith(extensions)\n    instances = []  # type: List[Tuple[str, int]]\n    folder_to_idx = {}\n    folder_to_instances = {}\n    idx = 0\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if is_valid_file(fname):\n                path = os.path.join(root, fname)\n                item = path, class_to_idx[path]\n                instances.append(item)\n                folder_to_instances.setdefault(root, []).append(item)\n        if class_to_idx is None and folder_to_instances:\n            folders = sorted(folder_to_instances.keys(), key=natural_key)\n            if leaf_name_only:\n                folders = [os.path.basename(os.path.normpath(f)) for f in folders]\n            for f in folders:\n                folder_to_idx[f] = idx\n                idx += 1\n            folder_to_instances.clear()\n    if class_to_idx is None:\n        # Remap labels to {0, 1, ..., n-1}\n        classes = sorted(folder_to_idx.keys(), key=natural_key)\n        if leaf_name_only:\n            classes = [os.path.basename(os.path.normpath(c)) for c in classes]\n        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n\n    if sort:\n        instances.sort(key=lambda x: natural_key(x[0]))\n\n    return instances, class_to_idx\n\n\n\n", "CodeBleu_score": 38.788811766881764}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if isinstance(types, str):\n        types = [types]\n    labels = []\n    filenames = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname)\n                item = path\n                if leaf_name_only:\n                    dirname = os.path.basename(os.path.dirname(path))\n                    item = os.path.join(dirname, fname)\n                filenames.append(item)\n                labels.append(os.path.basename(root))\n    if class_to_idx is None:\n        unique_labels = {k: idx for idx, k in enumerate(sorted(set(labels), key=natural_key))}\n    else:\n        unique_labels = class_to_idx\n    label_to_idx = {v: k for k, v in unique_labels.items()}\n    targets = [unique_labels[label] for label in labels]\n    if sort:\n        filenames, targets = zip(*sorted(zip(filenames, targets), key=lambda x: natural_key(x[0])))\n    return list(filenames), targets, unique_labels, label_to_idx\n\n\n\n", "CodeBleu_score": 47.7123233396747}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = wds_rank()\n    return seed\n\n", "CodeBleu_score": 56.72672169234647}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % 2**32\n    else:\n        seed = int(os.environ.get('WDS_WORKER_SEED', 0))\n    return seed\n\n", "CodeBleu_score": 49.92828313835012}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = int(worker_info.seed)\n    else:\n        seed = int(os.environ.get('WDS_RANK', 0))\n    return seed\n\n", "CodeBleu_score": 50.48010995068457}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if _PIL_VER >= (5, 0):\n        return img.point(lambda p: p + add if p < thresh else p)\n    return ImageOps.solarize(img, thresh, add)\n\n", "CodeBleu_score": 15.257787569055791}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \\\"L\\\":\n        img = img.point(lambda p: p + add if p <= thresh else p)\n    elif img.mode == \\\"RGB\\\":\n        img = img.point(lambda p: p + add if p <= thresh else p)\n    return img\n\n\n", "CodeBleu_score": 28.800548650327894}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \\\"L\\\":\n        lut = [min(255, v + add) for v in range(256)]\n        if thresh > 0:\n            lut = [min(255, max(0, v + add)) if v < thresh else v for v in range(256)]\n        return img.point(lut)\n    elif img.mode == \\\"RGB\\\":\n        lut = np.array([[min(255, v + add) for v in range(256)] for _ in range(3)])\n        if thresh > 0:\n            lut = np.array([[min(255, max(0, v + add)) if v < thresh else v for v in range(256)] for _ in range(3)])\n        return Image.fromarray(np.clip(np.dstack([img] * 3) + lut, 0, 255).astype(np.uint8))\n    else:\n        return img\n\n", "CodeBleu_score": 34.90883725495233}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_weights = []\n    for transform_name, weight in transforms.items():\n        transform_names.append(transform_name)\n        transform_weights.append(weight)\n    transform_weights = torch.tensor(transform_weights, dtype=torch.float)\n    transform_weights = transform_weights / transform_weights.sum()\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 27.75662115237062}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_weights = np.array(list(transforms.values()))\n    transform_weights /= transform_weights.sum()\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 21.07547959833958}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = []\n    transforms_weights = []\n    for transform, weight in transforms.items():\n        transforms_list.append(transform)\n        transforms_weights.append(weight)\n    transforms_weights = np.array(transforms_weights)\n    transforms_weights = transforms_weights / transforms_weights.sum()\n    return transforms_list, transforms_weights\n\n", "CodeBleu_score": 29.758633391840768}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = np.zeros(img_orig.shape)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        for op in ops:\n            img = op(img)\n        img_mixed = img_mixed + w * img\n    img_mixed = np.clip(img_mixed, 0, 255)\n    img_mixed = img_mixed.round().astype(np.uint8)\n    img_mixed = Image.fromarray(img_mixed)\n    img_mixed = img_mixed.resize(img_orig.size, resample=Image.BILINEAR)\n    if self.blended:\n        return Image.blend(img_orig, img_mixed, m)\n    else:\n        return img_mixed\n\n", "CodeBleu_score": 53.82659907796701}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = np.zeros_like(img_orig)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed = np.clip(img_mixed + img_aug * w, 0, 255)\n    img_mixed = np.clip(img_mixed, 0, 255).astype(np.uint8)\n    img_mixed = Image.fromarray(img_mixed)\n    return Image.blend(img_orig, img_mixed, m)\n\n\n\n", "CodeBleu_score": 56.310356771879974}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = np.zeros_like(img_orig)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed = img_mixed + img_aug * w\n    img_mixed = np.clip(img_mixed, 0, 255)\n    img_mixed = Image.fromarray(img_mixed.astype(np.uint8))\n    img = Image.blend(img, img_mixed, m)\n    return img\n\n\n", "CodeBleu_score": 57.80527668480681}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning(f\"Adapting input {x} to {n} channels\")\n        x = tuple(np.mean(x, axis=0))\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 43.371744213147736}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple(np.mean(x, axis=0))\n        _logger.warning(f'Input channels {len(x)} != {n} for {x}. Using mean.')\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 44.50412961511097}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (list, tuple)):\n        x = (x,)\n    if len(x) == 1:\n        x = x * n\n    elif len(x) != n:\n        x = tuple(np.mean(x, axis=0))\n        _logger.warning(f'adapt_to_chs: input has {len(x)} channels, expected {n}. Using mean.')\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 41.76012855460436}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    worker_id = worker_info.id\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    else:\n        raise ValueError(f'Invalid type for worker_seeding: {type(worker_seeding)}')\n\n", "CodeBleu_score": 47.995731297871025}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id == 0:\n        _logger.info(f'Initializing worker {worker_info.id} with seed {worker_seeding}')\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n    elif isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info.id)\n    else:\n        seed = worker_seeding\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n", "CodeBleu_score": 49.01993231004456}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id != worker_info.id:\n        return\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n    elif isinstance(worker_seeding, Callable):\n        seed = worker_seeding()\n    else:\n        raise ValueError('worker_seeding must be a string or a callable')\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n", "CodeBleu_score": 47.41502831263242}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(tf.cast([image_height, image_width], tf.float32) / 2.0, tf.int32)\n    offset_height = ((image_height - padded_center_crop_size[0]) / 2)\n    offset_width = ((image_width - padded_center_crop_size[1]) / 2)\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size[0], padded_center_crop_size[1]])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 42.66187400614266}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(tf.cast(tf.minimum(image_height, image_width), tf.float32) * 1.0 / 8, tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 55.18329236112769}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(tf.cast(tf.minimum(image_height, image_width), tf.float32) * 1.0 / 8, tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 55.18329236112769}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    else:\n        raise ValueError('Unknown interpolation method')\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 60.07829086034964}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BILINEAR if interpolation == 'bilinear' else tf.image.ResizeMethod.BICUBIC\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.image.convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 50.73701891695326}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image_size = tf.cast(image_size, tf.int32)\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.image.convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 58.788160660946346}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor.predict(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, f\"{output_file}.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}", "CodeBleu_score": 60.85973946215595}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    if self.predictor.pred_dir is None:\n        raise ValueError(\"predictor.pred_dir is None\")\n    self.predictor.predict(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, f\"{output_file}.npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 56.05968647031955}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is\", eval_dataloader)\n    outputs = []\n    for batch in tqdm(eval_dataloader):\n        output = model(**batch)\n        outputs.append(output)\n    outputs = torch.cat(outputs, dim=0)\n    outputs = outputs.cpu().numpy()\n    np.save(os.path.join(self.predictor.pred_dir, f\"{output_file}.npy\"), outputs)\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n    return {\"results\": results, \"metric\": self.metric}", "CodeBleu_score": 49.216411886513555}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print(f'rank1: {rank1:.4f} - rank5: {rank5:.4f} - miou: {miou:.4f}')\n\n", "CodeBleu_score": 21.63023417686435}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(f\"Rank1: {rank1:.4f} - Rank5: {rank5:.4f} - mIoU: {miou:.4f}\")\n\n\n def _eval_predictions(self, outputs, targets):\n    \"\"\"The function computes the rank1, rank5, and mIoU metrics for the provided outputs and targets.", "CodeBleu_score": 33.72137507699547}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\\\"rank1\\\"]\n    rank5 = metrics[\\\"rank5\\\"]\n    miou = metrics[\\\"miou\\\"]\n    print(\\\"Rank1: {:.4f} - Rank5: {:.4f} - mIoU: {:.4f}\\\".format(rank1, rank5, miou))\n\n\n\n", "CodeBleu_score": 24.461447046300904}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0.0\n    rank5 = 0.0\n    miou = 0.0\n    for i in range(len(segments)):\n        iou_list = []\n        rank_list = []\n        for j in range(len(data[i])):\n            iou_list.append(self._iou(segments[i], data[i][j]))\n            rank_list.append(self._rank(segments[i], data[i][j]))\n        miou += max(iou_list)\n        rank1 += min(rank_list)\n        rank5 += sum([x for x in rank_list if x <= 5])\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    miou /= len(segments)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 29.674224367981484}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    iou_list = []\n    rank_list = []\n    for segment in segments:\n        iou_list.append([self._iou(segment, gt) for gt in data])\n        rank_list.append([self._rank(segment, gt) for gt in data])\n    rank1 = np.mean([np.max(r) for r in rank_list])\n    rank5 = np.mean([np.sum([r <=5 for r in rl]) for rl in rank_list])\n    miou = np.mean([np.max(i) for i in iou_list])\n    return rank1, rank5, miou\n", "CodeBleu_score": 24.60235055706567}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for i in range(len(segments)):\n        iou = max([self._iou(segments[i], gt) for gt in data[i]])\n        rank = self._rank(segments[i], data[i])\n        ious.append(iou)\n        ranks.append(rank)\n    rank1 = float(sum([1 for r in ranks if r == 1])) / len(ranks)\n    rank5 = float(sum([1 for r in ranks if r <= 5])) / len(ranks)\n    miou = float(sum(ious)) / len(ious)\n    return rank1, rank5, miou\n\n", "CodeBleu_score": 31.954568085241846}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            input_ids = batch[\"input_ids\"].to(self.device)\n            attention_mask = batch[\"attention_mask\"].to(self.device)\n            token_type_ids = batch[\"token_type_ids\"].to(self.device)\n            output = model(input_ids, attention_mask, token_type_ids)\n            scores.append(output)\n    self.finalize(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 36.029399159015036}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            data = batch[0]\n            data = data.to(0)\n            output = model(data)\n            output = output.cpu()\n            scores.append(output.numpy())\n    return self.finalize(scores, output_file)\n\n", "CodeBleu_score": 45.6169469930503}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n            batch = self.process_batch(batch)\n            outputs = model(**batch)\n            scores.append(outputs)\n            self.update_outputs(batch, outputs)\n    return self.finalize_predictions(scores, output_file)\n\n", "CodeBleu_score": 44.394378735754614}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = []\n    return np.dot(full_scores[0], full_scores[1].T)\n\n", "CodeBleu_score": 43.30314904602149}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    assert len(scores[0]) == len(scores[1])\n    full_scores = np.concatenate(scores[0], axis=0)\n    full_scores = np.concatenate(scores[1], axis=0)\n    self.full_scores = []\n    return np.dot(full_scores[0], full_scores[1].T)\n\n", "CodeBleu_score": 65.38792926030901}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    full_scores = np.concatenate(scores, axis=0)\n    scores = np.dot(full_scores[0], full_scores[1].T)\n    return scores\n\n", "CodeBleu_score": 38.012554025558984}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0)\n    Y_true = torch.cat(Y_true, dim=0)\n    Y_pred = Y_pred.cpu().numpy()\n    Y_true = Y_true.cpu().numpy()\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 44.96896684781537}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).cpu()\n    Y_true = torch.cat(Y_true, dim=0).cpu()\n    if output_file is not None:\n        with open(\n            os.path.join(self.pred_dir, output_file + \".pkl\"),\n            \"wb\",\n        ) as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true},\n                fw,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 46.27463048255821}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError(\"Predicted and true labels have different shapes: {} and {}\".format(Y_pred.shape, Y_true.shape))\n    if Y_pred.dtype != np.uint8:\n        raise ValueError(\"Predicted labels must be of type np.uint8, but got {}\".format(Y_pred.dtype))\n    if Y_true.dtype != np.uint8:\n        raise ValueError(\"True labels must be of type np.uint8, but got {}\".format(Y_true.dtype))\n    if Y_pred.min() < 0 or Y_pred.max() > 1:\n        raise ValueError(\"Predicted labels must be in the range [0, 1], but got [{}, {}]\".format(Y_pred.min(), Y_pred.max()))\n    if Y_true.min() < 0 or Y_true.max() > 1:\n        raise ValueError(\"True labels must be in the range [0, 1], but got [{}, {}]\".format(Y_true.min(), Y_true.max()))\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw, protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 49.22232795240369}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size / 1000, sample_size, round=3)\n\n", "CodeBleu_score": 52.313986771499586}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size_sum = sum(log.get('sample_size', 0) for log in logging_outputs)\n    loss_sum /= sample_size_sum\n    metrics.log_scalar('loss', loss_sum, sample_size_sum, round=3)\n\n", "CodeBleu_score": 46.833449008855446}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size_sum = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size_sum, sample_size_sum, round=3)\n\n", "CodeBleu_score": 47.69327288758961}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n            attention_mask = torch.cat([cmasks, vmasks], dim=1)\n        else:\n            attention_mask = torch.cat([attention_mask, vmasks], dim=1)\n        token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n        return attention_mask, token_type_ids\n\n", "CodeBleu_score": 30.888891145419045}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n            attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n        token_type_ids = torch.zeros_like(attention_mask)\n        token_type_ids[cmasks == 0] = 1\n        return attention_mask, token_type_ids\n\n", "CodeBleu_score": 23.902751768627738}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n            attention_mask = torch.cat([\n                cmasks,\n                vmasks\n            ],\n            dim=1)\n        token_type_ids = torch.cat([\n            torch.zeros_like(cmasks, dtype=torch.long),\n            torch.ones_like(vmasks, dtype=torch.long)\n        ],\n        dim=1)\n        return attention_mask, token_type_ids\n\n", "CodeBleu_score": 37.21900102189667}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust the sequence length of input_ids to match the combined length of input_ids and input_video_embeds\n    input_ids = input_ids.view(-1, input_ids.size(-1))\n    input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-2), input_video_embeds.size(-1))\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    if attention_mask is not None:\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = torch.cat([attention_mask, torch.zeros(attention_mask.size(0), combined_length - attention_mask.size(1), dtype=torch.long, device=attention_mask.device)], dim=-1)\n    if token_type_ids is not None:\n        token_type_ids = torch.cat([token_type_ids, torch.zeros(token_type_ids.size(0), combined_length - token_type_ids.size(1), dtype=torch.long, device=token_type_ids.device)], dim=-1)\n\n    # Return the updated inputs\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 47.861651035037845}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if input_ids is not None and input_ids.dtype == torch.long:\n        input_ids = input_ids.to(self.device)\n    if attention_mask is not None and attention_mask.dtype == torch.long:\n        attention_mask = attention_mask.to(self.device)\n    if token_type_ids is not None and token_type_ids.dtype == torch.long:\n        token_type_ids = token_type_ids.to(self.device)\n\n    # adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    if attention_mask is not None:\n        attention_mask = self._extend_attention_mask(attention_mask, combined_length)\n    if token_type_ids is not None:\n        token_type_ids = self._extend_token_type_ids(token_type_ids, combined_length)\n\n    # modify the attention_mask and token_type_ids to fit the new sequence length\n    input_ids = input_ids.view(-1, input_ids.size(-1))\n    input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-1))\n    input_ids, input_video_embeds = self._extend_inputs(input_ids, input_video_embeds, combined_length)\n\n    # return the modified inputs as a dictionary\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 46.03791077539674}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    if attention_mask is not None:\n        if attention_mask.size(1) != combined_length:\n            attention_mask = attention_mask[:, :combined_length]\n    if token_type_ids is not None:\n        if token_type_ids.size(1) != combined_length:\n            token_type_ids = token_type_ids[:, :combined_length]\n\n    # Modify attention_mask and token_type_ids to fit the new sequence length\n    input_ids = input_ids.view(-1, input_ids.size(-1))\n    attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n    # Create a dictionary with the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    model_kwargs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n    return model_kwargs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.87588360842726}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'meta_keys' in pipeline[0]:\n        meta_keys = pipeline[0].meta_keys\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n            pipeline[0].meta_keys = meta_keys\n    idx = [i for i, t in enumerate(pipeline) if t.type == 'LoadImageFromFile'][0]\n    pipeline[idx].type = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 31.550857555469573}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'LoadImageFromFile' in pipeline:\n        load_img_idx = pipeline.index('LoadImageFromFile')\n        pipeline[load_img_idx] = dict(type='mmdet.InferencerLoader')\n        if 'meta_keys' in pipeline[load_img_idx]:\n            meta_keys = pipeline[load_img_idx]['meta_keys']\n            if 'img_id' in meta_keys:\n                meta_keys.remove('img_id')\n            pipeline[load_img_idx]['meta_keys'] = meta_keys\n    else:\n        raise ValueError('\\'LoadImageFromFile\\' is not found in the test pipeline.')\n    return Compose(pipeline)\n\n", "CodeBleu_score": 39.243686876616394}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'meta_keys' in pipeline[0]:\n        meta_keys = pipeline[0].meta_keys\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n        pipeline[0].meta_keys = meta_keys\n    load_image_from_file_idx = None\n    for idx, transform in enumerate(pipeline):\n        if isinstance(transform, LoadImageFromFile):\n            load_image_from_file_idx = idx\n            break\n    if load_image_from_file_idx is None:\n        raise ValueError('LoadImageFromFile transform not found in the pipeline')\n    pipeline[load_image_from_file_idx].type = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 35.980670755904804}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = list(sorted(glob.glob(os.path.join(inputs, '*'))))\n            if len(inputs) == 0:\n                raise FileNotFoundError(f'No images found in {inputs}')\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 49.77164124274815}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        if os.path.isdir(inputs):\n            inputs = list(sorted(glob.glob(os.path.join(inputs, '*'))))\n        else:\n            inputs = [inputs]\n    elif isinstance(inputs, (list, tuple)):\n        inputs = list(inputs)\n    else:\n        raise TypeError('The input must be a string, a list, or a tuple.')\n    return inputs\n\n", "CodeBleu_score": 42.94696084402611}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        inputs = list(sorted(glob.glob(str(inputs))))\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 31.380892614926438}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError('chunk_size must be greater than 0.')\n    chunk = []\n    for input_ in inputs:\n        if isinstance(input_, dict) and ('img' in input_ or 'img_path' in input_):\n            input_ = self.pipeline(input_)\n        chunk.append(input_)\n        if len(chunk) >= chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 26.571808770904692}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError('chunk_size must be greater than 0.')\n    if chunk_size == 1:\n        yield from map(self.pipeline, inputs)\n        return\n    chunk_inputs = []\n    for input_ in inputs:\n        if isinstance(input_, dict) and ('img' in input_ or 'img_path' in input_):\n            chunk_inputs.append(input_)\n            if len(chunk_inputs) == chunk_size:\n                yield self.pipeline(chunk_inputs)\n                chunk_inputs.clear()\n    if chunk_inputs:\n        yield self.pipeline(chunk_inputs)\n\n", "CodeBleu_score": 28.91043361368387}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if isinstance(inputs, dict):\n        if 'img' in inputs:\n            inputs = [inputs]\n        elif 'img_path' in inputs:\n            inputs = [inputs]\n        else:\n            raise ValueError('The input dict must contain an \"img\" or \"img_path\" key')\n    iter_inputs = iter(inputs)\n    while True:\n        chunk = list(islice(iter_inputs, chunk_size))\n        if not chunk:\n            break\n        yield list(map(self.pipeline, chunk))\n\n", "CodeBleu_score": 29.507361934064598}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir != '' and return_datasamples:\n        logger = get_logger('mmpretrain')\n        logger.warning('Saving predictions is not supported for now, please set '\\\n                       '`no_save_pred` to True')\n\n    results = {'predictions': [], 'visualization': visualization}\n    for pred in preds:\n        if return_datasamples:\n            results['predictions'].append(pred)\n        else:\n            results['predictions'].append(pred.to_dict())\n\n    if print_result:\n        print(f'{json.dumps(results, indent=4)}", "CodeBleu_score": 46.77235024993767}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    results = dict(predictions=[], visualization=visualization)\n\n    if return_datasamples:\n        if pred_out_dir != '':\n            logger.warning('Saving datasamples is not supported when return_datasamples=True.')\n        for pred in preds:\n            results['predictions'].append(pred.pred_instances.to_dict())\n\n        return results\n\n    if print_result:\n        for pred in preds:\n            print(pred.pred_instances.to_dict())\n\n    for pred in preds:\n        results['predictions'].append(pred.pred_instances.to_dict())\n\n    if pred_out_dir != '' and not no_save_pred:\n        for pred in preds:\n            out_file = osp.join(pred_out_dir, pred.pred_instances.file_name)\n            mmengine.dump(pred.pred_instances.to_dict(), out_file)\n\n    return results\n\n", "CodeBleu_score": 45.19137449961047}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    results = dict(predictions=[], visualization=visualization)\n    if return_datasamples:\n        if pred_out_dir != '':\n            warnings.warn('Saving datasamples is not supported when return_datasamples'\\\n                          ' is True.')\n        for pred in preds:\n            results['predictions'].append(pred.to_dict())\n    else:\n        for pred in preds:\n            results['predictions'].append(pred.to_dict())\n\n    if print_result:\n        print(results['predictions'][0]['pred_bboxes'])\n\n    return results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.89687315887052}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # build the pipeline\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline = replace_ImageToTensor(test_pipeline)\n    test_pipeline = replace_MultiScaleFlipAug(test_pipeline)\n    test_pipeline = replace_LoadImageFromFile(test_pipeline)\n    test_pipeline = replace_LoadAnnotations(test_pipeline)\n    test_pipeline = replace_LoadMultiChannelImageFromFiles(test_pipeline)\n    test_pipeline = replace_LoadMultiChannelAnnotations(test_pipeline)\n    test_pipeline = replace_LoadPanopticAnnotations(test_pipeline)\n    test_pipeline = replace_LoadProposals(test_pipeline)\n    test_pipeline = replace_PackDetInputs(test_pipeline)\n    test_pipeline = replace_Collect(test_pipeline)\n    test_pipeline = replace_DefaultFormatBundle(test_pipeline)\n    test_pipeline = replace_DefaultFormatBundle3D(test_pipeline)\n    test_pipeline = replace_DefaultFormatBundle3D_LIDAR(test_pipeline)\n    test_pipeline = replace_DefaultFormatBundle_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadPointsFromFile(test_pipeline)\n    test_pipeline = replace_LoadPointsFromMultiSweeps(test_pipeline)\n    test_pipeline = replace_LoadPointsFromFile_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadPointsFromMultiSweeps_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadAnnotations3D(test_pipeline)\n    test_pipeline = replace_LoadPointsFromMultiSweeps_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadPointsFromFile_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadPointsFromMultiSweeps_LIDAR(test_pipeline)\n    test_pipeline = replace_LoadPointsFromFile_LIDAR(test_pipeline)\\", "CodeBleu_score": 12.4941713766669}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = dict(type='Resize', **test_pipeline[0]['resize_cfg'])\n    test_pipeline[-1] = dict(type='Collect', **test_pipeline[-1]['collect_cfg'])\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 21.446134748810056}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    # modify the first transform to only include the 'Resize' transform\n    test_pipeline[0] = dict(type='Resize', **test_pipeline[0].get('resize_cfg', {}))\n    # copy the last transform\n    last_transform = copy.deepcopy(test_pipeline[-1])\n    # combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = [test_pipeline[0], last_transform]\n    return test_pipeline\n\n", "CodeBleu_score": 21.54905689597805}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_shape=img.shape, img_id=0,\n                video_len=video_len)\n    # build the data pipeline\n    cfg = model.cfg\n    cfg = cfg.copy()\n    # set loading pipeline type\n    cfg.data.test.pipeline[0].type = 'LoadImageFromNDArray'\n    test_pipeline = Compose(cfg.data.test.pipeline)\n    data = test_pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 30.8520384668551}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    test_pipeline = get_test_pipeline_cfg(cfg)\n    data = dict(img=img, frame_id=frame_id, original_shape=img.shape[:2],\n                img_id=0, video_len=video_len)\n    data = test_pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 28.324262334573035}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, orig_shape=img.shape[:2], img_id=0,\n                video_len=video_len)\n\n    # build the data pipeline\n    cfg = model.cfg\n    cfg = cfg.copy()\n    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n    test_pipeline = Compose(cfg.data.test.pipeline)\n    data = test_pipeline(data)\n\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n\n    return results\n\n", "CodeBleu_score": 28.36830084066592}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(0)\n        else:\n            tokens_b.pop()\n    assert len(tokens_a) >= 1\n    assert len(tokens_b) >= 1\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 54.874035713202474}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(rng.randint(0, len(tokens_a) - 1))\n        else:\n            tokens_b.pop(rng.randint(0, len(tokens_b) - 1))\n\n", "CodeBleu_score": 47.7497995444901}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(rng.randint(0, len(tokens_a) - 1))\n        else:\n            tokens_b.pop(rng.randint(0, len(tokens_b) - 1))\n\n", "CodeBleu_score": 47.7497995444901}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \\\"linear\\\":\n        return tf.identity\n    elif act == \\\"relu\\\":\n        return tf.nn.relu\n    elif act == \\\"gelu\\\":\n        return gelu\n    elif act == \\\"tanh\\\":\n        return tf.tanh\n    elif act == \\\"sigmoid\\\":\n        return tf.sigmoid\n    else:\n        raise ValueError(\\\"Unsupported activation: \\\", act)\n\n", "CodeBleu_score": 67.99733281570963}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, str):\n        act = activation_string.lower()\n        if act == \\\"linear\\\":\n            return None\n        elif act == \\\"relu\\\":\n            return tf.nn.relu\n        elif act == \\\"gelu\\\":\n            return gelu\n        elif act == \\\"tanh\\\":\n            return tf.tanh\n        elif act == \\\"sigmoid\\\":\n            return tf.sigmoid\n        else:\n            raise ValueError(\\\"Unsupported activation function: \\'{}\\'\\\".format(activation_string))\n    else:\n        return activation_string\n\n", "CodeBleu_score": 61.51497895820197}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, str):\n        act = activation_string.lower()\n        if act == \\\"linear\\\":\n            return tf.identity\n        elif act == \\\"relu\\\":\n            return tf.nn.relu\n        elif act == \\\"gelu\\\":\n            return gelu\n        elif act == \\\"tanh\\\":\n            return tf.tanh\n        elif act == \\\"sigmoid\\\":\n            return tf.sigmoid\n        else:\n            raise ValueError(\\\"Unsupported activation: \\\", activation_string)\n    else:\n        return activation_string\n\n", "CodeBleu_score": 47.78491832262276}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.84090845772291}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    to_shape = get_shape_list(to_mask, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(to_mask, dtype=tf.float32)\n    to_mask = tf.reshape(to_mask, [batch_size, 1, to_seq_length, to_seq_length])\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, to_seq_length])\n    broadcast_ones = tf.cast(broadcast_ones, dtype=tf.float32)\n    attention_mask = tf.matmul(broadcast_ones, to_mask)\n    return attention_mask  # (batch_size, from_seq_length, to_seq_length)\n\n", "CodeBleu_score": 38.48962024166661}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor)\n    to_shape = get_shape_list(to_mask)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, to_seq_length], dtype=tf.float32)\n    attention_mask = tf.minimum(broadcast_ones, to_mask)\n    return attention_mask\n\n", "CodeBleu_score": 42.623753338731206}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask)\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, to_seq_length], dtype=tf.float32)\n    attention_mask = tf.minimum(broadcast_ones, to_mask)\n    return attention_mask\n\n", "CodeBleu_score": 43.672834024006264}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 42.61845112285781}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    shape = get_shape_list(input_tensor)\n    if len(shape) < 2:\n        raise ValueError(\\\"Input tensor must have at least rank 2. Shape received: %s\\\" % shape)\n    if len(shape) == 2:\n        return input_tensor\n    width = shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 44.42279120697502}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = get_shape_list(input_tensor)\n    if len(input_shape) < 2:\n        raise ValueError(\\\"Input tensor rank must be >= 2. Received rank: %d\\\" %\n                     len(input_shape))\n  if len(input_shape) == 2:\n    return input_tensor\n\n  width = input_shape[-1]\n  output_shape = input_shape[:-1]\n  output_shape.append(width)\n  return tf.reshape(input_tensor, output_shape)\n\n", "CodeBleu_score": 38.24316219837046}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\\\"Input tensor must have rank 2 or higher, but got %d\\\" % len(input_tensor.shape))\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 45.88637310465443}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\")), \\\"train\\\")\n\n  def get_dev_examples(self, data_dir):\n    \\\"\\\"\\\"The function retrieves development examples from a specified directory.", "CodeBleu_score": 6.867120915011996}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \\\"xnli.train.tsv\\\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = \\\"train-%d\\\" % i\n      text_a = line[0]\n      text_b = line[1]\n      label = line[2]\n      if label == \\\"contradictory\\\":\n        label = \\\"contradiction\\\"\n      examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    \\\"\\\"\\\"The function retrieves development examples from a specified directory.", "CodeBleu_score": 36.86339391312063}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\")), \\\"train\\\")\n\n  def get_dev_examples(self, data_dir):\n    \\\"\\\"\\\"The function retrieves development examples from a specified directory.", "CodeBleu_score": 6.867120915011996}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    mean_loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n    return {\n        \\\"eval_accuracy\\\": accuracy,\n        \\\"eval_loss\\\": mean_loss\n    }\n\n", "CodeBleu_score": 31.118716613828145}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    mean_loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n\n    return {\n        \\\"eval_accuracy\\\": accuracy,\n        \\\"eval_loss\\\": mean_loss\n    }\n\n", "CodeBleu_score": 31.118716613828145}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    mean_loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n    return {\n        \\\"eval_accuracy\\\": accuracy,\n        \\\"eval_loss\\\": mean_loss\n    }\n\n", "CodeBleu_score": 43.33213150760238}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\\\"Writing example %d of %d\\\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.63382927637542}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (i, example) in enumerate(examples):\n        if i % 10000 == 0:\n            tf.logging.info(\\\"Converting example %d of %d\\\" % (i, len(examples)))\n        feature = convert_single_example(example, label_list, max_seq_length,\n                                         tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 58.49095760683731}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\\\"Writing example %d of %d\\\" % (ex_index, len(examples)))\n        features.append(convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer))\n    return features\n\n", "CodeBleu_score": 62.77700346006284}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The embedding for [CLS] is taken from the pooler layer, but only for classification tasks.\n    # This prevents the pre-trained BERT model from using the pooler layer.\n    first_token_tensor = input_tensor[:, 0]\n    input_tensor = gather_indexes(first_token_tensor, labels)\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n      output_weights = tf.get_variable(\n          \\\"output_weights\\\",\n          shape=[bert_config.hidden_size, 2],\n          initializer=tf.truncated_normal_initializer(\n              stddev=bert_config.initializer_range))\n\n      output_bias = tf.get_variable(\n          \\\"output_bias\\\",\n          shape=[2],\n          initializer=tf.zeros_initializer())\n\n      logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n      logits = tf.nn.bias_add(logits, output_bias)\n      log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n      labels = tf.reshape(labels, [-1])\n      one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n      per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n      loss = tf.reduce_mean(per_example_loss)\n\n  return (loss, per_example_loss, log_probs)\n\n\n\n", "CodeBleu_score": 74.78423383753578}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The embedding for [CLS] is taken from the embedding output layer.\n    first_token_tensor = input_tensor[:, 0]\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n        output_weights = tf.get_variable(\\\"output_weights\\\", [bert_config.hidden_size, 2],\n                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\\\"output_bias\\\", [2],\n                                 initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(first_token_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=[-1])\n        numerator = tf.reduce_sum(one_hot_labels * per_example_loss)\n        denominator = tf.reduce_sum(one_hot_labels) + 1e-5\n        loss = numerator / denominator\n\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 64.2261189280648}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n        output_weights = tf.get_variable(\n          \\\"output_weights\\\",\n          shape=[bert_config.hidden_size, 2],\n          initializer=tf.truncated_normal_initializer(\n              stddev=bert_config.initializer_range))\n        output_bias = tf.get_variable(\n          \\\"output_bias\\\",\n          shape=[2],\n          initializer=tf.zeros_initializer())\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 75.77131186205844}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(flat_offsets + positions, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 52.347858015846626}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + tf.cast(flat_offsets, tf.int32), [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 53.3479862144333}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name].dtype\n        if t == tf.int64:\n            t = tf.int32\n        example[name] = tf.cast(example[name], t)\n    return example\n\n", "CodeBleu_score": 49.8489612667469}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name].dtype\n        if t == tf.int64:\n            t = tf.int32\n        example[name] = tf.cast(example[name], t)\n    return example\n\n", "CodeBleu_score": 49.8489612667469}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name].dtype\n        if t == tf.int64:\n            t = tf.int32\n        example[name] = tf.cast(example[name], t)\n    return example\n\n", "CodeBleu_score": 49.8489612667469}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words.\n    def _canonicalize_text(text):\n      \"\"\"Returns text with whitespace normalized and punctuation removed.\"\"\"\n      text = tokenization.convert_to_unicode(text)\n\n      text = \\\" \\\" + text + \\\" \\\"\n      text = text.replace(u\\\"-\\\", u\\\" \\\")\n      text = re.sub(u\\\"\\\\\\\\s+\\\", u\\\" \\\", text)\n      text = text.replace(u\\\".,?!\\\", u\\\" \\\".encode(\\\"utf-8\\\"))\n      text = re.sub(u\\\"[^a-zA-Z0-9 \\\\\\\\s]+\\\", u\\\"\\\", text)\n      text = re.sub(u\\\"\\\\\\\\s+\\\", u\\\" \\\", text)\n      text = text.strip()\n      text = text.lower()\n      return text\n\n    tok_answer_text = \\\" \\\".join(tokenizer.tokenize(_canonicalize_text(orig_answer_text)))\n\n    start_position = None\n    end_position = None\n    for (start_position, end_position) in _get_best_text_segment(tok_answer_text, input_text_spans):\n      if start_position == None or end_position == None:\n        continue\n      if input_end < start_position or end_position < input_start:\n        continue\n      if input_start <= start_position and end_position <= input_end:\n        break\n\n    if start_position != None and end_position != None:\n      # We could find a match of the whole original span\n      return (start_position, end_position)\n\n    # If we cannot find a match of the whole original span, we resort to\n    # finding a sub-span of the original span.\n    for (start_position, end_position) in _get_best_text_segment(tok_answer_text, input_text_spans):\n      if start_position == None or end_position", "CodeBleu_score": 20.920859139581573}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words.\n    def _canonicalize_text(text):\n        \"\"\"Returns text with whitespaces and canonically combines tokens.", "CodeBleu_score": 4.513378175493185}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. This allows to break tokenization into smaller\n    # pieces.\n    tok_answer_text = \\\" \\\"\\\"\n    for c in orig_answer_text:\n      if c == \\\\'\\\\\\", "CodeBleu_score": 4.53160593095779}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n  best_span_index = None\n  for (span_index, doc_span) in enumerate(doc_spans):\n    start = doc_span.start\n    end = doc_span.start + doc_span.length - 1\n    if position < start or position > end:\n      continue\n\n    num_left_context = position - start\n    num_right_context = end - position\n    score = (num_left_context + num_right_context) / doc_span.length\n    if score > best_score:\n      best_score = score\n      best_span_index = span_index\n\n  return cur_span_index == best_span_index\n\n\n", "CodeBleu_score": 66.1787153464712}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        start = doc_span.start\n        end = doc_span.start + doc_span.length - 1\n        if position < start or position > end:\n            continue\n        num_left_context = position - start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context)\n        if (best_score is None or score > best_score):\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 66.09907763708648}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / doc_span.length\n        if (best_score is None) or (score > best_score):\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.46286119683016}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 66.58562471993214}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\" as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.97375895824288}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\" as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.97375895824288}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output_items = []\n    for item in items:\n        output_items.append(vocab[item])\n    return output_items\n\n", "CodeBleu_score": 65.46078808894123}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output_ids = []\n    for item in items:\n        output_ids.append(vocab[item])\n    return output_ids\n\n", "CodeBleu_score": 66.4943800131923}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output_tokens = []\n    for token in items:\n        if token in vocab:\n            output_tokens.append(vocab[token])\n        else:\n            output_tokens.append(vocab[UNKNOWN_TOKEN])\n    return output_tokens\n\n", "CodeBleu_score": 44.64857071705425}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\\\"Loading audio from %s\\\", path)\n        audio_data = self.load(path, offset, duration, sample_rate, dtype)\n        logger.info(\\\"Audio loaded successfully from %s\\\", path)\n        return audio_data, False\n    except Exception as e:\n        logger.error(\\\"Error loading audio from %s: %s\\\", path, e)\n        return -1.0, True\n\n    def load_waveform(self, audio_descriptor, offset=0.0, duration=1800.0, sample_rate=44100, dtype=b\\\"float32\\\", waveform_name=\\\"waveform\\\"):\n        \\\"\\\"\\\"\n        Load the audio and convert it to a tensorflow waveform.\n\n        Parameters:\n            audio_descriptor (Any):\n                Describe song to load, in case of file based audio adapter,\n                such descriptor would be a file path.\n            offset (float):\n                (Optional) Start offset to load from in seconds.\n            duration (float):\n                (Optional) Duration to load in seconds.\n            sample_rate (float):\n                (Optional) Sample rate to load audio with.\n            dtype (bytes):\n                (Optional) Data type to use, default to `b'float32'`.\n            waveform_name (str):\n                (Optional) Name of the key in output dict, default to\n                `'waveform'`.\n\n        Returns:\n            Dict[str, Any]:\n                TF output dict with waveform as `(T x chan numpy array)`\n                and a boolean that tells whether there were an error while\n                trying to load the waveform.\n        \\\"\\\"\\\"\n        # Cast parameters to TF format.\n        offset = tf.cast(offset, tf.float64)\n        duration = tf.cast(duration, tf.float64)\n\n        # Defined safe loading function.\n        audio_data, error = safe_load(audio_descriptor, offset, duration, sample_rate, dtype)\n", "CodeBleu_score": 31.63548753668355}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\\\"Loading audio from %s\\\", path)\n        audio = self.load(path, offset, duration, sample_rate, dtype)\n        return audio, False\n    except Exception as e:\n        logger.error(\\\"Error loading audio from %s: %s\\\", path, e)\n        return -1.0, True\n\n    def load_waveform(self, audio_descriptor, offset=0.0, duration=1800.0, sample_rate=44100, dtype=b\\\"float32\\\", waveform_name=\\\"waveform\\\"):\n        \\\"\\\"\\\"\n        Load the audio and convert it to a tensorflow waveform.\n\n        Parameters:\n            audio_descriptor (Any):\n                Describe song to load, in case of file based audio adapter,\n                such descriptor would be a file path.\n            offset (float):\n                (Optional) Start offset to load from in seconds.\n            duration (float):\n                (Optional) Duration to load in seconds.\n            sample_rate (float):\n                (Optional) Sample rate to load audio with.\n            dtype (bytes):\n                (Optional) Data type to use, default to `b'float32'`.\n            waveform_name (str):\n                (Optional) Name of the key in output dict, default to\n                `'waveform'`.\n\n        Returns:\n            Dict[str, Any]:\n                TF output dict with waveform as `(T x chan numpy array)`\n                and a boolean that tells whether there were an error while\n                trying to load the waveform.\n        \\\"\\\"\\\"\n        # Cast parameters to TF format.\n        offset = tf.cast(offset, tf.float64)\n        duration = tf.cast(duration, tf.float64)\n\n        # Load the audio data.\n        audio, error = safe_load(audio_descriptor, offset, duration, sample_rate, dtype)\n\n        # Create a TF output dict.\n        output = {waveform_name: audio, \\\"", "CodeBleu_score": 30.49692202436085}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n            # Load audio data using the provided parameters.\n            signal = self.load(path, offset, duration, sample_rate, dtype)\n        except Exception as e:\n            # Log the error and return a default value with a True flag.\n            logger.error(\\\"Error loading audio file %s: %s\\\", path, e)\n            return np.array([-1.0]), True\n        else:\n            # Return the audio data and a False flag indicating no error.\n            return signal, False\n\n    # Define a function to convert audio data to a TensorFlow waveform.\n    def tf_waveform(path, offset, duration, sample_rate, dtype, waveform_name):\n        \\\"\\\"\\\"\n        Convert audio data to a TensorFlow waveform.\n\n        Parameters:\n            path (Any):\n                Path to the audio file.\n            offset (float):\n                Start offset of the audio data in seconds.\n            duration (float):\n                Duration of the audio data in seconds.\n            sample_rate (float):\n                Sample rate of the audio data.\n            dtype (bytes):\n                Data type of the audio data.\n            waveform_name (str):\n                Name of the waveform key in the output dictionary.\n\n        Returns:\n            Dict[str, Any]:\n                A dictionary containing the waveform as a TensorFlow tensor.\n        \\\"\\\"\\\"\n        # Load the audio data and check for errors.\n        signal, error = safe_load(path, offset, duration, sample_rate, dtype)\n        if error:\n            # Return a default value with a True flag if there was an error.\n            return {waveform_name: tf.constant(signal), \\\"error\\\": True}\n\n        # Convert the audio data to a TensorFlow tensor.\n        signal = tf.convert_to_tensor(signal, dtype=dtype)\n\n        # Return the waveform as a TensorFlow tensor with a False flag.\n        return {waveform_name: signal, \\\"error\\\": False}\n\n    # Define a function to convert audio data to a TensorFlow", "CodeBleu_score": 29.02469627683764}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_channels = waveform.shape[-1]\n    if waveform_channels == n_channels:\n        return waveform\n    if waveform_channels > n_channels:\n        return waveform[..., :n_channels]\n    return tf.tile(waveform[..., None], [1] * (len(waveform.shape) - 1) + [n_channels])[..., :n_channels]\n\n", "CodeBleu_score": 42.53215628070164}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == 1:\n        return waveform[:, 0]\n\n    if n_channels == 2:\n        return tf.stack([waveform[:, 0], waveform[:, 1]], axis=1)\n\n    if n_channels > waveform.shape[1]:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1]])[:, :n_channels]\n\n    return waveform[:, :n_channels]\n\n", "CodeBleu_score": 47.512696622413145}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] == n_channels:\n        return waveform\n    if waveform.shape[1] < n_channels:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])[:, :n_channels]\n    return waveform[:, :n_channels]\n\n", "CodeBleu_score": 48.73984454227145}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    return from_float32_to_uint8(\n        tf.clip_by_value(gain_to_db(spectrogram), -db_range, 0.0), **kwargs\n    )\n\n", "CodeBleu_score": 28.330543086920624}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram)\n    max_db = tf.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    db_spectrogram = from_float32_to_uint8(db_spectrogram)\n    return db_spectrogram\n\n", "CodeBleu_score": 45.412255113841994}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # pylint: disable=unused-argument\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownGlobalType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownReturnType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: report", "CodeBleu_score": 17.938099389712292}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, dtype=waveform.dtype) ** window_exponent\n    windowed_waveform = waveform * window\n    stft_output = stft(\n        windowed_waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n    )\n    return tf.abs(stft_output) ** spec_exponent\n\n", "CodeBleu_score": 35.38215503316577}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the Hann window function\n    window = hann_window(frame_length, dtype=tf.float32) ** window_exponent\n\n    # Compute the spectrogram using STFT\n    spectrogram = tf.abs(\n        stft(\n            waveform,\n            frame_length,\n            frame_step,\n            fft_length=frame_length,\n            window_fn=lambda x: window,\n        )\n    ) ** spec_exponent\n\n    # Transpose the spectrogram tensor\n    spectrogram = tf.transpose(spectrogram, perm=[2, 1, 0])\n\n    return spectrogram\n\n", "CodeBleu_score": 40.99512034755978}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    window = hann_window(frame_length, dtype=waveform.dtype) ** window_exponent\n    stfts = stft(\n        tf.transpose(waveform),\n        frame_length,\n        frame_step,\n        window,\n        pad_end=True,\n    )\n    spectrogram = tf.abs(stfts) ** spec_exponent\n    return spectrogram\n\n", "CodeBleu_score": 41.168776692255335}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # pyright: reportMissingImports=false\n    # pylint: disable=import-error\n    import tensorflow as tf  # type: ignore\n\n    # pylint: enable=import-error\n\n    # Get the shape of the input spectrogram\n    shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(shape[0], tf.float32) * factor\n\n    # Resize the spectrogram using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=tf.cast(new_time_dim, tf.int32),\n        method=method,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    adjusted_spectrogram = tf.image.resize_with_crop_or_pad(\n        resized_spectrogram,\n        shape[0],\n        shape[1],\n    )\n\n    return adjusted_spectrogram\n\n", "CodeBleu_score": 35.21009098117914}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # pyright: reportGeneralTypeIssues=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: reportUnknownParameterType=false\n    # pyright: reportUnknownVariableType=false\n    # pyright: reportUnknownMemberType=false\n    # pyright: reportUnknownArgumentType=false\n    # pyright: report", "CodeBleu_score": 15.230481727574752}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # pylint: disable=no-member\n    return tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[0] * factor), spectrogram.shape[1]],\n        method=method,\n    )\n\n", "CodeBleu_score": 24.18669514126041}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = tf.pow(2.0, semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis\n    shifted_spectrogram = tf.image.resize_images(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resizing_factor, tf.int32)],\n        method=method,\n        align_corners=True,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    shifted_spectrogram = tf.image.resize_image_with_crop_or_pad(\n        shifted_spectrogram, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n\n    return shifted_spectrogram\n\n", "CodeBleu_score": 31.672576887834452}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resize_factor = tf.pow(2.0, semitone_shift / 12.0)\n\n    # Calculate the new shape for the spectrogram\n    new_shape = tf.cast(\n        tf.cast(tf.shape(spectrogram)[1], tf.float32) * resize_factor, tf.int32\n    )\n\n    # Resize the spectrogram along the frequency axis\n    ps_spec = tf.image.resize_images(\n        spectrogram, [new_shape, tf.shape(spectrogram)[1]], method=method\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1])\n\n", "CodeBleu_score": 34.7832078626499}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    shifted_spectrogram = tf.image.resize_images(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.cast(tf.cast(tf.shape(spectrogram)[1], tf.float32) * resizing_factor, tf.int32)[0]],\n        method=method,\n        align_corners=True,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.resize_image_with_crop_or_pad(\n        shifted_spectrogram, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 34.985139848024716}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\\\"conv_activation\\\", \\\"relu\\\")\n    if conv_activation == \\\"relu\\\":\n        return ReLU()\n    if conv_activation == \\\"elu\\\":\n        return ELU()\n    if conv_activation == \\\"leaky_relu\\\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\\\"Unknown activation layer: \\\" + conv_activation)\n\n", "CodeBleu_score": 31.093688737255892}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\\\"conv_activation\\\") == \\\"relu\\\":\n        return ReLU\n    elif params.get(\\\"conv_activation\\\") == \\\"elu\\\":\n        return ELU\n    elif params.get(\\\"conv_activation\\\") == \\\"leaky_relu\\\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\\\"Invalid activation layer: {}\\\".format(params.get(\\\"conv_activation\\\")))\n\n", "CodeBleu_score": 29.0502208524253}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\\\"conv_activation\\\", \\\"ReLU\\\") == \\\"ReLU\\\":\n        return ReLU()\n    if params.get(\\\"conv_activation\\\", \\\"ReLU\\\") == \\\"ELU\\\":\n        return ELU()\n    if params.get(\\\"conv_activation\\\", \\\"ReLU\\\") == \\\"LeakyReLU\\\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\\\"Unsupported activation layer.\\\")\n\n", "CodeBleu_score": 31.80036538045941}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(f\"Apply unet for {instruments}\")\n    conv_n_filters = params.get(\"conv_n_filters\", [16, 32, 64, 128, 256, 512])\n    conv_activation_layer = _get_conv_activation_layer(params)\n    deconv_activation_layer = _get_deconv_activation_layer(params)\n    kernel_initializer = he_uniform(seed=50)\n    conv2d_factory = partial(\n        Conv2D, strides=(2, 2), padding=\"same\", kernel_initializer=kernel_initializer\n    )\n    # First layer.\n    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n    batch1 = BatchNormalization(axis=-1)(conv1)\n    rel1 = conv_activation_layer(batch1)\n    # Second layer.\n    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n    batch2 = BatchNormalization(axis=-1)(conv2)\n    rel2 = conv_activation_layer(batch2)\n    # Third layer.\n    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n    batch3 = BatchNormalization(axis=-1)(conv3)\n    rel3 = conv_activation_layer(batch3)\n    # Fourth layer.\n    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n    batch4 = BatchNormalization(axis=-1)(conv4)\n    rel4 = conv_activation_layer(batch4)\n    # Fifth layer.\n    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n    batch5 = BatchNormalization(axis=-1)(conv5)\n    rel5 = conv_activation_layer", "CodeBleu_score": 20.024659886253428}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Initialize an empty dictionary to store the logit masks for each instrument.\n    logit_masks = {}\n    # Iterate over each instrument.\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for the current instrument.\n        logit_mask = apply_unet(input_tensor, instrument, params, output_mask_logit=True)\n        # Add the logit mask for the current instrument to the dictionary.\n        logit_masks[instrument] = logit_mask\n    # Combine the logit masks for all instruments using a softmax function along the channel axis.\n    softmax_logit_mask = tf.nn.softmax(logit_masks, axis=-1)\n    # Initialize an empty dictionary to store the spectrograms for each instrument.\n    spectrograms = {}\n    # Iterate over each instrument.\n    for instrument in instruments:\n        # Multiply the softmax logit mask for the current instrument with the input tensor to obtain the spectrogram.\n        spectrogram = tf.multiply(softmax_logit_mask[instrument], input_tensor)\n        # Add the spectrogram for the current instrument to the dictionary.\n        spectrograms[instrument] = spectrogram\n    # Return the dictionary of instrument spectrograms.\n    return spectrograms\n\n", "CodeBleu_score": 32.82137899392241}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply unet for softmax\")\n    conv_n_filters = params.get(\"conv_n_filters\", [16, 32, 64, 128, 256, 512])\n    conv_activation_layer = _get_conv_activation_layer(params)\n    deconv_activation_layer = _get_deconv_activation_layer(params)\n    kernel_initializer = he_uniform(seed=50)\n    conv2d_factory = partial(\n        Conv2D, strides=(2, 2), padding=\"same\", kernel_initializer=kernel_initializer\n    )\n    # First layer.\n    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n    batch1 = BatchNormalization(axis=-1)(conv1)\n    rel1 = conv_activation_layer(batch1)\n    # Second layer.\n    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n    batch2 = BatchNormalization(axis=-1)(conv2)\n    rel2 = conv_activation_layer(batch2)\n    # Third layer.\n    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n    batch3 = BatchNormalization(axis=-1)(conv3)\n    rel3 = conv_activation_layer(batch3)\n    # Fourth layer.\n    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n    batch4 = BatchNormalization(axis=-1)(conv4)\n    rel4 = conv_activation_layer(batch4)\n    # Fifth layer.\n    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n    batch5 = BatchNormalization(axis=-1)(conv5)\n    rel5 = conv_activation_layer(batch5)\n", "CodeBleu_score": 18.50736095966338}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 56.88014708665297}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 38.0396105771608}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \\\"rb\\\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 38.594244900163986}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\\\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\\\"No checksum found for model {name}\\\")\n    return index[name]\n\n\n", "CodeBleu_score": 39.77277305776239}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\\\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\\\"Failed to retrieve checksum index from {url}\\\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\\\"Model {name} not found in {url}\\\")\n    return index[name]\n\n\n\n", "CodeBleu_score": 42.55678072933578}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\\\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n        if response.status_code != 200:\n            raise ValueError(f\\\"Failed to fetch checksum index from {url}\\\")\n        index = response.json()\n        if name not in index:\n            raise ValueError(f\\\"Model {name} not found in index\\\")\n        return index[name]\n\n    def download(self, name: str, path: str) -> None:\n        \"\"\"The function downloads a model from a Github release and extracts it to a specified path.", "CodeBleu_score": 42.77910073750303}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \\\"/\\\".join(\n            (\n                self._host,\n                self._repository,\n                self.RELEASE_PATH,\n                self._release,\n                f\\\"{name}.tar.gz\\\",\n            )\n        )\n        logger.info(\\\"Downloading model %s from %s\\\", name, url)\n        response: httpx.Response = httpx.get(url, stream=True)\n        response.raise_for_status()\n        with NamedTemporaryFile(delete=False) as fd:\n            for chunk in response.iter_bytes():\n                fd.write(chunk)\n        checksum: str = compute_file_checksum(fd.name)\n        if checksum != self.checksum(name):\n            raise ValueError(f\\\"Checksum mismatch for model {name}\\\")\n        logger.info(\\\"Extracting model %s to %s\\\", name, path)\n        with tarfile.open(fd.name) as tar:\n            tar.extractall(path)\n        os.remove(fd.name)\n        logger.info(\\\"Model %s successfully extracted\\\", name)", "CodeBleu_score": 46.41603461391905}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \\\"/\\\".join(\n            (\n                self._host,\n                self._repository,\n                self.RELEASE_PATH,\n                self._release,\n                f\\\"{name}.tar.gz\\\",\n            )\n        )\n        logger.info(\\\"Downloading model from %s\\\", url)\n        response: httpx.Response = httpx.get(url, stream=True)\n        response.raise_for_status()\n        with NamedTemporaryFile(delete=False) as temp_file:\n            for chunk in response.iter_bytes():\n                temp_file.write(chunk)\n            temp_file.flush()\n            logger.info(\\\"Downloaded model to %s\\\", temp_file.name)\n            checksum: str = compute_file_checksum(temp_file.name)\n            logger.info(\\\"Computed checksum %s\\\", checksum)\n            if checksum != self.checksum(name):\n                raise ValueError(\\\"Invalid checksum for model\\\")\n            logger.info(\\\"Valid checksum for model\\\")\n            with tarfile.open(temp_file.name) as tar:\n                tar.extractall(path)\n            logger.info(\\\"Extracted model to %s\\\", path)\n            os.remove(temp_file.name)\n            logger.info(\\\"Removed downloaded model\\\")\n\n\n", "CodeBleu_score": 46.842596684786365}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \\\"/\\\".join(\n            (\n                self._host,\n                self._repository,\n                self.RELEASE_PATH,\n                self._release,\n                f\\\"{name}.tar.gz\\\",\n            )\n        )\n        logger.info(f\\\"Downloading {name} from {url}\\\")\n        response: httpx.Response = httpx.get(url, stream=True)\n        response.raise_for_status()\n        with NamedTemporaryFile(delete=False) as stream:\n            for chunk in response.iter_content(chunk_size=4096):\n                stream.write(chunk)\n        logger.info(f\\\"Downloaded {name} to {stream.name}\\\")\n        logger.info(f\\\"Validating {name} checksum\\\")\n        checksum: str = compute_file_checksum(stream.name)\n        if checksum != self.checksum(name):\n            raise ValueError(f\\\"Invalid {name} checksum\\\")\n        logger.info(f\\\"Extracting {name} to {path}\\\")\n        with tarfile.open(stream.name) as archive:\n            archive.extractall(path=path)\n        logger.info(f\\\"Extracted {name} to {path}\\\")\n        os.remove(stream.name)\n\n\n", "CodeBleu_score": 50.522153009517346}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            content = loader.read_text(resources, name)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"{name}\\\" is not a valid configuration name)\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\", encoding=\\\"utf-8\\\") as fd:\n            content = fd.read()\n    else:\n        raise SpleeterError(f\\\"{descriptor}\\\" is not a valid configuration name or file path)\n    return json.loads(content)", "CodeBleu_score": 45.16872373235939}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor.replace(_EMBEDDED_CONFIGURATION_PREFIX, \\\"\\\")\n        try:\n            configuration = json.loads(loader.read_text(resources, name))\n        except FileNotFoundError:\n            raise SpleeterError(\\\"Embedded configuration '{}' not found\\\".format(name)) from None\n    else:\n        if not exists(descriptor):\n            raise SpleeterError(\\\"Configuration file '{}' not found\\\".format(descriptor)) from None\n        with open(descriptor, \\\"r\\\") as f:\n            configuration = json.load(f)\n    return configuration", "CodeBleu_score": 44.22293010617292}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            configuration = loader.read_text(resources, configuration_name)\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration '{configuration_name}' not found.\\\")\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as configuration_file:\n            configuration = configuration_file.read()\n    else:\n        raise SpleeterError(f\\\"Configuration file '{descriptor}' not found.\\\")\n\n    return json.loads(configuration)\n", "CodeBleu_score": 43.16682976750278}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, but {} was given.\".format(concat_axis)\n        )\n    if concat_axis == 0:\n        # Concatenate tensors along axis 0\n        tensor_concat = tf.concat(list(tensor_dict.values()), axis=0)\n        # Apply function to concatenated tensor\n        tensor_processed = func(tensor_concat)\n        # Split processed tensor back into dictionary\n        tensor_dict_processed = {\n            key: tensor_processed[i : i + tensor_dict[key].shape[0]]\n            for i, key in enumerate(tensor_dict)\n        }\n    else:\n        # Concatenate tensors along axis 1\n        tensor_concat = tf.concat(list(tensor_dict.values()), axis=1)\n        # Apply function to concatenated tensor\n        tensor_processed = func(tensor_concat)\n        # Split processed tensor back into dictionary\n        tensor_dict_processed = {\n            key: tensor_processed[:, i : i + tensor_dict[key].shape[1]]\n            for i, key in enumerate(tensor_dict)\n        }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 48.063340889765705}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    # pylint: disable=unused-argument\n    if concat_axis == 0:\n        raise ValueError(\n            \"The concatenation axis should be either 0 or 1, but not 0 in sync_apply\"\n        )\n    if concat_axis == 1:\n        tensors = list(tensor_dict.values())\n        concat_tensor = tf.concat(tensors, axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        processed_dict = {\n            key: tensor\n            for key, tensor in zip(tensor_dict.keys(), tf.split(processed_tensor, len(tensors)))\n        }\n    else:\n        raise ValueError(\n            \"The concatenation axis should be either 0 or 1, but not {} in sync_apply\".format(\n                concat_axis\n            )\n        )\n    return processed_dict\n\n", "CodeBleu_score": 46.52169218581415}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis == 0:\n        concat_tensor = tf.concat(\n            [tensor for tensor in tensor_dict.values()], axis=concat_axis\n        )\n        output_tensor = func(concat_tensor)\n        return {\n            key: tensor\n            for key, tensor in zip(\n                tensor_dict.keys(), tf.split(output_tensor, len(tensor_dict), axis=concat_axis)\n            )\n        }\n    elif concat_axis == 1:\n        concat_tensor = tf.concat(\n            [tensor for tensor in tensor_dict.values()], axis=concat_axis\n        )\n        output_tensor = func(concat_tensor)\n        return {\n            key: tensor\n            for key, tensor in zip(\n                tensor_dict.keys(), tf.split(output_tensor, len(tensor_dict), axis=concat_axis)\n            )\n        }\n    else:\n        raise ValueError(\n            f\"Invalid concatenation axis {concat_axis}. Only 0 and 1 are allowed.\"\n        )\n\n", "CodeBleu_score": 40.805088498169326}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_min = tf.cast(tensor_min, tf.float32)\n    tensor_max = tf.cast(tensor_max, tf.float32)\n    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {tensor_key: tensor, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 60.266345609592776}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255.0, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 47.202142731066075}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast((tensor - min_value) / (max_value - min_value) * 255, tf.uint8),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 49.576519707314404}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = segment_len - tf.shape(tensor)[0] % segment_len\n    # Pad the tensor with zeros to the right\n    padded_tensor = tf.pad(tensor, [[0, padding_needed]])\n    # Reshape the padded tensor into a new tensor with the specified segment length\n    reshaped_tensor = tf.reshape(padded_tensor, (-1, segment_len))\n    return reshaped_tensor\n\n", "CodeBleu_score": 22.6211269633659}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_shape = tf.shape(tensor)\n    tensor_len = tensor_shape[0]\n    padding_len = (segment_len - (tensor_len % segment_len)) % segment_len\n    padding_shape = [padding_len] + list(tensor_shape[1:])\n    padded_tensor = tf.pad(tensor, [[0, padding_len]] + [[0, 0]] * (len(tensor_shape) - 1))\n    return tf.reshape(padded_tensor, [-1, segment_len] + list(tensor_shape[1:]))\n\n", "CodeBleu_score": 31.50623691065807}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor with zeros\n    padded_tensor = tf.pad(tensor, [[0, padding_needed], [0, 0]])\n    # Reshape the padded tensor into a new tensor with the specified segment length\n    reshaped_tensor = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[-1]])\n    return reshaped_tensor\n\n", "CodeBleu_score": 28.251211805366665}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - tf.shape(instr_spec)[0] % frame_length\n    # Create an extension of zeros with the same shape as the input spectrogram\n    zeros = tf.zeros((extra_rows, F), dtype=instr_spec.dtype)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_instr_spec = tf.concat([instr_spec, zeros], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    return tf.reshape(extended_instr_spec, (-1, F))\n\n", "CodeBleu_score": 15.916432841135556}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    frame_length_padded = frame_length + instr_spec.shape[1] % frame_length\n    padding = tf.zeros([F, frame_length_padded - instr_spec.shape[1]], dtype=tf.float32)\n    instr_spec_padded = tf.concat([instr_spec, padding], axis=1)\n    return tf.reshape(instr_spec_padded, [-1, F * frame_length])\n\n", "CodeBleu_score": 14.965943579464161}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    spec_length = tf.shape(instr_spec)[1]\n    spec_length_pad = tf.cast(tf.math.ceil(tf.cast(spec_length, tf.float32) / frame_length) * frame_length, tf.int32)\n    spec_length_pad_diff = spec_length_pad - spec_length\n    spec_length_pad_diff_left = spec_length_pad_diff // 2\n    spec_length_pad_diff_right = spec_length_pad - spec_length - spec_length_pad_diff_left\n    spec_length_pad_diff_left = tf.cast(spec_length_pad_diff_left, tf.int32)\n    spec_length_pad_diff_right = tf.cast(spec_length_pad_diff_right, tf.int32)\n    spec_length_pad_diff_left_tf = tf.tile([spec_length_pad_diff_left], [F])\n    spec_length_pad_diff_right_tf = tf.tile([spec_length_pad_diff_right], [F])\n    spec_length_pad_diff_left_tf = tf.reshape(spec_length_pad_diff_left_tf, [-1, 1])\n    spec_length_pad_diff_right_tf = tf.reshape(spec_length_pad_diff_right_tf, [-1, 1])\n    spec_length_pad_diff_left_tf = tf.cast(spec_length_pad_diff_left_tf, tf.int32)\n    spec_length_pad_diff_right_tf = tf.cast(spec_length_pad_diff_right_tf, tf.int32)\n    spec_length_pad_diff_left_tf = tf.transpose(spec_length_pad_diff_left_tf)\n    spec_length_pad_diff_right_tf = tf.transpose(spec_length_pad_diff_right_tf)\n    spec_length_pad_diff_left_tf = tf.reshape(spec", "CodeBleu_score": 15.374491504221865}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    data_dict = {col: tf.constant(df[col].values) for col in df.columns}\n    return tf.data.Dataset.from_tensor_slices(data_dict)\n\n", "CodeBleu_score": 40.35182712416512}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 37.4879361692088}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    tensor_dict = {col: tf.constant(df[col].values) for col in df.columns}\n    return tf.data.Dataset.from_tensor_slices(tensor_dict)\n\n", "CodeBleu_score": 39.28112611150217}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim in range(len(target_shape)):\n        result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[dim], target_shape[dim]))\n    return result\n\n", "CodeBleu_score": 37.21626169115478}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 26.678418515318214}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for target_dim, tensor_dim in zip(target_shape, tensor_tf.shape):\n        if target_dim is not None:\n            result = tf.math.logical_and(result, tf.equal(tensor_dim, target_dim))\n    return result\n\n", "CodeBleu_score": 34.16398401063373}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    logger.info(\\\"Creating training dataset...\\\")\n    # Initialize the DatasetBuilder with the provided audio parameters\n    builder = DatasetBuilder(audio_params)\n    # Build the training dataset with the specified parameters\n    dataset = builder.build(\n        train_csv_path=os.path.join(audio_path, \\\"train.csv\\\"),\n        cache_dir=os.path.join(audio_path, \\\"cache\\\"),\n        batch_size=audio_params[\\\"batch_size\\\"],  # type: ignore\n        n_chunks_per_song=audio_params[\\\"n_chunks_per_song\\\"],  # type: ignore\n        data_augmentation=audio_params[\\\"data_augmentation\\\"],  # type: ignore\n        cache=audio_params[\\\"cache\\\"],  # type: ignore\n        shuffle=audio_params[\\\"shuffle\\\"],  # type: ignore\n    )\n    return dataset\n\n", "CodeBleu_score": 34.60524250995666}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    chunk_duration = audio_params.get(\\\"chunk_duration\\\", 10)\n    random_seed = audio_params.get(\\\"random_seed\\\", 0)\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    return builder.build(train_csv=audio_params.get(\\\"train_csv\\\"), cache_dir=audio_params.get(\\\"cache_dir\\\"), batch_size=audio_params.get(\\\"batch_size\\\"), chunks_per_song=audio_params.get(\\\"chunks_per_song\\\"), random_seed=random_seed, augmentation=audio_params.get(\\\"augmentation\\\"), cache=audio_params.get(\\\"cache\\\"))\n\n", "CodeBleu_score": 33.91213278311191}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n\n    logger.info(\\\"Creating training dataset...\\\")\n    logger.info(\\\"Audio parameters: %s\\\", audio_params)\n    logger.info(\\\"Audio adapter: %s\\\", audio_adapter)\n    logger.info(\\\"Audio path: %s\\\", audio_path)\n\n    # Create dataset builder.\n    # pylint: disable=protected-access\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    # pylint: enable=protected-access\n    # pylint: disable=protected-access\n    builder.chunk_duration = audio_params[\\\"chunk_duration\\\"]\n    builder.random_seed = audio_params[\\\"random_seed\\\"]\n    # pylint: enable=protected-access\n    # pylint: disable=protected-access\n    builder.train_csv_path = audio_params[\\\"train_csv_path\\\"]\n    builder.cache_dir = audio_params[\\\"cache_dir\\\"]\n    # pylint: enable=protected-access\n    # pylint: disable=protected-access\n    builder.batch_size = audio_params[\\\"batch_size\\\"]\n    builder.chunks_per_song = audio_params[\\\"chunks_per_song\\\"]\n    # pylint: enable=protected-access\n    # pylint: disable=protected-access\n    builder.data_augmentation = audio_params[\\\"data_augmentation\\\"]\n    builder.cache = audio_params[\\\"cache\\\"]\n    # pylint: enable=protected-access\n    # pylint: disable=protected-access\n    builder.chunk_duration = audio_params[\\", "CodeBleu_score": 35.70259068997083}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(\n                audio_adapter=self._audio_adapter,\n                audio_params=self._audio_params,\n                audio_path=self._audio_path,\n                random_seed=self._random_seed,\n                instrument=instrument,\n            )\n            for instrument in self._audio_params[\"instrument_list\"]\n        ]\n    yield from self._instrument_builders\n", "CodeBleu_score": 33.775828355099705}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(\n                instrument,\n                self._audio_adapter,\n                self._audio_params,\n                self._audio_path,\n                self._random_seed,\n            )\n            for instrument in self._instruments\n        ]\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 45.49381061527945}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(\n                audio_adapter=self._audio_adapter,\n                audio_params=self._audio_params,\n                audio_path=self._audio_path,\n                chunk_duration=self._chunk_duration,\n                instrument=instrument,\n                random_seed=self._random_seed,\n            )\n            for instrument in self._instruments\n        ]\n    yield from self._instrument_builders\n\n", "CodeBleu_score": 32.97649448364384}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if wait:\n        while not tf.io.gfile.exists(os.path.join(cache, \\\"index.json\\\")):\n            tf.get_logger().info(\\\"Waiting for cache to be created...\\\")\n            time.sleep(1)\n    return tf.data.experimental.save(dataset, cache)\n\n    def get_dataset(self, split: str, n_chunks_per_song: int, cache: Optional[str] = None) -> Any:\n        \\\"\\\"\\\"\n        Get dataset.\n\n        Parameters:\n            split (str):\n                Split to get the dataset for.\n            n_chunks_per_song (int):\n                Number of segment per song to compute.\n            cache (Optional[str], optional):\n                Path to cache the dataset.\n\n        Returns:\n            Any:\n                Dataset.\n        \\\"\\\"\\\"\n        if split not in self._splits:\n            raise ValueError(f\\\"split must be in {self._splits}\\\")\n\n        if n_chunks_per_song <= 0:\n            raise ValueError(\\\"n_chunks_per_song must be positif\\\")\n\n        if cache is not None and not tf.io.gfile.exists(cache):\n            tf.io.gfile.makedirs(cache)\n\n        dataset = self._get_dataset(split)\n        dataset = self.compute_segments(dataset, n_chunks_per_song)\n        dataset = dataset.map(self.expand_path)\n        dataset = dataset.filter(self.filter_error)\n        dataset = dataset.filter(self.filter_waveform)\n        dataset = dataset.map(self.harmonize_spectrogram)\n        dataset = dataset.filter(self.filter_short_segments)\n        dataset = dataset.map(self.random_time_crop)\n        dataset = dataset.map(self.random_time_stretch)\n        dataset = dataset.map(self.random_pitch_shift)\n        dataset =", "CodeBleu_score": 38.47395749041668}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if not os.path.exists(cache):\n        os.makedirs(cache)\n    if not os.path.exists(os.path.join(cache, CACHE_INDEX_FILENAME)):\n        if wait:\n            logger.info(\\\"Waiting for cache index file to be created...\\\")\n            while not os.path.exists(os.path.join(cache, CACHE_INDEX_FILENAME)):\n                time.sleep(1)\n\n    return dataset.cache(cache)\n\n", "CodeBleu_score": 46.12519089175733}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    cache_dir = os.path.join(cache, self._name)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    cache_path = os.path.join(cache_dir, self._name)\n    if not os.path.exists(cache_path):\n        dataset.cache(cache_path).prefetch(tf.data.AUTOTUNE)\n        if wait:\n            while not os.path.exists(cache_path + \".index\"):\n                print(\\\"Waiting for cache to be created...\\\")\n                time.sleep(10)\n    return tf.data.Dataset.load(cache_path)\n\n", "CodeBleu_score": 48.98929599903875}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model_path(params['model'])\n    params['model_dir'] = model_dir\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.8\n    run_config = tf.estimator.RunConfig(session_config=config)\n    estimator = tf.estimator.Estimator(model_fn, params=params, model_dir=model_dir, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 34.77862282752361}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model(params['model'])\n    params['model_dir'] = model_dir\n\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = params['gpu_memory_fraction']\n    run_config = tf.estimator.RunConfig(session_config=config)\n\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 38.60801355176254}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params['model_dir'])\n    params.update(model_provider.params)\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.visible_device_list = params.get('visible_devices', '0')\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn, model_dir=model_provider.model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 37.9022072307996}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure stereo.\n    waveform = to_stereo(waveform, audio_descriptor)\n    # Get prediction generator.\n    prediction_generator = self._get_prediction_generator()\n    # Update data generator.\n    self._data_generator.update_data({\"waveform\": waveform, \\\"audio_id\\\": audio_descriptor.audio_id})\n    # Get prediction.\n    prediction = next(prediction_generator)\n    # Remove audio_id key.\n    prediction.pop(\\\"audio_id\\\")\n    return prediction\n\n", "CodeBleu_score": 43.54597191441775}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    with self._tf_graph.as_default():\n        # Ensure waveform is stereo.\n        waveform = to_stereo(waveform, self._sample_rate)\n        # Get prediction generator.\n        prediction_generator = self._get_prediction_generator()\n        # Update data generator.\n        self._data_generator.update_data({\"waveform\": waveform, \\\"audio_id\\\": audio_descriptor.filename})\n        # Get prediction.\n        prediction = next(prediction_generator)\n        # Remove audio_id key.\n        prediction.pop(\\\"audio_id\\\")\n        return prediction\n\n", "CodeBleu_score": 57.01134921216672}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    with self._tf_graph.as_default():\n        waveform = to_stereo(waveform)\n        self._data_generator.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor.id})\n        prediction = next(self._get_prediction_generator())\n        del prediction[\\\"audio_id\\\"]\n        return prediction\n\n", "CodeBleu_score": 50.71637300272366}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    if config.BACKBONE == \\\"resnet50\\\":\n        return {\n            \\\"res2_out\\\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \\\"res3_out\\\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \\\"res4_out\\\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \\\"res5_out\\\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    elif config.BACKBONE == \\\"resnet101\\\":\n        return {\n            \\\"res2_out\\\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \\\"res3_out\\\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \\\"res4_out\\\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \\\"res5_out\\\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    else:\n        raise NotImplementedError(\\\"Unknown backbone: \\\" + config.BACKBONE)\n\n", "CodeBleu_score": 36.92031191420192}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # This is the default case for ResNet50, ResNet101, and ResNet101_FPN\n    if config.BACKBONE == \\\"resnet50\\\":\n        return utils.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES, config.BACKBONE_SHAPES)\n    elif config.BACKBONE == \\\"resnet101\\\":\n        return utils.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES, config.BACKBONE_SHAPES)\n    elif config.BACKBONE == \\\"resnet101_fpn\\\":\n        return utils.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES, config.BACKBONE_SHAPES)\n    else:\n        raise ValueError(\\\"Backbone \\\"{}\\\" not recognized.\\\".format(config.BACKBONE))\n\n", "CodeBleu_score": 33.46617372394295}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    elif config.BACKBONE == \\\"resnet50\\\":\n        return {\n            \\\"R2\\\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \\\"R3\\\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \\\"R4\\\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \\\"R5\\\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    elif config.BACKBONE == \\\"resnet101\\\":\n        return {\n            \\\"R2\\\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \\\"R3\\\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \\\"R4\\\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \\\"R5\\\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    else:\n        raise NotImplementedError(\\\"Backbone \\\" + config.BACKBONE + \\\" is not implemented!\\\")\n\n", "CodeBleu_score": 35.54558212420706}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split via stacking into a list of tensors\n    x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)\n    window = tf.split(window, 4, axis=0)\n    # Stack to a pair of tensors\n    x0, y0 = x0[:, 0], y0[:, 0]\n    x1, y1 = x1[:, 0], y1[:, 0]\n    w0, h0 = window[0]\n    w1, h1 = window[1]\n    # Clip\n    x0 = tf.maximum(tf.minimum(x0, w1), w0)\n    y0 = tf.maximum(tf.minimum(y0, h1), h0)\n    x1 = tf.maximum(tf.minimum(x1, w1), w0)\n    y1 = tf.maximum(tf.minimum(y1, h1), h0)\n    clipped = tf.concat([y0, x0, y1, x1], axis=1, name=\\\"clipped_boxes\\\")\n    clipped.set_shape([None, 4])\n    return clipped\n\n", "CodeBleu_score": 24.951644236709594}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split via comma to avoid TensorFlow 1.15 deprecation\n    window, boxes = tf.split(window, num_or_size_splits=2, axis=0), tf.split(boxes, num_or_size_splits=2, axis=1)\n    window[0] = tf.maximum(window[0], 0.0)\n    window[1] = tf.minimum(window[1], 1.0)\n    boxes[0] = tf.maximum(boxes[0], window[0])\n    boxes[1] = tf.minimum(boxes[1], window[1])\n    boxes = tf.concat(boxes, axis=1, name=\"clip_boxes_out\")\n    boxes.set_shape((2, 4))\n    return boxes\n\n", "CodeBleu_score": 17.606625189722344}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope(\\\"clip_boxes\\\"):\n        # Convert to 2D tensor of shape (N, 4)\n        boxes = tf.reshape(boxes, (-1, 4))\n        window = tf.reshape(window, [1, 4])\n        boxes = tf.maximum(boxes, window[0, :2], name=\\\"roi_1\\\")\n        boxes = tf.minimum(boxes, window[0, 2:], name=\\\"roi_2\\\")\n        # Convert back to a 2D tensor of shape (N, 4)\n        boxes = tf.reshape(boxes, tf.shape(boxes))\n        return boxes\n\n", "CodeBleu_score": 23.707275669848922}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    # Note that the feature map size is reduced by a factor of 2\n    # because the RPN is applied to a coarser C5 feature map, and all of the RPN\n    # predictions are scaled down to the same resolution as the feature map.\n    # The RPN itself operates on the small region around the proposed region (called an\n    # \\\"anchor\\\") rather than the entire image, so this is a valid loss normalization\n    # factor.\n    shared = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding=\\\"same\\\", activation=\\\"relu\\\", strides=anchor_stride),\n                                name=\\\"rpn_conv_shared\\\")(feature_map)\n\n    # Anchor Score\n    x = KL.Conv2D(anchors_per_location * 2, (1, 1), padding=\\\"valid\\\", activation=\\\"linear\\\", name=\\\"rpn_class_raw\\\")(shared)\n    rpn_class_logits = KL.Reshape(name=\\\"rpn_class_logits\\\")(x)\n\n    # Bounding box refinement\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\\\"valid\\\", activation=\\\"linear\\\", name=\\\"rpn_bbox_pred\\\")(shared)\n    rpn_probs = KL.Activation(\\\"softmax\\\", name=\\\"rpn_class\\\")(rpn_class_logits)\n    rpn_bbox = KL.Reshape((-1, 4), name=\\\"rpn_bbox\\\")(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 26.985598433967727}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Anchor stride\n    # The stride length of an anchor determines the spatial scale\n    # of the features in a given layer. Anchors at smaller scales\n    # are applied to the small objects in an image.\n    # For example, kitti anchors are applied to 32 x 32 feature\n    # maps. Then anchors are applied to 16 x 16 feature maps\n    # and so on.\n    # It scales the anchor dimensions with the feature map size.\n    # anchor_stride = config.RPN_ANCHOR_STRIDE\n\n    # Reduce depth of feature map (because P2 has one channel for each anchor)\n    # (a.k.a. concatenation of anchors on each level)\n    # Shape: [batch, height, width, anchors_per_location * depth]\n    shared = layers.Conv2D(512, (3, 3), padding=\\\"SAME\\\", activation=\\\"relu\\\", strides=(anchor_stride, anchor_stride), name=\\\"rpn_conv_shared\\\")(feature_map)\n\n    # Classification\n    x = layers.Conv2D(2 * anchors_per_location, (1, 1), padding=\\\"VALID\\\", activation=None, name=\\\"rpn_class_raw\\\")(shared)\n    # Shape: [batch, height, width, anchors_per_location * 2]\n\n    # Reshape to [batch, height, width, num_anchors, 2]\n    rpn_class_logits = layers.Reshape((-1, 2), name=\\\"rpn_class_logits\\\")\n    rpn_class_logits = rpn_class_logits(x)\n\n    # Bounding box refinement\n    x = layers.Conv2D(4 * anchors_per_location, (1, 1), padding=\\\"VALID\\\", activation=None, name=\\\"rpn_bbox_pred\\\")(shared)\n    # Shape: [batch, height, width, anchors_per_location * 4]", "CodeBleu_score": 28.904137638281423}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN and classifier heads\n    # Layer names typically follow the \\\"stage{2,3,4}\\\" convention\n    shared = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding=\\\"same\\\", activation=\\\"relu\\\", strides=anchor_stride),\n                                name=\\\"rpn_shared\\\")\n    # Apply convolutions to get the class and bounding box predictions\n    x = shared(feature_map)\n    x_class = KL.Conv2D(2 * anchors_per_location, (1, 1), activation=\\\"linear\\\", name=\\\"rpn_class\\\")\n    rpn_class_logits = x_class(x)\n    rpn_probs = KL.Activation(\\\"softmax\\\", name=\\\"rpn_class_xxx\\\", dtype=\\\"float32\\\")(rpn_class_logits)\n    x_bbox = KL.Conv2D(4 * anchors_per_location, (1, 1), activation=\\\"linear\\\", name=\\\"rpn_bbox\\\")\n    rpn_bbox = layers.Reshape((-1, 4), name=\\\"rpn_bbox_xxx\\\")(x_bbox(x))\n    # rpn_class = layers.Reshape((-1, 2), name=\\\"rpn_class_xxx\\\")(rpn_class_logits)\n    # rpn_class_probs = layers.Activation(\\\"softmax\\\", name=\\\"rpn_class_xxx\\\", dtype=\\\"float32\\\")(rpn_class)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 23.986656805637494}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    absolute_difference = K.abs(y_true - y_pred)\n    square_difference = K.square(y_true - y_pred)\n    small_difference = 1.0 / 9.0 * (square_difference)\n    big_difference = absolute_difference - 0.5\n    loss = K.switch(K.less(absolute_difference, 1.0), lambda: small_difference, lambda: big_difference)\n    return K.mean(loss, axis=-1)\n\n", "CodeBleu_score": 38.95047691322151}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    abs_loss = K.abs(y_true - y_pred)\n    sq_loss = 0.5 * (y_true - y_pred) ** 2\n    loss = K.switch(K.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return K.sum(loss)\n\n", "CodeBleu_score": 41.08345481442018}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tturns the calculated loss.\"\"\"\n    absolute_difference = K.abs(y_true - y_pred)\n    squared_difference = K.square(y_true - y_pred)\n    sigma = 3.0\n    l1_loss = tf.where(K.less(absolute_difference, 1.0 / sigma), 0.5 * squared_difference * sigma ** 2, absolute_difference - 0.5 / sigma)\n    return K.sum(l1_loss)\n\n", "CodeBleu_score": 36.21074669021266}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    model = KM.Sequential()\n    model.add(KL.InputLayer(input_shape=x_train.shape[1:]))\n    model.add(KL.Conv2D(32, (3, 3), activation='relu'))\n    model.add(KL.MaxPooling2D((2, 2)))\n    model.add(KL.Conv2D(64, (3, 3), activation='relu'))\n    model.add(KL.MaxPooling2D((2, 2)))\n    model.add(KL.Flatten())\n    model.add(KL.Dense(64, activation='relu'))\n    model.add(KL.Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 25.626959381424676}, {"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    model = KM.Sequential()\n    model.add(KL.InputLayer(input_shape=x_train.shape[1:]))\n    model.add(KL.Conv2D(32, (3, 3), activation='relu'))\n    model.add(KL.Conv2D(64, (3, 3), activation='relu'))\n    model.add(KL.MaxPooling2D(pool_size=(2, 2)))\n    model.add(KL.Flatten())\n    model.add(KL.Dense(128, activation='relu'))\n    model.add(KL.Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 26.617059883641474}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = KM.Sequential()\n    model.add(KL.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(KL.Conv2D(64, (3, 3), activation='relu'))\n    model.add(KL.MaxPooling2D(pool_size=(2, 2)))\n    model.add(KL.Flatten())\n    model.add(KL.Dense(64, activation='relu'))\n    model.add(KL.Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 24.50150310622799}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    # Count non-zero pixels per mask layer\n    non_zeros = np.count_nonzero(mask, axis=(1, 2)) > 0\n    # Find bounding box for each mask layer\n    boxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    for i, m in enumerate(mask):\n        # Find horizontal and vertical indices where mask is present\n        horiz = np.any(m, axis=0)\n        vert = np.any(m, axis=1)\n\n        # Find bounding box\n        y1, y2 = np.where(vert)[0][[0, -1]]\n        x1, x2 = np.where(horiz)[0][[0, -1]]\n\n        # Store bounding box\n        boxes[i] = [x1, y1, x2, y2]\n\n    return boxes\n\n\n\n\n", "CodeBleu_score": 31.41220804067893}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1 = np.min(horizontal_indicies)\n            x2 = np.max(horizontal_indicies)\n            y1 = np.min(vertical_indicies)\n            y2 = np.max(vertical_indicies)\n            # cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n            boxes[i] = [x1, y1, x2, y2]\n        else:\n            # print(\\\"empty mask\\\", i)\n            boxes[i] = [0, 0, 0, 0]\n    return boxes\n\n\n\n\n", "CodeBleu_score": 53.872291291368214}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n\n        # Bounding box of 0 size means background.\n        # Cast to bool to prepare mask for numpy/scipy operations that use boolean indexing.\n        m = m.astype(bool)\n        if m.any():\n            # Bounding box of 0 size means background.\n            # Cast to bool to prepare mask for numpy/scipy operations that use boolean indexing.\n            m = m.astype(bool)\n            # Get indices of pixels where mask is 1\n            y, x = m.nonzero()\n            # Convert coordinates to be relative to the mask\n            # and to be between 0 and 1.\n            # Calculate bounding box from mask coordinates\n            x0 = x.min()\n            x1 = x.max()\n            y0 = y.min()\n            y1 = y.max()\n            # Store bounding box in array.\n            boxes[i] = np.array([x0, y0, x1, y1])\n        else:\n            # Set to zeros if no mask for this instance.\n            boxes[i] = np.zeros(4, dtype=np.int32)\n    return boxes\n\n", "CodeBleu_score": 37.33646533929249}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute IoU\n    iou = intersection / union\n    return iou\n\n\n\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\n\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\n\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either box is empty, return empty result", "CodeBleu_score": 0.6291343111878056}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If not a matrix, reshape to matrix.\n    if boxes1.ndim == 1:\n        boxes1 = np.expand_dims(boxes1, axis=0)\n    if boxes2.ndim == 1:\n        boxes2 = np.expand_dims(boxes2, axis=0)\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        overlaps[:, i] = compute_iou(boxes1, boxes2[i, :])\n    return overlaps\n\n", "CodeBleu_score": 41.04559475196935}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either set of boxes is empty, return empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # Compute intersections\n    ixmin = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    iymin = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    ixmax = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    iymax = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    iw = np.maximum(ixmax - ixmin, 0.0)\n    ih = np.maximum(iymax - iymin, 0.0)\n    intersections = iw * ih\n\n    # Compute unions\n    union = np.array((boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1]) +\n                      (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1]) -\n                      intersections)\n\n    overlaps = intersections / union\n    return overlaps\n\n", "CodeBleu_score": 49.22852972175494}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    assert info[\\\"source\\\"], \\\"Source not found for {}.\\\".format(image_id)\n    path = info[\\\"path\\\"]\n    img = skimage.io.imread(path)\n    if img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n\n", "CodeBleu_score": 44.07461644540474}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    assert info[\\\"source\\\"], \\\"Source not found for {}.\\\".format(image_id)\n    path = info[\\\"path\\\"].replace(\\\"\\\\\\\", \\\"/\\\")\n    # Load the image\n    image = skimage.io.imread(path)\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 75.57339041009172}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    assert info[\\\"source\\\"], \\\"Source not found for {}.\\\".format(image_id)\n    path = info[\\\"path\\\"]\n    img = skimage.io.imread(path)\n    # Convert PIL/Pillow's RGBA2RGB\n    if img.shape[-1] == 4:\n        img = img[:, :, :3]\n    return img\n\n", "CodeBleu_score": 46.53668083859127}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis(\\'off\\')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm,\n                   interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 70.54266743786172}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\"] * len(images)\n    rows = len(images) // cols + 1\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.title(title, fontsize=9)\n        plt.axis(\\\"off\\\")\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm,\n                   interpolation=interpolation)\n    plt.show()\n\n\n", "CodeBleu_score": 51.42668700841205}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\"]*len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis(\\\"off\\\")\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm,\n                   interpolation=interpolation)\n        i += 1\n    plt.show()\n\n\n\n", "CodeBleu_score": 69.91323241754046}]}
