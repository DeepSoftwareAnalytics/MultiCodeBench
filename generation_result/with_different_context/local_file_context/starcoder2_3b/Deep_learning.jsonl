{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load the model from the specified path\n    if self.cache_model:\n        # Check if a cached model is available and valid\n        if cached_ldsr_model is not None and cached_ldsr_model.config.model_path == self.model_path:\n            # Use the cached model if it is valid\n            print(\"Using cached LDSR model\")\n            return cached_ldsr_model\n        else:\n            # Clear the cached model if it is invalid\n            cached_ldsr_model = None\n    model = self.load_model_from_path(self.model_path, half_attention=half_attention)\n    if self.cache_model:\n        # Cache the loaded model\n        cached_ldsr_model = model\n    return model\n", "CodeBleu_score": 16.901055157294095}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if cached_ldsr_model is not None and cached_ldsr_model.device == self.device:\n        return cached_ldsr_model\n    else:\n        if self.config.model.path.endswith(\".safetensors\"):\n            model = torch.load(self.config.model.path, map_location=self.device)\n        else:\n            model = torch.load(self.config.model.path, map_location=self.device)\n        if half_attention:\n            model = model.half()\n        if self.config.model.memory_format == \"channels_last\":\n            model = model.to(memory_format=\"channels_last\")\n        model.eval()\n        if cached_ldsr_model is not None:\n            cached_ldsr_model.cpu()\n            del cached_ldsr_model\n            gc.collect()\n        cached_ldsr_model = model\n        return model\n\n", "CodeBleu_score": 34.02984832169794}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    global cached_ldsr_model\n    if cached_ldsr_model is not None:\n        if cached_ldsr_model.device == self.device and cached_ldsr_model.half_attention == half_attention:\n            return cached_ldsr_model\n    if self.config.model.cache_model:\n        cached_ldsr_model = self.load_model_from_cache()\n        if cached_ldsr_model is not None:\n            return cached_ldsr_model\n    if self.config.model.model_path.endswith('.safetensors'):\n        model = safetensors.torch.load_file(self.config.model.model_path)\n    else:\n        model = torch.load(self.config.model.model_path, map_location=self.device)\n    if self.config.model.half_attention:\n        model = shared.half_attention(model)\n    model = model.to(self.device)\n    model.eval()\n    if self.config.model.cache_model:\n        cached_ldsr_model = model\n    return model\n\n", "CodeBleu_score": 36.5139372488765}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    sampler.set_state({\"step\": 0, \"eta\": eta})\n    samples = sampler.sample(batch_size, shape, cond, callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, x_t)\n    return samples\n\n", "CodeBleu_score": 21.006766929109695}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    sampler = DDIMSampler(model, cond, shape, eta=eta, callback=callback,\n                          normals_sequence=normals_sequence, mask=mask, x0=x0,\n                          quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_T=x_t)\n    return sampler.sample(steps, batch_size=batch_size)\n", "CodeBleu_score": 23.2405691785279}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    ddim_sampler = DDIMSampler(model, shape, cond, eta, callback, normals_sequence, mask, x0,\n                               quantize_x0, temperature, score_corrector, corrector_kwargs, x_t)\n    samples = ddim_sampler.sample(steps)\n    return samples\n", "CodeBleu_score": 15.735004486473128}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        with self.model_ema.ema_scope():\n            yield\n        if context is not None:\n            print(f\\\"{context}: EMA weights restored.\\\")\n    else:\n        yield\n        if context is not None:\n            print(f\\\"{context}: EMA weights disabled.\\\")\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=[]):\n        ckpt = torch.load(ckpt_path, map_location=torch.device(\\\"cpu\\\"))\n        ckpt = {k: v for k, v in ckpt.items() if k not in ignore_keys}\n        self.load_state_dict(ckpt)\n        print(f\\\"{self.__class__.__name__}: Loaded from checkpoint {ckpt_path}.\\\")\n\n    def forward(self, x, y, t, **kwargs):\n        x = self.encoder(x, t, **kwargs)\n        x = self.quant_conv(x)\n        x = self.quantize(x)\n        z = self.quantize.embed(x)\n        x_hat = self.post_quant_conv(self.quantize.decode(z))\n        loss = self.loss(x_hat, x)\n        return loss, z\n\n    def training_step(self, batch, batch_idx):\n        x, y, t, t_out = batch\n        loss, z = self.forward(x, y, t)\n        if self.monitor is not None:\n            self.monitor.log(\\\"train/loss\\\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        if self.scheduler_config is not None:\n            self.scheduler_config[\\\"optimizer\\\"] = self.optimizers()[0]\n            self.scheduler_config[\\\"lr_scheduler\\\"] = self.lr_schedulers()[0]\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y, t, t_out = batch\n        loss, z = self.forward(x, y, t)\n        if self.monitor is not", "CodeBleu_score": 38.13871054708305}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = \\\"\\\".join([\\\"\\", "CodeBleu_score": 8.36381743550275}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            print(f\\\"{self.__class__.__name__}: Entering EMA scope.\\\")\n        self.model_ema.ema_model.eval()\n        with torch.no_grad():\n            with self.model_ema.ema_model.ema.no_grad():\n                yield\n        if context is not None:\n            print(f\\\"{self.__class__.__name__}: Exiting EMA scope.\\\")\n        self.model_ema.ema_model.train()\n        self.model_ema.update_model()\n    else:\n        yield\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=None):\n        print(f\\\"{self.__class__.__name__}: Initializing from checkpoint {ckpt_path}.\\\")\n        ckpt = torch.load(ckpt_path, map_location=torch.device(\\\"cpu\\\"))\n        ckpt = {k: v for k, v in ckpt.items() if k not in ignore_keys}\n        self.load_state_dict(ckpt, strict=False)\n\n    def forward(self, x, *args, **kwargs):\n        x = self.encoder(x)\n        z = self.quantize(x)\n        z = self.quant_conv(z)\n        x_hat = self.decoder(z)\n        x_hat = self.post_quant_conv(x_hat)\n        loss = self.loss(x, x_hat)\n        return loss, z\n\n    def training_step(self, batch, batch_idx):\n        loss, z = self.forward(batch[self.image_key])\n        self.log(\\\"train_loss\\\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, z = self.forward(batch[self.image_key])\n        self.log(\\\"val_loss\\\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss, z = self.forward(batch[self.image_key])\n       ", "CodeBleu_score": 37.45415709967782}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    print(f\\\"{self.__class__.__name__}: Loading from {path}\\\")\n    ckpt = torch.load(path, map_location=\\\"cpu\\\")\n    state_dict = ckpt[\\\"state_dict\\\"]\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n            else:\n                print(f\\\"{self.__class__.__name__}: Key {key} not found in checkpoint\\\")\n    self.load_state_dict(state_dict, strict=False)\n    print(f\\\"{self.__class__.__name__}: Loaded from {path}\\\")\n    return\n\n\n\n", "CodeBleu_score": 31.452583989299974}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    print(f\\\"{self.__class__.__name__}: Loading weights from {path}.\\\")\n    ckpt = torch.load(path, map_location=\\\"cpu\\\")\n    if \\\"state_dict\\\" in ckpt:\n        state_dict = ckpt[\\\"state_dict\\\"]\n    elif \\\"model\\\" in ckpt:\n        state_dict = ckpt[\\\"model\\\"]\n    else:\n        state_dict = ckpt\n    self.load_state_dict(state_dict, strict=False)\n    if ignore_keys is not None:\n        for key in list(self.state_dict().keys()):\n            if any(key.startswith(ignore_key) for ignore_key in ignore_keys):\n                del self.state_dict()[key]\n    missing, unexpected = self.state_dict()._get_missing_keys(), self.state_dict()._get_unexpected_keys()\n    if len(missing) > 0:\n        print(f\\\"{self.__class__.__name__}: Missing keys: {missing}\\\")\n    if len(unexpected) > 0:\n        print(f\\\"{self.__class__.__name__}: Unexpected keys: {unexpected}\\\")\n    self.eval()\n    self.requires_grad_(False)\n\n\n", "CodeBleu_score": 32.77808972735035}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    ckpt = torch.load(path, map_location=self.device)\n    if \"state_dict\" in ckpt:\n        ckpt = ckpt[\"state_dict\"]\n    self.load_state_dict(ckpt, strict=False)\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in self.state_dict():\n                del self.state_dict()[key]\n            else:\n                print(f\\\"{self.__class__.__name__}: Unexpected key {key} in checkpoint\\\")\n    print(f\\\"{self.__class__.__name__}: Loaded checkpoint from {path}\\\")\n", "CodeBleu_score": 24.681258886403423}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # encode\n    quant_a, emb_loss, info = self.encode(input)\n    # decode\n    dec_a = self.decode(quant_a)\n    # calculate loss\n    loss_a = self.loss(dec_a, input)\n    # return results\n    return dec_a, loss_a, emb_loss, info, quant_a, info, quant_a, info\n\n", "CodeBleu_score": 23.46638242672372}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # encode\n    z, emb_loss, info = self.encode(input)\n    # decode\n    x_hat = self.decode(z)\n    # compute diffusion loss\n    diffusion_loss = self.loss(x_hat, input)\n    # compute total loss\n    loss = diffusion_loss + emb_loss\n    # return prediction indices if requested\n    if return_pred_indices:\n        pred_indices = self.quantize.embed_code_to_indices(z)\n        return loss, x_hat, pred_indices\n    # otherwise, just return the loss\n    return loss, x_hat\n\n", "CodeBleu_score": 31.427988141063796}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.batch_resize_range is not None:\n        batch_size = input.size(0)\n        input = input.view(-1, *input.shape[2:])\n        input = F.interpolate(input, size=(batch_size, *self.batch_resize_range), mode=\\\"bilinear\\\")\n        input = input.view(batch_size, *input.shape[1:])\n    quant_a, emb_loss_a, info_a = self.encode(input)\n    if return_pred_indices:\n        pred_indices = info_a[\\\"pred_indices\\\"]\n    else:\n        pred_indices = None\n    dec = self.decode(quant_a)\n    loss_a = self.loss(dec, input, info_a)\n    return dec, loss_a, pred_indices, info_a, emb_loss_a\n\n    def forward(self, input, return_pred_indices=False):\n        \"\"\"This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.", "CodeBleu_score": 26.79113378500907}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    if self.batch_resize_range is not None:\n        batch = self.batch_resize(batch, self.batch_resize_range)\n    input = batch[k]\n    input = input.permute(0, 3, 1, 2)\n    input = input.contiguous().float()\n    return input\n\n\n", "CodeBleu_score": 20.87979853400063}, {"code": "def get_input(self, batch, k)\n\t\n    if self.batch_resize_range is not None:\n        batch_size = batch[self.image_key].size(0)\n        resize_factor = np.random.uniform(self.batch_resize_range[0], self.batch_resize_range[1])\n        new_size = [int(x * resize_factor) for x in batch[self.image_key].size()[2:]]\n        batch[self.image_key] = F.interpolate(batch[self.image_key], new_size, mode=\\\"bicubic\\\")\n    batch[self.image_key] = batch[self.image_key].permute(0, 3, 1, 2)\n    batch[self.image_key] = batch[self.image_key].contiguous().float()\n    batch[self.image_key] = batch[self.image_key].to(self.device)\n    return batch[self.image_key]\n\n", "CodeBleu_score": 39.84770332545617}, {"code": "def get_input(self, batch, k)\n\t\n    # get the input data from the batch\n    input = batch[self.image_key]\n    # check if the input data is a single image or a batch of images\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    # check if the input data is a batch of images\n    if len(input.shape) == 4:\n        # check if the input data is a single image\n        if input.shape[0] == 1:\n            input = input.squeeze(0)\n        # check if the input data is a batch of images with a single image in each batch\n        if input.shape[0] == batch[self.image_key].shape[0]:\n            input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.squeeze(1)\n    # check if the input data is a batch of images with a single image in each batch\n    if len(input.shape) == 4 and input.shape[0] == batch[self.image_key].shape[0]:\n        input = input.", "CodeBleu_score": 18.08322901314622}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    x = self.get_input(batch, self.image_key)\n    if optimizer_idx == 0:\n        pred, diff = self.forward(x)\n        loss = self.loss(x, pred, diff)\n        self.log(\\\"loss\\\", loss)\n        self.log(\\\"loss_diffusion\\\", diff)\n        return loss\n    elif optimizer_idx == 1:\n        pred, diff = self.forward(x)\n        loss = self.loss(x, pred, diff)\n        self.log(\\\"loss\\\", loss)\n        self.log(\\\"loss_diffusion\\\", diff)\n        return loss\n\n    return None\n\n\n\n", "CodeBleu_score": 23.790296789722852}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    x = self.get_input(batch, self.image_key)\n\n    if optimizer_idx == 0:\n\n        # autoencoder training\n\n        pred, diff = self(x)\n\n        loss = self.loss(pred, x, diff)\n\n        self.log(\\\"loss\\\", loss, on_epoch=True, on_step=True, prog_bar=True)\n\n        return loss\n\n    elif optimizer_idx == 1:\n\n        # discriminator training\n\n        pred, diff = self(x, return_pred_indices=True)\n\n        loss = self.loss(pred, x, diff)\n\n        self.log(\\\"loss\\\", loss, on_epoch=True, on_step=True, prog_bar=True)\n\n        return loss\n\n    else:\n\n        raise ValueError(f\\\"Invalid optimizer index {optimizer_idx}\\\")\n\n", "CodeBleu_score": 32.81240764585277}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    # get the input data\n    x = self.get_input(batch, self.image_key)\n\n    if optimizer_idx == 0:\n\n        # autoencoder\n\n        pred, diff = self(x)\n\n        loss = self.loss(pred, x, diff)\n\n        self.log(\\\"loss\\\", loss)\n\n        return loss\n\n    elif optimizer_idx == 1:\n\n        # discriminator\n\n        pred, diff = self(x, return_pred_indices=True)\n\n        loss = self.loss(pred, x, diff)\n\n        self.log(\\\"loss\\\", loss)\n\n        return loss\n\n    else:\n\n        raise ValueError(\\\"Invalid optimizer index\\\")\n\n", "CodeBleu_score": 27.729956078562612}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss, ind = self(x, return_pred_indices=True)\n    if only_inputs:\n        xrec = x\n    x = x.detach().cpu()\n    xrec = xrec.detach().cpu()\n    x = self.post_process(x)\n    xrec = self.post_process(xrec)\n    x = self.post_process_img(x)\n    xrec = self.post_process_img(xrec)\n    x = self.post_process_img(x)\n    xrec = self.post_process_img(xrec)\n    if self.normalize_input:\n        x = self.normalize_input(x)\n        xrec = self.normalize_input(xrec)\n    if self.normalize_output:\n        x = self.normalize_output(x)\n        xrec = self.normalize_output(xrec)\n    if self.normalize_input_img:\n        x = self.normalize_input_img(x)\n        xrec = self.normalize_input_img(xrec)\n    if self.normalize_output_img:\n        x = self.normalize_output_img(x)\n        xrec = self.normalize_output_img(xrec)\n    if self.normalize_input_img and self.normalize_output_img:\n        x = self.normalize_input_img(x)\n        xrec = self.normalize_output_img(xrec)\n    if self.normalize_input and self.normalize_output:\n        x = self.normalize_input(x)\n        xrec = self.normalize_output(xrec)\n    if self.normalize_input and self.normalize_output_img:\n        x = self.normalize_input(x)\n        xrec = self.normalize_output_img(xrec)\n    if self.normalize_output and self.normalize_output_img:\n        x = self.normalize_output(x)\n        xrec = self.normalize_output_img(xrec)\n    if self.normalize_input and self.normalize_output and self.normalize_input_", "CodeBleu_score": 29.57175893521503}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss, ind = self(x, return_pred_indices=True)\n    if not only_inputs:\n        xrec_ema, _, _ = self(x, return_pred_indices=True, ema=True)\n        xrec_ema = xrec_ema.detach()\n        xrec = xrec.detach()\n        x = x.detach()\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = upper_size\n        else:\n            new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n        if new_resize!= x.shape[2]:\n            x = F.interpolate(x, size=new_resize, mode=\\\"bicubic\\\")\n    if self.color_jitter:\n        x = self.color_jitter(x)\n    x = x.permute(0, 2, 3, 1).cpu().numpy()\n    xrec = xrec.permute(0, 2, 3, 1).cpu().numpy()\n    if plot_ema:\n        xrec_ema = xrec_ema.permute(0, 2, 3, 1).cpu().numpy()\n    if self.color_jitter:\n        x = self.color_jitter(x)\n        xrec = self.color_jitter(xrec)\n        if plot_ema:\n            xrec_ema = self.color_jitter(xrec_ema)\n    if plot_ema:\n        self.logger.experiment.add_images(\\\"reconstruction/", "CodeBleu_score": 30.111009555500633}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss, ind = self(x, return_pred_indices=True)\n    if only_inputs:\n        self.logger.experiment.add_images(\\\"inputs\\\", x, global_step=self.global_step)\n        return\n\n    self.logger.experiment.add_images(\\\"inputs\\\", x, global_step=self.global_step)\n    self.logger.experiment.add_images(\\\"reconstructions\\\", xrec, global_step=self.global_step)\n    if plot_ema:\n        with self.ema_scope():\n            xrec_ema, _, _ = self(x, return_pred_indices=True)\n            self.logger.experiment.add_images(\\\"reconstructions_ema\\\", xrec_ema, global_step=self.global_step)\n\n    if self.loss.discriminator:\n        self.logger.experiment.add_images(\\\"quantization_code\\\", qloss, global_step=self.global_step)\n        if plot_ema:\n            with self.ema_scope():\n                self.logger.experiment.add_images(\\\"quantization_code_ema\\\", self.loss.discriminator(qloss), global_step=self.global_step)\n\n    if self.loss.quantize.embed_code is not None:\n        code_b = self.loss.quantize.embed_code(ind)\n        self.logger.experiment.add_images(\\\"code_b\\\", code_b, global_step=self.global_step)\n        if plot_ema:\n            with self.ema_scope():\n                code_b_ema = self.loss.quantize.embed_code(ind)\n                self.logger.experiment.add_images(\\\"code_b_ema\\\", code_b_ema, global_step=self.global_step)\n\n    if self.loss.quantize.embed_code is not None:\n        code_b = self.loss.quantize.embed_code(ind)\n        self.logger.experiment.add_images(\\\"code_b\\\", code_b, global_step=self.global_step)\n        if plot_", "CodeBleu_score": 29.928838662410918}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.sum(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return torch.sum((pred - target) ** 2)\n    else:\n        raise ValueError('Unknown loss type.')\n\n", "CodeBleu_score": 35.336741966892916}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.sum(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return torch.sum((pred - target) ** 2)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n", "CodeBleu_score": 34.62090326222844}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.sum(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return torch.sum((pred - target) ** 2)\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n", "CodeBleu_score": 34.37738417388201}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        if self.time_embedding is not None:\n            if self.time_embedding_type == \\\"positional\\\":\n                time_embedding = self.time_embedding(self.time_step)\n            elif self.time_embedding_type == \\\"learned\\\":\n                time_embedding = self.time_embedding(c)\n            else:\n                raise ValueError(\\\"time_embedding_type must be \\\"positional\\\" or \\\"learned\\\". Received: {}\\\".format(self.time_embedding_type))\n            x = torch.cat([x, time_embedding], dim=1)\n        if self.conditioning_type == \\\"learned\\\":\n            x = torch.cat([x, c], dim=1)\n        return self.forward_step(x, *args, **kwargs)\n    else:\n        if self.time_embedding is not None:\n            if self.time_embedding_type == \\\"positional\\\":\n                time_embedding = self.time_embedding(self.time_step)\n            elif self.time_embedding_type == \\\"learned\\\":\n                time_embedding = self.time_embedding(c)\n            else:\n                raise ValueError(\\\"time_embedding_type must be \\\"positional\\\" or \\\"learned\\\". Received: {}\\\".format(self.time_embedding_type))\n            x = torch.cat([x, time_embedding], dim=1)\n        if self.conditioning_type == \\\"learned\\\":\n            x = torch.cat([x, c], dim=1)\n        return self.forward_step(x, *args, **kwargs)\n\n\n\n", "CodeBleu_score": 28.58987251052839}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model\n    # TODO: allow conditioning to be applied to all layers of the model", "CodeBleu_score": 2.188995215311005}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        # randomly select time steps\n        if self.time_embedding is not None:\n            if self.time_embedding.requires_grad:\n                time_steps = torch.randint(0, self.time_embedding.shape[0], (x.shape[0],), device=x.device)\n            else:\n                time_steps = torch.zeros((x.shape[0],), dtype=torch.long, device=x.device)\n        else:\n            time_steps = torch.zeros((x.shape[0],), dtype=torch.long, device=x.device)\n\n        # apply learned conditioning\n        if self.conditioning_key is not None:\n            c = self.get_conditioning(batch, self.conditioning_key)\n            if self.conditioning_key in self.conditioning_key_to_embed:\n                c = self.conditioning_embedder(c)\n            c = self.conditioning_encoder(c)\n        else:\n            c = None\n\n        # process inputs according to conditioning requirements\n        if self.conditioning_key is not None:\n            x = self.conditioning_processor(x, c, time_steps)\n        else:\n            x = self.conditioning_processor(x)\n\n        # compute loss\n        loss = self.shared_step(x, c, *args, **kwargs)\n        return loss\n    else:\n        # randomly select time steps\n        if self.time_embedding is not None:\n            if self.time_embedding.requires_grad:\n                time_steps = torch.randint(0, self.time_embedding.shape[0], (x.shape[0],), device=x.device)\n            else:\n                time_steps = torch.zeros((x.shape[0],), dtype=torch.long, device=x.device)\n        else:\n            time_steps = torch.zeros((x.shape[0],), dtype=torch.long, device=x.device)\n\n        # apply learned conditioning\n        if self.conditioning_key is not None:\n            c = self.get_conditioning(batch,", "CodeBleu_score": 39.60312600524686}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    indices = indices.reshape(shape)\n    entry = self.embedding(indices).view(shape)\n    return entry\n\n", "CodeBleu_score": 33.36943218668452}, {"code": "def get_codebook_entry(self, indices, shape)\n\t    assert isinstance(indices, torch.Tensor)\n    assert len(indices.shape) == 2\n    assert indices.shape[1] == 1\n    indices = indices.squeeze(1)\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    if self.sane_index_shape:\n        indices = indices.reshape(indices.shape[0], indices.shape[2], indices.shape[3])\n    return self.embedding(indices).reshape(shape)\n\n", "CodeBleu_score": 44.51931126267902}, {"code": "def get_codebook_entry(self, indices, shape)\n\t    indices = indices.reshape(-1, 1)\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    codebook = self.embedding.weight[indices].reshape(shape)\n    return codebook\n", "CodeBleu_score": 35.20201881033438}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        m = factor\n        n = dimension // factor\n    else:\n        m = 1\n        n = dimension\n        while m < n:\n            if dimension % m == 0:\n                break\n            m += 1\n    return m, n\n\n", "CodeBleu_score": 28.11290295392813}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor < 0:\n        m = 1\n        n = dimension\n        while n > factor:\n            n -= 1\n            m += 1\n    else:\n        m = dimension // factor\n        n = dimension // m\n    return m, n\n", "CodeBleu_score": 24.669467365557562}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        m = dimension\n        n = 1\n        while m > 1:\n            if m % -factor == 0:\n                m //= -factor\n                n *= -factor\n            else:\n                m -= 1\n        return n, m\n    raise ValueError(f\"Invalid factor {factor} for dimension {dimension}.\")\n", "CodeBleu_score": 35.261364078348585}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if updown == \\\"up\\\":\n        if orig_weight.device!= self.device or orig_weight.dtype!= self.dtype:\n            orig_weight = orig_weight.to(self.device, self.dtype)\n        if self.scale is not None:\n            combined_weight = self.scale * orig_weight\n        else:\n            combined_weight = orig_weight\n        if self.alpha is not None:\n            combined_weight = self.alpha * combined_weight\n        combined_weight = F.normalize(combined_weight, dim=self.dora_norm_dims)\n        adjusted_weight = self.weight - combined_weight\n        return adjusted_weight\n    elif updown == \\\"down\\\":\n        if orig_weight.device!= self.device or orig_weight.dtype!= self.dtype:\n            orig_weight = orig_weight.to(self.device, self.dtype)\n        if self.dora_scale is not None:\n            combined_weight = self.dora_scale * orig_weight\n        else:\n            combined_weight = orig_weight\n        if self.alpha is not None:\n            combined_weight = self.alpha * combined_weight\n        combined_weight = F.normalize(combined_weight, dim=self.dora_norm_dims)\n        adjusted_weight = self.weight + combined_weight\n        return adjusted_weight\n    else:\n        raise Exception(\\\"Invalid updown value\\\")\n\n", "CodeBleu_score": 22.214895478784044}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if self.dora_scale is None:\n        return orig_weight\n\n    if updown == 'up':\n        weight = orig_weight.to(self.dora_scale.device).to(self.dora_scale.dtype)\n    else:\n        weight = orig_weight.to(self.dora_scale.device).to(self.dora_scale.dtype)\n\n    if self.dora_norm_dims == 1:\n        weight = weight.unsqueeze(-1)\n\n    weight = weight * self.dora_scale\n    weight = weight / weight.norm(dim=self.dora_norm_dims, keepdim=True)\n\n    if updown == 'up':\n        return weight - orig_weight\n    else:\n        return orig_weight - weight\n", "CodeBleu_score": 30.56144957321811}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t  if self.dora_scale is None:\n        return orig_weight\n    else:\n        orig_weight = orig_weight.to(updown.device)\n        orig_weight = orig_weight.type_as(updown)\n        orig_weight = orig_weight.to(updown.dtype)\n        combined_weight = updown * self.dora_scale + orig_weight\n        combined_weight = F.normalize(combined_weight, dim=self.dora_norm_dims)\n        adjusted_weight = orig_weight - combined_weight\n        return adjusted_weight\n\n", "CodeBleu_score": 29.835045207311676}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if self.bias is not None and ex_bias is None:\n        updown = updown + self.bias\n    elif ex_bias is not None:\n        updown = updown + ex_bias\n    if self.dim is not None and self.alpha is not None:\n        updown = updown * self.alpha / self.dim\n\n    if self.shape is None:\n        updown = updown.reshape(output_shape)\n    elif self.shape!= output_shape:\n        updown = updown.reshape(self.shape)\n    return updown, self.calc_scale()\n\n", "CodeBleu_score": 44.84265454860833}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Reshape updown\n    if len(updown.shape) == len(output_shape):\n        updown = updown.reshape(output_shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    # Apply weight decomposition\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    # Return the final updown tensor\n    return updown, ex_bias\n\n", "CodeBleu_score": 42.25941137472853}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n    # Apply bias\n    if ex_bias is not None:\n        updown += ex_bias\n    # Reshape\n    if updown.shape == output_shape:\n        return updown\n    elif updown.shape[0] == output_shape[0]:\n        return updown.reshape(output_shape)\n    else:\n        raise ValueError(f\\\"updown shape {updown.shape} does not match output shape {output_shape}\\\")\n\n", "CodeBleu_score": 25.985306983470384}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    orig_weight = orig_weight.to(self.weight.device)\n    return self._calc_updown_weights(orig_weight, self.weight)\n\n", "CodeBleu_score": 7.932631707650831}, {"code": "def calc_updown(self, orig_weight)\n\t\n    with torch.no_grad():\n        # transfer the current weight tensor to the device of the original weight tensor\n        self.weight = self.weight.to(orig_weight.device)\n        self.ex_bias = self.ex_bias.to(orig_weight.device) if self.ex_bias is not None else None\n        return self.finalize_weights(orig_weight)\n\n", "CodeBleu_score": 46.838128838577944}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.device!= self.weight.device:\n        self.weight = self.weight.to(self.device)\n\n        if self.ex_bias is not None:\n\n            self.ex_bias = self.ex_bias.to(self.device)\n\n    return self.finalize_weights(orig_weight)\n\n\n", "CodeBleu_score": 44.496009466010875}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if shared.sd_model.network_count > shared.network_limit:\n        shared.sd_model.network_count = 0\n        for name, network in shared.sd_model.networks.items():\n            shared.sd_model.network_count += 1\n            if shared.sd_model.network_count > shared.network_limit:\n                shared.sd_model.networks.pop(name)\n                break\n\n    gc.collect()\n\n\n\n", "CodeBleu_score": 29.21206693342241}, {"code": "def purge_networks_from_memory()\n\t\n    if shared.sd_model is None:\n        return\n\n    if len(shared.sd_model.networks) <= shared.max_networks:\n\n        return\n\n    logging.info(\\\"Purging networks from memory\\\")\n\n    for name in list(shared.sd_model.networks.keys()):\n\n        if name in shared.sd_model.networks_to_purge:\n\n            del shared.sd_model.networks[name]\n\n    shared.sd_model.networks_to_purge = set()\n\n    gc.collect()\n\n\n", "CodeBleu_score": 30.689082663241578}, {"code": "def purge_networks_from_memory()\n\t\n    if shared.sd_model.networks_in_memory_count > shared.sd_model.networks_in_memory_limit:\n        # purge networks from memory\n        for key in list(shared.sd_model.networks_in_memory.keys()):\n            if key not in shared.sd_model.networks_to_keep:\n                shared.sd_model.networks_in_memory_count -= 1\n                del shared.sd_model.networks_in_memory[key]\n                shared.sd_model.networks_in_memory_size -= shared.sd_model.networks_in_memory[key].size()\n\n        # trigger garbage collection\n        gc.collect()\n\n", "CodeBleu_score": 31.080856371114017}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if not hasattr(self, 'backup_weights') or self.backup_weights is None:\n        return\n\n    for i, weight in enumerate(self.backup_weights):\n        self.weight.data[i] = weight\n\n    if hasattr(self, 'backup_biases') and self.backup_biases is not None:\n        for i, bias in enumerate(self.backup_biases):\n            self.bias.data[i] = bias\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight.data = self.backup_in_proj_weight\n        self.in_proj_bias.data = self.backup_in_proj_bias\n        self.out_proj.weight.data = self.backup_out_proj_weight\n        self.out_proj.bias.data = self.backup_out_proj_bias\n\n\n\n\n", "CodeBleu_score": 34.99265818645981}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    backup_filename = self.get_backup_filename()\n\n    if backup_filename is None:\n        return\n\n    backup_file = open(backup_filename, \\\"rb\\\")\n    backup_data = pickle.load(backup_file)\n    backup_file.close()\n\n    if self.weight_backup_key in backup_data:\n        self.weight = backup_data[self.weight_backup_key]\n\n    if self.bias_backup_key in backup_data:\n        self.bias = backup_data[self.bias_backup_key]\n\n    if self.proj_weight_backup_key in backup_data:\n        self.proj_weight = backup_data[self.proj_weight_backup_key]\n\n    if self.proj_bias_backup_key in backup_data:\n        self.proj_bias = backup_data[self.proj_bias_backup_key]\n\n    if self.q_proj_weight_backup_key in backup_data:\n        self.q_proj_weight = backup_data[self.q_proj_weight_backup_key]\n\n    if self.q_proj_bias_backup_key in backup_data:\n        self.q_proj_bias = backup_data[self.q_proj_bias_backup_key]\n\n    if self.k_proj_weight_backup_key in backup_data:\n        self.k_proj_weight = backup_data[self.k_proj_weight_backup_key]\n\n    if self.k_proj_bias_backup_key in backup_data:\n        self.k_proj_bias = backup_data[self.k_proj_bias_backup_key]\n\n    if self.v_proj_weight_backup_key in backup_data:\n        self.v_proj_weight = backup_data[self.v_proj_weight_backup_key]\n\n    if self.v_proj_bias_backup_key in backup_data:\n        self.v_proj_bias = backup_data[self.v_proj_bias_backup_key]\n\n    if self.out_proj_weight", "CodeBleu_score": 28.61812500319086}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \\\"_backup_weights\\\") and hasattr(self, \\\"_backup_biases\\\"):\n        if hasattr(self, \\\"in_proj_weight\\\") and hasattr(self, \\\"in_proj_bias\\\"):\n            self.in_proj_weight.copy_(self._backup_weights)\n            self.in_proj_bias.copy_(self._backup_biases)\n        elif hasattr(self, \\\"weight\\\") and hasattr(self, \\\"bias\\\"):\n            self.weight.copy_(self._backup_weights)\n            self.bias.copy_(self._backup_biases)\n        elif hasattr(self, \\\"proj_weight\\\") and hasattr(self, \\\"proj_bias\\\"):\n            self.proj_weight.copy_(self._backup_weights)\n            self.proj_bias.copy_(self._backup_biases)\n        elif hasattr(self, \\\"k_proj\\\") and hasattr(self, \\\"v_proj\\\") and hasattr(self, \\\"out_proj\\\") and hasattr(self, \\\"out_proj_bias\\\"):\n            self.k_proj.copy_(self._backup_weights)\n            self.v_proj.copy_(self._backup_weights)\n            self.out_proj.copy_(self._backup_weights)\n            self.out_proj_bias.copy_(self._backup_biases)\n        elif hasattr(self, \\\"attn\\\") and hasattr(self, \\\"attn_bias\\\"):\n            self.attn.copy_(self._backup_weights)\n            self.attn_bias.copy_(self._backup_biases)\n\n\n\n\n", "CodeBleu_score": 27.93985030839124}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not loaded_networks:\n        return original_forward(input)\n    input = input.to(devices.cpu, copy=True)\n    network_restore_weights_from_backup(org_module)\n    org_module.network_current_names = ()\n    for net in loaded_networks:\n        module = net.modules.get(org_module.network_layer_name, None)\n        if module is not None:\n            try:\n                with torch.no_grad():\n                    if getattr(org_module, 'fp16_weight', None) is None:\n                        weight = org_module.weight\n                        bias = org_module.bias\n                    else:\n                        weight = org_module.fp16_weight.clone().to(org_module.weight.device)\n                        bias = getattr(org_module, 'fp16_bias', None)\n                        if bias is not None:\n                            bias = bias.clone().to(org_module.bias.device)\n                    updown, ex_bias = module.calc_updown(weight)\n                    if len(weight.shape) == 4 and weight.shape[1] == 9:\n                        # inpainting model. zero pad updown to make channel[1]  4 to 9\n                        updown = torch.nn.functional.pad(updown, (0, 0, 0, 0, 0, 5))\n                    org_module.weight.copy_((weight.to(dtype=updown.dtype) + updown).to(dtype=org_module.weight.dtype))\n                    if ex_bias is not None and hasattr(org_module, 'bias'):\n                        if org_module.bias is None:\n                            org_module.bias = torch.nn.Parameter(ex_bias).to(org_module.weight.dtype)\n                        else:\n                            org_module.bias.copy_((bias + ex_bias).to(dtype=org_module.bias.dtype))\n            except RuntimeError as e:\n                logging.debug(f\"Network {net.name} layer {org_module.network_layer_name}: {e}\")\n                extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n                continue\n            org_module.network_current_names += (net", "CodeBleu_score": 35.63478902006363}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n\n    if not loaded_networks:\n        return original_forward(input)\n\n    if isinstance(org_module, torch.nn.Linear):\n        input = input.to(org_module.weight.device)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        output = original_forward(input)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        return output\n\n    if isinstance(org_module, torch.nn.Conv2d):\n        input = input.to(org_module.weight.device)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        output = original_forward(input)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        return output\n\n    if isinstance(org_module, torch.nn.GroupNorm):\n        input = input.to(org_module.weight.device)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        output = original_forward(input)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_backup)\n        return output\n\n    if isinstance(org_module, torch.nn.LayerNorm):\n        input = input.to(org_module.weight.device)\n        org_module.weight.copy_(org_module.network_weights_backup)\n        if org_module.bias is not None:\n            org_module.bias.copy_(org_module.network_bias_", "CodeBleu_score": 28.206678630685044}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if loaded_networks:\n        if isinstance(org_module, torch.nn.MultiheadAttention):\n            # MultiheadAttention has a different way of handling weights\n            weights_backup = (org_module.in_proj_weight.to(devices.cpu, copy=True), org_module.out_proj.weight.to(devices.cpu, copy=True))\n        else:\n            weights_backup = org_module.weight.to(devices.cpu, copy=True)\n        org_module.network_weights_backup = weights_backup\n        org_module.network_bias_backup = getattr(org_module, 'bias', None)\n        org_module.network_current_names = ()\n\n        if org_module.network_layer_name is None:\n            return original_forward(input)\n\n        for net in loaded_networks:\n            module = net.modules.get(org_module.network_layer_name, None)\n            if module is not None and hasattr(org_module, 'weight'):\n                try:\n                    with torch.no_grad():\n                        if getattr(org_module, 'fp16_weight', None) is None:\n                            weight = org_module.weight\n                            bias = org_module.bias\n                        else:\n                            weight = org_module.fp16_weight.clone().to(org_module.weight.device)\n                            bias = getattr(org_module, 'fp16_bias', None)\n                            if bias is not None:\n                                bias = bias.clone().to(org_module.bias.device)\n                        updown, ex_bias = module.calc_updown(weight)\n\n                        if len(weight.shape) == 4 and weight.shape[1] == 9:\n                            # inpainting model. zero pad updown to make channel[1]  4 to 9\n                            updown = torch.nn.functional.pad(updown, (0, 0, 0, 0, 0, 5))\n\n                        org_module.weight.copy_((weight.to(dtype=updown.dtype) + updown).to(dtype=org_module.weight.dtype))\n                        if ex_bias is not None and hasattr(org_module, 'bias'):\n                            if org_module.bias is None:\n                                org_module.bias = torch.nn.Parameter(ex_bias).", "CodeBleu_score": 35.09956965590604}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t  return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.32156763555954}, {"code": "def trainables(self)\n\t  return [p for p in self.parameters() if p.requires_grad]\n", "CodeBleu_score": 6.32156763555954}, {"code": "def trainables(self)\n\t  return [param for layer in self.linear for param in layer.parameters()]\n", "CodeBleu_score": 9.655828003224789}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    if len(names)!= len(multipliers):\n        raise ValueError(\\\"Multipliers must have same length as names\\\")\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.hypernetworks:\n            shared.hypernetworks[name].set_multiplier(multiplier)\n        else:\n            hypernetwork = load_hypernetwork(name)\n            if hypernetwork is not None:\n                hypernetwork.set_multiplier(multiplier)\n                shared.hypernetworks[name] = hypernetwork\n    return shared.hypernetworks.values()\n\n", "CodeBleu_score": 36.30999418174504}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if shared.hypernetworks is None:\n        shared.hypernetworks = {}\n\n    if names is None:\n        return\n\n    if multipliers is None:\n        multipliers = [1] * len(names)\n\n    if len(names)!= len(multipliers):\n        raise ValueError(f\\\"Number of hypernetworks ({len(names)}) does not match number of multipliers ({len(multipliers)})\\\")\n\n    shared.hypernetworks.clear()\n    for i, name in enumerate(names):\n        if name not in shared.hypernetworks:\n            shared.hypernetworks[name] = load_hypernetwork(name)\n\n        if shared.hypernetworks[name] is not None:\n            shared.hypernetworks[name].set_multiplier(multipliers[i])\n            shared.hypernetworks[name].name = name\n            shared.hypernetworks[name].step = shared.step\n            shared.hypernetworks[name].filename = shared.hypernetworks[name].shorthash()\n\n    for name in names:\n        if name not in shared.hypernetworks:\n            shared.hypernetworks[name] = None\n\n", "CodeBleu_score": 34.78466874454949}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    shared.hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            shared.hypernetworks[name] = hypernetwork\n    return shared.hypernetworks\n", "CodeBleu_score": 22.265693450979963}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n\n\n\n\n", "CodeBleu_score": 70.4402416707376}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context.clone()\n    context_v = context.clone()\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 66.63562884756048}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 70.4402416707376}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # The query, key, and value projections are calculated from the input and context.\n    query = self.query_projection(x)\n    key = self.key_projection(context)\n    value = self.value_projection(context)\n    # The context is passed through a hypernetwork to produce a new context.\n    context, _ = apply_hypernetworks(self.hypernetworks, context)\n    # The context is reshaped for multi-head attention.\n    context = rearrange(context, 'b c (h d) -> b h (c d)', h=self.num_heads)\n    # The scaled dot-product attention is calculated.\n    attn_output, attn_weights = scaled_dot_product_attention(query, key, value, mask=mask, dropout=self.dropout)\n    # The attention output is reshaped back.\n    attn_output = rearrange(attn_output, 'b h (c d) -> b c (h d)', h=self.num_heads)\n    # The attention output is passed through a final linear layer to produce the output.\n    return self.output_projection(attn_output)\n\n", "CodeBleu_score": 16.449422908359207}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # pylint: disable=too-many-locals,too-many-statements,too-many-branches,too-many-return-statements\n    # pylint: disable=too-many-arguments,too-many-instance-attributes\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments,too-many-locals,too-many-statements\n    # pylint: disable=too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-branches,too-many-return-statements,too-many-arguments,too-many-locals,too-many-statements,too-many-", "CodeBleu_score": 1.9834764933446987}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if self.is_hypernetwork:\n        x, context = apply_hypernetworks(self.hypernetworks, x, context)\n\n    query = self.query_projection(x)\n    key = self.key_projection(context)\n    value = self.value_projection(context)\n\n    query = self.reshape_for_attention(query)\n    key = self.reshape_for_attention(key)\n    value = self.reshape_for_attention(value)\n\n    attention_scores = torch.einsum('bij,bkj->bik', query, key)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    if mask is not None:\n        attention_scores = self.apply_mask(attention_scores, mask)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores = self.attention_scores_projection(attention_scores)\n\n    attention_scores = self.attention_scores_projection(attention_scores)\n    attention_scores =", "CodeBleu_score": 25.305857868515396}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    hypernetwork.save(filename)\n    hypernetwork.checkpoint = None\n    hypernetwork.hypernetwork_name = None\n\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.checkpoint = None\n        hypernetwork.hypernetwork_name = None\n        raise\n    hypernetwork.checkpoint = None\n    hypernetwork.hypernetwork_name = None\n\n    return hypernetwork\n\n\n\n    def save(self, filename):\n        \"\"\"This function saves the state of a hypernetwork to a file with a specified filename.", "CodeBleu_score": 26.61384918422324}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_name = hypernetwork.name\n    original_hypernetwork_optimizer_name = hypernetwork.optimizer_name\n    original_hypernetwork_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    original_hypernetwork_step = hypernetwork.step\n    original_hypernetwork_optimizer_step = hypernetwork.optimizer_step\n    original_hypernetwork_optimizer_epoch = hypernetwork.optimizer_epoch\n    original_hypernetwork_optimizer_epoch_step = hypernetwork.optimizer_epoch_step\n    original_hypernetwork_optimizer_epoch_step_offset = hypernetwork.optimizer_epoch_step_offset\n    original_hypernetwork_optimizer_epoch_offset = hypernetwork.optimizer_epoch_offset\n    original_hypernetwork_optimizer_epoch_offset_step = hypernetwork.optimizer_epoch_offset_step\n    original_hypernetwork_optimizer_epoch_offset_step_offset = hypernetwork.optimizer_epoch_offset_step_offset\n    original_hypernetwork_optimizer_epoch_offset_step_offset = hypernetwork.optimizer_epoch_offset_step_offset\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step = hypernetwork.optimizer_epoch_offset_step_offset_step\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step_offset = hypernetwork.optimizer_epoch_offset_step_offset_step_offset\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step_offset = hypernetwork.optimizer_epoch_offset_step_offset_step_offset\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step_offset_step = hypernetwork.optimizer_epoch_offset_step_offset_step_offset_step\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step_offset_step = hypernetwork.optimizer_epoch_offset_step_offset_step_offset_step\n    original_hypernetwork_optimizer_epoch_offset_step_offset_step_offset_step_offset = hypernetwork.optimizer_epoch_offset_step_offset_step_offset_step_offset\n    original_hypernetwork_optimizer_epoch_offset_", "CodeBleu_score": 11.577286151881166}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_hypernetwork_name = hypernetwork.name\n\n    old_hypernetwork_optimizer_name = hypernetwork.optimizer_name\n\n    old_hypernetwork_optimizer_state_dict = hypernetwork.optimizer_state_dict\n\n    old_hypernetwork_step = hypernetwork.step\n\n    old_hypernetwork_optimizer_step = hypernetwork.optimizer_step\n\n    old_hypernetwork_optimizer_epoch = hypernetwork.optimizer_epoch\n\n    old_hypernetwork_optimizer_epoch_step = hypernetwork.optimizer_epoch_step\n\n    old_hypernetwork_optimizer_epoch_steps_per_epoch = hypernetwork.optimizer_epoch_steps_per_epoch\n\n    old_hypernetwork_optimizer_epoch_steps_done = hypernetwork.optimizer_epoch_steps_done\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save = hypernetwork.optimizer_epoch_steps_done_since_last_save\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save_since_last_save = hypernetwork.optimizer_epoch_steps_done_since_last_save_since_last_save\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save = hypernetwork.optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save_since_last_save = hypernetwork.optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save_since_last_save\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save_since_last_save_since_last_save = hypernetwork.optimizer_epoch_steps_done_since_last_save_since_last_save_since_last_save_since_last_save_since_last_save\n\n    old_hypernetwork_optimizer_epoch_steps_done_since_last_save_", "CodeBleu_score": 10.12505907061218}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self._backup_ema()\n        yield\n        self._restore_ema()\n    else:\n        yield\n\n    def _backup_ema(self):\n        \"\"\"Backup the model parameters and copy EMA weights to the model.\"\"\"\n        self._ema_backup = {}\n        for name, param in self.named_parameters():\n            self._ema_backup[name] = param.data.clone()\n            param.data.copy_(self.model_ema.get_buffer(name))\n        if self.monitor is not None:\n            self.monitor.ema_backup(self.model_ema)\n\n    def _restore_ema(self):\n        \"\"\"Restore the model parameters from the backup and copy EMA weights from the model to the model.\"\"\"\n        for name, param in self.named_parameters():\n            param.data.copy_(self._ema_backup[name])\n        if self.monitor is not None:\n            self.monitor.ema_restore(self.model_ema)\n\n    def init_from_ckpt(self, ckpt_path, ignore_keys=None, only_model=False):\n        \"\"\"Initialize the model from a checkpoint.\n\n        Args:\n            ckpt_path (str): path to the checkpoint.\n            ignore_keys (list[str]): keys to ignore when initializing the model.\n            only_model (bool): if True, only initialize the model weights and ignore the rest of the checkpoint.\n        \"\"\"\n        ckpt = torch.load(ckpt_path, map_location=\\\"cuda:0\\\")\n        if only_model:\n            ckpt = ckpt[\\\"model\\\"]\n        if ignore_keys is not None:\n            ckpt = {k: v for k, v in ckpt.items() if k not in ignore_keys}\n        self.load_state_dict(ckpt, strict=False)\n\n    def forward(self, x, t, return_dict=False, decode=True, return_latents=False, return_alphas=False, return_losses=False, return_samples=False, return_latents_and_alphas=False, return_latents_and_samples=False, return_latents_and_losses=", "CodeBleu_score": 28.61723318440213}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = dict()\n        if 'ema_weight_backup' not in context:\n            context['ema_weight_backup'] = dict()\n        if 'ema_weight_backup' not in context:\n            context['original_weight'] = dict()\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                context['ema_weight_backup'][name] = param.data.clone()\n                context['original_weight'][name] = param.data.clone()\n                param.data.copy_(self.model_ema.get_param(name))\n        yield context\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data.copy_(context['original_weight'][name])\n        if self.monitor is not None:\n            self.monitor.ema_weight_backup = context['ema_weight_backup']\n    else:\n        yield\n\n    @torch.no_grad()\n    def step(self, x, t, noise=None, noise_schedule=None, clip_denoised=True, log_every_t=10, return_dict=False, **kwargs):\n        \"\"\"\n        This function performs a single diffusion step, given the current noise\n        vector and the current timestep t.\n        \"\"\"\n        assert not (noise is not None and noise_schedule is not None), 'noise and noise_schedule cannot be used at the same time'\n        assert noise is None or noise.shape == x.shape, 'noise and x must have the same shape'\n\n        if noise is None:\n            noise = torch.randn_like(x)\n\n        if self.use_scheduler:\n            noise, _ = self.scheduler.step(noise, t, self.model, **kwargs)\n\n        if noise_schedule is not None:\n            noise = noise_schedule(t, noise)\n\n        if self.use_ema:\n            with self.ema_scope():\n                self.model.set_timesteps(t)\n                x_t = self.model(x,", "CodeBleu_score": 30.917189421838476}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = [self.model, self.model_ema]\n        else:\n            context = [self.model, self.model_ema] + context\n        with torch.no_grad():\n            for obj in context:\n                obj.save_for_update(self.ema_decay)\n        yield\n        with torch.no_grad():\n            for obj in context:\n                obj.restore_from_update()\n    else:\n        yield\n    \n\n    def forward(self, x, t, noise_level=1., return_dict=False, return_losses=False, return_all_losses=False):\n        if self.use_ema:\n            # make sure that the model is in eval mode\n            self.model.eval()\n            self.model_ema.eval()\n        else:\n            self.model.train()\n        \n        if self.use_scheduler:\n            # make sure that the model is in eval mode\n            self.scheduler.set_timesteps(t)\n            self.scheduler.set_beta_schedule(self.betas)\n        \n        if self.use_ema:\n            with ema_scope(self):\n                loss, losses = self.model(x, t, noise_level=noise_level, return_dict=True, return_losses=True,\n                                             return_all_losses=True)\n        else:\n            loss, losses = self.model(x, t, noise_level=noise_level, return_dict=True, return_losses=True,\n                                             return_all_losses=True)\n        \n        if return_losses:\n            return loss, losses\n        elif return_all_losses:\n            return loss, losses, self.scheduler.alphas, self.scheduler.alphas_cumprod\n        else:\n            return loss\n    \n    def sample(self, x_t, t, num_samples=1, noise_level=1., return_dict=False, return_all_losses=False):\n        if self.use_ema:\n            # make sure that the model is in eval mode\n", "CodeBleu_score": 29.87708183134646}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n\n    if self.parameterization == \\\"eps\\\":\n        eps = model_output\n        mean = eps\n        posterior_variance = 1.0\n        posterior_log_variance_clipped = 0.0\n    elif self.parameterization == \\\"x0\\\":\n        x_start = self.predict_start_from_noise(x, t, model_output)\n        if clip_denoised:\n            x_start = torch.clamp(x_start, -1.0, 1.0)\n        mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior(x_start, x, t)\n    else:\n        raise NotImplementedError(\\\"mu not supported\\\")\n    return mean, posterior_variance, posterior_log_variance_clipped\n\n\n", "CodeBleu_score": 43.35603742971751}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output, _ = self.model(x, t, return_dict=True, return_noise=True)\n\n    if self.parameterization == \\\"eps\\\":\n        mean = model_output.mean\n        log_variance = model_output.log_variance\n        variance = torch.exp(log_variance)\n    elif self.parameterization == \\\"x0\\\":\n        mean = model_output.mean\n        variance = model_output.variance\n        log_variance = torch.log(variance)\n    else:\n        raise NotImplementedError(\\\"mu not supported\\\")\n    if clip_denoised:\n        mean = torch.clamp(mean, -1.0, 1.0)\n    posterior_mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior(x, mean, t)\n    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n\n\n", "CodeBleu_score": 34.25704339141025}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    if clip_denoised:\n        x = torch.clip(x, -1.0, 1.0)\n    if self.parameterization == \\\"eps\\\":\n        mean = self.model(x)[0]\n    elif self.parameterization == \\\"x0\\\":\n        mean = self.model(x)[1]\n    else:\n        raise NotImplementedError(\\\"mu not supported\\\")\n    variance = 1.0 - self.alphas_cumprod[t]\n    log_variance = torch.log(variance)\n    return mean, variance, log_variance\n", "CodeBleu_score": 35.97614747531521}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.square(pred - target)\n        if not self.loss_reduction:\n            loss = torch.sum(loss)\n        return loss\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n", "CodeBleu_score": 45.65951369749622}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.square(pred - target)\n        if not self.reduction:\n            loss = torch.sum(loss)\n        return loss\n    else:\n        raise ValueError('Unknown loss type:'+ self.loss_type)\n\n", "CodeBleu_score": 45.44135683569684}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}.')\n    return loss\n", "CodeBleu_score": 51.82035670071178}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    # get batch size\n    batch_size = x_start.shape[0]\n    # create tensor with final timestep for each batch element\n    t = torch.zeros((batch_size, 1)).to(self.device)\n    # calculate mean and log variance of predicted distribution at final timestep\n    mean, log_var = self.model.get_mean_logvar(x_start, t)\n    # calculate KL divergence between predicted and standard normal distribution\n    kl_div = self.model.get_kl_div(mean, log_var)\n    # calculate mean KL divergence\n    mean_kl_div = kl_div.mean()\n    # normalize mean KL divergence by logarithm of 2\n    bpd = mean_kl_div / math.log(2)\n    return bpd\n", "CodeBleu_score": 21.405752535316328}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b, c, h, w = x_start.shape\n    t = torch.arange(1, h * w + 1).to(self.device)\n    t = t.view(1, 1, 1, h * w).expand(b, c, h, w)\n    x_t = self.prior_model(x_start, t)\n    x_t = x_t.view(b, c, h, w)\n    pred_xstart = x_t[:, :, 0, 0]\n    log_var = x_t[:, :, 1, 0]\n    mean = x_t[:, :, 2, 0]\n    kl_div = (log_var - 2.0 + (mean ** 2) + (pred_xstart ** 2)) / 2.0\n    return kl_div.mean().log2()\n\n", "CodeBleu_score": 31.03513011680994}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size).to(self.device) * self.max_timestep\n    x_t = self.apply_model(x_start, t, None, return_ids=False)\n    x_t = x_t.view(batch_size, -1)\n    pred_xstart = self._predict_eps_from_xstart(x_t, t, x_start)\n    mean, logvar = self.model.get_mean_logvar(x_t)\n    kl_div = self.kl_div(mean, logvar)\n    return kl_div.mean().item() / np.log(2)\n\n", "CodeBleu_score": 37.12439207175521}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4294967296, min_file_size=262144)\n    caches[subsection] = cache\n    return cache\n\n", "CodeBleu_score": 48.745231305922836}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    global cache_lock\n    global caches\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024**3, min_file_size=256 * 1024)\n        return caches[subsection]\n\n\n\n\n", "CodeBleu_score": 53.891083870506066}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    with cache_lock:\n        if subsection not in caches:\n            caches[subsection] = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4294967296, min_file_size=262144)\n    return caches[subsection]\n\n", "CodeBleu_score": 51.30308587312767}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches, cache_lock\n    with cache_lock:\n        cache_obj = caches.get(subsection)\n        if cache_obj is None:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 40.803411579694284}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                if not os.path.exists(cache_dir):\n                    os.makedirs(cache_dir)\n                if not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 65.26060068648904}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                if not os.path.exists(cache_dir):\n                    os.makedirs(cache_dir)\n                if not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 65.26060068648904}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    try:\n        cached_data = cache_obj[title]\n    except KeyError:\n        pass\n    else:\n        if cached_data[\"mtime\"] > os.path.getmtime(filename):\n            cache_obj.pop(title)\n    cache_obj[title] = {\"data\": func(), \"mtime\": os.path.getmtime(filename)}\n    dump_cache()\n    return cache_obj[title][\"data\"]\n", "CodeBleu_score": 28.450249567713325}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache_obj.get(title)\n    if cached_data is None or mtime > cached_data[\"mtime\"]:\n        data = func()\n        data[\"mtime\"] = mtime\n        cache_obj[title] = data\n        dump_cache()\n    return cache_obj[title][\"data\"]\n", "CodeBleu_score": 20.50421678063025}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    data = cache.get(title)\n    if data and data[1] >= mtime:\n        cache.pop(title)\n    if not data:\n        data = func()\n        cache[title] = (data, mtime)\n        dump_cache()\n    return data\n\n", "CodeBleu_score": 17.833765144497278}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line)\n    if exc_info:\n        print(get_exceptions())\n        print(\"---\")\n\n", "CodeBleu_score": 41.70236301597443}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    if exc_info:\n        print(\"***\")\n        for line in textwrap.indent(str(get_exceptions()), \"    \").splitlines():\n            print(line)\n        print(\"---\")\n    print(\"***\", message)\n\n", "CodeBleu_score": 38.6807106652398}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message)\n    if exc_info:\n        print(\"***\", \"---\")\n        for line in format_traceback(sys.exc_info()[2]):\n            print(\"***\", \"    \", line[0])\n            if line[1]:\n                print(\"***\", \"    \", textwrap.indent(line[1], \"        \"))\n\n", "CodeBleu_score": 40.47900234756455}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    positive_prompt, negative_prompt = prompt, negative_prompt\n    if style.prompt:\n        positive_prompt, negative_prompt = extract_style_text_from_prompt(style.prompt, prompt), extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    return positive_prompt, negative_prompt\n\n", "CodeBleu_score": 45.678309909363016}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted or not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 61.132798834908385}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted or not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 67.58733757990271}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 64.07063310277653}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    return [u.random_partial(n_frames) for u in self.utterance_cycler.sample(count)]\n", "CodeBleu_score": 49.87186720514033}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are True. Only one can be True.\")\n    if increase_only:\n        change_dBFS = target_dBFS - dBFS_to_linear(wav.max())\n    elif decrease_only:\n        change_dBFS = target_dBFS - dBFS_to_linear(wav.min())\n    else:\n        change_dBFS = target_dBFS - dBFS_to_linear(wav.mean())\n    return wav * np.power(10, change_dBFS / 20)\n\n", "CodeBleu_score": 41.69181670368322}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only cannot both be set to True.\")\n    if increase_only:\n        change_dBFS = target_dBFS - max_db\n    elif decrease_only:\n        change_dBFS = target_dBFS - min_db\n    else:\n        change_dBFS = target_dBFS - np.max(wav)\n    if change_dBFS > 0:\n        wav = wav * 10 ** (change_dBFS / 20)\n    elif change_dBFS < 0:\n        wav = wav * 10 ** (change_dBFS / 20)\n    return wav\n\n", "CodeBleu_score": 42.726156386417216}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t   if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only are both set to True.\")\n    if increase_only:\n        change_in_dBFS = target_dBFS - librosa.amplitude_to_db(np.max(np.abs(wav)))\n    elif decrease_only:\n        change_in_dBFS = librosa.amplitude_to_db(np.max(np.abs(wav))) - target_dBFS\n    else:\n        change_in_dBFS = target_dBFS - librosa.amplitude_to_db(np.max(np.abs(wav)))\n    return librosa.db_to_amplitude(change_in_dBFS + librosa.amplitude_to_db(wav))\n\n", "CodeBleu_score": 36.70347405444324}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    global _device\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _device = device\n    _model = SpeakerEncoder()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.to(device)\n    _model.eval()\n    print(f'Loaded model from {weights_fpath}')\n    print(f'Training step: {_model.training_step}')\n\n", "CodeBleu_score": 41.80268557288499}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    global _device\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _device = device\n    _model = SpeakerEncoder()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.to(device)\n    _model.eval()\n    print(f'Loaded model {weights_fpath} to {_device}')\n\n", "CodeBleu_score": 40.27315926293127}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    global _device\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _device = device\n    _model = SpeakerEncoder()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.to(device)\n    _model.eval()\n    print(f\"Loaded model {weights_fpath} on {device}\")\n\n", "CodeBleu_score": 40.178227266407596}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    global _model, _device\n    if not _model:\n        raise RuntimeError(\\\"Model not loaded\\\")\n    if not isinstance(frames_batch, np.ndarray):\n        raise TypeError(\\\"frames_batch should be a NumPy array\\\")\n    frames_batch = torch.from_numpy(frames_batch).float()\n    frames_batch = frames_batch.to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 43.53265288115588}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    global _model, _device\n    if _model is None:\n        raise Exception(\\\"No model loaded. Call load_model() first.\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.embed_utterance(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 65.73554160201923}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    global _model, _device\n    if not is_loaded():\n        raise RuntimeError(\\\"Model not loaded\\\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model.embed_utterance(frames_batch).cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 49.03444687378381}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert partial_utterance_n_frames > 0, \"Partial utterance n frames must be greater than 0\"\n    assert min_pad_coverage > 0 and min_pad_coverage < 1, \"Minimum pad coverage must be between 0 and 1\"\n    assert overlap > 0 and overlap < 1, \"Overlap must be between 0 and 1\"\n    n_frames = int(n_samples / audio_sampling_rate)\n    frame_step = int(n_frames * overlap)\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames - partial_utterance_n_frames + 1, frame_step):\n        wav_slices.append(i)\n        mel_slices.append(i)\n    if mel_slices[-1] + partial_utterance_n_frames > n_frames:\n        mel_slices.pop()\n    wav_slices = np.array(wav_slices)\n    mel_slices = np.array(mel_slices)\n    if wav_slices.shape[0] == 0:\n        return wav_slices, mel_slices\n    coverage = (wav_slices[-1] + partial_utterance_n_frames) / n_frames\n    if coverage < min_pad_coverage:\n        if wav_slices.shape[0] > 1:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.213577704696014}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert min_pad_coverage >= 0.0 and min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0.0 and 1.0\"\n    assert overlap >= 0.0 and overlap <= 1.0, \"overlap must be between 0.0 and 1.0\"\n    assert partial_utterance_n_frames >= 1, \"partial_utterance_n_frames must be at least 1\"\n    assert n_samples >= 1, \"n_samples must be at least 1\"\n    n_frames = int(n_samples / audio.SAMPLE_RATE)\n    frame_step = int(n_frames * (1 - overlap))\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames, frame_step):\n        wav_slice = audio.slice_wav(audio.wav, i, partial_utterance_n_frames)\n        mel_slice = audio.slice_mel(audio.mel, i, partial_utterance_n_frames)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    if len(wav_slices) > 1:\n        last_wav_slice = wav_slices[-1]\n        last_mel_slice = mel_slices[-1]\n        last_wav_slice_coverage = audio.compute_slice_coverage(last_wav_slice)\n        last_mel_slice_coverage = audio.compute_slice_coverage(last_mel_slice)\n        if last_wav_slice_coverage < min_pad_coverage and last_mel_slice_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.716269566890446}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 <= overlap < 1.0, \"overlap must be in [0, 1)\"\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be in [0, 1]\"\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be > 0\"\n    assert n_samples > 0, \"n_samples must be > 0\"\n    n_frames = int(n_samples / (1 - overlap))\n    frame_step = int(n_frames * (1 - overlap))\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames, frame_step):\n        wav_slices.append(i)\n        mel_slices.append(i)\n    if len(wav_slices) > 1:\n        last_wav_slice = wav_slices[-1]\n        last_mel_slice = mel_slices[-1]\n        if last_wav_slice + partial_utterance_n_frames > n_samples:\n            wav_slices.pop()\n            mel_slices.pop()\n        elif last_mel_slice + partial_utterance_n_frames > n_samples:\n            wav_slices.pop()\n            mel_slices.pop()\n    wav_slices = np.array(wav_slices)\n    mel_slices = np.array(mel_slices)\n    assert wav_slices[-1] + partial_utterance_n_frames <= n_samples, \"wav_slices[-1] + partial_utterance_n_frames > n_samples\"\n    assert mel_slices[-1] + partial_utterance_n_frames <= n_samples, \"mel_slices[-1] + partial_utterance_n_frames > n_samples\"\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 31.20100636856076}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    ax.matshow(embed.reshape(shape, shape), cmap=\"viridis\")\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(ticks=np.linspace(*color_range, 5), label=\"Embedding\")\n    plt.show()\n\n", "CodeBleu_score": 38.77455856258929}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    ax.imshow(embed, cmap=\"hot\")\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(ax.get_children()[0], ax=ax, shrink=0.5)\n    return ax\n\n", "CodeBleu_score": 46.751960255338574}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    ax.imshow(embed.reshape(shape, shape), cmap=\"hot\")\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(label=\"Similarity\", ax=ax, fraction=0.046, pad=0.04)\n    ax.set_clim(color_range[0], color_range[1])\n    return ax\n\n", "CodeBleu_score": 41.325235331967264}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Number of speakers\n    n_speakers = embeds.shape[0]\n    # Number of utterances per speaker\n    n_utterances = embeds.shape[1]\n    # Initialize the similarity matrix with zeros\n    similarity_matrix = torch.zeros(n_speakers, n_speakers, device=self.loss_device)\n    # Initialize the mask matrix\n    mask_matrix = torch.zeros(n_speakers, n_speakers, device=self.loss_device)\n    # Initialize the centroids\n    centroids_incl = torch.zeros(n_speakers, embeds.shape[2], device=self.loss_device)\n    centroids_excl = torch.zeros(n_speakers, embeds.shape[2], device=self.loss_device)\n    # Iterate over speakers\n    for i in range(n_speakers):\n        # Exclude the current speaker from the centroid calculation\n        excl_embeds = embeds[torch.arange(n_speakers)!= i]\n        # Exclude the current speaker from the similarity calculation\n        mask_matrix[i, i] = 1\n        # Calculate the centroids\n        centroids_incl[i] = torch.mean(embeds[i].unsqueeze(0), dim=0)\n        centroids_excl[i] = torch.mean(excl_embeds, dim=0)\n        # Calculate the similarity scores\n        similarity_incl = torch.sum(centroids_incl[i] * centroids_excl, dim=1)\n        similarity_excl = torch.sum(centroids_excl[i] * centroids_incl, dim=1)\n        # Update the similarity matrix\n        similarity_matrix[i, i:] = similarity_incl.unsqueeze(1)\n        similarity_matrix[i:, i] = similarity_excl.unsqueeze(1)\n    # Scale and bias the similarity matrix\n    similarity_matrix = self.similarity_weight * similarity_matrix + self.similarity_bias\n    return similarity_matrix\n", "CodeBleu_score": 30.021061525791353}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the embeddings\n    batch_size, n_utterances = embeds.shape[0], embeds.shape[1]\n\n    # Initialize the similarity matrix with zeros on the specified device\n    similarity_matrix = torch.zeros(batch_size, batch_size, device=self.loss_device)\n\n    # Initialize the mask matrix to exclude self-similarity\n    mask_matrix = torch.ones(batch_size, batch_size, device=self.loss_device) - torch.eye(batch_size, device=self.loss_device)\n\n    # Iterate over the speakers in the batch\n    for i in range(batch_size):\n        # Get the current speaker embeddings\n        speaker_embeds = embeds[i]\n\n        # Initialize the centroids_incl and centroids_excl tensors\n        centroids_incl = torch.zeros(n_utterances, model_embedding_size, device=self.loss_device)\n        centroids_excl = torch.zeros(n_utterances, model_embedding_size, device=self.loss_device)\n\n        # Iterate over the utterances in the speaker\n        for j in range(n_utterances):\n            # Get the current utterance embedding\n            utterance_embed = speaker_embeds[j]\n\n            # Update the centroids_incl tensor by including the current utterance embedding\n            centroids_incl[j] = centroids_incl[j] + utterance_embed\n\n            # Update the centroids_excl tensor by excluding the current utterance embedding\n            centroids_excl[j] = centroids_excl[j] + utterance_embed\n\n        # Compute the mean of the centroids_incl tensor\n        centroids_incl = centroids_incl / (n_utterances + 1e-5)\n\n        # Compute the mean of the centroids_excl tensor\n        centroids_excl = centroids_excl / n_utterances\n\n        # Compute the similarity scores using the centroids_incl and centroids_excl tensors\n        similarity_scores = torch.matmul(centroids_incl, centroids_excl.transpose(0, 1))\n\n        # Scale and bias the similarity scores\n        similarity_scores = self.similarity_weight * similarity_scores + self.similarity_bias\n\n        # Update the similarity matrix with the similarity", "CodeBleu_score": 28.36104240859074}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker\n    n_speakers, n_utterances = embeds.shape[0], embeds.shape[1]\n\n    # Compute the centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl = (embeds.sum(dim=1, keepdim=True) - embeds) / (n_utterances - 1)\n\n    # Initialize the similarity matrix and mask matrix\n    similarity_matrix = torch.zeros(n_speakers, n_speakers, device=self.loss_device)\n    mask_matrix = torch.zeros(n_speakers, n_speakers, device=self.loss_device)\n\n    # Compute the similarity scores\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            if i!= j:\n                similarity_matrix[i, j] = (self.similarity_weight * torch.norm(centroids_incl[i] - centroids_excl[j], dim=1) + self.similarity_bias).sum()\n                mask_matrix[i, j] = 1\n\n    # Scale and bias the similarity matrix\n    similarity_matrix = similarity_matrix / (similarity_matrix.max() + 1e-5)\n    similarity_matrix = similarity_matrix - 1\n\n    # Return the similarity matrix\n    return similarity_matrix, mask_matrix\n\n", "CodeBleu_score": 28.71882635104045}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n    ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n    #                           ).to(self.loss_device)\n    # eye = np.eye(speakers_per_batch, dtype=np.int)\n    # mask = np.where(1 - eye)\n    # sim_matrix2[mask] = (", "CodeBleu_score": 35.42006360780721}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n    # Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n    #                           ).to(self.loss_device)\n    # eye = np.eye(speakers_per_batch, dtype=np.int)\n    # mask = np.where(1 - eye)\n    # sim_matrix2[mask] = (", "CodeBleu_score": 35.42006360780721}, {"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker,\n                                    speakers_per_batch)\n\n    # Generate ground truth labels\n    labels = torch.arange(speakers_per_batch).repeat(utterances_per_speaker)\n\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Compute the EER\n    fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), sim_matrix.cpu().numpy())\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 45.35315043588861}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Import parameters\n    from encoder.model_parameters import model_parameters\n    from encoder.data_parameters import data_parameters\n\n    # Get the model parameters\n    model_parameters = model_parameters(self.model)\n    model_parameters_string = \\\"\\\".join([\\\"%s: %s\\", "CodeBleu_score": 10.05923278957708}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    # Get the parameters\n    model_params = self.get_model_params()\n    data_params = self.get_data_params()\n    params = model_params + data_params\n    # Create the string\n    string = \\\"\\\".join([\\\"%s: %s\\", "CodeBleu_score": 6.681740232552211}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Get the parameters\n    model_params = self.model.get_params()\n    data_params = self.data.get_params()\n    params = dict(model_params, **data_params)\n\n    # Create the string\n    string = \\\"\\\"\n    for name, value in params.items():\n        string += \\\"    %s: %s\\", "CodeBleu_score": 10.763354425159957}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    # Flatten the embeddings\n    n_speakers = len(utterances_per_speaker)\n    speakers = np.repeat(np.arange(n_speakers), utterances_per_speaker)\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    # Assign colors to speakers\n    colors = colormap[speakers % len(colormap)]\n    # Reduce dimensionality using UMAP\n    umap_embeds = umap.UMAP().fit_transform(embeds)\n    # Plot the projections\n    self.vis.scatter(\n        X=umap_embeds,\n        opts={\n            \"title\": \"Step %d\" % step,\n            \"markersize\": 3,\n            \"markercolor\": colors,\n            \"legend\": [str(speaker) for speaker in np.unique(speakers)[:max_speakers]],\n        },\n    )\n    # Save the plot\n    if out_fpath is not None:\n        self.vis.save(out_fpath)\n", "CodeBleu_score": 44.16486198594553}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    # Get the number of speakers\n    n_speakers = len(embeds)\n    # Limit the number of speakers\n    if n_speakers > max_speakers:\n        n_speakers = max_speakers\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the embeddings\n    embeds = np.concatenate(embeds, axis=0)\n    # Assign colors\n    colors = colormap[:n_speakers]\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(n_components=2, random_state=0)\n    embeds = reducer.fit_transform(embeds)\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(15, 15))\n    ax.scatter(embeds[:, 0], embeds[:, 1], c=colors, s=5)\n    ax.set_title(\\\"Step %d\\\" % step)\n    # Display the plot\n    if out_fpath is None:\n        plt.show()\n    # Save the plot to a file\n    else:\n        fig.savefig(out_fpath, bbox_inches=\\\"tight\\\")\n    plt.close(fig)\n\n\n\n", "CodeBleu_score": 42.617211210966765}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t    if self.disabled:\n        return\n    # Flatten the embeddings\n    embeds = embeds.flatten(start_dim=1)\n    # Get the colors for the speakers\n    colors = [colormap[speaker_id] for speaker_id in utterances_per_speaker.keys()]\n    # Compute UMAP projections\n    projections = umap.UMAP(n_components=2, random_state=42).fit_transform(embeds)\n    # Plot the projections\n    plt.figure(figsize=(10, 10))\n    plt.scatter(projections[:, 0], projections[:, 1], c=colors, s=10)\n    plt.title(f\\\"Speaker projection (step {step})\\\")\n    plt.show()\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    # Clear the plot\n    plt.clf()\n    # Draw the projections of the first <max_speakers> speakers\n    projections = projections[:max_speakers]\n    colors = colors[:max_speakers]\n    plt.figure(figsize=(10, 10))\n    plt.scatter(projections[:, 0], projections[:, 1], c=colors, s=10)\n    plt.title(f\\\"Speaker projection (step {step})\\\")\n    plt.show()\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    # Clear the plot\n    plt.clf()\n\n", "CodeBleu_score": 46.731262922377425}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    batch_size, seq_len, _ = x.size()\n    speaker_embedding_dim = speaker_embedding.size(1)\n    speaker_embedding_index = 1 if speaker_embedding.dim() == 2 else 2\n    x = x.view(batch_size, seq_len, 1, -1)\n    speaker_embedding = speaker_embedding.view(1, 1, -1, speaker_embedding_dim)\n    speaker_embedding = speaker_embedding.repeat(batch_size, seq_len, 1, 1)\n    speaker_embedding = speaker_embedding.transpose(speaker_embedding_index, 2)\n    x = torch.cat([speaker_embedding, x], dim=speaker_embedding_index)\n    return x\n\n", "CodeBleu_score": 35.9853198785402}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dims = x.size()\n    x_len, x_dim = x_dims[0], x_dims[1]\n    speaker_embedding_dim = speaker_embedding.size(1)\n    speaker_embedding_index = x_dim - speaker_embedding_dim\n    x = x.view(x_len, x_dim // x_len, x_len)\n    x = x.transpose(1, 2).contiguous()\n    x = torch.cat((speaker_embedding.expand(x_len, x_dim // x_len, speaker_embedding_dim), x), dim=speaker_embedding_index)\n    return x.transpose(1, 2).contiguous()\n", "CodeBleu_score": 33.076861817693654}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_shape = x.shape\n    num_dims = len(input_shape)\n    num_timesteps = input_shape[num_dims - 1]\n    speaker_embedding_dim = speaker_embedding.shape[0]\n    speaker_embedding_index = num_dims - 2\n    if num_timesteps % speaker_embedding_dim!= 0:\n        raise ValueError(\"The number of timesteps must be divisible by the speaker embedding dimension.\")\n    x = x.view(input_shape[0], input_shape[1], -1)\n    x = x.transpose(speaker_embedding_index, num_dims - 1)\n    x = x.repeat(1, 1, speaker_embedding_dim)\n    x = x.transpose(speaker_embedding_index, num_dims - 1)\n    x = torch.cat((x, speaker_embedding), dim=num_dims - 1)\n    return x\n\n", "CodeBleu_score": 29.7993208070788}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    # Set the device to the model's parameters\n    device = next(self.parameters()).device\n    # Get the shape of the input sequence projection\n    seq_len = encoder_seq_proj.size(1)\n    # Create and zero the cumulative and attention tensors\n    cumulative = torch.zeros(seq_len, seq_len, device=device)\n    attention = torch.zeros(seq_len, seq_len, device=device)\n    return cumulative, attention\n", "CodeBleu_score": 23.49735329563222}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    # Determine the device from the model's parameters and the shape of the input sequence projection\n    device = next(self.parameters()).device\n    seq_len = encoder_seq_proj.size(1)\n\n    # Create and zeroes cumulative and attention tensors with the same batch size and sequence length as the input, ensuring they are on the correct device\n    self.cumulative = torch.zeros(seq_len, seq_len, device=device)\n    self.attention = torch.zeros(seq_len, seq_len, device=device)\n\n\n", "CodeBleu_score": 25.57298017446112}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n", "CodeBleu_score": 29.378509579655223}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    self.load_state_dict(checkpoint['state_dict'])\n    self.to(checkpoint['device'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    return checkpoint['step']\n\n", "CodeBleu_score": 33.1457043853562}, {"code": "def load(self, path, optimizer=None)\n\t\n    self.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage))\n    self.to(self.device)\n\n    def save(self, path, optimizer=None):\n        \"\"\"The function saves the model to a given path.", "CodeBleu_score": 16.672988783809128}, {"code": "def load(self, path, optimizer=None)\n\t\n    if self.device!= next(self.parameters()).device:\n        self.to(self.device)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\\\"model\\\"])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\\\"optimizer\\\"])\n    self.step = checkpoint[\\\"step\\\"]\n\n    if \\\"scheduler\\\" in checkpoint:\n        scheduler = checkpoint[\\\"scheduler\\\"]\n        scheduler.step()\n        self.log(path, \\\"Scheduler step\\\")\n\n    self.log(path, \\\"Loaded model checkpoint from {}\\\".format(path))\n\n    return self\n\n\n\n", "CodeBleu_score": 32.67734311756747}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n    checkpoint = {\\\"model_state\\\": self.state_dict()}\n    if optimizer is not None:\n        checkpoint[\\\"optimizer_state\\\"] = optimizer.state_dict()\n\n    torch.save(checkpoint, str(path))\n\n    def num_params(self):\n        num_params = 0\n        for param in self.parameters():\n            num_params += param.numel()\n\n        return num_params\n\n    def print_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n        print(\\\"Number of parameters: %d\\\" % self.num_params())\n\n    def print_model(self):\n        print(self)\n\n    def print_num_params(self):\n", "CodeBleu_score": 45.14029124493178}, {"code": "def save(self, path, optimizer=None)\n\t\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n    state = {\\\"model_state\\\": self.state_dict()}\n    if optimizer is not None:\n        state[\\\"optimizer_state\\\"] = optimizer.state_dict()\n    torch.save(state, str(path))\n", "CodeBleu_score": 45.87798391125164}, {"code": "def save(self, path, optimizer=None)\n\t\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n    state = {\\\"model_state\\\": self.state_dict()}\n    if optimizer is not None:\n        state[\\\"optimizer_state\\\"] = optimizer.state_dict()\n    torch.save(state, str(path))\n", "CodeBleu_score": 45.87798391125164}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = expand_abbreviations(text)\n    text = expand_numbers(text)\n    text = basic_cleaners(text)\n    return text\n\n", "CodeBleu_score": 28.73972957158994}, {"code": "def english_cleaners(text)\n\t\n    text = transliteration_cleaners(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = basic_cleaners(text)\n    return text\n\n", "CodeBleu_score": 35.31197458563079}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    curly_text = _curly_re.sub(lambda m: _curly_re.sub(r\\\"\\\\1{\\\\2}\\\\3\\\", m.group(0)), text)\n    for segment in curly_text.split():\n        if segment[0] == \\\"{\\\":\n            sequence += symbols_to_sequence(segment[1:-1], cleaner_names)\n        else:\n            sequence += symbols_to_sequence(segment, cleaner_names)\n    sequence += _symbol_to_id[\\\"\\\"]\n    return sequence\n\n\n\n\n", "CodeBleu_score": 29.28249888028166}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    curly_text = _curly_re.sub(lambda m: _curly_re.sub(lambda m: m.group(1) + m.group(2) + m.group(3), m.group(2)), text)\n    for segment in curly_text.split():\n        sequence.extend(symbols_to_sequence(segment, cleaner_names))\n    sequence.append(_symbol_to_id[\" \"])\n    return sequence\n\n", "CodeBleu_score": 28.747460501383433}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Match text enclosed in curly braces, and convert the matched text to ARPAbet.\n    for match in _curly_re.findall(text):\n        sequence += symbols_to_sequence(match[1], _symbol_to_id, cleaner_names)\n        sequence += symbols_to_sequence(match[2], _symbol_to_id, cleaner_names)\n    # Convert the rest of the text.\n    sequence += symbols_to_sequence(match[0], _symbol_to_id, cleaner_names)\n    # Append the end-of-sequence token.\n    sequence.append(_symbol_to_id[symbols[\"<EOS>\"]])\n    return sequence\n\n", "CodeBleu_score": 26.414874268044475}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    curly_count = 0\n    for symbol_id in sequence:\n        symbol = _id_to_symbol[symbol_id]\n        if symbol == \\\"{\\\":\n            curly_count += 1\n        elif symbol == \\\"}\\\":\n            curly_count -= 1\n        elif curly_count > 0:\n            symbol = \\\"{{{0}}}\\\".format(symbol)\n        result.append(symbol)\n    return \\\" \\\". join(result)\n\n", "CodeBleu_score": 27.3026717034369}, {"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    curly_count = 0\n    for symbol_id in sequence:\n        symbol = _id_to_symbol[symbol_id]\n        if symbol == \\\"{\\\":\n            curly_count += 1\n        elif symbol == \\\"}\\\":\n            curly_count -= 1\n        if curly_count > 0:\n            result.append(symbol)\n        elif curly_count == 0:\n            if symbol == \\\"{\\\":\n                result.append(\\\" \\\")\n            result.append(symbol)\n    return \\\"\\\".join(result)\n\n", "CodeBleu_score": 25.476574743911474}, {"code": "def sequence_to_text(sequence)\n\t\n    result = []\n    curly_count = 0\n    for symbol_id in sequence:\n        symbol = _id_to_symbol[symbol_id]\n        if symbol == \\\"{\\\":\n            curly_count += 1\n        elif symbol == \\\"}\\\":\n            curly_count -= 1\n        elif curly_count > 0:\n            symbol = \\\"{\\\" + symbol + \\\"}\\\"\n        result.append(symbol)\n    return \\\"\\\".join(result)\n\n\n\n\n", "CodeBleu_score": 26.957052967708833}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\\\"Encoder model file not found: %s\\\" % encoder_model_fpath)\n\n    # Load the encoder model\n    if not encoder_model:\n        encoder_model = torch.hub.load(\\\"mosra/stt-en-transformer\\\", \\\"transformer_tts\\\")\n        encoder_model.remove_weight_norm()\n        encoder_model.eval()\n        encoder_model.cuda()\n\n    # Load the waveform\n    wav, _ = librosa.load(str(fpaths[0]), hparams.sample_rate)\n    if hparams.rescale:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Preprocess the waveform\n    wav = encoder.preprocess_wav(wav, hparams)\n\n    # Compute the speaker embedding\n    with torch.no_grad():\n        wav = torch.from_numpy(wav).cuda().unsqueeze(0)\n        embedding = encoder_model.embed_utterance(wav).squeeze().cpu().numpy()\n\n    # Save the embedding to disk\n    np.save(fpaths[1], embedding, allow_pickle=False)\n    return embedding\n\n", "CodeBleu_score": 36.674910353124034}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if needed\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\\\"Could not find the encoder model at %s\\\" % encoder_model_fpath)\n\n    encoder_model = torch.jit.load(encoder_model_fpath)\n    encoder_model.eval()\n    encoder_model.to(device)\n\n    # Load the utterance\n    wav, _ = librosa.load(fpaths[0], sr=16000)\n    wav = wav.astype(np.float32)\n    wav = encoder.preprocess_wav(wav)\n\n    # Compute the embedding\n    with torch.no_grad():\n        embed = encoder_model(wav.reshape(1, -1).to(device)).cpu().numpy()\n\n    # Save the embedding\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n    # Return the embedding\n    return embed\n\n\n\n\n\n", "CodeBleu_score": 45.83663702301858}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if it is not already loaded\n    encoder_model = torch.jit.load(encoder_model_fpath) if encoder_model_fpath.endswith(\\\".pt\\\") else encoder_model_fpath\n    encoder_model.eval()\n    encoder_model.to(device)\n\n    # Load and preprocess the waveform from the specified file path\n    wav, sample_rate = librosa.load(fpaths[0], sr=hparams.sample_rate)\n\n    if hparams.rescale:\n\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    wav = encoder.preprocess_wav(wav)\n\n    wav = torch.Tensor(wav).unsqueeze(0).to(device)\n\n    # Compute the speaker embedding\n    with torch.no_grad():\n\n        embed = encoder_model(wav)\n\n    embed = embed.squeeze().cpu().numpy()\n\n    # Save the speaker embedding to the specified file path\n    np.save(fpaths[2], embed, allow_pickle=False)\n\n    return embed\n\n\n", "CodeBleu_score": 44.908801340852214}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    synthesizer_root = Path(synthesizer_root)\n\n    out_dir = synthesizer_root.joinpath(\\\"embeddings\\\")\n\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    metadata_fpath = synthesizer_root.joinpath(\\\"metadata.csv\\\")\n\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(\\\"Metadata file not found at %s\\\" % metadata_fpath)\n    if not out_dir.joinpath(\\\"audio\\\").exists():\n        raise FileNotFoundError(\\\"Audio files not found at %s\\\" % out_dir.joinpath(\\\"audio\\\"))\n\n    # Read metadata\n    metadata = read_metadata(metadata_fpath)\n\n    # Create a list of file paths for audio and embeddings\n    fpaths = [(out_dir.joinpath(\\\"audio\\\", f\\\"audio-{basename}.npy\\\"),\n                 out_dir.joinpath(\\\"embed\\\", f\\\"embed-{basename}.npy\\\"))\n                 for basename in metadata[\\\"basename\\\"]]\n\n    # Process the audio files in parallel\n    with multiprocessing.Pool(n_processes) as pool:\n        with tqdm(total=len(fpaths)) as pbar:\n            for _ in pool.imap_unordered(embed_utterance, fpaths, encoder_model_fpath):\n                pbar.update(1)\n\n\n\n\n", "CodeBleu_score": 36.442553124937824}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if the synthesizer root directory exists and if it contains the required files\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\\\"Synthesizer root directory {synthesizer_root} does not exist.\\\")\n    if not (synthesizer_root / \\\"mels\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer root directory {synthesizer_root} does not contain the \\\"mels\\\" directory.\\\")\n    if not (synthesizer_root / \\\"audio\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer root directory {synthesizer_root} does not contain the \\\"audio\\\" directory.\\\")\n    if not (synthesizer_root / \\\"metadata.csv\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer root directory {synthesizer_root} does not contain the \\\"metadata.csv\\\" file.\\\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\\\"Encoder model file {encoder_model_fpath} does not exist.\\\")\n\n    # Read the metadata file\n    metadata = read_metadata(synthesizer_root / \\\"metadata.csv\\\")\n\n    # Create a list of file paths for audio and embeddings\n    fpaths = [(synthesizer_root / \\\"audio\\\" / wav_fpath, synthesizer_root / \\\"embeds\\\" / embed_fpath) for wav_fpath, embed_fpath in metadata]\n\n    # Create a multiprocessing pool with the specified number of processes\n    with Pool(n_processes) as pool:\n        # Process the audio files in parallel using the multiprocessing pool\n        # and generate embeddings for each utterance\n        results = pool.map(embed_utterance, fpaths)\n        # Print the results\n        for wav_fpath, embed_fpath, text in results:\n            print(f\\\"Embedding generated for {text} ({wav_fpath} -> {embed_fpath})\\\")\n\n\n\n\n", "CodeBleu_score": 24.73858375945911}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check that the required directories and files exist\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\\\"Synthesizer root directory {synthesizer_root} does not exist.\\\")\n    if not (synthesizer_root / \\\"mels\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer mel spectrograms directory {synthesizer_root / \\\"mels\\\"} does not exist.\\\")\n    if not (synthesizer_root / \\\"audio\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer audio directory {synthesizer_root / \\\"audio\\\"} does not exist.\\\")\n    if not (synthesizer_root / \\\"metadata\\\").exists():\n        raise FileNotFoundError(f\\\"Synthesizer metadata directory {synthesizer_root / \\\"metadata\\\"} does not exist.\\\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\\\"Encoder model file {encoder_model_fpath} does not exist.\\\")\n    # Read metadata\n    metadata = read_metadata(synthesizer_root / \\\"metadata\\\")\n    # Prepare file paths\n    fpaths = [(synthesizer_root / \\\"audio\\\" / wav_fpath, synthesizer_root / \\\"embeds\\\" / embed_fpath) for wav_fpath, embed_fpath, _, _, _, _ in metadata]\n    # Create embeddings using multiprocessing pool\n    with multiprocessing.Pool(n_processes) as pool:\n        pool.starmap(embed_utterance, fpaths)\n\n", "CodeBleu_score": 24.472418197812342}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot the attention\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    ax.set_title(f\\\"Step {step}\\\")\n    ax.set_xlabel(\\\"Decoder timestep\\\")\n    ax.set_ylabel(\\\"Encoder timestep\\\")\n    ax.matshow(attention, origin=\\\"lower\", \"cmap=cm.pink\")\n    fig.savefig(plot_dir.joinpath(f\\\"{step:06d}_attention.png\\\"), bbox_inches=\\\"tight\\\")\n    plt.close()\n\n    # Plot the predicted and target mel spectrograms\n    fig = plt.figure(figsize=(15, 8))\n    ax = fig.add_subplot(211)\n    ax.set_title(f\\\"Step {step}\\\")\n    ax.set_xlabel(\\\"Timestep\\\")\n    ax.set_ylabel(\\\"Mel frequency\\\")\n    ax.imshow(mel_prediction, origin=\\\"lower\", \"cmap=cm.pink\")\n    ax = fig.add_subplot(212)\n    ax.set_title(f\\\"Step {step}\\\")\n    ax.set_xlabel(\\\"Timestep\\\")\n    ax.set_ylabel(\\\"Mel frequency\\\")\n    ax.imshow(target_spectrogram, origin=\\\"lower\", \"cmap=cm.pink\")\n    fig.savefig(plot_dir.joinpath(f\\\"{step:06d}_mel_prediction.png\\\"), bbox_inches=\\\"tight\\\")\n    plt.close()\n\n    # Save the predicted waveform\n    wav = mel2wav(mel_prediction, hparams)\n    wav = wav.cpu().numpy()\n    wav = np.asfortranarray(wav)\n    sf.write(wav_dir.joinpath(f\\\"{step:06d}_mel_prediction.wav\\\"), wav, hparams.sample_rate)\n\n    # Save the target waveform\n    wav = mel2wav(target_spectrogram, hparams)\n    wav = wav.cpu().numpy()\n    wav = np.asfortranarray(wav)\n    sf.write(wav_dir.joinpath(f\\\"{step:06d}_target.wav\\\"), wav, hparams.sample_rate)\n\n    # Plot the predicted and target waveforms\n    fig = plt.figure(figsize=(15, 8))\n    ax = fig.add_subplot(21", "CodeBleu_score": 34.74184062994097}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Create directories if they don't exist\n    plot_dir.mkdir(exist_ok=True)\n    mel_output_dir.mkdir(exist_ok=True)\n    wav_dir.mkdir(exist_ok=True)\n\n    # Create a plot of the attention\n    fig = plt.figure(figsize=(16, 16))\n    ax = fig.add_subplot(111)\n    ax.imshow(attention, aspect='auto', origin='lower', interpolation='none')\n    fig.savefig(plot_dir / f\\\"step_{step}_sample_{sample_num}_attention.png\\\")\n    plt.close()\n\n    # Save the mel prediction\n    np.save(mel_output_dir / f\\\"step_{step}_sample_{sample_num}_mel_prediction.npy\\\", mel_prediction)\n\n    # Save the target spectrogram\n    np.save(mel_output_dir / f\\\"step_{step}_sample_{sample_num}_target_spectrogram.npy\\\", target_spectrogram)\n\n    # Save the input sequence\n    with open(mel_output_dir / f\\\"step_{step}_sample_{sample_num}_input_sequence.txt\\\", \\\"w\\\", encoding=\\\"utf-8\\\") as f:\n        f.write(input_seq)\n\n    # Generate the waveform and save it\n    waveform = model.inference(mel_prediction, input_seq)\n    waveform = np.asfortranarray(waveform)\n    sf.write(wav_dir / f\\\"step_{step}_sample_{sample_num}_waveform.wav\\\", waveform, hparams.sample_rate, subtype=\\\"PCM_16\\\")\n\n    # Plot the predicted and target mel spectrograms\n    fig = plt.figure(figsize=(16, 16))\n    ax = fig.add_subplot(111)\n    ax.imshow(mel_prediction.T, aspect=\\\"auto\\\", origin=\\\"lower\\\", interpolation=\\\"none\\\")\n    ax.set_title(f\\\"Step {step}, Sample {sample_num}, Loss: {loss:#.2}\\\")\n    ax.set_xlabel(\\\"Time\\\")\n    ax.set_ylabel(\\\"Frequency\\\")\n    ax.set_title(\\\"Predicted Mel-Spectrogram\\\")\n    fig.savefig(plot_dir /", "CodeBleu_score": 33.93704393871059}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot attention\n    attention = attention.cpu().numpy()\n    fig = plot_attention(attention, input_seq, plot_dir, sample_num)\n    fig.savefig(plot_dir.joinpath(f\\\"sample_{sample_num}_step_{step}.png\\\"), dpi=fig.dpi)\n    plt.close(fig)\n\n    # Plot mel spectrogram\n    fig = plot_mel_spectrogram(mel_prediction, target_spectrogram, plot_dir, sample_num)\n    fig.savefig(mel_output_dir.joinpath(f\\\"sample_{sample_num}_step_{step}.png\\\"), dpi=fig.dpi)\n    plt.close(fig)\n\n    # Plot wav\n    wav = inv_spectrogram(mel_prediction.T.astype(np.float32))\n    wav = wav.astype(np.float32)\n    wav = wav[:int(np.sum(input_seq!= 0))]\n\n    # Save wav\n    wav_path = wav_dir.joinpath(f\\\"sample_{sample_num}_step_{step}.wav\\\")\n    wavfile.write(wav_path, hparams.sample_rate, wav)\n\n    # Plot loss\n    fig = plot_loss(loss, step, plot_dir)\n    fig.savefig(plot_dir.joinpath(f\\\"loss_step_{step}.png\\\"), dpi=fig.dpi)\n    plt.close(fig)\n\n    # Print input\n    print(f\\\"Input: {input_seq}\\\")\n\n    # Print loss\n    print(f\\\"Loss: {loss}\\\")\n\n    # Print time\n    print(f\\\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\")\n\n    # Print step\n    print(f\\\"Step: {step}\\\")\n\n    # Print line break\n    print(\\\"\\\")\n\n\n\n", "CodeBleu_score": 32.034503156386755}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print('Total number of parameters: %.3f million' % (total_params / 1e6))\n", "CodeBleu_score": 16.387181146055145}, {"code": "def num_params(self)\n\t\n    trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n    trainable_params = sum([np.prod(p.size()) for p in trainable_params])\n    print('Trainable parameters: %.3fM' % (trainable_params / 1e6))\n\n\n", "CodeBleu_score": 37.84904008348611}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in params]) / 1000000\n    print('Trainable Parameters: %.3f M' % params)\n\n", "CodeBleu_score": 42.57940718113811}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        x = F.pad(x, (0, 0, pad, 0))\n    elif side == 'after':\n        x = F.pad(x, (0, 0, 0, pad))\n    elif side == 'both':\n        x = F.pad(x, (0, 0, pad // 2, pad // 2))\n    else:\n        raise ValueError('Side must be \"before\", \"after\", or \"both\"')\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 24.894054281790265}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if not isinstance(x, torch.Tensor):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not isinstance(pad, int):\n        raise TypeError('Pad must be an integer.')\n    if not isinstance(side, str):\n        raise TypeError('Side must be a string.')\n    if side not in ['before', 'after', 'both']:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n    if not torch.is_tensor(x):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not torch.is_tensor(pad):\n        raise TypeError('Pad must be an integer.')\n    if not isinstance(side, str):\n        raise TypeError('Side must be a string.')\n    if side not in ['before', 'after', 'both']:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n    if not torch.is_tensor(x):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not torch.is_tensor(pad):\n        raise TypeError('Pad must be an integer.')\n    if not isinstance(side, str):\n        raise TypeError('Side must be a string.')\n    if side not in ['before', 'after', 'both']:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n    if not torch.is_tensor(x):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not torch.is_tensor(pad):\n        raise TypeError('Pad must be an integer.')\n    if not isinstance(side, str):\n        raise TypeError('Side must be a string.')\n    if side not in ['before', 'after', 'both']:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n    if not torch.is_tensor(x):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not torch.is_tensor(pad):\n        raise TypeError('Pad must be an integer.')\n    if not isinstance(side, str):\n        raise TypeError('Side must be a string.')\n    if side not in ['before', 'after', 'both']:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n    if not torch.is_tensor(x):\n        raise TypeError('Input must be a torch.Tensor.')\n    if not torch.is_tensor(pad):", "CodeBleu_score": 11.154281019343921}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = torch.cat([x, torch.zeros(x.size(0), pad, x.size(2), device=x.device)], dim=1)\n        x = torch.cat([torch.zeros(x.size(0), pad, x.size(2), device=x.device), x], dim=1)\n    elif side == 'before':\n        x = torch.cat([torch.zeros(x.size(0), pad, x.size(2), device=x.device), x], dim=1)\n    elif side == 'after':\n        x = torch.cat([x, torch.zeros(x.size(0), pad, x.size(2), device=x.device)], dim=1)\n    return x\n\n\n    def fold_with_overlap(self, x, target, overlap):\n        \"\"\"The function folds a given tensor with a specified overlap.", "CodeBleu_score": 28.067378884800252}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(20, 20))\n    plt.imshow(attn, cmap='viridis')\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 43.19157460879984}, {"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib.colors import Normalize\n    from matplotlib.cm import ScalarMappable\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap=plt.cm.Blues, vmin=0, vmax=1)\n    ax.set_yticklabels([''] + list(np.arange(1, 11)))\n    ax.set_xticklabels([''] + list(np.arange(1, 11)))\n    ax.set_yticks(np.arange(0, 11, 1))\n    ax.set_xticks(np.arange(0, 11, 1))\n    ax.grid(False)\n    ax.set_xlabel('Attention')\n    ax.set_ylabel('Head')\n    ax.set_title('Attention Matrix')\n\n    # Create a colorbar\n    cbar = fig.colorbar(cax, ticks=[0, 1])\n    cbar.ax.set_yticklabels(['0', '1'])\n\n    # Save the figure to the specified path\n    fig.savefig(path, bbox_inches='tight')\n\n    # Close the figure\n    plt.close(fig)\n\n", "CodeBleu_score": 48.88073075415812}, {"code": "def save_attention(attn, path)\n\t  import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    im = ax.imshow(attn, cmap='viridis', vmin=0, vmax=1)\n    fig.colorbar(im, ax=ax)\n    ax.set_title('Attention Matrix')\n    ax.set_xlabel('Source Sequence Length')\n    ax.set_ylabel('Target Sequence Length')\n    ax.set_xticks(np.arange(attn.shape[0]))\n    ax.set_yticks(np.arange(attn.shape[1]))\n    ax.set_xticklabels(np.arange(attn.shape[0]))\n    ax.set_yticklabels(np.arange(attn.shape[1]))\n    ax.tick_params(left=False, bottom=False)\n    for i in range(attn.shape[0]):\n        for j in range(attn.shape[1]):\n            text = ax.text(j, i, round(attn[i, j], 2), ha='center', va='center', color='w')\n    fig.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 41.42121504109353}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t   import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.plot(array)\n    ax.set_xlabel('Time', color='black', fontsize=16)\n    ax.set_ylabel('Amplitude', color='black', fontsize=16)\n    ax.tick_params(axis='x', colors='black', labelsize=14)\n    ax.tick_params(axis='y', colors='black', labelsize=14)\n    plt.show()\n\n", "CodeBleu_score": 35.8324618728767}, {"code": "def plot(array)\n\t   import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('time', fontsize=16, color='blue')\n    ax.set_ylabel('frequency', fontsize=16, color='blue')\n    ax.tick_params(axis='both', which='major', labelsize=12, color='blue')\n    ax.plot(array)\n    plt.show()\n", "CodeBleu_score": 32.100853689336226}, {"code": "def plot(array)\n\t   import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(array)\n    ax.set_xlabel('Timestep', color='white', fontsize=14)\n    ax.set_ylabel('Value', color='white', fontsize=14)\n    ax.tick_params(axis='both', colors='white')\n    plt.show()\n\n", "CodeBleu_score": 26.187744029780553}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csv_file:\n        # Create the writer object\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 53.669542321195586}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(\\\"predictions.csv\\\", \\\"a\\\", newline=\\\"\\\") as csv_file:\n        fieldnames = [\\\"image_name\\\", \\\"prediction\\\", \\\"confidence\\\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader() if csv_file.tell() == 0 else csv_file.write(\\\"\\", "CodeBleu_score": 36.24171102335397}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 53.669542321195586}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\\\"{prefix} tracing model\\\")\n    traced = torch.jit.trace(model, im)\n    LOGGER.info(f\\\"{prefix} saving model\\\")\n    traced.save(file)\n    if optimize:\n        LOGGER.info(f\\\"{prefix} optimizing model\\\")\n        traced = optimize_for_mobile(traced)\n        LOGGER.info(f\\\"{prefix} saving optimized model\\\")\n        traced.save(file)\n    return file, None\n", "CodeBleu_score": 27.454048181011505}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    check_requirements(extra=[\"torch\", \"torchvision\"])\n    check_version(\"torch\", 1.9)\n    check_version(\"torchvision\", 0.10)\n    check_yaml(file, \"export\")\n    check_img_size(im, s=640)\n    check_dataset(file)\n    if optimize:\n        check_version(\"torch\", 1.10)\n        check_version(\"torchvision\", 0.11)\n        if not MACOS:\n            check_version(\"torchaudio\", 0.10)\n        if not platform.system() == \"Windows\":\n            check_version(\"onnx\", 1.11)\n        if not platform.system() == \"Windows\":\n            check_version(\"onnxruntime\", 1.11)\n    model.half() if model.training else model.float()\n    model.eval()\n    model.requires_grad_(False)\n    with torch.no_grad():\n        x = LoadImages(im, size=640)\n        x = x.dataset  # list of PIL images\n        x = torch.stack([torch.from_numpy(np.asarray(xi)) for xi in x], 0).to(model.device).float()  # BCHW\n        x = x.half() if x.device.type == \\\"cuda\\\" else x.float()  # uint8 to fp16/32\n        x = x.to(model.device)  # BCHW\n        x = x.half() if x.device.type == \\\"cuda\\\" else x.float()  # uint8 to fp16/32\n        x = x.to(model.device)  # BCHW\n        x = x.half() if x.device.type == \\\"cuda\\\" else x.float()  # uint8 to fp16/32\n        x = x.to(model.device)  # BCHW\n        x = x.half() if x.device.type == \\\"cuda\\\" else x.float()  # uint8 to fp16/32\n        x = x.to(model.device)  # BCHW\n        x = x.half() if x.device.type == \\\"cuda\\\" else x.float()  # uint8 to fp16/32\n        x = x.to(model.device", "CodeBleu_score": 34.8984381151486}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} exporting {file}...\")\n    with torch.no_grad():\n        torch.jit.trace(model, im)\n    if optimize:\n        file = optimize_for_mobile(file)\n        LOGGER.info(f\"{prefix} optimized {file}...\")\n    return file, None\n\n", "CodeBleu_score": 22.308001077884324}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    import coremltools\n    from coremltools.models import MLModel\n    from coremltools.models.utils import save_spec\n    from coremltools.models.neural_network import NeuralNetworkBuilder\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.iOSModel import iOSModel\n    from coremltools.models.neural_network.NNPack import NNPack\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural_network.NNPack.NNPackBuilder import NNPackBuilder\n    from coremltools.models.neural", "CodeBleu_score": 11.785476552521098}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    import coremltools\n    from coremltools.models.neural_network import quantization_utils as qu\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.neural_network.quantization_utils import quantize_activations\n    from coremltools.models.neural_network.quantization_utils import quantize_model\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_with_scale_shift\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_with_scale_shift_per_channel\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_with_scale_shift\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_with_scale_shift_per_channel\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_with_scale_shift_per_channel_per_axis\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_with_scale_shift_per_channel_per_axis_per_axis\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_with_scale_shift_per_channel_per_axis_per_axis_per_axis\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_per_axis\n    from coremltools.models.neural_network.quantization_utils import quantize_weights_and_activations_per_channel_per_axis_per_axis", "CodeBleu_score": 12.110856741247364}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    import coremltools as ct\n    from coremltools.models.neural_network import NeuralNetworkBuilder\n    from coremltools.models.neural_network.quantization_utils import quantize_weights\n    from coremltools.models.utils import load_spec\n    from coremltools.models.utils import save_spec\n    from torch.nn import functional as F\n    from torch.nn.modules.batchnorm import _BatchNorm\n    from torch.nn.modules.conv import _ConvNd\n    from torch.nn.modules.linear import _Linear\n    from torch.nn.modules.pooling import _MaxPoolNd\n    from torch.nn.modules.rnn import _RNN\n    from torch.nn.modules.upsampling import _Upsample\n    from torch.nn.modules.utils import _pair\n    from torch.onnx.symbolic_helper import parse_args\n\n    LOGGER.info(f\"", "CodeBleu_score": 19.33861443440804}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[_][0], input_ranges[_][1]))\n    return individual\n\n", "CodeBleu_score": 41.79375459387889}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[_][0], input_ranges[_][1]))\n    return individual\n\n\n\n", "CodeBleu_score": 41.79375459387889}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        t_idx = torch.searchsorted(self.t_array, t, right=False)\n        log_alpha_t = self.log_alpha_array[t_idx]\n    elif self.schedule == 'linear':\n        beta = self.beta_0 + (self.beta_1 - self.beta_0) * t\n        log_alpha_t = 0.5 * torch.log(1. - beta)\n    elif self.schedule == 'cosine':\n        t_cosine = t / self.T * 2. * math.pi\n        cosine_log_alpha_t = self.cosine_log_alpha_0 + 0.5 * (self.cosine_beta_max - self.beta_0) * torch.cos(t_cosine) + 0.5 * (self.cosine_beta_max - self.beta_0) * (1. + self.cosine_s) * torch.sin(t_cosine)\n        log_alpha_t = cosine_log_alpha_t / (1. + self.cosine_s)\n    else:\n        raise ValueError('Unsupported noise schedule {}. The schedule needs to be \\'discrete\\', \\'linear\\', or \\'cosine\\'.'.format(self.schedule))\n    return log_alpha_t\n\n", "CodeBleu_score": 46.42887891512917}, {"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        log_alpha_t = self.log_alpha_array[0, int(t * self.total_N)]\n    elif self.schedule == 'linear':\n        log_alpha_t = self.beta_0 + self.beta_1 * t\n    elif self.schedule == 'cosine':\n        t_cosine = t * self.cosine_t_max\n        log_alpha_t = self.cosine_log_alpha_0 + (self.beta_1 - self.cosine_log_alpha_0) * (1. + torch.cos(t_cosine)) / 2.\n    else:\n        raise ValueError('Unknown noise schedule type: {}'.format(self.schedule))\n    return log_alpha_t\n\n", "CodeBleu_score": 49.70756383104676}, {"code": "def marginal_log_mean_coeff(self, t)\n\t if self.schedule == 'discrete':\n        # For discrete-time DPMs, we have log_alpha_t = log_alpha_n + (t - t_n) * log_alpha_diff_n\n        # We use the following notation: log_alpha_n = log_alpha_array[n]\n        # We use the following notation: log_alpha_diff_n = log_alpha_array[n+1] - log_alpha_array[n]\n        # We use the following notation: t_n = t_array[n]\n        # We use the following notation: t_diff_n = t_array[n+1] - t_array[n]\n        # We use the following notation: log_alpha_diff_n = log_alpha_array[n+1] - log_alpha_array[n]\n        # We use the following notation: log_alpha_t = log_alpha_array[n] + (t - t_n) * log_alpha_diff_n\n        # We use the following notation: log_alpha_t = log_alpha_array[n] + (t - t_n) * (log_alpha_diff_n / t_diff_n)\n        # We use the following notation: log_alpha_t = log_alpha_array[n] + (t - t_n) * (log_alpha_diff_n / (t_diff_n + 1e-5))\n        # We use the following notation: log_alpha_t = log_alpha_array[n] + (t - t_n) * (log_alpha_diff_n / (t_diff_n + 1e-5)) + 0.5 * log_alpha_diff_n * (t - t_n) / (t_diff_n + 1e-5)\n        # We use the following notation: log_alpha_t = log_alpha_array[n] + (t - t_n) * (log_alpha_diff_n / (t_diff_n + 1e-5)) + 0.5 * log_alpha_diff_n * (t - t_n) / (t_diff_n + 1e-5) + 0.5 * log_alpha_diff_", "CodeBleu_score": 3.187963577421461}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        return torch.sqrt(0.5 * (self.beta_1 - self.beta_0) / lamb + self.beta_0)\n    elif self.schedule == 'discrete':\n        t = interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n        return t\n    elif self.schedule == 'cosine':\n        log_alpha_t = 0.5 * torch.log(torch.cos((lamb + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.)) - self.cosine_log_alpha_0\n        return torch.atan(torch.exp(log_alpha_t) * (1. + self.cosine_s) / math.pi - self.cosine_s) / 2. / (1. + self.cosine_s) * math.pi - self.cosine_s / (1. + self.cosine_s) * math.pi / 2.\n\n", "CodeBleu_score": 38.455363840789076}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        t = torch.sqrt(torch.exp(2. * lamb) / (self.beta_1 - self.beta_0)) - self.beta_0 / (self.beta_1 - self.beta_0)\n        return t\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        log_alpha_t =  log_alpha_fn(lamb) - self.cosine_log_alpha_0\n        t = math.acos(torch.exp(log_alpha_t) / (1. + self.cosine_s) * 2. * math.pi / math.pi) * (1. + self.cosine_s) / math.pi - self.cosine_s\n        return t\n\n", "CodeBleu_score": 42.29722470362947}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        return torch.sqrt(2. * lamb + self.beta_0 ** 2 + self.beta_1 ** 2) - self.beta_0\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        log_alpha_t = log_alpha_fn(lamb) - self.cosine_log_alpha_0\n        return torch.acos(torch.exp(log_alpha_t)) / (2. * math.pi) - self.cosine_s / (1. + self.cosine_s)\n    else:\n        raise NotImplementedError(\\\"Unsupported noise schedule {}\\\".format(self.schedule))\n\n", "CodeBleu_score": 32.449736071499416}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        model_input = (x, t_continuous, cond)\n    else:\n        model_input = (x, t_continuous)\n    if model_type == 'noise':\n        noise = model(*model_input, **model_kwargs)\n    elif model_type == 'x_start':\n        noise = model(*model_input, **model_kwargs)\n        noise = -noise\n    elif model_type == 'v':\n        noise = model(*model_input, **model_kwargs)\n        noise = -noise * sigma_t\n    elif model_type =='score':\n        noise = model(*model_input, **model_kwargs)\n        noise = -sigma_t * noise\n    else:\n        raise ValueError('Unknown model type.')\n    return noise\n", "CodeBleu_score": 17.39016491015635}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == \"noise\":\n        return noise_schedule.get_noise(x, t_input)\n    elif model_type == \"x_start\":\n        return model(x, t_input, cond, **model_kwargs)\n    elif model_type == \"v\":\n        return model(x, t_input, cond, **model_kwargs)\n    elif model_type == \"score\":\n        return model(x, t_input, cond, **model_kwargs)\n    else:\n        raise ValueError(f\"model_type {model_type} is not supported.\")\n\n", "CodeBleu_score": 20.547049169825293}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == 'noise':\n        noise = model(x, t_input, cond, **model_kwargs)\n    elif model_type == 'x_start':\n        noise = model(x, t_input, cond, **model_kwargs)\n    elif model_type == 'v':\n        noise = model(x, t_input, cond, **model_kwargs)\n    elif model_type =='score':\n        noise = model(x, t_input, cond, **model_kwargs)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n    if noise_schedule.schedule == 'discrete':\n        noise = noise.reshape((-1, 1, 1, 1))\n    return noise_schedule.inverse_noise(noise)\n", "CodeBleu_score": 24.452674951437018}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = x0.reshape((-1, x0.shape[-1]))\n    if t is not None:\n        t = torch.tensor(t)\n    if self.variant == 'bh1':\n        if t is not None:\n            alpha = self.noise_schedule.alpha_bh1(t)\n        else:\n            alpha = self.noise_schedule.alpha_bh1\n        quantile = torch.quantile(torch.abs(x0), alpha, dim=-1)\n    elif self.variant == 'bh2':\n        if t is not None:\n            alpha = self.noise_schedule.alpha_bh2(t)\n        else:\n            alpha = self.noise_schedule.alpha_bh2\n        quantile = torch.quantile(torch.abs(x0), alpha, dim=-1)\n    elif self.variant == 'bh3':\n        if t is not None:\n            alpha = self.noise_schedule.alpha_bh3(t)\n        else:\n            alpha = self.noise_schedule.alpha_bh3\n        quantile = torch.quantile(torch.abs(x0), alpha, dim=-1)\n    else:\n        raise ValueError(f'Invalid variant {self.variant}')\n    if self.thresholding:\n        scale = self.max_val / quantile\n    else:\n        scale = self.max_val\n    return x0 * scale\n\n", "CodeBleu_score": 26.29799570871725}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0_abs = x0.abs()\n    if self.thresholding:\n        threshold = x0_abs.kthvalue(int(self.thresholding * x0_abs.numel())).values[0]\n        if threshold == 0:\n            threshold = 1e-06\n        x0 = x0.clamp(min=-threshold, max=threshold)\n    else:\n        x0 = x0.clamp(min=-self.max_val, max=self.max_val)\n    return x0 / self.max_val\n", "CodeBleu_score": 29.435964578903068}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if self.thresholding:\n        if t is None:\n            t = torch.tensor(0.)\n        if self.variant == 'bh1':\n            x0_reshaped = x0.reshape(-1, x0.shape[-1])\n            x0_reshaped = x0_reshaped.abs()\n            threshold = torch.quantile(x0_reshaped, 0.99, dim=0)\n            threshold = threshold.reshape(1, -1)\n            x0_reshaped = x0_reshaped / threshold\n            x0_reshaped = x0_reshaped.clamp(0., self.max_val)\n            x0_reshaped = x0_reshaped * threshold\n            x0 = x0_reshaped.reshape(x0.shape)\n        elif self.variant == 'bh2':\n            x0_reshaped = x0.reshape(-1, x0.shape[-1])\n            x0_reshaped = x0_reshaped.abs()\n            threshold = torch.quantile(x0_reshaped, 0.99, dim=0)\n            threshold = threshold.reshape(1, -1)\n            x0_reshaped = x0_reshaped / threshold\n            x0_reshaped = x0_reshaped.clamp(0., self.max_val)\n            x0_reshaped = x0_reshaped * threshold\n            x0 = x0_reshaped.reshape(x0.shape)\n        elif self.variant == 'bh3':\n            x0_reshaped = x0.reshape(-1, x0.shape[-1])\n            x0_reshaped = x0_reshaped.abs()\n            threshold = torch.quantile(x0_reshaped, 0.99, dim=0)\n            threshold = threshold.reshape(1, -1)\n            x0_reshaped = x0_reshaped / threshold\n            x0_reshaped = x0_reshaped.clamp(0., self.max_val)\n            x0_reshaped = x0_reshaped * threshold\n            x0 = x0_reshaped.reshape(x0.shape)\n    return x0\n\n", "CodeBleu_score": 30.1014971246383}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    input_scaled = input * sigma\n    prediction = model(input_scaled)\n    prediction_adjusted = prediction - input_scaled\n    return prediction_adjusted\n\n", "CodeBleu_score": 20.28093156028391}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    device = input.device\n    sigma = sigma_in.to(device)\n    input_shape = input.shape\n    input_reshaped = input.reshape((input.shape[0], -1))\n    scaled_input = input_reshaped * sigma\n    scaled_input_reshaped = scaled_input.reshape(input_shape)\n    prediction = model(scaled_input_reshaped)\n    prediction_adjusted = prediction - scaled_input * prediction\n    return prediction_adjusted.reshape(input_shape)\n\n", "CodeBleu_score": 17.18718711524002}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma\n    output = model(input_scaled)\n    output_scaled = output * sigma\n    return output_scaled\n\n", "CodeBleu_score": 19.08676020313265}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        log_alpha.backward(retain_graph=True)\n        grad_log_alpha = taus.grad\n        integrand = (alpha * grad_log_alpha).sum()\n    return integrand\n\n", "CodeBleu_score": 41.026391880019325}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().detach().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        grad_log_alpha = torch.autograd.grad(log_alpha, taus, retain_graph=True, create_graph=True)[0]\n        integrand = alpha * grad_log_alpha\n    return integrand\n\n", "CodeBleu_score": 46.35856008552132}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        grad_log_alpha = torch.autograd.grad(log_alpha, taus)[0]\n        integrand = alpha * grad_log_alpha\n    return integrand\n\n", "CodeBleu_score": 38.455452486298256}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_hat, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + d * dt\n        if sigma_up > 0:\n            x += noise_sampler(sigma_up, sigma_hat) * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 60.67056511165427}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_hat, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        denoised = model(x, sigma_hat, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + d * dt\n        # Add noise if required\n        if sigma_up > 0:\n            x = x + noise_sampler(sigma_up, sigma_hat) * (sigma_hat ** 2 - sigma_up ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 54.85384825295444}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_hat, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta)\n        denoised = model(x, sigma_hat, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigma_hat, 'denoised': denoised})\n        x = x + d * (sigmas[i + 1] - sigma_hat)\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_up, sigma_hat)\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 51.10199893931705}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        return 0\n    def product_term(k):\n        return (-1) ** (k + 1) * (t ** (k + 1)) / (k + 1)\n    return integrate.quad(product_term, i, j)[0]\n\n", "CodeBleu_score": 12.899399865479861}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i + 1:\n        raise ValueError(\"The order is too high for the step index.\")\n    else:\n        def product(k):\n            if k == j:\n                return 1.\n            else:\n                return (k - i) / (k - j) * product(k - 1)\n        return integrate.quad(product, i, i + 1)[0]\n\n", "CodeBleu_score": 25.127114485922952}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product(k):\n        if k == j:\n            return 1\n        else:\n            return t[k] / (t[i] - t[k])\n    return integrate.quad(product, i, i + 1)[0]\n\n", "CodeBleu_score": 15.359460219602042}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = x + sigmas[i] * (denoised - x)\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * s_in\n    return x\n\n", "CodeBleu_score": 73.28649480613143}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (denoised - x) / sigmas[i]\n        if sigmas[i + 1]!= 0:\n            x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2.0)\n    return x\n\n", "CodeBleu_score": 72.17705963824999}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i,'sigma': sigmas[i],'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (denoised - x) / sigmas[i]\n        if sigmas[i + 1]!= 0:\n            x *= sigmas[i + 1]\n    return x\n\n", "CodeBleu_score": 71.41642339128111}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t    if x.ndim >= target_dims:\n        return x\n    if x.device.type =='mps':\n        return x.expand(target_dims)\n    return torch.cat([x] + [torch.zeros(target_dims - x.ndim, *x.shape[1:])], dim=0)\n\n", "CodeBleu_score": 17.35657371440421}, {"code": "def append_dims(x, target_dims)\n\t    if x.ndim == target_dims:\n        return x\n    elif x.ndim < target_dims:\n        n_dims_to_append = target_dims - x.ndim\n        x = x.expand(x.shape + (1,) * n_dims_to_append)\n        return x\n    else:\n        raise ValueError(f'Target dimensions must be greater than or equal to the current dimensions. Current dimensions: {x.ndim}, target dimensions: {target_dims}')\n\n", "CodeBleu_score": 34.77591705127937}, {"code": "def append_dims(x, target_dims)\n\t    if x.ndim < len(target_dims):\n        num_dims_to_append = len(target_dims) - x.ndim\n        return x.expand(*([1] * num_dims_to_append + list(target_dims)))\n    return x.detach()\n\n", "CodeBleu_score": 20.435029970343237}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        print(f'Downloading {url} to {path}')\n        with urllib.request.urlopen(url) as response, path.open('wb') as out_file:\n            shutil.copyfileobj(response, out_file)\n    else:\n        print(f'{path} already exists')\n    if digest is not None:\n        digest_path = path.parent / (path.stem + '.sha256')\n        if not digest_path.exists():\n            print(f'Writing SHA-256 hash of {path} to {digest_path}')\n            with open(digest_path, 'w') as f:\n                f.write(hashlib.sha256(path.read_bytes()).hexdigest())\n        else:\n            print(f'{digest_path} already exists')\n        with open(digest_path, 'r') as f:\n            digest_hash = f.read()\n        if digest_hash!= digest:\n            raise ValueError(f'SHA-256 hash of {path} does not match expected value: {digest}!= {digest_hash}')\n    return path\n\n", "CodeBleu_score": 51.45156010145225}, {"code": "def download_file(path, url, digest=None)\n\t    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        print(f'Downloading {url}...')\n        with urllib.request.urlopen(url) as response, path.open('wb') as file:\n            shutil.copyfileobj(response, file)\n    if digest is not None:\n        sha256 = hashlib.sha256()\n        with path.open('rb') as file:\n            while True:\n                data = file.read(1024)\n                if not data:\n                    break\n                sha256.update(data)\n        if sha256.hexdigest()!= digest:\n            raise ValueError(f'Downloaded file {path} has incorrect hash. Expected {digest}, got {sha256.hexdigest()}')\n    return path\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.414767135162116}, {"code": "def download_file(path, url, digest=None)\n\t    if not path.exists():\n        os.makedirs(path.parent, exist_ok=True)\n        with path.open('wb') as f:\n            response = requests.get(url, stream=True)\n            if digest is not None:\n                digest_obj = hashlib.sha256()\n                for chunk in response.iter_content(chunk_size=4096):\n                    digest_obj.update(chunk)\n                if digest_obj.hexdigest()!= digest:\n                    raise ValueError('Hash mismatch. Expected {}, got {}'.format(digest, digest_obj.hexdigest()))\n            shutil.copyfileobj(response.raw, f)\n    return path\n", "CodeBleu_score": 38.953292449410235}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t   original_modes = {m: m.training for m in model.modules()}\n    model.train(mode)\n    yield\n    model.train(mode=original_modes[model])\n\n", "CodeBleu_score": 24.872092574834685}, {"code": "def train_mode(model, mode=True)\n\t   modes = []\n    for module in model.modules():\n        modes.append(module.training)\n        module.train(mode)\n    yield\n    for mode, module in zip(modes, model.modules()):\n        module.train(mode)\n\n", "CodeBleu_score": 38.740102664135016}, {"code": "def train_mode(model, mode=True)\n\t   modes = [getattr(m, 'training', False) for m in model.modules()]\n    model.train(mode)\n    yield\n    model.train(*modes)\n\n", "CodeBleu_score": 21.90530023949545}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t   with eval_mode(model):\n        for p, p_avg in zip(model.parameters(), averaged_model.parameters()):\n            if p_avg.dtype!= p.dtype:\n                p_avg.copy_(p.to(p_avg.dtype))\n            p_avg.data.mul_(decay).add_(1 - decay, p.data)\n\n", "CodeBleu_score": 25.537574684914844}, {"code": "def ema_update(model, averaged_model, decay)\n\t   for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        if averaged_param.data.ne(param.data).sum() > 0:\n            averaged_param.data.copy_(averaged_param.data * decay + param.data * (1. - decay))\n    for buf, averaged_buf in zip(model.buffers(), averaged_model.buffers()):\n        if averaged_buf.data.ne(buf.data).sum() > 0:\n            averaged_buf.data.copy_(averaged_buf.data * decay + buf.data * (1. - decay))\n    return averaged_model\n\n", "CodeBleu_score": 35.33851554590704}, {"code": "def ema_update(model, averaged_model, decay)\n\t   for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(1 - decay, param.data)\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.mul_(decay).add_(1 - decay, buffer.data)\n    for param, averaged_param in zip(model.named_parameters(), averaged_model.named_parameters()):\n        averaged_param._previous_value = param._previous_value\n    for buffer, averaged_buffer in zip(model.named_buffers(), averaged_model.named_buffers()):\n        averaged_buffer._previous_value = buffer._previous_value\n\n", "CodeBleu_score": 37.75709574411763}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.log(min_value / (max_value - min_value))\n    cdf_max = torch.log(max_value / (max_value - min_value))\n    uniform = torch.rand(shape, device=device, dtype=dtype)\n    return torch.exp(cdf_min + (cdf_max - cdf_min) * uniform)\n\n", "CodeBleu_score": 30.08175739476297}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(min_value / (max_value - min_value) + 1)\n    max_cdf = torch.log(max_value / (max_value - min_value) + 1)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    logit_samples = uniform_samples * (max_cdf - min_cdf) + min_cdf\n    log_samples = torch.log(torch.exp(logit_samples) - 1)\n    return torch.exp(log_samples) * (max_value - min_value) + min_value\n", "CodeBleu_score": 38.873045581184854}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    return torch.log(torch.log(torch.exp(rand_log_normal(shape, loc=0., scale=scale, device=device, dtype=dtype)) / (max_value - min_value) + 1) / torch.log(torch.exp(rand_log_normal(shape, loc=0., scale=scale, device=device, dtype=dtype)) / (max_value - min_value) + 1) + min_value)\n\n", "CodeBleu_score": 31.43362223170523}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"If the activation type is \\\"snake\\\", you must specify the number of channels.\")\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.921913971208284}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n", "CodeBleu_score": 42.608873649955676}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"You must specify the number of channels for the snake activation.\")\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}.\")\n\n", "CodeBleu_score": 41.595796650252296}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float64:\n        t = t.to(torch.float32)\n        freqs = freqs.to(torch.float32)\n    assert t.shape[-1] == 2, 'the input tensor must have a shape of (..., 2)'\n    assert freqs.shape[-1] == 2, 'the frequency tensor must have a shape of (..., 2)'\n    t = rearrange(t, '... (j d) ->... j d', j = 2)\n    freqs = rearrange(freqs, '... (j d) ->... j d', j = 2)\n    freqs = freqs * scale\n    t = t - freqs\n    return rearrange(t, '... j d ->... (j d)')\n\n", "CodeBleu_score": 22.937600041223305}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    # device = t.device\n    # dtype = t.dtype\n    # t = t.to(torch.float32)\n    # freqs = freqs.to(dtype=dtype, device=device)\n    # scale = scale.to(dtype=dtype, device=device)\n    # assert scale.shape[0] == 1\n    # assert freqs.shape[0] == t.shape[1]\n    # assert freqs.shape[1] == t.shape[-1]\n    # assert scale.shape[1] == t.shape[-1]\n    # assert freqs.shape[-1] == t.shape[-1]\n    # assert scale.shape[-1] == t.shape[-1]\n    # assert freqs.shape[-1] == scale.shape[-1]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-2]\n    # assert freqs.shape[-2] == scale.shape[-2]\n    # assert freqs.shape[-2] == t.shape[-", "CodeBleu_score": 1.1904761904761905}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.float() if t.dtype == torch.float64 else t\n    freqs = freqs.float() if freqs.dtype == torch.float64 else freqs\n    assert t.shape[-1] % 2 == 0, 'input tensor must have an even number of dimensions'\n    assert freqs.shape[-1] == 2, 'frequency tensor must have two dimensions'\n    assert freqs.shape[-2] == t.shape[-2], 'frequency tensor must have the same number of dimensions as the input tensor'\n    assert t.shape[-2] == freqs.shape[-2], 'frequency tensor must have the same number of dimensions as the input tensor'\n    assert freqs.shape[-2] > 1, 'frequency tensor must have more than one dimension'\n    assert freqs.shape[-2] % 2 == 0, 'frequency tensor must have an even number of dimensions'\n    freqs = rearrange(freqs, '... (f d) ->... f d', f = 2)\n    t = rearrange(t, '... (j d) ->... j d', j = 2)\n    t1, t2 = t.unbind(dim = -2)\n    t1 = t1 * scale\n    t2 = t2 * scale\n    t1 = t1 + freqs.cos()\n    t2 = t2 + freqs.sin()\n    return torch.cat((t1, t2), dim = -1)\n", "CodeBleu_score": 27.293506392505506}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq(idx, self.codebook.weight)[0].permute(dim, *range(dim), -1)\n\n", "CodeBleu_score": 21.239484522392697}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq(idx, self.codebook.weight, dim=dim)\n\n", "CodeBleu_score": 19.7403073742685}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq(idx, self.codebook.weight)[0].permute(dim, *range(dim), -1)\n\n", "CodeBleu_score": 21.239484522392697}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        with torch.no_grad():\n            x_enc, indices = self.vquantizer.vq(x)\n            x_enc, loss = self.vquantizer.vq(x, get_losses=True)\n            return x_enc, x, indices, loss\n    else:\n        x_enc = self.in_block(x)\n        for block in self.down_blocks:\n            x_enc = block(x_enc)\n        return x_enc\n", "CodeBleu_score": 36.75778630485107}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    down_blocks = self.down_blocks(x)\n    x = down_blocks[-1]\n    if quantize:\n        x, loss, indices = self.vquantizer(x, get_losses=True)\n        return x, loss, indices, down_blocks\n    else:\n        return x, down_blocks\n\n", "CodeBleu_score": 29.27770689179167}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.vquantizer(x, get_losses=True)\n        return z_q_x, x, indices, (vq_loss, commit_loss)\n    else:\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        return z_e_x, x, None, None\n\n", "CodeBleu_score": 34.92835492296244}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        with torch.no_grad():\n            # Store current parameters\n            if context is None:\n                context = dict()\n            for k, v in self.named_parameters():\n                context[k] = v.detach().clone()\n                # Replace parameters with EMA weights\n                v.data.copy_(self.model_ema.get_buffer(k))\n            # Log switch\n            if context is not None:\n                context[\\\"ema_switch\\\"] = True\n    yield\n    # Restore parameters\n    if self.use_ema:\n        with torch.no_grad():\n            for k, v in self.named_parameters():\n                v.data.copy_(context[k])\n            # Log switch\n            if context is not None:\n                context[\\\"ema_switch\\\"] = False\n    # Log EMA parameters\n    if self.use_ema:\n        with torch.no_grad():\n            for k, v in self.model_ema.named_buffers():\n                context[k] = v.detach().clone()\n    return context\n\n    def get_trainable_parameters(self) -> Any:\n        yield from ()\n\n    def forward(self, batch: Any) -> Any:\n        raise NotImplementedError()\n\n    def get_input(self, batch: Any) -> Any:\n        raise NotImplementedError()\n\n    def get_output(self, batch: Any) -> Any:\n        raise NotImplementedError()\n\n    def get_loss(self, batch: Any, **kwargs) -> Any:\n        raise NotImplementedError()\n\n    def get_log(self, batch: Any, **kwargs) -> Any:\n        raise NotImplementedError()\n\n    def on_train_batch_end(self, batch: Any, **kwargs):\n        raise NotImplementedError()\n\n    def on_train_epoch_end(self, **kwargs):\n        raise NotImplementedError()\n\n    def on_test_epoch_end(self, **kwargs):\n        raise NotImplementedError()\n\n    def on_save_checkpoint(self, checkpoint: dict, **kwargs):\n        raise NotImplementedError()\n\n    def on_", "CodeBleu_score": 36.899479001662996}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        with self.ema_context():\n            yield\n    else:\n        yield\n\n", "CodeBleu_score": 12.283850562912775}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is None:\n            context = []\n        else:\n            context = list(context)\n        with contextlib.ExitStack() as stack:\n            stack.enter_context(self.model_ema.ema_scope(self))\n            yield stack\n    else:\n        yield\n\n", "CodeBleu_score": 31.59297677578687}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = (x + attn) * gate_msa\n    x = self.norm2(x)\n    if not self.pre_only:\n        x = modulate(x, shift_mlp, scale_mlp)\n        x = self.mlp(x)\n    return x\n\n", "CodeBleu_score": 37.46558696772049}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    if not self.pre_only:\n        x = x + self.attn.post_attention(attn)\n    if not self.scale_mod_only:\n        x = modulate(self.norm2(x), shift_mlp, scale_mlp)\n    if not self.pre_only:\n        x = self.mlp(x)\n    return x\n\n", "CodeBleu_score": 51.750222200360184}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn\n    x = self.norm2(x)\n    if not self.scale_mod_only:\n        x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x, gate_msa, shift_mlp, scale_mlp, gate_mlp\n\n", "CodeBleu_score": 40.572443264938094}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv, context_intermediates = context_block.pre_attention(context, c)\n    x_qkv, x_intermediates = x_block.pre_attention(x, c)\n    qkv = torch.cat([context_qkv, x_qkv], dim=-1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context_out = context_block.post_attention(attn, *context_intermediates)\n    x_out = x_block.post_attention(attn, *x_intermediates)\n    return context_out, x_out\n\n", "CodeBleu_score": 24.905283744153632}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv, context_intermediates = context_block.pre_attention(context, c)\n    x_qkv, x_intermediates = x_block.pre_attention(x, c)\n    qkv = torch.cat([context_qkv, x_qkv], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    context = context + context_intermediates[0] * attn[:, : context.shape[0], :]\n    x = x + x_intermediates[0] * attn[:, context.shape[0] :, :]\n    context = context_block.post_attention(attn, *context_intermediates)\n    x = x_block.post_attention(attn, *x_intermediates)\n    return context, x\n\n", "CodeBleu_score": 33.49376313580495}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv, intermediates = context_block.pre_attention(x, c)\n    qkv = torch.cat([qkv, x_block], dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    x = context_block.post_attention(attn, *intermediates)\n    return x, context\n\n", "CodeBleu_score": 11.122972494449549}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.flatten()\n    log_scale = math.log(10000)\n    position_embeddings = torch.zeros(timesteps.shape[0], embedding_dim)\n    for i in range(embedding_dim):\n        if i % 2 == 0:\n            position_embeddings[:, i] = torch.sin(timesteps * (i * math.pi / embedding_dim))\n        else:\n            position_embeddings[:, i] = torch.cos(timesteps * (i * math.pi / embedding_dim))\n    position_embeddings = position_embeddings * math.sqrt(log_scale)\n    if embedding_dim % 2!= 0:\n        position_embeddings = torch.cat([position_embeddings, torch.zeros(position_embeddings.shape[0], 1)], dim=1)\n    return position_embeddings\n\n", "CodeBleu_score": 29.328680148558195}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    if len(timesteps.shape)!= 1:\n        raise ValueError(\"timesteps must be one-dimensional\")\n    if embedding_dim % 2 == 1:\n        embedding_dim += 1\n    log_timestep_scale = math.log(10000) / (embedding_dim // 2)\n    embeddings = torch.zeros(timesteps.shape[0], embedding_dim)\n    for i in range(embedding_dim):\n        position = i // 2\n        embeddings[:, i] = torch.sin(timesteps[:, None] * (2 ** (position * log_timestep_scale)))\n        if position!= 0:\n            embeddings[:, i] += torch.cos(timesteps[:, None] * (2 ** ((position - 1) * log_timestep_scale)))\n    return embeddings\n\n", "CodeBleu_score": 35.36809134915235}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    if timesteps.ndim == 2:\n        timesteps = timesteps.squeeze(1)\n    if timesteps.ndim!= 1:\n        raise ValueError(\"Timesteps must be a 1-dimensional tensor.\")\n    max_timestep = timesteps.max()\n    min_timestep = timesteps.min()\n    if min_timestep < 0:\n        raise ValueError(\"Timesteps must be non-negative.\")\n    if max_timestep > 10000:\n        raise ValueError(\"Timesteps must be less than 10000.\")\n    if embedding_dim % 2!= 0:\n        raise ValueError(\"Embedding dimension must be even.\")\n    embedding_dim_div2 = embedding_dim // 2\n    embedding = torch.zeros(len(timesteps), embedding_dim)\n    embedding[:, 0::2] = torch.sin(timesteps.float() * (2 * math.pi / (max_timestep - min_timestep)))\n    embedding[:, 1::2] = torch.cos(timesteps.float() * (2 * math.pi / (max_timestep - min_timestep)))\n    embedding = embedding * math.log(10000) / embedding_dim_div2\n    return embedding\n\n", "CodeBleu_score": 30.161244264959624}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False, q_proj_weight=None, k_proj_weight=None, v_proj_weight=None, out_proj_weight=None, out_proj_bias=None, use_cache=False, self_attention=False, need_weights=False, q_proj_bias=None, k_proj_bias=None, v_proj_bias=None, static_kv=False, batch_first=False)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n", "CodeBleu_score": 63.52365732368385}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w) # b,c,hw\n\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n", "CodeBleu_score": 12.757288899204358}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n\n", "CodeBleu_score": 12.432097210731676}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    control = control.get(name, [])\n    if not control:\n        return h\n    if not isinstance(control, list):\n        control = [control]\n    if not isinstance(h, dict):\n        h = {\"\": h}\n    for k in control:\n        if k not in h:\n            h[k] = None\n    try:\n        h.update(control)\n    except RuntimeError as e:\n        logger.warning(f\\\"Failed to apply control {name} to {h} with error {e}\\\")\n    return h\n\n", "CodeBleu_score": 36.056593571505836}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        control_value = control.pop(name)\n        try:\n            h = h + control_value\n        except Exception as e:\n            logger.warning(f\\\"Failed to apply control {name} to {h.__class__.__name__}: {e}\\\")\n    return h\n\n", "CodeBleu_score": 41.18998907093129}, {"code": "def apply_control(h, control, name)\n\t\n    if name in control:\n        if len(control[name]) == 0:\n            return h\n        control_val = control[name].pop(0)\n        try:\n            h = h.add(control_val)\n        except Exception as e:\n            warnings.warn(f\\\"Could not apply control {name} to {h.shape}: {e}\\\")\n    return h\n\n\n", "CodeBleu_score": 41.958882773901074}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t  # one-hot encoding\n    one_hot = torch.zeros(shape[0], self.n_e, *shape[2:]).to(indices.device)\n    one_hot.scatter_(1, indices.unsqueeze(1), 1)\n    # get quantized latent vectors\n    z_q = torch.matmul(one_hot, self.embedding.weight).view(shape)\n    # preserve gradients\n    z_q = z_q + (z_q - z).detach()\n    return z_q\n\n", "CodeBleu_score": 25.804762166797428}, {"code": "def get_codebook_entry(self, indices, shape)\n\t  indices = indices.view(-1, 1)\n    one_hot_indices = torch.zeros(indices.shape[0], self.n_e).to(indices.device)\n    one_hot_indices.scatter_(1, indices, 1)\n    codebook_entry = torch.matmul(one_hot_indices, self.embedding.weight).view(shape)\n    codebook_entry = codebook_entry.permute(0, 3, 1, 2).contiguous()\n    return codebook_entry\n\n", "CodeBleu_score": 51.812910979988324}, {"code": "def get_codebook_entry(self, indices, shape)\n\t  if shape is not None:\n        indices = indices.view(-1, shape[1], shape[2], shape[3])\n        indices = indices.permute(0, 3, 1, 2)\n    indices = indices.unsqueeze(1)\n    one_hot = torch.zeros(indices.shape[0], self.n_e, indices.shape[2], indices.shape[3]).to(indices)\n    one_hot.scatter_(1, indices, 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight).view(indices.shape)\n    if shape is not None:\n        quantized = quantized.permute(0, 2, 3, 1).contiguous()\n        quantized = quantized.view(indices.shape[0], indices.shape[2], indices.shape[3], -1)\n    return quantized\n\n", "CodeBleu_score": 48.56349265312725}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    self.noises = nn.ModuleList()\n    for layer_idx in range(self.num_layers):\n        resolution = 2 ** ((layer_idx + 5) // 2)\n        shape = [1, 1, resolution, resolution]\n        self.noises.register_buffer(f'noise{layer_idx}', torch.randn(*shape))\n    return self.noises\n\n", "CodeBleu_score": 24.42488225110784}, {"code": "def make_noise(self)\n\t\n    self.noises = nn.ModuleList()\n    self.noises.append(self.constant_input.weight.repeat(1, 1, 4, 4).to(self.device))\n    for layer_idx in range(self.num_layers):\n        resolution = 2**((layer_idx + 5) // 2)\n        shape = [1, 1, resolution, resolution]\n        self.noises.append(torch.randn(*shape).to(self.device))\n    return self.noises\n\n", "CodeBleu_score": 30.626439612403416}, {"code": "def make_noise(self)\n\t\n    # initialize the list\n    noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n    # append the noise tensors\n    for i in range(3, self.log_size + 1):\n        size = 2 ** i\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n        noises.append(torch.randn(1, 1, size // 2, size // 2, device=self.device))\n    return noises\n\n", "CodeBleu_score": 40.012231222725184}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n    latent = self.style_mlp(latent)\n    latent = torch.mean(latent, dim=0, keepdim=True)\n    return latent\n", "CodeBleu_score": 50.058629810523556}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent = torch.randn(num_latent, self.num_style_feat, device=x.device)\n    latent = self.style_mlp(latent)\n    return latent.mean(0, keepdim=True)\n\n", "CodeBleu_score": 32.18068261649158}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent = torch.randn(num_latent, self.num_style_feat, device=x.device)\n    latent = self.style_mlp(latent)\n    return latent.mean(0, keepdim=True)\n\n", "CodeBleu_score": 32.18068261649158}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t    img = img.float()\n    for c in range(3):\n        img[c] += torch.rand(1) * shift\n    return torch.clamp(img, 0., 1.)\n\n", "CodeBleu_score": 23.20284309812545}, {"code": "def color_jitter(img, shift)\n\t    img = img.copy()\n    if np.random.rand() < shift:\n        r = np.random.rand() * 2 - 1\n        img[:, :, 0] += r\n    if np.random.rand() < shift:\n        g = np.random.rand() * 2 - 1\n        img[:, :, 1] += g\n    if np.random.rand() < shift:\n        b = np.random.rand() * 2 - 1\n        img[:, :, 2] += b\n    img = np.clip(img, 0, 1)\n    return img\n", "CodeBleu_score": 39.828759312205456}, {"code": "def color_jitter(img, shift)\n\t    img = img.copy()\n    r_shift = np.random.uniform(-shift, shift)\n    g_shift = np.random.uniform(-shift, shift)\n    b_shift = np.random.uniform(-shift, shift)\n    img[:, :, 0] += r_shift\n    img[:, :, 1] += g_shift\n    img[:, :, 2] += b_shift\n    img = np.clip(img, 0., 1.)\n    return img\n", "CodeBleu_score": 21.250396870447435}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.81975192363866}, {"code": "def feed_data(self, data)\n\t\n    if self.opt['isTrain']:\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n        if 'left_eye_locs' in data:\n            self.left_eye_locs = data['left_eye_locs'].to(self.device)\n        if 'right_eye_locs' in data:\n            self.right_eye_locs = data['right_eye_locs'].to(self.device)\n        if'mouth_locs' in data:\n            self.mouth_locs = data['mouth_locs'].to(self.device)\n    else:\n        self.lq = data['lq'].to(self.device)\n        self.gt = data['gt'].to(self.device)\n\n", "CodeBleu_score": 50.3226641014887}, {"code": "def feed_data(self, data)\n\t\n    self.input = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.target = data['gt'].to(self.device)\n    else:\n        self.target = None\n    if self.use_facial_disc:\n        self.left_eye_gt = data['left_eye_gt'].to(self.device)\n        self.right_eye_gt = data['right_eye_gt'].to(self.device)\n        self.mouth_gt = data['mouth_gt'].to(self.device)\n\n", "CodeBleu_score": 46.58391682212443}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    gt_img = self.gt\n    pyramid = [gt_img]\n    for i in range(self.opt['pyramid_depth']):\n        gt_img = F.interpolate(gt_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n        pyramid.insert(0, gt_img)\n    return pyramid\n\n", "CodeBleu_score": 25.16833573676728}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt.detach().clone()]\n    for i in range(self.opt['scale_pyramid']):\n        img_pyramid.insert(0, F.interpolate(\n            img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.384997066565834}, {"code": "def construct_img_pyramid(self)\n\t\n    gt_img = self.gt\n    img_pyramid = [gt_img]\n    for _ in range(self.num_pyramid_levels - 1):\n        gt_img = F.interpolate(gt_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n        img_pyramid.insert(0, gt_img)\n    return img_pyramid\n\n", "CodeBleu_score": 44.700407178765126}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['is_train']:\n        save_dir = self.opt['path']['model']\n        if not osp.exists(save_dir):\n            os.makedirs(save_dir)\n        save_filename = osp.join(save_dir, f'{self.opt[\"name\"]}_{epoch}_{current_iter}.pth')\n        logger = get_root_logger()\n        logger.info(f'Saving model at {save_filename}')\n        save_dict = {'epoch': epoch, 'current_iter': current_iter, 'opt': self.opt, 'net_g': self.net_g, 'net_g_ema': self.net_g_ema, 'net_d': self.net_d, 'net_d_left_eye': self.net_d_left_eye, 'net_d_right_eye': self.net_d_right_eye, 'net_d_mouth': self.net_d_mouth, 'optimizer_g': self.optimizer_g, 'optimizer_d': self.optimizer_d, 'optimizer_d_left_eye': self.optimizer_d_left_eye, 'optimizer_d_right_eye': self.optimizer_d_right_eye, 'optimizer_d_mouth': self.optimizer_d_mouth, 'train_state': self.train_state}\n        torch.save(save_dict, save_filename)\n", "CodeBleu_score": 34.77210210057158}, {"code": "def save(self, epoch, current_iter)\n\t\n    save_dir = self.opt['path']['models']\n    save_filename = 'latest.pt'\n    save_path = osp.join(save_dir, save_filename)\n    state = {\n        'net_g': self.net_g.state_dict(),\n        'net_g_ema': self.net_g_ema.state_dict() if hasattr(self, 'net_g_ema') else None,\n        'net_d': self.net_d.state_dict() if self.use_disc else None,\n        'net_d_left_eye': self.net_d_left_eye.state_dict() if self.use_facial_disc else None,\n        'net_d_right_eye': self.net_d_right_eye.state_dict() if self.use_facial_disc else None,\n        'net_d_mouth': self.net_d_mouth.state_dict() if self.use_facial_disc else None,\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'opt': self.opt,\n    }\n    torch.save(state, save_path)\n    logger = get_root_logger()\n    logger.info(f'Model saved to {save_path}.')\n", "CodeBleu_score": 37.208834878872224}, {"code": "def save(self, epoch, current_iter)\n\t\n    save_path = osp.join(self.opt['path']['model'], f'{self.opt[\"name\"]}_epoch_{epoch}_iter_{current_iter}.pth')\n    save_dict = {'net_g': self.net_g.state_dict(), 'net_g_ema': self.net_g_ema.state_dict(), 'net_d': self.net_d.state_dict()}\n    if self.use_facial_disc:\n        save_dict['net_d_left_eye'] = self.net_d_left_eye.state_dict()\n        save_dict['net_d_right_eye'] = self.net_d_right_eye.state_dict()\n        save_dict['net_d_mouth'] = self.net_d_mouth.state_dict()\n    save_dict['opt'] = self.opt\n    save_dict['train_state'] = self.train_state\n    torch.save(save_dict, save_path)\n\n", "CodeBleu_score": 34.45128061838138}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as error:\n            print(f\"Error deleting file or directory: {item_path}. Error: {error}\")\n\n", "CodeBleu_score": 49.246092203381245}, {"code": "def clean_folder(folder)\n\t\n    try:\n        for item in os.listdir(folder):\n            item_path = os.path.join(folder, item)\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n    except Exception as error:\n        print(f'Error while deleting files in folder {folder}: {error}')\n\n", "CodeBleu_score": 49.58324815529562}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as error:\n                print(f\"Error deleting file: {item_path} - {error}\")\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except Exception as error:\n                print(f\"Error deleting directory: {item_path} - {error}\")\n\n", "CodeBleu_score": 45.01396726754948}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    random_indices = random.sample(range(len(self.exps)), sample_size)\n    batch = [self.exps[i] for i in random_indices]\n    for i in random_indices:\n        self.exps.pop(i)\n    return batch\n\n", "CodeBleu_score": 48.415527258141886}, {"code": "def next_batch(self, sample_size=1)\n\t    if len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = [exp for exp in self.exps if exp not in batch]\n    return batch\n\n", "CodeBleu_score": 53.783743680009025}, {"code": "def next_batch(self, sample_size=1)\n\t    if len(self.exps) < sample_size:\n        sample_size = len(self.exps)\n    sampled_exps = random.sample(self.exps, sample_size)\n    for exp in sampled_exps:\n        self.exps.remove(exp)\n    return sampled_exps\n\n", "CodeBleu_score": 46.58529701206906}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\tll_configs = self.all_configs\n    dims = self.dims\n    num_all_configs = self.num_all_configs\n    metric = self.metric\n    cost_model = self.cost_model\n    best_iter = self.best_iter\n    random_exploration_ratio = self.random_exploration_ratio\n    tuning_space = self.tuning_space\n    logger = self.logger\n    logger.info(f\\\"Find estimated top configs\\\")\n    all_configs = {all_configs}\n    dims = {dims}\n    num_all_configs = {num_all_configs}\n    metric = {metric}\n    cost_model = {cost_model}\n    best_iter = {best_iter}\n    random_exploration_ratio = {random_exploration_ratio}\n    tuning_space = {tuning_space}\n\n    all_configs_flattened = []\n    for config in all_configs:\n        all_configs_flattened.append(dict_to_flattened_dict(config))\n\n    all_configs_flattened = np.array(all_configs_flattened)\n    all_configs_flattened = all_configs_flattened.astype(np.float32)\n    logger.info(f\\\"all_configs_flattened: {all_configs_flattened}\\\")\n\n    all_configs_flattened = all_configs_flattened[best_iter:]\n    logger.info(f\\\"all_configs_flattened: {all_configs_flattened}\\\")\n\n    all_configs_flattened = all_configs_flattened[:num_all_configs]\n    logger.info(f\\\"all_configs_flattened: {all_configs_flattened}\\\")\n\n    all_configs_flattened = all_configs_flattened.astype(np.float32)\n    logger.info(f\\\"all_configs_flattened: {all_configs_flattened}\\\")\n\n    all_configs_flattened_numerical = []\n    for config in all_configs_flattened:\n        config_numerical = []\n        for dim in dims:\n            config_numerical.append(config[dim])", "CodeBleu_score": 19.393634455376922}, {"code": "def find_estimated_top_configs(self)\n\tll_configs = [e['ds_config'] for e in self.exps]\n    num_all_configs = len(all_configs)\n    dims = dict_to_dims(self.tuning_space)\n    logger.info(f\\\"Create config dim: {dims}, all configs: {num_all_configs}\\\")\n\n    visited = set([])\n    trials = []\n    trial_pt = 0\n\n    init_num = min(INIT_NUM, num_all_configs)\n\n    for _ in range(init_num):\n        exp_feature = np.random.randint(num_all_configs)\n        while exp_feature in visited:\n            exp_feature = np.random.randint(num_all_configs)\n        trials.append(exp_feature)\n        visited.add(exp_feature)\n\n    cost_model = XGBoostCostModel(\\\"rank\\\")\n\n    evaluated_configs = []\n    evaluated_perf = []\n\n    train_ct = 0\n\n    random_exploration_ratio = 0.2  # do random exploration\n\n    for i in trials:\n        ds_config = all_configs[i]\n        ds_config = flatten_dict(ds_config, dims)\n        ds_config = [float(x) for x in ds_config]\n        ds_config = np.array(ds_config)\n        cost = cost_model.predict(ds_config)\n        evaluated_configs.append(ds_config)\n        evaluated_perf.append(cost)\n        train_ct += 1\n\n    logger.info(f\\\"Train cost model {train_ct} times\\\")\n\n    evaluated_configs = np.array(evaluated_configs)\n    evaluated_perf = np.array(evaluated_perf)\n\n    logger.info(f\\\"Evaluated configs: {evaluated_configs}\\\")\n    logger.info(f\\\"Evaluated perf: {evaluated_perf}\\\")\n\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        sorted_indices = np.argsort(evaluated_perf)\n    else:\n        sorted_indices = np.argsort(-evaluated_perf)\n\n    top_configs = evaluated_configs[sorted_indices[:self.num_top_configs]]\n    top_configs = dims_to_dict(top_configs, dims)\n\n    return top_configs\n\n", "CodeBleu_score": 24.831805301042024}, {"code": "def find_estimated_top_configs(self)\n\t Find the estimated top configurations based on a cost model\n    # Flatten each configuration in all_configs, collect numerical values, and convert them into a numpy array\n    all_configs_flattened = [flatten_config(config) for config in self.all_configs]\n    all_configs_flattened = np.array(all_configs_flattened)\n    # Predict estimates for these configurations using the cost model\n    estimates = self.cost_model.predict(all_configs_flattened)\n    # Sort the estimates in descending order\n    sorted_indices = np.argsort(estimates)[::-1]\n    # Return the indices of the top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 17.97041914115354}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.23325210077819}, {"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.23325210077819}, {"code": "def flatten(d, parent_key='', sep='_')\n\t    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.23325210077819}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\n            f\\\"Running the optimal DeepSpeed command found by autotuning: {self.optimal_cmd}\\\"\n        )\n        try:\n            p = subprocess.Popen(self.optimal_cmd)\n            p.wait()\n            logger.info(\\\"Autotuning completed successfully.\\\")\n        except Exception as e:\n            logger.info(\n                f\\\"Autotuning completed with error: {e}. Please check the log for details.\\\")\n    else:\n        logger.info(\\\"No optimal DeepSpeed command found by autotuning.\\\")\n\n\n\n", "CodeBleu_score": 47.75835905438539}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(f\\\"Running optimal command: {self.optimal_cmd}\\\")\n        process = subprocess.Popen(self.optimal_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        process.wait()\n        if process.returncode!= 0:\n            logger.error(f\\\"Command failed with return code {process.returncode}\\\")\n            logger.error(f\\\"Command output: {process.stdout.read()}\\\")\n            logger.error(f\\\"Command error: {process.stderr.read()}\\\")\n        else:\n            logger.info(\\\"Command finished successfully.\\\")\n    else:\n        logger.info(\\\"No optimal command found.\\\")\n    return\n\n", "CodeBleu_score": 40.28667706263181}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\\\"Running the optimal DeepSpeed configuration found by autotuning...\\\")\n        p = subprocess.Popen(self.optimal_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        out, err = p.communicate()\n        if p.returncode == 0:\n            logger.info(\\\"Successfully ran the optimal DeepSpeed configuration found by autotuning.\\\")\n        else:\n            logger.info(\\\"Failed to run the optimal DeepSpeed configuration found by autotuning.\\\")\n            logger.info(\\\"stdout: \\\" + out.decode(\\\"utf-8\\\"))\n            logger.info(\\\"stderr: \\\" + err.decode(\\\"utf-8\\\"))\n\n", "CodeBleu_score": 47.09818036196217}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t  exp_id = exp['exp_id']\n    result_dir = exp['result_dir']\n    master_port = exp['master_port']\n    exp['ds_config']['master_port'] = master_port\n    exp['ds_config']['train_batch_size'] = exp['ds_config']['train_batch_size'] * self.num_gpus_per_node\n    exp['ds_config']['train_micro_batch_size_per_gpu'] = exp['ds_config']['train_micro_batch_size_per_gpu'] * self.num_gpus_per_node\n    exp['ds_config']['train_micro_batch_size'] = exp['ds_config']['train_micro_batch_size_per_gpu'] * self.num_gpus_per_node\n    exp['ds_config']['train_steps'] = int(exp['ds_config']['train_steps'] * self.num_gpus_per_node)\n    exp['ds_config']['gradient_accumulation_steps'] = int(exp['ds_config']['gradient_accumulation_steps'] * self.num_gpus_per_node)\n    exp['ds_config']['gradient_clipping'] = exp['ds_config']['gradient_clipping'] * self.num_gpus_per_node\n    exp['ds_config']['optimizer']['lr'] = exp['ds_config']['optimizer']['lr'] * self.num_gpus_per_node\n    exp['ds_config']['optimizer']['weight_decay'] = exp['ds_config']['optimizer']['weight_decay'] * self.num_gpus_per_node\n    exp['ds_config']['optimizer']['max_grad_norm'] = exp['ds_config']['optimizer']['max_grad_norm'] * self.num_gpus_per_node\n    exp['ds_config']['optimizer']['momentum'] = exp['ds_config']['optimizer']['momentum'] * self.num_gpus_per_node\n    exp['ds_config']['optimizer']['adam_w_mode'] = exp['ds_config']['optimizer']['adam_w_mode']\n    exp['ds_config']['optimizer']['adam_w_decay'] = exp['ds_config']['optimizer']['adam_w_decay'] * self.num_gpus_per_node\n    exp['ds_", "CodeBleu_score": 9.324079566395088}, {"code": "def run_job(self, exp: dict, reservations)\n\t  result_dir = exp[\"result_dir\"] = os.path.join(self.results_dir, exp['name'])\n    if AUTOTUNING in exp[\"ds_config\"]:\n        metric_file = os.path.join(result_dir, \"metrics.json\")\n        exp[\"ds_config\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n    stderr_file = os.path.join(result_dir, \"stderr.log\")\n    model_info_file = os.path.join(result_dir, \"model_info.json\")\n    metric_file = os.path.join(result_dir, \"metrics.json\")\n\n    # skip existing experiments (except for the ones that were interrupted)\n    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n        if not was_interruptted(stderr_file):\n            err = search_error(stderr_file)\n            exp_id = exp[\"exp_id\"]\n            self.finished_experiments[exp_id] = (exp, err)\n            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                logger.info(f\"Skipping exp {exp['name']} whose result already exists\")\n                return\n\n    # update user arguments based on argument mappings\n    if self.arg_mappings is not None:\n        for k, v in self.arg_mappings.items():\n            if k in exp[\"args\"]:\n                exp[\"args\"][v] = exp[\"args\"].pop(k)\n\n    # set the master port\n    exp[\"args\"][\"master_port\"] = self.nodes[0].master_port\n\n    # set the result directory\n    exp[\"args\"][\"result_dir\"] = result_dir\n\n    # set the model info file\n    exp[\"args\"][\"model_info_file\"] = model_info_file\n\n    # start the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # store the running experiment details\n    exp_id = exp[\"exp_id\"]\n    self.running_experiments[exp_id] = (exp, time.time())\n\n", "CodeBleu_score": 34.94358156296162}, {"code": "def run_job(self, exp: dict, reservations)\n\t  exp_id = exp[\"exp_id\"]\n    exp_name = exp[\"name\"]\n    exp_dir = exp[\"exp_dir\"]\n    result_dir = exp[\"result_dir\"]\n    master_port = exp[\"master_port\"]\n    exp[\"result_dir\"] = result_dir\n    exp[\"master_port\"] = master_port\n    exp[\"ds_config\"][\"train_batch_size\"] = self.args.batch_size\n    exp[\"ds_config\"][\"train_micro_batch_size_per_gpu\"] = self.args.micro_batch_size\n    exp[\"ds_config\"][\"train_micro_batch_size_per_node\"] = self.args.micro_batch_size\n    exp[\"ds_config\"][\"train_num_steps\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_print\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_checkpoint\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_validation\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_test\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_validation\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_test\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_train\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_train_validation\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_train_test\"] = self.args.num_steps\n    exp[\"ds_config\"][\"train_num_steps_per_eval_train_train_train\"] = self.args.num_steps\n   ", "CodeBleu_score": 11.722759042478877}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if not ranks:\n            ranks = [dist.get_rank()]\n        if path is None:\n            path = os.path.join(self.args.results_dir, \\\"exp_results.json\\\")\n        with open(path, \\\"a\\\") as f:\n            for rank in ranks:\n                f.write(json.dumps({\\\"rank\\\": rank, \\\"message\\\": message}) + \\\"\\", "CodeBleu_score": 26.092440383639925}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized():\n        return\n    if ranks is None:\n        ranks = [dist.get_rank()]\n    if path is None:\n        path = os.path.join(self.args.results_dir, \\\"results.json\\\")\n    if self.args.log_results:\n        with open(path, \\\"a\\\") as f:\n            f.write(json.dumps({\\\"message\\\": message, \\\"rank\\\": ranks}, indent=4))\n            f.write(\\\"\\", "CodeBleu_score": 28.11541282704445}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = [dist.get_rank()]\n    if path is None:\n        path = self.args.results_path\n    if dist.get_world_size() == 1:\n        logger.info(message)\n        with open(path, \\\"a\\\") as fd:\n            fd.write(json.dumps({\\\"rank\\\": 0, \\\"message\\\": message}))\n            fd.write(\\\"\\", "CodeBleu_score": 23.016064727247205}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if layer_index not in self.layer_keys:\n        raise ValueError(f'Layer index {layer_index} not found in layer keys')\n\n    layer_prefix = self.layer_keys[layer_index]\n    layer_files = [fname for fname in self.mp_rank_files if layer_prefix in fname]\n    if len(layer_files) == 0:\n        return {}\n\n    tp_to_files_map = {}\n    for tp_index in range(self.tp_degree):\n        tp_to_files_map[tp_index] = []\n\n    for layer_file in layer_files:\n        tp_index = int(layer_file.split(layer_prefix)[1].split('.')[0])\n        assert tp_index < self.tp_degree\n        tp_to_files_map[tp_index].append(layer_file)\n\n    return tp_to_files_map\n\n", "CodeBleu_score": 34.84837658739808}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    assert layer_index in self.layer_keys.values()\n    layer_name = self.layer_keys[layer_index]\n    layer_file_prefix = f'{layer_name}.'\n    layer_file_list = [fname for fname in self.mp_rank_files if fname.startswith(layer_file_prefix)]\n    assert len(layer_file_list) > 0\n    layer_file_list.sort()\n    tp_to_layer_file_map = {}\n    for i in range(self.tp_degree):\n        tp_to_layer_file_map[i] = []\n    for fname in layer_file_list:\n        tp_index = int(fname.split(layer_file_prefix)[1].split('.')[0])\n        assert tp_index < self.tp_degree\n        tp_to_layer_file_map[tp_index].append(fname)\n    return tp_to_layer_file_map\n\n", "CodeBleu_score": 27.936381415458023}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = self.get_layer_files(layer_index=layer_index)\n\n    if len(layer_files) == 0:\n        return {}\n\n    layer_prefix = self.layer_keys[layer_index]\n    layer_files = [fname for fname in layer_files if fname.startswith(layer_prefix)]\n\n    assert len(layer_files) > 0\n    assert self.tp_degree > 0\n    assert len(layer_files) % self.tp_degree == 0\n\n    tp_file_map = {}\n    for i in range(self.tp_degree):\n        tp_file_map[i] = layer_files[i::self.tp_degree]\n\n    return tp_file_map\n\n\n", "CodeBleu_score": 38.33379392827617}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    info = {}\n    info['OS'] = platform.system()\n    info['Architecture'] = platform.architecture()[0]\n    info['Version'] = platform.version()\n    info['Processor'] = platform.processor()\n    info['Python version'] = sys.version.split()[0]\n    return info\n\n", "CodeBleu_score": 41.45496039705369}, {"code": "def system_info()\n\t\n    system = platform.system()\n    architecture = platform.architecture()\n    version = platform.version()\n    processor = platform.processor()\n    python_version = platform.python_version()\n    return {'system': system, 'architecture': architecture,'version': version, 'processor': processor, 'python_version': python_version}\n\n", "CodeBleu_score": 42.83095944238641}, {"code": "def system_info()\n\t\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version,\n    }\n\n", "CodeBleu_score": 76.1373025040652}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        return {\n            \"CUDA_device_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n            \"CUDA_status\": \"Available\",\n            \"CUDA_version\": torch.version.cuda,\n        }\n    else:\n        return {\"CUDA_status\": \"Not available\"}\n\n", "CodeBleu_score": 62.11384284028419}, {"code": "def cuda_info()\n\t\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    cuda_device_names = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    cuda_device_count = torch.cuda.device_count()\n    return {\n        \"cuda_available\": cuda_available,\n        \"cuda_version\": cuda_version,\n        \"cuda_device_names\": cuda_device_names,\n        \"cuda_device_count\": cuda_device_count,\n    }\n\n", "CodeBleu_score": 48.26879288547447}, {"code": "def cuda_info()\n\t\n    return torch.cuda.get_device_name(0), torch.cuda.is_available(), torch.version.cuda\n\n", "CodeBleu_score": 14.6235428821345}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\", None)\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\", None)\n    if class_name_key is None:\n        raise ValueError(\"class_name_key is required to compute the encoder accuracy\")\n    if class_id_to_name_mapping is None:\n        class_id_to_name_mapping = {}\n    class_name_to_id_mapping = {v: k for k, v in class_id_to_name_mapping.items()}\n    if class_name_key not in dataset_items[0]:\n        raise ValueError(f\"{class_name_key} is not found in the dataset items\")\n    if class_name_key not in class_id_to_name_mapping:\n        raise ValueError(f\"{class_name_key} is not found in the class ID to name mapping\")\n    accuracy_per_class = {}\n    for item in tqdm(dataset_items):\n        audio_path = item[\"audio_path\"]\n        embedding = encoder_manager.encode(audio_path)\n        if class_name_key in item:\n            true_class_name = item[class_name_key]\n            if true_class_name in class_name_to_id_mapping:\n                true_class_id = class_name_to_id_mapping[true_class_name]\n                predicted_class_id = encoder_manager.predict(embedding)\n                if true_class_id == predicted_class_id:\n                    if true_class_id not in accuracy_per_class:\n                        accuracy_per_class[true_class_id] = 0\n                    accuracy_per_class[true_class_id] += 1\n    for class_id, accuracy in accuracy_per_class.items():\n        print(f\"Accuracy for class {class_id_to_name_mapping[class_id]}: {accuracy / len(dataset_items):.2f}\")\n    print(f\"Average accuracy: {sum(accuracy_per_class.values()) / len(dataset_items):.2f}\")\n\n", "CodeBleu_score": 34.218264869936526}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    class_name_mapping = encoder_manager.config.class_name_mapping\n    criteria = encoder_manager.config.criteria\n    accuracy = 0\n    for item in dataset_items:\n        audio_file = item[0]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        if criteria and class_name_key in item and class_name_mapping:\n            true_class_name = item[class_name_key]\n            predicted_class_name = encoder_manager.predict_class_name(embedding, class_name_mapping)\n            if true_class_name == predicted_class_name:\n                accuracy += 1\n        elif criteria and class_name_key in item:\n            true_class_name = item[class_name_key]\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n            if true_class_name == predicted_class_name:\n                accuracy += 1\n        elif class_name_key in item:\n            true_class_name = item[class_name_key]\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n            if true_class_name == predicted_class_name:\n                accuracy += 1\n    accuracy /= len(dataset_items)\n    return accuracy\n\n", "CodeBleu_score": 24.415933210945216}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_name_mapping = encoder_manager.config.class_id_to_name_mapping\n    # Initialize the accuracy dictionary and class count dictionary\n    accuracy_dict = {}\n    class_count_dict = {}\n    # Iterate over the dataset items\n    for item in tqdm(dataset_items):\n        # Get the audio file path and the class name key from the item\n        audio_file_path = item[0]\n        class_name_key = item[1]\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file_path)\n        # If the class name key is available, get the class ID from the item\n        if class_name_key is not None:\n            class_id = item[2]\n        # If the class ID to class name mapping is available, use it to get the class name\n        elif class_id_to_name_mapping is not None:\n            class_name = class_id_to_name_mapping[class_id]\n        # If the class name key and class ID to class name mapping are not available, use the class name key to get the class name\n        else:\n            class_name = item[2]\n        # If the class name is not in the accuracy dictionary, add it with an empty list as the value\n        if class_name not in accuracy_dict:\n            accuracy_dict[class_name] = []\n            class_count_dict[class_name] = 0\n        # Predict the class label for the embedding\n        predicted_label = encoder_manager.predict_label(embedding)\n        # If the predicted label is the same as the class name, increment the accuracy for the class\n        if predicted_label == class_name:\n            accuracy_dict[class_name].append(1)\n        else:\n            accuracy_dict[class_name].append(0)\n        class_count_dict[class_name] += 1\n    # Calculate the average accuracy for each class and the overall average accuracy\n    overall_accuracy = 0\n    for class_name, accuracies in accuracy_dict.items():\n        accuracy = sum(accuracies) / len(accuracies)\n        print(f", "CodeBleu_score": 32.52667523436784}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Get the file name and file extension\n    file_name = os.path.splitext(os.path.basename(audio_path))[0]\n    file_extension = os.path.splitext(audio_path)[1]\n    # Create the output path\n    output_path = os.path.join(args.output_dir, os.path.basename(audio_path))\n    # Check if the output file already exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    # Remove silence from the audio\n    vad_model, vad_utils = get_vad_model_and_utils(args.vad_model, args.vad_threshold, args.vad_window_length, args.vad_overlap_length)\n    if vad_model is not None:\n        audio, sampling_rate = vad_utils.load_audio(audio_path, sampling_rate=args.sampling_rate)\n        if sampling_rate!= args.sampling_rate:\n            audio = librosa.resample(audio, sampling_rate, args.sampling_rate)\n            sampling_rate = args.sampling_rate\n        audio = remove_silence(audio, sampling_rate, vad_model, args.vad_threshold, args.vad_window_length, args.vad_overlap_length)\n    else:\n        audio, sampling_rate = librosa.load(audio_path, sr=args.sampling_rate)\n    # Save the processed audio\n    librosa.output.write_wav(output_path, audio, sampling_rate)\n    return output_path, True\n\n", "CodeBleu_score": 41.23067112268068}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_directory, output_directory)\n    output_path = pathlib.Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not force:\n        return output_path, False\n    else:\n        if not force:\n            if output_path.exists():\n                print(\"File already exists. Skipping.\")\n                return output_path, False\n        else:\n            if output_path.exists():\n                output_path.unlink()\n        audio_data = torch.load(audio_path)\n        audio = audio_data[\"audio\"]\n        audio = audio.transpose(0, 1)\n        audio = audio.squeeze(0)\n        audio = audio.numpy()\n        audio = torch.from_numpy(audio)\n        audio = audio.unsqueeze(0)\n        audio = audio.unsqueeze(0)\n        audio = audio.float()\n        audio = audio.to(device)\n        with torch.no_grad():\n            audio_len = audio.size(1)\n            audio_len = torch.tensor(audio_len).to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n            audio_len = audio_len.unsqueeze(0)\n            audio_len = audio_len.float()\n            audio_len = audio_len.to(device)\n           ", "CodeBleu_score": 29.62738895040261}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    output_path = pathlib.Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not force:\n        return output_path, False\n    else:\n        audio, sampling_rate = librosa.load(audio_path, sr=16000)\n        audio = remove_silence(audio, sampling_rate, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n        librosa.output.write_wav(output_path, audio, sampling_rate)\n        return output_path, True\n\n", "CodeBleu_score": 39.21812520065526}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n    copytree(input_dir, output_dir)\n    input_filenames = glob.glob(os.path.join(input_dir, f\"*.{file_ext}\"))\n    output_filenames = [\n        os.path.join(output_dir, os.path.basename(filename)) for filename in input_filenames\n    ]\n    func_args = zip(input_filenames, [output_sr] * len(input_filenames))\n    with Pool(n_jobs) as pool:\n        for filename, _ in tqdm(\n            pool.imap_unordered(resample_file, func_args), total=len(input_filenames)\n        ):\n            pass\n    print(f\"Resampling complete. Resampled files saved to {output_dir}.\")\n\n", "CodeBleu_score": 41.481759410364965}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    copytree(input_dir, output_dir)\n    filenames = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    if n_jobs == 1:\n        for filename in tqdm(filenames):\n            resample_file((filename, output_sr))\n    else:\n        with Pool(processes=n_jobs) as pool:\n            for _ in tqdm(pool.imap_unordered(resample_file, ((filename, output_sr) for filename in filenames)), total=len(filenames)):\n                pass\n\n", "CodeBleu_score": 43.818270209002456}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n    copytree(input_dir, output_dir)\n    file_paths = glob.glob(os.path.join(output_dir, f\"**/*.{file_ext}\"), recursive=True)\n    print(f\"Found {len(file_paths)} files with extension {file_ext}\")\n    if len(file_paths) == 0:\n        return\n    with Pool(n_jobs) as pool:\n        for filename in tqdm(file_paths):\n            pool.apply_async(resample_file, args=((filename, output_sr),))\n    print(f\"Completed resampling of {len(file_paths)} files with extension {file_ext} to {output_sr} Hz\")\n\n", "CodeBleu_score": 40.07707291122286}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   model.eval()\n    losses = []\n    with torch.no_grad():\n        for step, batch in enumerate(data_loader):\n            # load data\n            if use_cuda:\n                batch = {k: v.cuda() for k, v in batch.items()}\n            inputs, labels, lengths, _ = batch[c.inputs_key], batch[c.labels_key], batch[c.lengths_key], batch[c.mel_targets_key]\n            # compute loss\n            loss = criterion(model(inputs), labels, lengths, mel_targets=lengths)\n            losses.append(loss.item())\n            if step == 0:\n                plot_embeddings(model, inputs, labels, lengths, mel_targets=lengths, global_step=global_step)\n    return sum(losses) / len(losses)\n\n", "CodeBleu_score": 17.981018044391263}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   # set model to evaluation mode\n    model.eval()\n    # initialize variables to accumulate average loss\n    total_loss = 0.0\n    # iterate over batches\n    for i, batch in enumerate(data_loader):\n        # move data to GPU if available\n        if use_cuda:\n            batch = {k: v.cuda() for k, v in batch.items()}\n        # get inputs and labels\n        inputs, labels = batch[\\\"inputs\\\"], batch[\\\"labels\\\"]\n        # get embeddings\n        embeddings = model(inputs)\n        # reshape inputs and labels to group samples of each class\n        inputs = inputs.view(-1, inputs.size(-1))\n        labels = labels.view(-1)\n        # compute loss\n        loss = criterion(embeddings, labels)\n        # update average loss\n        total_loss += loss.item()\n        # log progress\n        if i % 100 == 0:\n            print(\\\"[EVAL] Step: {}, Batch: {}, Avg Loss: {:.4f}\\\".format(global_step, i, total_loss / (i + 1)))\n    # generate and log UMAP plot of embeddings from last batch\n    # compute average loss\n    avg_loss = total_loss / len(data_loader)\n    print(\\\"[EVAL] Step: {}, Avg Loss: {:.4f}\\\".format(global_step, avg_loss))\n    # generate and log UMAP plot of embeddings from last batch\n    plot_embeddings(model, data_loader.dataset.get_map_classid_to_classname(), global_step, \\\"eval\\\")\n    # return average loss\n    return avg_loss\n\n\n\n\n", "CodeBleu_score": 28.27936834440784}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t   # set the model in evaluation mode\n    model.eval()\n    # initialize the average evaluation loss\n    avg_eval_loss = 0.0\n    # initialize the total number of batches\n    num_batches = 0\n    # iterate over the data loader\n    for batch in data_loader:\n        # get the input and target data\n        inputs, targets = batch[\"input\"], batch[\"target\"]\n        # move the input and target tensors to the GPU if available\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        # reshape the input and target tensors to group samples of each class\n        inputs = inputs.reshape(-1, inputs.size(2), inputs.size(3))\n        targets = targets.reshape(-1, targets.size(2))\n        # compute the model output\n        with torch.no_grad():\n            outputs = model(inputs)\n        # compute the loss\n        loss = criterion(outputs, targets)\n        # update the average evaluation loss\n        avg_eval_loss += loss.item()\n        # increment the total number of batches\n        num_batches += 1\n    # compute the average evaluation loss\n    avg_eval_loss /= num_batches\n    # log the evaluation statistics\n    print(\n        f\" > Evaluation loss at global step {global_step}: {avg_eval_loss:.4f}\"\n    )\n    # generate and log the UMAP plot of the embeddings from the last batch\n    if global_step % c.log_every == 0:\n        plot_embeddings(\n            model,\n            data_loader,\n            global_step,\n            num_classes=len(train_classes),\n            num_gpus=num_gpus,\n        )\n    # return the average evaluation loss\n    return avg_eval_loss\n\n", "CodeBleu_score": 31.13264409253504}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class = None\n    if model_name == \"xtts\":\n        config_class = xtts_config.XTTSConfig\n    else:\n        config_class = find_module(model_name, CONFIG_PATHS)\n    if config_class is None:\n        raise ModuleNotFoundError(f\\\"No configuration class found for model {model_name}.\\\")\n    return config_class()\n\n", "CodeBleu_score": 26.241767456535257}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        from TTS.config.xtts_configs import XTtsConfig\n        return XTtsConfig\n    config_name = f\"{model_name.capitalize()}Config\"\n    config_path = find_module(config_name)\n    if config_path:\n        config_class = getattr(__import__(config_path, fromlist=[config_name]), config_name)\n        return config_class\n    raise ModuleNotFoundError(f\"Configuration class for model '{model_name}' not found\")\n\n", "CodeBleu_score": 37.116266309564395}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        from TTS.config.xtts_configs import XTtsConfig\n    else:\n        config_module = find_module(model_name, CONFIG_MODULE_PATHS)\n        if config_module is None:\n            raise ModuleNotFoundError(f\\\"No configuration class found for model {model_name}.\\\")\n        config_class = getattr(config_module, CONFIG_CLASS_NAME)\n    return config_class()\n\n", "CodeBleu_score": 29.471096871467168}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = os.path.expanduser(config_path)\n    config_dict = None\n    if config_path.endswith(\\\".json\\\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\\\".yml\\\") or config_path.endswith(\\\".yaml\\\"):\n        with open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise ValueError(f\\\" [!] Unknown config file format: {config_path}\\\")\n    config_dict = config_dict.get(\\\"config\\\") if config_dict.get(\\\"config\\\") is not None else config_dict\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n\n\n\n", "CodeBleu_score": 42.61846671821367}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    config_ext = os.path.splitext(config_path)[1]\n    if config_ext == \\\".yaml\\\" or config_ext == \\\".yml\\\":\n        with open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_ext == \\\".json\\\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\\\" [!] Unknown config file extension: {config_ext}\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n\n", "CodeBleu_score": 41.48262605227652}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\\\".yml\\\"):\n        with open(config_path, \\\"r\\\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\\\".json\\\"):\n        with open(config_path, \\\"r\\\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\\\" [!] Unknown file extension for {config_path}.\\\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class(config_dict)\n    return config\n\n\n", "CodeBleu_score": 34.59731946146283}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for f in files:\n            if contains is None or contains in f:\n                ext = os.path.splitext(f)[1]\n                if validExts is None or ext.lower() in validExts:\n                    yield os.path.join(root, f)\n\n", "CodeBleu_score": 32.61988040887639}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # initialize the list of files found in the path\n    filePaths = []\n    # check to see if the base path is a directory (or is a file)\n    if os.path.isdir(basePath):\n        # loop over the items in the directory\n        for (root, _, filenames) in os.walk(basePath):\n            # loop over the filenames in the directory\n            for filename in filenames:\n                # construct the full path to the file\n                path = os.path.join(root, filename)\n                # check to see if the file should be ignored based on the\n                # supplied list of valid extentions\n                if validExts is None or filename.endswith(validExts):\n                    # check to see if the file should be ignored based on the\n                    # supplied list of contains strings\n                    if contains is None or contains in filename:\n                        # add the fully-qualified file path to the list of\n                        # files found in the directory\n                        filePaths.append(path)\n    # otherwise, the supplied path is not a valid path to a directory\n    else:\n        # check to see if the file should be ignored based on the supplied\n        # list of valid extentions\n        if validExts is None or basePath.endswith(validExts):\n            # check to see if the file should be ignored based on the supplied\n            # list of contains strings\n            if contains is None or contains in basePath:\n                # add the fully-qualified file path to the list of files\n                # found in the directory\n                filePaths.append(basePath)\n    # return the list of files found in the path\n    return filePaths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.88836364554247}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    assert os.path.isdir(basePath), f\\\"Invalid base path: {basePath}\\\"\n    if validExts is None:\n        validExts = []\n    if contains is None:\n        contains = []\n    for dirName, subdirList, fileList in os.walk(basePath):\n        for fname in fileList:\n            if any(fname.endswith(ext) for ext in validExts):\n                if any(fname.find(cont)!= -1 for cont in contains):\n                    yield os.path.join(dirName, fname)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 34.64980264268942}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the required paths for checkpoint, config, and vocab.\"\n    print(\"Loading configuration...\")\n    config = XttsConfig.from_pretrained(xtts_config)\n    print(\"Loading model...\")\n    model = Xtts(config)\n    print(\"Loading model checkpoint and vocabulary...\")\n    model.load_pretrained(xtts_checkpoint, xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"Model loaded successfully.\")\n    return model\n\n", "CodeBleu_score": 48.97963171220634}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide all paths for the XTTS model checkpoint, config, and vocabulary.\"\n    print(\"Loading XTTS model...\")\n    xtts_config = XttsConfig.from_pretrained(xtts_config)\n    xtts_vocab = xtts_vocab\n    xtts_model = Xtts(xtts_config, xtts_vocab)\n    xtts_model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    if torch.cuda.is_available():\n        xtts_model = xtts_model.cuda()\n    print(\"XTTS model loaded.\")\n    return \"XTTS model loaded.\"\n\n", "CodeBleu_score": 50.25447383631471}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide a checkpoint, config, and vocab path.\"\n    else:\n        print(\"Loading XTTS model...\")\n        config = XttsConfig.from_pretrained(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n        if torch.cuda.is_available():\n            model.cuda()\n        print(\"XTTS model loaded successfully.\")\n        return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 48.35826648549606}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    assert x.size(1) == self.audio_config[\\\"sample_rate\\\"] * self.audio_config[\\\"duration\\\"]\n    num_frames = int(num_frames)\n    num_eval = int(num_eval)\n\n    if self.audio_config[\\\"use_torch_stft\\\"]:\n        x = self.get_torch_mel_spectrogram_class(self.audio_config)(x)\n\n    if num_frames > x.size(1):\n        num_frames = x.size(1)\n    x = x[:, :num_frames]\n\n    if x.size(1) % num_eval!= 0:\n        num_eval = x.size(1) // num_frames\n\n    if num_eval > 1:\n        x = x.unfold(1, num_frames, num_frames)\n\n    x = x.reshape(x.size(0), num_eval, num_frames, x.size(-1))\n    x = x.permute(0, 2, 1, 3)\n    x = x.reshape(-1, num_frames, x.size(-1))\n\n    x = x.to(self.device)\n    x = self.inference(x, l2_norm)\n\n    if return_mean:\n        x = x.mean(dim=1)\n\n    return x\n\n", "CodeBleu_score": 35.66108554707067}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    x = x.to(self.device)\n    if self.use_torch:\n        # x = self.get_torch_mel_spectrogram_class(self.audio_config)(x)\n        # x = x.to(self.device)\n        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), \"reflect\")\n        x = torch.nn.functional.conv1d(x, self.filter).squeeze(1)\n    else:\n        x = torchaudio.transforms.MelSpectrogram(\n            sample_rate=self.audio_config[\"sample_rate\"],\n            n_fft=self.audio_config[\"fft_size\"],\n            win_length=self.audio_config[\"win_length\"],\n            hop_length=self.audio_config[\"hop_length\"],\n            window_fn=torch.hamming_window,\n            n_mels=self.audio_config[\"num_mels\"],\n        )(x)\n    if self.use_torch:\n        x = x.to(self.device)\n    else:\n        x = x.to(self.device)\n    if self.use_torch:\n        x = x.to(self.device)\n    else:\n        x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x = x.to(self.device)\n    x", "CodeBleu_score": 26.70186058859305}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch:\n        x = self.get_torch_mel_spectrogram_class(self.audio_config)(x)\n        x = x.squeeze(0)\n    else:\n        x = torch.stft(\n            x,\n            n_fft=self.audio_config[\\\"fft_size\\\"],\n            hop_length=self.audio_config[\\\"hop_length\\\"],\n            win_length=self.audio_config[\\\"win_length\\\"],\n            window=torch.hamming_window(self.audio_config[\\\"win_length\\\"]),\n            center=True,\n            pad_mode=\\\"reflect\\\",\n            normalized=False,\n            onesided=True,\n        )\n        x = torch.sqrt(torch.mean(torch.square(x), dim=-1))\n    if self.use_torch:\n        x = x.squeeze(0)\n    if self.audio_config[\\\"num_mels\\\"]!= 128:\n        x = torch.nn.functional.interpolate(\n            x,\n            size=(self.audio_config[\\\"num_mels\\\"],),\n            mode=\\\"linear\\\",\n            align_corners=False,\n        )\n    x = x.transpose(1, 2)\n    if self.use_torch:\n        x = x.squeeze(0)\n    num_frames = int(np.ceil(x.size(1) / self.audio_config[\\\"hop_length\\\"]) * self.audio_config[\\\"hop_length\\\"])\n    x = x[:, :num_frames]\n    if x.size(1) < num_frames:\n        x = torch.nn.functional.pad(x, (0, num_frames - x.size(1)))\n    if self.use_torch:\n        x = x.squeeze(0)\n    x = x.reshape(1, x.size(0), x.size(1))\n    if self.use_torch:\n        x = x.squeeze(0)\n    x = self.inference(x, l2_norm)\n    if return_mean:\n        return x.mean(dim=0)\n    return x\n\n", "CodeBleu_score": 29.991257968668588}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.82383622378171}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.42502344469808}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.82383622378171}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    umap_model = umap.UMAP(n_components=2, random_state=42)\n    umap_model.fit(embeddings)\n    umap_embeddings = umap_model.transform(embeddings)\n    num_classes = np.unique(num_classes_in_batch).shape[0]\n    if num_classes > 10:\n        num_classes = 10\n    plt.figure(figsize=(10, 10))\n    plt.scatter(\n        umap_embeddings[:, 0],\n        umap_embeddings[:, 1],\n        s=10,\n        c=colormap[num_classes_in_batch % 10],\n    )\n    plt.title(\"UMAP projection\")\n    plt.axis(\"equal\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(\"umap.png\")\n    return plt.gcf()\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.10735066603024}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = len(np.unique(num_classes_in_batch))\n    if num_classes > 10:\n        num_classes = 10\n    umap_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0, n_components=2).fit_transform(embeddings)\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(\n        umap_embedding[:, 0],\n        umap_embedding[:, 1],\n        s=0.1,\n        c=colormap[num_classes_in_batch][:num_classes],\n    )\n    plt.title(\"UMAP projection\", fontsize=24)\n    plt.axis(\"off\")\n    return fig\n\n", "CodeBleu_score": 31.696569883372717}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    fig = plt.figure()\n", "CodeBleu_score": 0.6518953882372035}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim = []\n    for spkr in range(dvecs.shape[0]):\n        excl = torch.cat((dvecs[spkr, :utt], dvecs[spkr, utt + 1 :]))\n        excl = torch.mean(excl, 0)\n        new_centroids = []\n        for i, centroid in enumerate(centroids):\n            if i == spkr:\n                new_centroids.append(excl)\n            else:\n                new_centroids.append(centroid)\n        new_centroids = torch.stack(new_centroids)\n        sim.append(torch.clamp(torch.cosine_similarity(dvecs[spkr, utt].view(1, -1), new_centroids), min=1e-12))\n    return torch.stack(sim)\n\n", "CodeBleu_score": 32.04100434133937}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for spkr in range(dvecs.size(0)):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cosine_sim.append(F.cosine_similarity(dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1).clamp(min=1e-12))\n    return torch.stack(cosine_sim)\n\n", "CodeBleu_score": 24.939018377876348}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    new_centroids = []\n    for i, centroid in enumerate(centroids):\n        excl = torch.cat((dvecs[i, :], dvecs[i, 1:]))\n        excl = torch.mean(excl, 0)\n        new_centroids.append(excl)\n    new_centroids = torch.stack(new_centroids)\n    cosine_sim = torch.mm(dvecs, new_centroids.t())\n    cosine_sim = torch.clamp(cosine_sim, min=1e-12)\n    return cosine_sim\n", "CodeBleu_score": 24.91774190399284}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # pylint: disable=R0914\n    # pylint: disable=R0912\n    # pylint: disable=R0915\n    # pylint: disable=R0913\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0915\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0915\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0915\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0915\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0915\n    # pylint: disable=R0916\n    # pylint: disable=R0917\n    # pylint: disable=R0911\n    # pylint: disable=R0912\n    # pylint: disable=R0913\n    # pylint: disable=R0914\n    # pylint: disable=R0915\n    # pylint: disable=", "CodeBleu_score": 2.26010101010101}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # pylint: disable=R0914\n    losses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        loss_row = []\n        for utt_idx, utterance in enumerate(speaker):\n            # calculate the loss for each utterance\n            loss = self.w * torch.log(\n                torch.sum(\n                    torch.exp(\n                        self.b\n                        - cos_sim_matrix[spkr_idx][utt_idx]\n                        * self.w\n                        + cos_sim_matrix[spkr_idx][utt_idx]\n                    )\n                )\n            )\n            loss_row.append(loss)\n        loss_row = torch.stack(loss_row, dim=0)\n        losses.append(loss_row)\n    return torch.stack(losses, dim=0)\n", "CodeBleu_score": 28.14359693975198}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.size(0)):\n        loss = F.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    return torch.stack(losses)\n\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 33.06886864735405}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            excl = torch.cat((dvecs[j, :i], dvecs[j, i + 1:]))\n            excl = torch.mean(excl, 0)\n            new_centroids = torch.stack((excl, dvecs[j, i]))\n            cos_sim_row = torch.sigmoid(torch.mm(dvecs[j, i].unsqueeze(1).transpose(0, 1), new_centroids.transpose(0, 1)))\n            L_row.append(-torch.log(cos_sim_row[0]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 52.89818488473132}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n            # vector based cosine similarity for speed\n            cs_row = torch.clamp(torch.mm(utterance.unsqueeze(1).transpose(0, 1), new_centroids.transpose(0, 1)) / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)), 1e-6)\n            # compute contrast loss\n            L_row.append(-F.logsigmoid(cs_row[j]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 50.87722891645012}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            excl = torch.cat((dvecs[j, :i], dvecs[j, i + 1 :]))\n            excl = torch.mean(excl, 0)\n            new_centroids = self.calc_new_centroids(dvecs, centroids, j, i)\n            # vector based cosine similarity for speed\n            cs_row = torch.clamp(torch.mm(utterance.unsqueeze(1).transpose(0, 1), new_centroids.transpose(0, 1)) / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)), 1e-6)\n            # contrast loss\n            L_row.append(-F.logsigmoid(cs_row[j]))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 50.27270107352868}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if x.size(1) < 2:\n        raise ValueError(\\\"Input must have at least two utterances per speaker\\\")\n    if x.size(0) < 2:\n        raise ValueError(\\\"Input must have at least two speakers\\\")\n    # calculate the centroids\n    centroids = x.mean(1)\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # calculate the embedding loss\n    embed_loss = self.embed_loss(x, cos_sim_matrix)\n    # average the embedding loss\n    embed_loss = embed_loss.mean()\n    return embed_loss\n", "CodeBleu_score": 54.10025945964083}, {"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=E1103\n    # pylint: disable=E1101\n    # pylint: disable=E1102\n    # pylint: disable=", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size(1) < 2:\n        raise ValueError(\\\"Input must have at least two utterances per speaker\\\")\n    num_speakers, num_utts_per_speaker, dvec_feats = x.size()\n    if num_speakers < 2:\n        raise ValueError(\\\"Input must have at least two speakers\\\")\n    centroids = x.mean(dim=1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift cosine similarity matrix\n    cos_sim_matrix = (cos_sim_matrix - self.b) / self.w\n    # calculate loss\n    if self.loss_method == \\\"softmax\\\":\n        loss = self.embed_loss_softmax(x, cos_sim_matrix)\n    if self.loss_method == \\\"contrast\\\":\n        loss = self.embed_loss_contrast(x, cos_sim_matrix)\n    return loss.mean()\n\n", "CodeBleu_score": 42.92496164251395}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(extra_voice_dir):\n            for filename in filenames:\n                if filename.endswith(\".npz\"):\n                    voices[os.path.basename(dirpath)] = [os.path.join(dirpath, filename)]\n                elif filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n                    voices[os.path.basename(dirpath)] = [os.path.join(dirpath, filename)]\n    return voices\n\n", "CodeBleu_score": 26.052111381314358}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for voice_dir in glob(os.path.join(extra_voice_dir, \"**\"), recursive=True):\n            if os.path.isdir(voice_dir):\n                voice_name = voice_dir.split(os.sep)[-1]\n                voices[voice_name] = []\n                for file in glob(os.path.join(voice_dir, \"*.npz\")):\n                    voices[voice_name].append(file)\n                for file in glob(os.path.join(voice_dir, \"*.wav\")):\n                    voices[voice_name].append(file)\n                for file in glob(os.path.join(voice_dir, \"*.mp3\")):\n                    voices[voice_name].append(file)\n                if not voices[voice_name]:\n                    for file in glob(os.path.join(voice_dir, \"*.wav\")):\n                        voices[voice_name].append(file)\n                    for file in glob(os.path.join(voice_dir, \"*.mp3\")):\n                        voices[voice_name].append(file)\n    return voices\n\n", "CodeBleu_score": 22.716034078644114}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for dirpath, _, filenames in os.walk(extra_voice_dir):\n            for filename in filenames:\n                if filename.endswith(\".npz\"):\n                    voice_dir = os.path.basename(dirpath)\n                    voices.setdefault(voice_dir, []).append(os.path.join(dirpath, filename))\n                elif filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n                    voice_dir = os.path.basename(dirpath)\n                    voices.setdefault(voice_dir, []).append(os.path.join(dirpath, filename))\n    return voices\n\n", "CodeBleu_score": 26.842872324895673}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   # get voice paths\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found. Available voices are: {voices.keys()}\\\")\n    if len(voices[voice]) == 0:\n        raise ValueError(f\\\"No audio files found for voice {voice}.\\\")\n    if len(voices[voice]) > 1:\n        raise ValueError(f\\\"Multiple audio files found for voice {voice}.\\\")\n    voice_path = voices[voice][0]\n    if voice_path.endswith(\\\".npz\\\"):\n        # load npz file\n        semantic, coarse, fine = load_npz(voice_path)\n        return semantic, coarse, fine\n    else:\n        # generate npz file and load\n        npz_path = voice_path.replace(\\\"wav\\\", \\\"npz\\\")\n        if not os.path.exists(npz_path):\n            logger.info(f\\\"Generating npz file for voice {voice_path}\\\")\n            convert_audio(voice_path, npz_path)\n        return load_npz(npz_path)\n\n", "CodeBleu_score": 40.836292264711524}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   voices = get_voices(extra_voice_dirs=extra_voice_dirs)\n    if voice == \\\"random\\\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found. Available voices: {voices.keys()}\\\".format(voice))\n    if len(voices[voice]) == 0:\n        raise ValueError(f\\\"No voice found for {voice}.\\\".format(voice))\n    if len(voices[voice]) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice}.\\\".format(voice))\n    voice_path = voices[voice][0]\n    if voice_path.endswith(\\\".npz\\\"):\n        semantic, coarse, fine = load_npz(voice_path)\n        return semantic, coarse, fine\n    else:\n        logger.info(f\\\"Generating voice {voice} for {model} from {voice_path}.\\\")\n        semantic, coarse, fine = model.generate_voice(voice_path)\n        np.savez(voice_path, semantic_prompt=semantic, coarse_prompt=coarse, fine_prompt=fine)\n        return semantic, coarse, fine\n\n\n\n\n", "CodeBleu_score": 38.51154291785345}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   voices = get_voices(extra_voice_dirs=extra_voice_dirs)\n    if voice == \\\"random\\\":\n        return None, None, None\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found. Available voices: {voices.keys()}\\\")\n    if len(voices[voice]) == 0:\n        raise ValueError(f\\\"No voice found for {voice}.\\\")\n    if len(voices[voice]) > 1:\n        raise ValueError(f\\\"Multiple voices found for {voice}.\\\")\n    voice_path = voices[voice][0]\n    if voice_path.endswith(\\\".npz\\\"):\n        return load_npz(voice_path)\n    # generate npz file\n    logger.info(f\\\"Generating npz file for {voice_path}.\\\")\n    convert_audio(voice_path)\n    return load_npz(voice_path)\n\n", "CodeBleu_score": 35.58911609334586}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, _, num_heads, d_head = pos_score.size()\n    zero_tensor = torch.zeros_like(pos_score)\n    zero_tensor = zero_tensor.view(batch_size, num_heads, d_head, 1)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, -1)\n    pos_score = pos_score[:, :, :-1]\n    return pos_score\n", "CodeBleu_score": 30.89856322135251}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_length, num_heads, _ = pos_score.size()\n    pos_score_shape = (batch_size, seq_length, num_heads, 1)\n    pos_score = pos_score.view(*pos_score_shape)\n    zero_tensor = torch.zeros(pos_score_shape, device=pos_score.device)\n    zero_tensor = zero_tensor.expand_as(pos_score)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score.reshape(batch_size, -1, num_heads, 2)\n    pos_score = pos_score[:, :, :, 1:]\n    pos_score = pos_score.reshape(batch_size, -1, num_heads)\n    return pos_score\n\n", "CodeBleu_score": 33.7564407898262}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length, _ = pos_score.size()\n    pos_score = pos_score.view(batch_size, num_heads, seq_length, 1)\n    zero_tensor = torch.zeros_like(pos_score)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score[:, :, :, 1:].view(batch_size, num_heads, seq_length, -1)\n    return pos_score\n\n", "CodeBleu_score": 31.677006549540078}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t   batch_size = context.size(0)\n    max_len = max(lens)\n    context_padded = context.new_zeros(batch_size, max_len, context.size(2))\n    context_padded.fill_(self.padding_idx)\n    for i in range(batch_size):\n        context_padded[i, :lens[i]] = context[i, :lens[i]]\n    context_padded = context_padded.to(context.device)\n    context_padded = context_padded.transpose(1, 2)\n    context_padded = self.dropout(context_padded)\n    for conv in self.convolutions:\n        context_padded = conv(context_padded)\n        context_padded = F.relu(context_padded)\n        context_padded = self.dropout(context_padded)\n    if not self.use_linear:\n        return context_padded\n    context_padded = context_padded.transpose(1, 2)\n    context_padded = self.dropout(context_padded)\n    if self.lstm_type!= \"\":\n        context_padded, _ = self.bilstm(context_padded)\n        context_padded = self.dropout(context_padded)\n    context_padded = self.dense(context_padded)\n    return context_padded\n\n", "CodeBleu_score": 26.537640911025424}, {"code": "def run_padded_sequence(self, context, lens)\n\t   # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    #", "CodeBleu_score": 1.459942084942085}, {"code": "def run_padded_sequence(self, context, lens)\n\t   # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-statements\n    #", "CodeBleu_score": 1.459942084942085}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_results = fn(sorted_context)\n    sorted_results, _ = nn.utils.rnn.pad_packed_sequence(sorted_results, batch_first=True)\n    results = sorted_results[torch.argsort(sorted_indices)]\n    return results\n", "CodeBleu_score": 18.869168067762242}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    result = fn(sorted_context)\n    result, _ = nn.utils.rnn.pad_packed_sequence(result, batch_first=True)\n    result = result[sorted_indices]\n    return result\n\n", "CodeBleu_score": 15.27578252732349}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t   sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_result = fn(sorted_context)\n    sorted_result, _ = nn.utils.rnn.pad_packed_sequence(sorted_result, batch_first=True)\n    _, original_indices = torch.sort(sorted_indices)\n    result = sorted_result[original_indices]\n    return result\n", "CodeBleu_score": 17.377120399789916}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre[0].remove_weight_norm()\n    for block in self.conv_blocks:\n        block[0].remove_weight_norm()\n\n", "CodeBleu_score": 33.88695421713756}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for i in range(len(self.conv_blocks)):\n        self.conv_blocks[i][0].remove_weight_norm()\n        self.conv_blocks[i][1].remove_weight_norm()\n\n", "CodeBleu_score": 40.31394303553177}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        for layer in block:\n            if hasattr(layer, \\\"remove_weight_norm\\\"):\n                layer.remove_weight_norm()\n", "CodeBleu_score": 46.64168943309364}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t   name = mx_name.lower()\n    name = name.replace('se_','se_resnext')\n    name = name.replace('senet_','senet1_0')\n    name = name.replace('inceptionv3', 'inception_v3')\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 33.99948988421511}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   # convert input name to lowercase\n    name = mx_name.lower()\n    # apply specific replacements for'se_','senet_', and 'inceptionv3' prefixes\n    name = name.replace('se_','senet_')\n    name = name.replace('senet_','senet_')\n    name = name.replace('inceptionv3', 'inception_v3')\n    # prefix the name with 'gluon_'\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 33.98244997601225}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   if'se_' in mx_name:\n        return 'gluon_se_' + mx_name.replace('se_', '').lower()\n    if'senet_' in mx_name:\n        return 'gluon_senet_' + mx_name.replace('senet_', '').lower()\n    if 'inceptionv3' in mx_name:\n        return 'gluon_inceptionv3'\n    return 'gluon_' + mx_name.lower()\n\n", "CodeBleu_score": 23.692693806722616}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t  if self.is_training and self.dist_num_replicas > 1:\n        num_samples = self.num_samples // self.dist_num_replicas\n        num_samples = math.ceil(num_samples) if num_samples % self.dist_num_replicas!= 0 else num_samples\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is not None:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 48.70174229980314}, {"code": "def _num_samples_per_worker(self)\n\t  if self.repeats > 1:\n        num_samples = math.ceil(self.num_samples / self.repeats)\n    else:\n        num_samples = self.num_samples\n    if self.batch_size:\n        num_samples = (num_samples // self.batch_size) * self.batch_size\n    if self.dist_num_replicas > 1:\n        num_samples = math.ceil(num_samples / self.dist_num_replicas)\n    return int(num_samples)\n", "CodeBleu_score": 40.0303707833087}, {"code": "def _num_samples_per_worker(self)\n\t  if self.repeats == 0:\n        return self.num_samples\n    if self.is_training or self.dist_num_replicas > 1:\n        return math.ceil(self.num_samples / self.global_num_workers)\n    return self.num_samples\n\n", "CodeBleu_score": 21.25635520100928}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if not os.path.exists(folder):\n        raise FileNotFoundError(f\\\"Folder {folder} does not exist\\\")\n    if not os.path.isdir(folder):\n        raise NotADirectoryError(f\\\"{folder} is not a folder\\\")\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = load_class_map(folder, leaf_name_only=leaf_name_only)\n    images, targets = [], []\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.lower().endswith(tuple(types)):\n                images.append(os.path.join(root, file))\n                targets.append(class_to_idx[root.split(os.sep)[-1]])\n    if sort:\n        images, targets = zip(*sorted(zip(images, targets), key=natural_key))\n    return images, targets\n\n", "CodeBleu_score": 33.318015917587175}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if not os.path.isdir(folder):\n        raise ValueError(f\\\"{folder} is not a valid folder\\\")\n    if types is None:\n        types = get_img_extensions()\n    if not isinstance(types, (list, tuple, set)):\n        raise ValueError(f\\\"types must be a list, tuple or set, got {type(types)} instead\\\")\n    if not isinstance(class_to_idx, dict):\n        class_to_idx = load_class_map(folder, leaf_name_only)\n    img_paths = []\n    img_labels = []\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if os.path.splitext(file)[1].lower() in types:\n                img_paths.append(os.path.join(root, file))\n                img_labels.append(root.split(os.sep)[-1])\n    if sort:\n        img_paths, img_labels = zip(*sorted(zip(img_paths, img_labels), key=natural_key))\n    img_paths = list(img_paths)\n    img_labels = list(img_labels)\n    img_labels = [class_to_idx[label] for label in img_labels]\n    return img_paths, img_labels, class_to_idx\n\n", "CodeBleu_score": 38.558184471554746}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t  if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = load_class_map(folder)\n    if sort:\n        class_to_idx = {k: v for k, v in sorted(class_to_idx.items(), key=natural_key)}\n    if leaf_name_only:\n        class_to_idx = {k: v for k, v in class_to_idx.items() if k.split(os.path.sep)[-1] == k}\n    return _find_images_and_targets(folder, types, class_to_idx)\n\n", "CodeBleu_score": 21.019426348566146}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return random.randint(0, 2**32)\n", "CodeBleu_score": 50.83553154363633}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    return random.randint(0, 2**32 - 1)\n\n", "CodeBleu_score": 52.16829408764257}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is None:\n        rank = int(os.environ.get('WDS_RANK', 0))\n        seed = int(os.environ.get('WDS_SEED', rank))\n    else:\n        seed = worker_info.seed\n    return seed\n\n", "CodeBleu_score": 48.583193512423925}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in ('L', 'RGB'):\n        return img\n    if _PIL_VER >= (5, 0):\n        return img.point(lambda p: p + add if p < thresh else p, '1')\n    else:\n        return img.point(lambda p: p + add if p < thresh else p)\n\n", "CodeBleu_score": 24.6235790927177}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in {'L', 'RGB'}:\n        return img\n    if img.mode == 'L':\n        lut = [i + add if i < thresh else i for i in img.getdata()]\n        return Image.new(img.mode, img.size, tuple(lut))\n    if img.mode == 'RGB':\n        lut = [\n            (i[0] + add if i[0] < thresh else i[0], i[1] + add if i[1] < thresh else i[1], i[2] + add if i[2] < thresh else i[2])\n            for i in img.getdata()\n        ]\n        return Image.new(img.mode, img.size, tuple(lut))\n\n", "CodeBleu_score": 34.65656680216191}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in {'L', 'RGB'}:\n        return img\n    if img.mode == 'L':\n        lut = [min(i + add, 255) for i in range(256)]\n    else:\n        lut = []\n        for i in range(256):\n            lut.append((i + add, i + add, i + add))\n    return img.point(lut, 'L')\n\n", "CodeBleu_score": 42.89403365329983}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    names = []\n    weights = []\n    for name, prob in transforms.items():\n        names.append(name)\n        weights.append(prob)\n    weights = np.array(weights)\n    weights /= np.sum(weights)\n    return names, weights\n\n", "CodeBleu_score": 35.077630653558636}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    names = []\n    probs = []\n    for name, prob in transforms.items():\n        names.append(name)\n        probs.append(prob)\n    probs = np.array(probs)\n    probs = probs / np.sum(probs)\n    return names, probs\n\n", "CodeBleu_score": 55.81301394450423}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms = list(transforms.items())\n    total = sum(p for t, p in transforms)\n    probs = [p / total for t, p in transforms]\n    return transforms, probs\n\n", "CodeBleu_score": 26.956877975402104}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    ws = self._calc_blended_weights(mixing_weights, m)\n    for w in ws:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img = Image.blend(img, img_aug, w)\n    return img\n\n", "CodeBleu_score": 47.953487792391634}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    ws = self._calc_blended_weights(mixing_weights, m)\n    for w in ws:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img = Image.blend(img, img_aug, w)\n    return img\n\n", "CodeBleu_score": 47.953487792391634}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    ws = self._calc_weights(mixing_weights, m)\n    img_mix = np.zeros_like(img_orig)\n    for w in ws:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mix = img_mix + (img_aug * w)\n    img_mix = np.clip(img_mix, 0, 1)\n    img = Image.fromarray(np.uint8(img_mix))\n    return img\n\n", "CodeBleu_score": 52.17376843219096}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tf isinstance(x, (tuple, list)):\n        assert len(x) == n, f'Expected {n} channels, but got {len(x)}'\n        return x\n    else:\n        return tuple(repeat(x, n))\n", "CodeBleu_score": 19.302159947427338}, {"code": "def adapt_to_chs(x, n)\n\tf not isinstance(x, (tuple, list)):\n        return tuple(repeat(x, n))\n    if len(x) == n:\n        return x\n    if len(x) == 1:\n        _logger.warning(f\\\"Adapted tuple {x} to length {n} by repeating {x[0]}.\\\")\n        return tuple(repeat(x[0], n))\n    _logger.warning(f\\\"Adapted tuple {x} to length {n} by averaging and repeating. Original length: {len(x)}.\\\")\n    return tuple(repeat(np.mean(x, axis=0), n))\n\n", "CodeBleu_score": 39.14493418633393}, {"code": "def adapt_to_chs(x, n)\n\tf isinstance(x, (list, tuple)):\n        if len(x) == n:\n            return x\n        elif len(x) < n:\n            _logger.warning(f\\\"Adapted tuple {x} to match {n} channels by repeating the mean of {x}.\\\")\n            return tuple(repeat(np.mean(x, axis=0), n))\n        else:\n            assert len(x) == n\n            return x\n    else:\n        return tuple(repeat(x, n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.652686743123656}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t worker_seeding == 'all':\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise ValueError('Cannot initialize worker seeding with all worker seeding for non-distributed data loading.')\n        worker_id = worker_info.id\n        seed = worker_info.seed\n        seed_torch = seed * 2 ** 32 + worker_id\n        seed_numpy = seed * 2 ** 32 + worker_id\n        random.seed(seed_torch)\n        np.random.seed(seed_numpy)\n    elif callable(worker_seeding):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise ValueError('Cannot initialize worker seeding with a callable worker seeding for non-distributed data loading.')\n        worker_id = worker_info.id\n        seed = worker_info.seed\n        worker_seeding(worker_id, seed)\n    elif isinstance(worker_seeding, str):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise ValueError('Cannot initialize worker seeding with a string worker seeding for non-distributed data loading.')\n        worker_id = worker_info.id\n        seed = worker_info.seed\n        if worker_seeding == 'all':\n            seed_torch = seed * 2 ** 32 + worker_id\n            seed_numpy = seed * 2 ** 32 + worker_id\n            random.seed(seed_torch)\n            np.random.seed(seed_numpy)\n        elif worker_seeding == 'numpy':\n            seed_numpy = seed * 2 ** 32 + worker_id\n            np.random.seed(seed_numpy)\n        elif worker_seeding == 'torch':\n            seed_torch = seed * 2 ** 32 + worker_id\n            random.seed(seed_torch)\n        else:\n            raise ValueError('Unrecognized worker seeding type.')\n    else:\n        raise ValueError('Unrecognized worker seeding type.')\n", "CodeBleu_score": 38.79355997881453}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\trker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id!= worker_id:\n        return\n    seed = worker_info.seed\n    if worker_seeding == 'all':\n        seed = seed * 2 + worker_info.id\n    elif callable(worker_seeding):\n        seed = worker_seeding(seed)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = seed * 2 + worker_info.id\n        else:\n            seed = int(worker_seeding)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n", "CodeBleu_score": 48.099490000791825}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\trker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id!= worker_id:\n        return\n    seed = worker_info.seed\n    if worker_seeding == 'all':\n        seed = seed * 2 + worker_id\n    elif callable(worker_seeding):\n        seed = worker_seeding(seed, worker_id)\n    elif isinstance(worker_seeding, str):\n        seed = seed * 2 + worker_id if worker_seeding == 'all' else seed\n    else:\n        raise TypeError('worker_seeding must be a string or a callable.')\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n", "CodeBleu_score": 43.68669991178203}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        tf.math.minimum(image_height, image_width) * 0.875, tf.int32)\n    padding = tf.cast(\n        (padded_center_crop_size - tf.minimum(image_height, image_width)) / 2,\n        tf.int32)\n    padded_shape = tf.cast(\n        tf.concat([\n            tf.shape(image_bytes)[:2],\n            tf.constant([padded_center_crop_size], dtype=tf.int32)\n        ],\n                   axis=0),\n        tf.int32)\n    image = tf.image.decode_and_crop_jpeg(image_bytes, [padding, padding, 0],\n                                          padded_shape)\n    image = tf.image.resize(image, [image_size, image_size], resize_method)\n    return image\n\n", "CodeBleu_score": 41.95865606328646}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image = tf.image.decode_and_crop_jpeg(image_bytes, [0, 0, shape[0], shape[1]], channels=3)\n    padded_center_crop_size = tf.cast(tf.math.ceil(tf.math.divide(image_size, 2.0)), tf.int32)\n    offset_height = ((shape[0] - image_size) + 1) // 2\n    offset_width = ((shape[1] - image_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, image_size, image_size)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 33.46359972830125}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image = tf.image.decode_and_crop_jpeg(image_bytes, [0, 0, shape[0], shape[1]], channels=3)\n    padded_center_crop_size = tf.cast(\n        (tf.cast(image_size, tf.float32) + tf.cast(CROP_PADDING, tf.float32)) / 2.0, tf.int32)\n    offset_height = ((shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((shape[1] - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n", "CodeBleu_score": 42.54682360229632}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.image.resize([image], [image_size, image_size], interpolation)[0]\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n", "CodeBleu_score": 36.64646935974326}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.cast(image, tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 31.97202119587172}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_and_crop_jpeg(image_bytes, [0, 0, IMAGE_SIZE, IMAGE_SIZE], channels=3)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.image.resize_images(image, [image_size, image_size], interpolation)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n", "CodeBleu_score": 37.99354733412353}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 73.57471023696938}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\\\"No eval_dataloader specified. Please specify one.\\\")\n    eval_dataloader.dataset.set_model(model)\n    eval_dataloader.dataset.set_output_file(output_file)\n    eval_dataloader.dataset.set_batch_size(self.config.batch_size)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set_shuffle(False)\n    eval_dataloader.dataset.set_collate_fn(self.collate_fn)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set_shuffle(False)\n    eval_dataloader.dataset.set_collate_fn(self.collate_fn)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set_shuffle(False)\n    eval_dataloader.dataset.set_collate_fn(self.collate_fn)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set_shuffle(False)\n    eval_dataloader.dataset.set_collate_fn(self.collate_fn)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set_shuffle(False)\n    eval_dataloader.dataset.set_collate_fn(self.collate_fn)\n    eval_dataloader.dataset.set_num_workers(self.config.num_workers)\n    eval_dataloader.dataset.set_pin_memory(self.config.pin_memory)\n    eval_dataloader.dataset.set", "CodeBleu_score": 39.54982234137917}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\\\"No eval_dataloader specified.\\\")\n    if not isinstance(eval_dataloader, DataLoader):\n        raise ValueError(\\\"eval_dataloader should be a DataLoader, but got {}.\\\".format(\n            type(eval_dataloader)))\n    self.predictor.pred_dir = self.output_dir\n    self.predictor.save_dir = self.output_dir\n    self.predictor.output_file = output_file\n    self.predictor.save_predictions(model, eval_dataloader)\n    results = self.evaluator(self.config)\n    return results\n\n", "CodeBleu_score": 47.56771091148264}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t   self.predictor.predict(model, eval_dataloader, output_file)\n    results = self.metric.compute_metrics(output_file)\n    self.metric.print_computed_metrics(results)\n    return results\n\n", "CodeBleu_score": 31.296897312255677}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f} - rank5: {:.4f} - miou: {:.4f}'.format(rank1, rank5, miou))\n", "CodeBleu_score": 28.338008680076392}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f} - rank5: {:.4f} - miou: {:.4f}'.format(rank1, rank5, miou))\n\n\n def _eval_predictions(self, outputs, targets):\n    \"\"\"The function evaluates the predictions of a model on the DiDeMo dataset.", "CodeBleu_score": 28.739919525848094}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f} - rank5: {:.4f} - miou: {:.4f}'.format(rank1, rank5, miou))\n", "CodeBleu_score": 28.338008680076392}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    miou = 0\n    for segment, data in zip(segments, data):\n        # Compute IoUs for each segment-data pair\n        ious = [self._iou(segment, datum) for datum in data]\n        # Compute ranks for each segment-data pair\n        ranks = [self._rank(segment, datum) for datum in data]\n        # Calculate rank1 and rank5\n        if ranks[0] == 1:\n            rank1 += 1\n        if ranks[0] == 1 or ranks[0] == 2:\n            rank5 += 1\n        # Calculate mean IoU\n        miou += np.mean(ious)\n    return rank1/len(segments), rank5/len(segments), miou/len(segments)\n\n", "CodeBleu_score": 25.575205074196695}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0.0\n    rank5 = 0.0\n    miou = 0.0\n    for i, segment in enumerate(segments):\n        gt = data[i]\n        iou = []\n        for pred in segment:\n            iou.append(self._iou(pred, gt))\n        iou = np.array(iou)\n        rank1 += self._rank(segment, gt)\n        rank5 += self._rank(np.array(segment)[:5], gt)\n        miou += np.mean(iou)\n    return rank1/len(segments), rank5/len(segments), miou/len(segments)\n\n", "CodeBleu_score": 28.867237888639742}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Calculate IoU for each segment-data pair\n    ious = [self._iou(pred, gt) for pred, gt in zip(segments, data)]\n    # Calculate rank for each segment-data pair\n    ranks = [self._rank(pred, gt) for pred, gt in zip(segments, data)]\n    # Calculate average rank for top 1 and top 5 predictions\n    rank1 = np.mean(ranks[:1])\n    rank5 = np.mean(ranks[:5])\n    # Calculate average IoU for top 1 and top 5 predictions\n    iou1 = np.mean(ious[:1])\n    iou5 = np.mean(ious[:5])\n    # Calculate average IoU for all predictions\n    mean_iou = np.mean(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 16.743167428774903}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda()\n    for batch in tqdm(eval_dataloader):\n        batch = tuple(t.cuda() for t in batch)\n        with torch.no_grad():\n            outputs = model(*batch)\n        scores.extend(outputs)\n    return scores\n\n", "CodeBleu_score": 37.85652117300397}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.cuda(0)\n    for batch in tqdm(eval_dataloader):\n        batch = tuple(t.cuda(0) for t in batch)\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if self.config.model_type!= 'distilbert':\n                inputs['labels'] = batch[3]\n            outputs = model(**inputs)\n            logits = outputs[0]\n            logits = logits.detach().cpu().numpy()\n            scores.extend(logits)\n    return np.array(scores)\n\n", "CodeBleu_score": 38.69880655055174}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.cuda()\n    for batch in tqdm(eval_dataloader):\n        batch = tuple(t.cuda() for t in batch)\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n            outputs = model(**inputs)\n            self.scores.extend(outputs[0].cpu().numpy())\n    self.scores = np.array(self.scores)\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump(self.scores, f)\n    return self.scores\n\n", "CodeBleu_score": 33.872293335313465}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = np.concatenate(scores, axis=0)\n    scores = np.mean(scores, axis=0)\n    return scores\n", "CodeBleu_score": 33.412385589073565}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    if len(scores[0]) == 0:\n        return []\n    scores = np.concatenate(scores, axis=0)\n    self.full_scores = []\n    return np.dot(scores[0], scores[1])\n\n", "CodeBleu_score": 45.43268979759857}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = []\n    return np.dot(full_scores[0], full_scores[1])\n\n", "CodeBleu_score": 43.10426912939108}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(\n                os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 35.894741350406214}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(\n                os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 35.894741350406214}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if output_file is not None:\n        with open(\n                os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 35.894741350406214}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\\\"loss\\\", 0) for log in logging_outputs)\n    loss_count = sum(log.get(\\\"ntokens\\\", 0) for log in logging_outputs)\n    sample_size_sum = sum(log.get(\\\"sample_size\\\", 0) for log in logging_outputs)\n    sample_size = sample_size_sum if sample_size_sum > 0 else 1\n    metrics.log_scalar(\\\"loss\\\", loss_sum / loss_count / sample_size)\n\n", "CodeBleu_score": 39.05017139500181}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\\\"loss\\\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\\\"sample_size\\\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"loss\\\", loss_sum / sample_size, sample_size)\n\n", "CodeBleu_score": 48.3346830552295}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\\\"loss\\\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\\\"sample_size\\\", 0) for log in logging_outputs)\n    metrics.log_scalar(\\\"loss\\\", loss_sum / sample_size, sample_size)\n\n", "CodeBleu_score": 48.3346830552295}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t# TODO: add a config to decide how to handle the attention mask.\n    # TODO: add a config to decide how to handle the token type IDs.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens.\n    # TODO: add a config to decide how to handle the visual tokens.\n    # TODO: add a config to decide how to handle the text tokens", "CodeBleu_score": 2.431943651246906}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tbsz, seq_len = cmasks.size()\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask = torch.zeros((bsz, seq_len), dtype=torch.bool)\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids = torch.zeros((bsz, seq_len), dtype=torch.long)\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[cmasks] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[~cmasks] = 1\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[vmasks] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[~vmasks] = 1\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[attention_mask == 0] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[token_type_ids == 0] = 1\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[attention_mask == 0] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[token_type_ids == 0] = 1\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[attention_mask == 0] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[token_type_ids == 0] = 1\n    # The attention mask is 1 for visual tokens and 0 for text tokens.\n\n    attention_mask[attention_mask == 0] = 1\n    # The token type IDs are 0 for visual tokens and 1 for text tokens.\n\n    token_type_ids[token_type_", "CodeBleu_score": 21.70601885654333}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tattention_mask = cmasks.clone()\n    attention_mask.masked_fill_(attention_mask.ne(1), -10000)\n    # if vmasks is not None:\n    #     attention_mask.masked_fill_(vmasks.ne(1), -10000)\n    # else:\n    #     attention_mask.masked_fill_(cmasks.ne(1), -10000)\n    token_type_ids = torch.zeros(cmasks.size(), dtype=torch.long, device=cmasks.device)\n    token_type_ids.masked_fill_(cmasks.ne(1), 1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 12.71747234690428}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t # TODO: support `input_ids` and `input_video_embeds` are None.\n    # TODO: support `attention_mask` and `token_type_ids` are None.\n    # TODO: support `input_ids` is a list of tensors.\n    # TODO: support `input_video_embeds` is a list of tensors.\n    # TODO: support `attention_mask` is a list of tensors.\n    # TODO: support `token_type_ids` is a list of tensors.\n    # TODO: support `input_ids` is a list of strings.\n    # TODO: support `input_video_embeds` is a list of strings.\n    # TODO: support `attention_mask` is a list of strings.\n    # TODO: support `token_type_ids` is a list of strings.\n    # TODO: support `input_ids` is a list of list of tensors.\n    # TODO: support `input_video_embeds` is a list of list of tensors.\n    # TODO: support `attention_mask` is a list of list of tensors.\n    # TODO: support `token_type_ids` is a list of list of tensors.\n    # TODO: support `input_ids` is a list of list of strings.\n    # TODO: support `input_video_embeds` is a list of list of strings.\n    # TODO: support `attention_mask` is a list of list of strings.\n    # TODO: support `token_type_ids` is a list of list of strings.\n    # TODO: support `input_ids` is a list of list of list of tensors.\n    # TODO: support `input_video_embeds` is a list of list of list of tensors.\n    # TODO: support `attention_mask` is a list of list of list of tensors.\n    # TODO: support `token_type_ids` is a list of list of list of tensors.\n    # TODO: support `input_ids` is a list of list of list of strings.\n    # TODO: support `input_video_embeds` is a list of list of list of strings.\n    # TODO: support `attention_mask` is a list of list of list of strings.\n    # TODO: support `token_type_ids` is a list of list of list of strings.\n    # TODO: support `input", "CodeBleu_score": 5.36986301369863}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t # The following code is copied from\n\n    # https://github.com/huggingface/transformers/blob/v4.27.1/src/transformers/generation_utils.py#L1004-L1023\n\n    # with some modifications.\n\n    # The original code is licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n\n    # you may not use this file except in compliance with the License.\n\n    # You may obtain a copy of the License at\n\n    #\n\n    #     http://www.apache.org/licenses/LICENSE-2.0\n\n    #\n\n    # Unless required by applicable law or agreed to in writing, software\n\n    # distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n\n    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n    # See the License for the specific language governing permissions and\n\n    # limitations under the License.\n\n    #\n\n    # The code is modified to fit our use case.\n\n    #\n\n    # The original code is licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n\n    # you may not use this file except in compliance with the License.\n\n    # You may obtain a copy of the License at\n\n    #\n\n    #     http://www.apache.org/licenses/LICENSE-2.0\n\n    #\n\n    # Unless required by applicable law or agreed to in writing, software\n\n    # distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n\n    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n    # See the License for the specific language governing permissions and\n\n    # limitations under the License.\n\n    #\n\n    # The code is modified to fit our use case.\n\n    #\n\n    # The original code is licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n\n    # you may not use this file except in compliance with the License.\n\n    # You may obtain a copy of the License at\n\n    #\n\n    #     http://www.apache.org/licenses/LICENSE-2.0\n\n    #\n\n    # Unless required by applicable law or agreed to in writing, software\n\n   ", "CodeBleu_score": 5.36986301369863}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t # adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1) + 2  # [CLS] [SEP] text [SEP]\n    if attention_mask is not None:\n        attention_mask = attention_mask.repeat(1, 1, 1)\n        attention_mask[:, :, 0, :] = 1  # [CLS]\n        attention_mask[:, :, combined_length - 1, :] = 1  # [SEP]\n        attention_mask[:, :, combined_length - 2, :] = 1  # [SEP]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.repeat(1, 1, 1)\n        token_type_ids[:, :, 0, :] = 1  # [CLS]\n        token_type_ids[:, :, combined_length - 1, :] = 1  # [SEP]\n        token_type_ids[:, :, combined_length - 2, :] = 1  # [SEP]\n\n    # prepare the inputs for text generation\n    model_kwargs[\"input_ids\"] = input_ids\n    model_kwargs[\"input_video_embeds\"] = input_video_embeds\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n    return model_kwargs\n\n", "CodeBleu_score": 35.670261938252665}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tnd the index of LoadImageFromFile\n    load_img_idx = -1\n    for i, transform in enumerate(cfg.test_pipeline):\n        if isinstance(transform, LoadImageFromFile):\n            load_img_idx = i\n            break\n    if load_img_idx == -1:\n        raise ValueError('LoadImageFromFile is not in the test_pipeline.')\n    # update the transform type\n    cfg.test_pipeline[load_img_idx] = InferencerLoader(\n        cfg.test_pipeline[load_img_idx])\n    # remove 'img_id' from meta_keys\n    if 'img_id' in cfg.test_pipeline[load_img_idx].meta_keys:\n        cfg.test_pipeline[load_img_idx].meta_keys.remove('img_id')\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 37.52116021691826}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tmove 'img_id' from meta_keys if present\n    meta_keys = cfg.test_pipeline.pop('meta_keys', [])\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    # find the index of 'LoadImageFromFile' transform\n    load_img_idx = None\n    for i, transform in enumerate(cfg.test_pipeline):\n        if isinstance(transform, LoadImageFromFile):\n            load_img_idx = i\n            break\n    if load_img_idx is None:\n        raise RuntimeError('`LoadImageFromFile` is not found in the test pipeline.')\n    # update the transform type to'mmdet.InferencerLoader'\n    cfg.test_pipeline[load_img_idx] = InferencerLoader(\n        cfg.test_pipeline[load_img_idx])\n    return Compose(cfg.test_pipeline)\n", "CodeBleu_score": 33.78302816125875}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tmove 'img_id' key from meta_keys if present\n    if 'img_id' in cfg.test_pipeline:\n        cfg.test_pipeline.meta_keys.remove('img_id')\n    # Find the index of the 'LoadImageFromFile' transform\n    load_img_idx = cfg.test_pipeline.index(LoadImageFromFile())\n    # Update the transform type to'mmdet.InferencerLoader'\n    cfg.test_pipeline[load_img_idx].type ='mmdet.InferencerLoader'\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 17.877968886679415}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = sorted(glob.glob(os.path.join(inputs, '*')))\n        else:\n            raise TypeError('Input must be a directory or a list of files')\n    elif isinstance(inputs, (np.ndarray, torch.Tensor)):\n        inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 46.46414246144279}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if not os.path.exists(inputs):\n            raise FileNotFoundError(f'Cannot find the directory: {inputs}')\n        if not os.path.isdir(inputs):\n            raise ValueError(f'The input path {inputs} is not a directory')\n        inputs = glob.glob(os.path.join(inputs, '*'))\n    elif isinstance(inputs, (tuple, list)):\n        pass\n    else:\n        raise TypeError(f'Invalid input type: {type(inputs)}')\n    return inputs\n\n", "CodeBleu_score": 32.95157512675479}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        inputs = list(Path(inputs).glob('*.*'))\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 34.06571514200964}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    # The chunked data is a list of data\n    chunked_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                img = input['img']\n            elif 'img_path' in input:\n                img = mmcv.imread(input['img_path'])\n            else:\n                raise KeyError('img or img_path is not found in input')\n            data = self.pipeline(img)\n        else:\n            data = input\n        chunked_data.append(data)\n        if len(chunked_data) == chunk_size:\n            yield chunked_data\n            chunked_data = []\n    if chunked_data:\n        yield chunked_data\n\n", "CodeBleu_score": 41.187733948272445}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunked_data = []\n    for input in inputs:\n        if isinstance(input, dict) and 'img' in input or 'img_path' in input:\n            chunked_data.append(self.pipeline(input))\n        else:\n            chunked_data.append(input)\n        if len(chunked_data) == chunk_size:\n            yield chunked_data\n            chunked_data = []\n    if chunked_data:\n        yield chunked_data\n\n", "CodeBleu_score": 26.61214710689232}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    data_iter = iter(inputs)\n    while True:\n        data_chunk = []\n        for _ in range(chunk_size):\n            try:\n                data = next(data_iter)\n            except StopIteration:\n                break\n            if 'img' in data or 'img_path' in data:\n                data = self.pipeline(data)\n            data_chunk.append(data)\n        if not data_chunk:\n            break\n        yield data_chunk\n\n", "CodeBleu_score": 31.932336618733142}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if return_datasamples is False and pred_out_dir!= '':\n        warnings.warn('Saving datasamples is not supported when return_datasamples is False.')\n\n    if print_result is True:\n        print(f'Prediction results of {self.__class__.__name__}:')\n        for pred in preds:\n            print(pred)\n\n    results = {'predictions': [], 'visualization': []}\n    for pred, vis in zip(preds, visualization):\n        if return_datasamples is False:\n            results['predictions'].append(pred.to_dict())\n        else:\n            results['predictions'].append(pred)\n\n        if vis is not None:\n            results['visualization'].append(vis)\n\n    return results\n", "CodeBleu_score": 46.40204222814794}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    results = dict()\n    if not no_save_pred and pred_out_dir!= '':\n        if return_datasamples:\n            warnings.warn('Saving datasamples is not supported when return_datasamples is True.')\n        else:\n            self.save_predictions(pred_out_dir, preds, visualization)\n    if print_result:\n        self.print_results(preds, visualization)\n    if return_datasamples:\n        results['predictions'] = preds\n        if visualization is not None:\n            results['visualization'] = visualization\n    else:\n        results['predictions'] = self.to_dict(preds)\n        if visualization is not None:\n            results['visualization'] = visualization\n    return results\n\n", "CodeBleu_score": 47.12462113500425}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir!= '':\n        if return_datasamples:\n            warnings.warn('Saving datasamples is not supported when'\\\n                         'return_datasamples is True.')\n        self._init_pred_dir(pred_out_dir)\n    results = {'predictions': [], 'visualization': []}\n    if return_datasamples:\n        results['datasamples'] = []\n    if print_result:\n        print('Predictions:')\n    for pred in preds:\n        if isinstance(pred, DetDataSample):\n            pred = pred.pred_to_dict()\n        if isinstance(pred, dict):\n            if return_datasamples:\n                results['datasamples'].append(pred)\n            results['predictions'].append(pred)\n        else:\n            raise ValueError(f'Unsupported type for predictions: {type(pred)}')\n        if print_result:\n            print(pred)\n    if visualization is not None:\n        results['visualization'] = visualization\n    return results\n\n", "CodeBleu_score": 48.06547555412693}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline.copy()\n    if len(test_pipeline) < 2:\n        raise ValueError('test_pipeline should contain at least 2 transforms')\n    # replace the first transform in the test pipeline\n    first_transform = test_pipeline.pop(0)\n    if first_transform.type == 'LoadImageFromFile':\n        first_transform.type = 'LoadImageFromWebp'\n    elif first_transform.type == 'LoadImageFromWebp':\n        first_transform.type = 'LoadImageFromNDArray'\n    else:\n        raise ValueError(f'Unsupported type of first transform in test_pipeline: {first_transform.type}')\n    test_pipeline.insert(0, first_transform)\n    # replace the last transform in the test pipeline\n    last_transform = test_pipeline.pop(-1)\n    if last_transform.type == 'LoadImageFromFile':\n        last_transform.type = 'LoadImageFromWebp'\n    elif last_transform.type == 'LoadImageFromWebp':\n        last_transform.type = 'LoadImageFromNDArray'\n    else:\n        raise ValueError(f'Unsupported type of last transform in test_pipeline: {last_transform.type}')\n    test_pipeline.append(last_transform)\n    test_pipeline.append(LoadImageFromNDArray())\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 22.732152930334138}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0]['type'] ='mmdet.Resize'\n    test_pipeline[-1]['type'] ='mmdet.PackDetInputs'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 16.020623139431503}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline.copy()\n    test_pipeline[0]['type'] ='mmdet.Resize'\n    test_pipeline[-1]['type'] ='mmdet.PackDetInputs'\n    test_pipeline[-1]['meta'] = cfg.data.test.get('meta', {})\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 23.67009089335371}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n    data = dict(\n        img=img,\n        img_id=frame_id,\n        img_shape=img.shape[:2],\n        ori_shape=img.shape[:2],\n        video_len=video_len)\n    data = test_pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 33.212294649790806}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n\n    # prepare data\n    data = dict(\n        img=img,\n        img_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_shape=img.shape[:2],\n        video_len=video_len,\n    )\n\n    # build the data pipeline\n    data = test_pipeline(data)\n\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n\n    return results\n\n", "CodeBleu_score": 29.116923271599575}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, original_shape=img.shape, img_id=0,\n                video_len=video_len)\n    # build the data pipeline\n    data = test_pipeline(data)\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 25.113349760195664}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n", "CodeBleu_score": 49.224850661546036}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    upper_activation_string = activation_string.upper()\n    if upper_activation_string == 'LINEAR':\n        return tf.identity\n    if upper_activation_string == 'RELU':\n        return tf.nn.relu\n    if upper_activation_string == 'TANH':\n        return tf.tanh\n    if upper_activation_string == 'SIGMOID':\n        return tf.sigmoid\n    if upper_activation_string == 'GELU':\n        return gelu\n    if upper_activation_string == 'SWISH':\n        return swish\n    if upper_activation_string == 'RELU6':\n        return tf.nn.relu6\n    if upper_activation_string == 'HARD_SIGMOID':\n        return hard_sigmoid\n    if upper_activation_string == 'SOFTPLUS':\n        return tf.nn.softplus\n    if upper_activation_string == 'LEAKY_RELU':\n        return leaky_relu\n    if upper_activation_string == 'ELU':\n        return tf.nn.elu\n    if upper_activation_string == 'NONE':\n        return None\n    if upper_activation_string == 'PRELU':\n        return tf.nn.relu\n    if upper_activation_string == 'RELU_USE_THRESHOLD':\n        return relu_use_threshold\n    raise ValueError(\\\"Unsupported activation: %s\\\" % upper_activation_string)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 57.15215335444452}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string == \\\"relu\\\":\n        return tf.nn.relu\n    if activation_string == \\\"gelu\\\":\n        return gelu\n    if activation_string == \\\"tanh\\\":\n        return tf.tanh\n    raise ValueError(\\\"Unsupported activation function %s\\\" %\n                         activation_string)\n\n\n\n\n", "CodeBleu_score": 41.87250671393314}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string == \\\"relu\\\":\n        return tf.nn.relu\n    if activation_string == \\\"gelu\\\":\n        return gelu\n    if activation_string == \\\"tanh\\\":\n        return tf.tanh\n    raise ValueError(\\\"Unsupported activation: %s\\\" % (activation_string))\n\n", "CodeBleu_score": 44.828817080715986}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, 1 - dropout_prob)\n\n", "CodeBleu_score": 54.23477033608736}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n        return output\n\n", "CodeBleu_score": 82.89511352687609}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.84090845772291}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=2)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    n = [batch_size, from_seq_length, to_seq_length]\n    attention_mask = tf.ones(n)\n    return attention_mask * to_mask\n\n", "CodeBleu_score": 42.20537198822659}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=2)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 55.77723835788818}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=2)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    n = [batch_size, from_seq_length, to_seq_length]\n    return tf.reshape(to_mask, n)\n\n", "CodeBleu_score": 40.23494294813993}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads,\n                                         width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 49.59282999401731}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\\\"Input tensor must have at least 2 dimensions.\\\")\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 68.68525415635254}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\\\"Input tensor must have at least 2 dimensions.\\\")\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n", "CodeBleu_score": 68.68525415635254}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 76.20079198293527}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\"))):\n        if i == 0:\n            continue\n        guid = line[0]\n        text_a = tokenization.convert_to_unicode(line[1])\n        text_b = tokenization.convert_to_unicode(line[2])\n        label = tokenization.convert_to_unicode(line[3])\n        if label == \\\"contradictory\\\":\n            label = \\\"contradiction\\\"\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 49.7555743372749}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\"))):\n        if i == 0:\n            continue\n        guid = str(i)\n        text_a = tokenization.convert_to_unicode(line[1])\n        text_b = tokenization.convert_to_unicode(line[2])\n        label = tokenization.convert_to_unicode(line[3])\n        if label == \\\"contradictory\\\":\n            label = \\\"contradiction\\\"\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 50.67051316467435}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\"))):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (self.task_name, i)\n        text_a = tokenization.convert_to_unicode(line[1])\n        text_b = tokenization.convert_to_unicode(line[2])\n        label = tokenization.convert_to_unicode(line[0])\n        if label == \\\"contradictory\\\":\n            label = \\\"contradiction\\\"\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n", "CodeBleu_score": 54.632697079645645}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(labels=label_ids, predictions=predictions,\n        weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\\\"eval_accuracy\\\": accuracy, \\\"eval_loss\\\": loss}\n\n", "CodeBleu_score": 50.79773722547301}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    labels = tf.reshape(labels, [-1])\n    labels = tf.cast(labels, tf.int32)\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    next_sentence_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, bert_config.embedding_table, transpose_b=True)\n    logits = tf.nn.bias_add(logits, next_sentence_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    next_sentence_loss = -tf.reduce_mean(\n        tf.reduce_sum(one_hot_labels * log_probs, axis=[-1]))\n    return (next_sentence_loss, log_probs)\n\n", "CodeBleu_score": 35.14210234270626}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n        # We apply one more non-linear transformation before the output layer.\n        # This matrix is not used after pre-training.\n        with tf.variable_scope(\\\"transform\\\"):\n            input_tensor = tf.layers.dense(\n                input_tensor,\n                units=bert_config.hidden_size,\n                activation=modeling.get_activation(bert_config.hidden_act),\n                kernel_initializer=modeling.create_initializer(\n                    bert_config.initializer_range))\n            input_tensor = modeling.layer_norm(input_tensor)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        output_bias = tf.get_variable(\n            \\\"output_bias\\\",\n            shape=[bert_config.vocab_size],\n            initializer=tf.zeros_initializer())\n        logits = tf.matmul(input_tensor, bert_config.embedding_table,\n                           transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n\n        # The loss is computed in the `next_sentence` function so that the loss\n        # is accumulated for the host CPU and not the GPU.\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs,\n                                          axis=[-1])\n        loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, log_probs)\n\n\n\n\n", "CodeBleu_score": 60.19542583760593}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n    output_weights = tf.get_variable(\n        \\\"output_weights\\\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(\n            bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \\\"output_bias\\\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, log_probs)\n\n\n\n\n", "CodeBleu_score": 83.60406958513639}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length,\n                             [batch_size])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 53.61552094126423}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length,\n                              [batch_size])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 53.58363526502367}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length,\n                             [batch_size])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 53.61552094126423}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32. So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 81.53206367966708}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \\\"\\\".join(tokenizer.tokenize(orig_answer_text)).strip()\n    tok_start_position = None\n    tok_end_position = None\n    if tok_answer_text:\n      tok_start_position = input_start\n      while tok_start_position < input_end:\n        tok_end_position = tok_start_position + len(tokenizer.tokenize(tok_answer_text)) - 1\n        if tok_end_position >= input_end:\n          tok_end_position = input_end - 1\n        if tok_start_position >= tok_end_position:\n          break\n        (start_position, end_position) = _improve_answer_span_one_example(\n            doc_tokens, tok_start_position, tok_end_position, tokenizer,\n            orig_answer_text)\n        if start_position == tok_start_position and end_position == tok_end_position:\n          tok_start_position += 1\n        else:\n          break\n    return (tok_start_position, tok_end_position)\n\n", "CodeBleu_score": 24.492565354002274}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n      for new_end in range(input_end, new_start - 1, -1):\n        text_span = \\\" \\\" * (new_end - new_start + 1)\n        if text_span in tok_answer_text:\n          return (new_start, new_end)\n    return (input_start, input_end)\n\n\n\n\n", "CodeBleu_score": 66.75957646393354}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.92310555989161}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.92310555989161}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span)\n            in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.4271648090102}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.72848186278928}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.72848186278928}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 68.72848186278928}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\n\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        # Load the audio data from the file path using the provided parameters.\n        # Cast the audio data to the desired data type.\n        audio_data = tf.cast(\n            tf.io.read_file(path), dtype=dtype\n        )  # type: ignore\n        audio_data = tf.audio.decode_wav(audio_data, desired_channels=1)[\n            0\n        ]  # type: ignore\n        audio_data = tf.squeeze(audio_data)  # type: ignore\n        audio_data = tf.cast(audio_data, dtype=dtype)  # type: ignore\n        # Adjust the audio data's start and end time based on the provided offset and duration.\n        start_index = tf.cast(offset * sample_rate, dtype=tf.int32)\n        end_index = tf.cast(\n            (offset + duration) * sample_rate, dtype=tf.int32\n        )  # type: ignore\n        audio_data = audio_data[start_index:end_index]  # type: ignore\n        # Return the audio data and a False flag indicating no error.\n        return audio_data, False\n    except Exception as e:\n        # Log the error and return a default value of -1.0 with a True flag indicating an error.\n        logger.error(f\\\"Error while loading audio from path {path}: {e}\\\")\n        return -1.0, True\n\n", "CodeBleu_score": 33.15400458658113}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        logger.info(\\\"Loading audio file from '%s'\\\", path)\n        audio_data = load_audio(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.error(\\\"Failed to load audio file from '%s': %s\\\", path, e)\n        return -1.0, True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.64199142062911}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t  try:\n        # Load the audio data using provided parameters.\n        waveform, sample_rate = load(path, offset, duration, sample_rate, dtype)\n        # Return the loaded audio data and a False flag indicating no error.\n        return waveform, sample_rate, False\n    except Exception as e:\n        # Log the error and return a default value of -1.0 with a True flag indicating an error.\n        logger.error(f\\\"Error while loading audio from {path}:", "CodeBleu_score": 20.267887510886577}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    waveform = tf.cast(waveform, tf.float32)\n    waveform = tf.reshape(waveform, (-1, 1))\n    if waveform.shape[0] == n_channels:\n        return waveform\n    if waveform.shape[0] < n_channels:\n        waveform = tf.tile(waveform, (1, n_channels))\n        return waveform[:n_channels]\n    return waveform[:n_channels]\n\n", "CodeBleu_score": 45.702492589526}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    n_channels_waveform = waveform.shape[1]\n    if n_channels_waveform == n_channels:\n        return waveform\n    if n_channels_waveform > n_channels:\n        return tf.slice(waveform, [0, 0], [waveform.shape[0], n_channels])\n    else:\n        waveform_tiled = tf.tile(waveform, [1, n_channels // n_channels_waveform])\n        return tf.slice(waveform_tiled, [0, 0], [waveform.shape[0], n_channels])\n\n", "CodeBleu_score": 42.56928237230137}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t    # pylint: disable=import-outside-toplevel\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels])\n\n", "CodeBleu_score": 52.5970842989419}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    return from_float32_to_uint8(db_spectrogram, **kwargs)\n\n", "CodeBleu_score": 45.82286584280558}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-locals\n    # pylint: disable", "CodeBleu_score": 17.938099389712292}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t    return from_float32_to_uint8(\n        gain_to_db(spectrogram, **kwargs),\n        max_value=db_range,\n        min_value=0.0,\n    )\n\n", "CodeBleu_score": 28.530929464667963}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # pylint: disable=too-many-arguments\n    window = tf.pow(hann_window(frame_length), window_exponent)\n    stft_window = tf.expand_dims(window, 1)\n    spectrogram = tf.pow(\n        tf.abs(\n            stft(\n                waveform,\n                frame_length=frame_length,\n                frame_step=frame_step,\n                window=stft_window,\n                pad_end=True,\n            )\n        ),\n        spec_exponent,\n    )\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n    # pylint: enable=too-many-arguments\n    return spectrogram\n\n", "CodeBleu_score": 39.86859146041959}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = tf.pow(hann_window(frame_length), window_exponent)\n    fft_window = tf.cast(tf.signal.fftshift(window), tf.complex64)\n    stft_result = tf.signal.stft(\n        waveform,\n        frame_length,\n        frame_step,\n        fft_window=fft_window,\n        pad_end=True,\n    )\n    magnitude = tf.abs(stft_result)\n    return tf.pow(magnitude, spec_exponent)\n\n", "CodeBleu_score": 37.14562114137144}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n    stft_result = stft(\n        waveform, frame_length, frame_step, window=window, pad_end=True\n    )\n    stft_result = tf.transpose(stft_result)\n    return tf.abs(stft_result) ** spec_exponent\n\n", "CodeBleu_score": 34.339004377289925}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    factor = tf.cast(factor, tf.float32)\n    time_axis = 1\n    n_samples = tf.shape(spectrogram)[time_axis]\n    new_n_samples = tf.cast(tf.round(n_samples * factor), tf.int32)\n    new_spectrogram = tf.image.resize(\n        spectrogram,\n        [new_n_samples, tf.shape(spectrogram)[2]],\n        method=method,\n    )\n    if new_n_samples > n_samples:\n        new_spectrogram = tf.pad(\n            new_spectrogram,\n            tf.constant([[0, new_n_samples - n_samples], [0, 0], [0, 0]]),\n        )\n    else:\n        new_spectrogram = new_spectrogram[\n            :, :n_samples, :\n        ]\n    return new_spectrogram\n\n", "CodeBleu_score": 39.44747704752135}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    new_time_dim = tf.cast(tf.math.ceil(tf.shape(spectrogram)[0] * factor), tf.int32)\n    spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n    spectrogram = tf.image.resize_with_crop_or_pad(\n        spectrogram, [tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]], method=method\n    )\n    return spectrogram\n\n", "CodeBleu_score": 38.492958085874456}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    new_time_dim = tf.cast(tf.shape(spectrogram)[0] * factor, tf.int32)\n    new_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method\n    )\n    new_spectrogram = tf.image.crop_to_bounding_box(\n        new_spectrogram, 0, 0, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n    return new_spectrogram\n\n", "CodeBleu_score": 37.71865995978867}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    shift_factor = tf.cast(tf.cast(F, tf.float32) * semitone_shift, tf.int32)[0]\n    resized_spectrogram = tf.image.resize_images(\n        spectrogram, [T, F + shift_factor], method=method, align_corners=True\n    )\n    padded_spectrogram = tf.pad(resized_spectrogram, [[0, 0], [0, shift_factor], [0, 0]])\n    return padded_spectrogram[0:T, 0:F]\n\n", "CodeBleu_score": 42.72475957824319}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    semitone_shift = tf.cast(semitone_shift, tf.float32)\n    resize_factor = tf.exp(tf.math.log(2) * semitone_shift)\n    resized_spectrogram = tf.image.resize_images(\n        spectrogram, [T, tf.cast(F * resize_factor, tf.int32)], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(resized_spectrogram, T, F)\n\n", "CodeBleu_score": 38.19135541753025}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    resizing_factor = tf.cast(tf.cast(T, tf.float32) * (1.0 + semitone_shift), tf.int32)[0]\n    resizing_factor = tf.maximum(resizing_factor, 1)\n    resizing_factor = tf.minimum(resizing_factor, T)\n    resizing_factor = tf.cast(resizing_factor, tf.float32)\n    resizing_factor = tf.cast(resizing_factor, tf.int32)\n    resizing_factor = tf.cast(resizing_factor, tf.float32)\n    pitch_shifted_spec = tf.image.resize_images(\n        spectrogram, [resizing_factor, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(pitch_shifted_spec, T, F)\n\n", "CodeBleu_score": 34.002922131156474}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\"Unknown convolutional activation: %s\" % params[\"conv_activation\"])\n\n", "CodeBleu_score": 34.085400524639574}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\"Unknown conv_activation: {}\".format(params[\"conv_activation\"]))\n\n", "CodeBleu_score": 34.37220079906579}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(f\\\"Unknown activation layer: {params['conv_activation']}\\\"\")\n\n", "CodeBleu_score": 36.03067824831723}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    conv_n_filters = params.get(\"conv_n_filters\", [16, 32, 64, 128, 256, 512])\n    conv_activation_layer = _get_conv_activation_layer(params)\n    deconv_activation_layer = _get_deconv_activation_layer(params)\n    kernel_initializer = he_uniform(seed=50)\n    conv2d_factory = partial(\n        Conv2D, strides=(2, 2), padding=\"same\", kernel_initializer=kernel_initializer\n    )\n    # First layer.\n    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n    batch1 = BatchNormalization(axis=-1)(conv1)\n    rel1 = conv_activation_layer(batch1)\n    # Second layer.\n    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n    batch2 = BatchNormalization(axis=-1)(conv2)\n    rel2 = conv_activation_layer(batch2)\n    # Third layer.\n    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n    batch3 = BatchNormalization(axis=-1)(conv3)\n    rel3 = conv_activation_layer(batch3)\n    # Fourth layer.\n    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n    batch4 = BatchNormalization(axis=-1)(conv4)\n    rel4 = conv_activation_layer(batch4)\n    # Fifth layer.\n    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n    batch5 = BatchNormalization(axis=-1)(conv5)\n    rel5 = conv_activation_layer(batch5)\n    # Sixth layer\n    conv6 = conv2d_factory(conv_n_filters[5], (5, 5))(rel5)\n    batch6 = BatchNormalization(axis=-1)(conv6)\n    _ = conv_activation_layer(batch6)\n    #\n    #\n    conv2d_transpose_factory = partial(\n       ", "CodeBleu_score": 18.073718923878253}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_mask_logit = True\n    if not params:\n        params = {}\n    output_dict = {}\n    for instrument in instruments:\n        output_name = instrument + \"_output\"\n        output_dict[output_name] = apply_unet(\n            input_tensor, output_name, params, output_mask_logit\n        )\n    # Combine logit masks using softmax along the channel dimension.\n    logit_masks = [output_dict[instrument + \"_output\"] for instrument in instruments]\n    combined_logit_masks = tf.math.reduce_sum(\n        tf.stack(logit_masks, axis=0), axis=0\n    )\n    # Create output dictionary.\n    for instrument in instruments:\n        output_name = instrument + \"_output\"\n        output_dict[output_name] = tf.multiply(\n            output_dict[output_name], combined_logit_masks\n        )\n    return output_dict\n\n", "CodeBleu_score": 41.05006060681499}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Applying softmax U-Net\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor,\n            instrument,\n            params,\n            output_mask_logit=True,\n        )\n    output_tensor = tf.zeros(\n        shape=(\n            output_dict[instruments[0]].shape[0],\n            output_dict[instruments[0]].shape[1],\n            output_dict[instruments[0]].shape[2],\n            len(instruments),\n        )\n    )\n    for i, instrument in enumerate(instruments):\n        output_tensor[:, :, :, i] = tf.multiply(\n            output_dict[instrument], tf.expand_dims(input_tensor, axis=-1)\n        )\n    output_tensor = tf.reduce_sum(\n        output_tensor, axis=-1, keepdims=True, name=\"output\"\n    )\n    return output_tensor\n\n", "CodeBleu_score": 42.4317460110624}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t   hash_function = hashlib.sha256()\n    with open(path, 'rb') as file:\n        for chunk in iter(lambda: file.read(4096), b''):\n            hash_function.update(chunk)\n    return hash_function.hexdigest()\n\n", "CodeBleu_score": 50.689767028778945}, {"code": "def compute_file_checksum(path)\n\t   sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 56.64636532708959}, {"code": "def compute_file_checksum(path)\n\t   with open(path, 'rb') as f:\n        hash_object = hashlib.sha256()\n        for chunk in iter(lambda: f.read(4096), b''):\n            hash_object.update(chunk)\n        return hash_object.hexdigest()\n\n", "CodeBleu_score": 59.44526508600774}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t   url = f\\\"{self._host}/{self._repository}/{self._release_path}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\\\"Failed to retrieve checksum index from {url}.\\\")\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\\\"Model {name} is not available in {url}.\\\")\n    return checksums[name]\n\n", "CodeBleu_score": 39.11446695692505}, {"code": "def checksum(self, name: str) -> str\n\t   url = f\\\"{self._host}/{self._repository}/{self._release_path}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    data = response.json()\n    if name not in data:\n        raise ValueError(f\\\"Model {name} not found\\\")\n    return data[name]\n\n", "CodeBleu_score": 34.11392281110503}, {"code": "def checksum(self, name: str) -> str\n\t   url = f\\\"{self._host}/{self._repository}/{self._release_path}/{self.CHECKSUM_INDEX}\\\"\n    response = httpx.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\\\"Failed to retrieve checksum index from {url}\\\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\\\"Model {name} not found in {url}\\\")\n    return index[name]\n\n", "CodeBleu_score": 41.573745834051195}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = f\\\"{self._host}/{self._repository}/archive/{self._release}.tar.gz\\\"", "CodeBleu_score": 13.966052442418297}, {"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\\\"Downloading model {name} from {url}\\\")\n    response: httpx.Response = httpx.get(url, stream=True)\n    response.raise_for_status()\n    with NamedTemporaryFile(delete=False) as tmp:\n        for chunk in response.iter_bytes(chunk_size=4096):\n            tmp.write(chunk)\n        tmp.flush()\n        checksum: str = compute_file_checksum(tmp.name)\n        if checksum!= self.checksum(name):\n            raise ValueError(f\\\"Checksum mismatch for model {name}\\\")\n        logger.info(f\\\"Extracting model {name} to {path}\\\")\n        with tarfile.open(tmp.name, \\\"r:gz\\\") as archive:\n            archive.extractall(path)\n        os.remove(tmp.name)\n        logger.info(f\\\"Extraction of model {name} completed\\\")\n\n", "CodeBleu_score": 47.092581200026316}, {"code": "def download(self, name: str, path: str) -> None:\n\t   url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\\\"Downloading {name} from {url} to {path}\\\")\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    with NamedTemporaryFile(delete=False) as tmp:\n        tmp.write(response.content)\n        tmp.flush()\n        checksum: str = compute_file_checksum(tmp.name)\n        if checksum!= self.checksum(name):\n            raise ValueError(f\\\"Checksum mismatch for {name}\\\")\n        with tarfile.open(tmp.name, \\\"r:gz\\\") as archive:\n            archive.extractall(path)\n        os.remove(tmp.name)\n        logger.info(f\\\"Downloaded {name} to {path}\\\")\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.30849385687543}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[_EMBEDDED_CONFIGURATION_PREFIX:]\n        try:\n            return json.loads(loader.read_text(resources, name))\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration \\\"{name} not found.\\\")\n    elif exists(descriptor):\n        with open(descriptor, \\\"r\\\") as f:\n            return json.load(f)\n    else:\n        raise SpleeterError(f\\\"Configuration file \\\"{descriptor} not found.\\\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.35656371759345}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[_EMBEDDED_CONFIGURATION_PREFIX:]\n        try:\n            with loader.open(resources, name) as f:\n                return json.load(f)\n        except loader.ResourceNotFound:\n            raise SpleeterError(f\\\"Could not find embedded configuration \\\"\\\"\\\"{name}\\\"\\\"\\\". Please make sure the configuration is correctly installed.\\\")\n    elif exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    else:\n        raise SpleeterError(f\\\"Could not find configuration file \\\"\\\"\\\"{descriptor}\\\"\\\"\\\". Please make sure the file is correctly installed.\\\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.282890030924726}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        try:\n            return json.loads(loader.read_text(resources, name))\n        except FileNotFoundError:\n            raise SpleeterError(f\\\"Embedded configuration {name} not found\\\")\n    else:\n        if exists(descriptor):\n            with open(descriptor, \\\"r\\\") as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\\\"Configuration file {descriptor} not found\\\")\n\n", "CodeBleu_score": 42.592111761610056}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis!= 0 and concat_axis!= 1:\n        raise ValueError(\n            f\\\"Concatenation axis must be either 0 or 1, but got {concat_axis}.\\\"\n        )\n    if concat_axis == 0:\n        return {\n            k: tf.concat(\n                [func(tensor_dict[k][:, i, :]) for i in range(tensor_dict[k].shape[0])],\n                axis=0,\n            )\n            for k in tensor_dict\n        }\n    else:\n        return {\n            k: tf.concat(\n                [func(tensor_dict[k][i, :, :]) for i in range(tensor_dict[k].shape[0])],\n                axis=1,\n            )\n            for k in tensor_dict\n        }\n\n", "CodeBleu_score": 33.723681002544105}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n    if concat_axis == 0:\n        tensors = [\n            tf.concat(\n                [\n                    tf.expand_dims(tensor_dict[key], 0)\n                    for key in tensor_dict.keys()\n                ],\n                axis=0,\n            )\n            for tensor_dict in tensor_dict.values()\n        ]\n    else:\n        tensors = [\n            tf.concat(\n                [\n                    tf.expand_dims(tensor_dict[key], 1)\n                    for key in tensor_dict.keys()\n                ],\n                axis=1,\n            )\n            for tensor_dict in tensor_dict.values()\n        ]\n    tensors = [func(tensor) for tensor in tensors]\n    if concat_axis == 0:\n        tensors = [tf.squeeze(tensor, 0) for tensor in tensors]\n    else:\n        tensors = [tf.squeeze(tensor, 1) for tensor in tensors]\n    return {\n        key: tf.concat(tensors, axis=concat_axis)\n        for key, tensor in zip(tensor_dict.keys(), tensor_dict.values())\n    }\n\n", "CodeBleu_score": 39.54270468033626}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(f\\\"Concatenation axis must be either 0 or 1, but got {concat_axis}.\\\")\n    for key in tensor_dict:\n        tensor_dict[key] = tf.concat(list(tensor_dict[key]), axis=concat_axis)\n    tensor_dict = func(tensor_dict)\n    for key in tensor_dict:\n        tensor_dict[key] = tf.split(tensor_dict[key], num_or_size_splits=len(tensor_dict[key]), axis=concat_axis)\n    return tensor_dict\n\n\n\n\n", "CodeBleu_score": 29.05128149745134}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_range = tensor_max - tensor_min\n    tensor_scaled = tf.cast(\n        (tensor - tensor_min) * (255 / tensor_range), dtype=tf.uint8\n    )\n    tensor_dict = {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n    return tensor_dict\n\n", "CodeBleu_score": 62.89711903646602}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_range = tensor_max - tensor_min\n    tensor_scaled = tf.cast(\n        (tensor - tensor_min) * (255 / tensor_range), dtype=tf.uint8\n    )\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 64.25787452793779}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_min_max = tensor_max - tensor_min\n    tensor_min_max = tf.where(\n        tensor_min_max == 0,\n        tf.zeros_like(tensor_min_max),\n        tensor_min_max,\n    )\n    tensor_scaled = (tensor - tensor_min) / tensor_min_max\n    tensor_scaled = tf.cast(tensor_scaled * 255, tf.uint8)\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 59.84740631734204}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_needed = segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_needed]])\n    reshaped_tensor = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[-1]])\n    return reshaped_tensor\n\n", "CodeBleu_score": 27.722284557870562}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = segment_len - tensor_len % segment_len\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    padded_tensor = tf.reshape(padded_tensor, [padding_len // segment_len, segment_len])\n    return padded_tensor\n\n", "CodeBleu_score": 24.61980119828808}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padded_tensor = tf.pad(tensor, [[0, 0], [0, segment_len - tf.shape(tensor)[1] % segment_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[-1]])\n\n", "CodeBleu_score": 20.520506472720072}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    if instr_spec.shape[1] % frame_length!= 0:\n        extra_rows = frame_length - instr_spec.shape[1] % frame_length\n        instr_spec = np.pad(instr_spec, ((0, 0), (0, extra_rows)), \"constant\")\n    instr_spec = np.concatenate((instr_spec, np.zeros((instr_spec.shape[0], extra_rows))), axis=1)\n    instr_spec = np.reshape(instr_spec, (instr_spec.shape[0], instr_spec.shape[1] // F, F))\n    return instr_spec\n\n", "CodeBleu_score": 27.425930106700818}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_extra_rows = frame_length - tf.shape(instr_spec)[0]\n    num_extra_cols = tf.shape(instr_spec)[1]\n    instr_spec = tf.pad(instr_spec, [[0, num_extra_rows], [0, 0], [0, 0]])\n    instr_spec = tf.reshape(instr_spec, [1, -1, num_extra_cols])\n    return instr_spec\n\n", "CodeBleu_score": 14.98310443183733}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # pad\n    n_pad = frame_length - F.shape[1] % frame_length\n    if n_pad > 0:\n        F_pad = np.zeros((F.shape[0], n_pad))\n        F = np.concatenate((F, F_pad), axis=1)\n    # reshape\n    F = F.reshape((F.shape[0], F.shape[1] // frame_length, frame_length))\n    return F\n\n", "CodeBleu_score": 17.802606104475032}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(df)\n\n", "CodeBleu_score": 26.55075431370808}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df)\n\n", "CodeBleu_score": 17.232156492595102}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(df)\n\n", "CodeBleu_score": 26.55075431370808}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i in range(len(target_shape)):\n        if isinstance(target_shape[i], int):\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n        elif isinstance(target_shape[i], tuple):\n            result = result and (\n                tensor_tf.shape[i] == target_shape[i][0]\n                and tensor_tf.shape[i + 1] == target_shape[i][1]\n            )\n    return result\n\n", "CodeBleu_score": 34.80890287656829}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_dim in enumerate(target_shape):\n        if isinstance(target_dim, int):\n            result = result and tf.math.equal(tf.shape(tensor_tf)[i], target_dim)\n    return result\n\n", "CodeBleu_score": 40.17927716425793}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i]!= -1:\n            result = result and (tf.shape(tensor_tf)[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 34.40193418984336}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(\n        audio_params[\"frame_length\"],\n        audio_params[\"frame_step\"],\n        audio_params[\"sample_rate\"],\n        audio_params[\"F\"],\n        audio_params[\"T\"],\n        audio_params[\"instrument_list\"],\n        audio_params[\"mix_name\"],\n        audio_params[\"random_seed\"],\n    )\n    builder.build(\n        audio_path,\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return builder.get_dataset()\n\n", "CodeBleu_score": 34.079196821610225}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many", "CodeBleu_score": 9.542815674891148}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, random_seed=42\n    ).set_chunk_duration(10)\n    return builder.build(\n        train_csv=os.path.join(audio_path, \"train.csv\"),\n        cache_dir=os.path.join(audio_path, \"cache\"),\n        batch_size=32,\n        num_chunks_per_song=10,\n        cache_options={\"num_parallel_calls\": 1},\n        data_augmentation_options={\n            \"time_stretch\": {\"min\": 0.8, \"max\": 1.2},\n            \"pitch_shift\": {\"min\": -10, \"max\": 10},\n        },\n    )\n\n", "CodeBleu_score": 43.24495469501609}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._audio_params[\\\"instrument_list\\\"]:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_params,\n                    self._audio_adapter,\n                    self._audio_path,\n                    self._random_seed,\n                    self._chunk_duration,\n                    instrument,\n                )\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n    def __len__(self) -> Any:\n        \\\"\\\"\\\"Returns the number of segments in the dataset.\\\"\\\"\\\"\n        return sum([len(builder) for builder in self.instruments()])\n\n    def __iter__(self) -> Any:\n        \\\"\\\"\\\"Yields each segment of the dataset.\\\"\\\"\\\"\n        for builder in self.instruments():\n            yield from builder\n\n    def __getitem__(self, index: int) -> Any:\n        \\\"\\\"\\\"Returns the segment at the given index.\\\"\\\"\\\"\n        for builder in self.instruments():\n            try:\n                return builder[index]\n            except IndexError:\n                pass\n        raise IndexError(\\\"Index out of range.\\\")\n\n    def __add__(self, other: Any) -> Any:\n        \\\"\\\"\\\"Concatenates the dataset with the other dataset.\\\"\\\"\\\"\n        return ConcatDataset(self, other)\n\n    def __radd__(self, other: Any) -> Any:\n        \\\"\\\"\\\"Concatenates the dataset with the other dataset.\\\"\\\"\\\"\n        return ConcatDataset(other, self)\n\n    def __len__(self) -> Any:\n        \\\"\\\"\\\"Returns the number of segments in the dataset.\\\"\\\"\\\"\n        return sum([len(builder) for builder in self.instruments()])\n\n    def __iter__(self) -> Any:\n        \\\"\\\"\\\"Yields each segment of the dataset.\\\"\\\"\\\"\n        for builder in self.instruments():\n            yield from builder\n\n    def __getitem__(self, index: int) -> Any:\n        \\\"\\\"\\\"Returns the segment at the given index.\\\"\\\"\\\"\n        for builder in self.instruments():\n", "CodeBleu_score": 54.2510885060367}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._audio_params[\\\"instrument_list\\\"]:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_params,\n                    self._audio_adapter,\n                    self._audio_path,\n                    self._random_seed,\n                    self._chunk_duration,\n                )\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n    def build(self) -> Any:\n        \\\"\\\"\\\"Builds the dataset.\n\n        Returns:\n            Any:\n                The built dataset.\n        \\\"\\\"\\\"\n        dataset = self.build_instruments()\n        dataset = self.build_mix()\n        return dataset\n\n    def build_instruments(self) -> Any:\n        \\\"\\\"\\\"Builds the instruments dataset.\n\n        Returns:\n            Any:\n                The built instruments dataset.\n        \\\"\\\"\\\"\n        dataset = tf.data.Dataset.from_generator(\n            lambda: self.instruments(),\n            output_types=self._audio_params[\\\"output_types\\\"],\n            output_shapes=self._audio_params[\\\"output_shapes\\\"],\n        )\n        dataset = dataset.map(\n            self.filter_error,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.map(\n            self.filter_waveform,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.map(\n            self.harmonize_spectrogram,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.map(\n            self.filter_short_segments,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.map(\n            self.random_time_crop,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.map(\n            self", "CodeBleu_score": 48.281942853327955}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_adapter,\n                    self._audio_params,\n                    self._audio_path,\n                    self._sample_rate,\n                    self._frame_length,\n                    self._frame_step,\n                    self._mix_name,\n                    self._n_channels,\n                    self._instruments,\n                    self._random_seed,\n                    self._chunk_duration,\n                )\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n    @property\n    def MARGIN(self) -> float:\n        \\\"\\\"\\\"Returns the margin between chunks.\n        \\\"\\\"\\\"\n        return (self._chunk_duration - self._frame_length) / 2\n\n    @property\n    def T(self) -> int:\n        \\\"\\\"\\\"Returns the length of a segment in frames.\n        \\\"\\\"\\\"\n        return self._T\n\n    @property\n    def F(self) -> int:\n        \\\"\\\"\\\"Returns the number of frequency bins used in the spectrogram.\n        \\\"\\\"\\\"\n        return self._F\n\n    @property\n    def sample_rate(self) -> int:\n        \\\"\\\"\\\"Returns the sample rate of the audio.\n        \\\"\\\"\\\"\n        return self._sample_rate\n\n    @property\n    def frame_length(self) -> int:\n        \\\"\\\"\\\"Returns the length of the frame in seconds.\n        \\\"\\\"\\\"\n        return self._frame_length\n\n    @property\n    def frame_step(self) -> int:\n        \\\"\\\"\\\"Returns the step between frames in seconds.\n        \\\"\\\"\\\"\n        return self._frame_step\n\n    @property\n    def mix_name(self) -> str:\n        \\\"\\\"\\\"Returns the name of the mix.\n        \\\"\\\"\\\"\n        return self._mix_name\n\n    @property\n    def n_channels(self) -> int:\n        \\\"\\\"\\\"", "CodeBleu_score": 56.99965765724922}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if not tf.io.gfile.exists(cache):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        if wait:\n            print(\n                f\\\"Waiting for {cache + \\\"/index.txt\\\"} to exist, \\\"\n                f\\\"this may take a while depending on the size of the dataset...\\\"\n            )\n            while not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n                time.sleep(10)\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise ValueError(\n            f\\\"Cache path {cache} does not exist. Please create it or provide a different path.\\\"\\\"\n        )\n    if not tf.io.gfile.exists(cache + \\\"/index.txt\\\"):\n        raise", "CodeBleu_score": 39.95985858241964}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if not tf.io.gfile.exists(cache):\n        tf.io.gfile.makedirs(cache)\n        tf.io.gfile.copy(\n            self.INDEX_FILE,\n            tf.strings.join((cache, self.INDEX_FILE)),\n        )\n    if wait:\n        while not tf.io.gfile.exists(tf.strings.join((cache, self.INDEX_FILE))):\n            time.sleep(self.WAIT_TIME)\n            print(\n                f\\\"Waiting for cache {cache} to be generated.\\\"\n                f\\\"(It may take a few minutes.)\\\",\n            )\n    return dataset.map(\n        lambda sample: dict(\n            sample,\n            **sync_apply(\n                {\n                    f\\\"{instrument}_path\\\": tf.strings.join(\n                        (cache, sample[f\\\"{instrument}_path\\\"]), SEPARATOR\n                    )\n                    for instrument in self._instruments\n                },\n                lambda x: tf.io.read_file(x),\n            ),\n        )\n    )\n\n    @property\n    def instruments(self) -> Any:\n        \\\"\\\"\\\"\n        Instrument dataset builder generator.\n\n        Yields:\n            Any:\n                InstrumentBuilder instance.\n        \\\"\\\"\\\"\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument)\n                )\n        for builder in self._instrument_builders:\n            yield builder\n\n\n", "CodeBleu_score": 39.691061015437505}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not os.path.exists(cache):\n                print(\n                    f\\\"Waiting for {cache} to be created...\\\"\n                )\n                time.sleep(1)\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        dataset = dataset.cache(cache)\n        return dataset\n    return dataset\n", "CodeBleu_score": 58.09232802208237}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_provider = ModelProvider(params['model_dir'], params['model_name'], params['model_weights'], MWF)\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=params['model_dir'], params=params, config=run_config)\n\n", "CodeBleu_score": 32.23155052616651}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_provider = ModelProvider(params['model_dir'], MWF)\n    params = model_provider.update_params(params)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    config.log_device_placement = False\n    run_config = tf.estimator.RunConfig(session_config=config)\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params['model_dir'],\n        params=params,\n        config=run_config)\n\n", "CodeBleu_score": 32.62370295183368}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tl_provider = ModelProvider(params)\n    params = model_provider.update_params(params)\n    params[\"MWF\"] = MWF\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=config)\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=model_provider.model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 27.634740936212292}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tnvert waveform to stereo.\n    waveform = to_stereo(waveform)\n    # Update data generator.\n    self._data_generator.update_data((waveform, audio_descriptor))\n    # Get prediction generator.\n    prediction_generator = self._get_prediction_generator()\n    # Get prediction.\n    prediction = next(prediction_generator)\n    # Remove audio_id key from prediction.\n    prediction.pop(\\\"audio_id\\\")\n    # Return prediction.\n    return prediction\n\n", "CodeBleu_score": 42.82661141848646}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tform = to_stereo(waveform)\n    self._data_generator.update_data((waveform, audio_descriptor))\n    prediction_generator = self._get_prediction_generator()\n    prediction = next(prediction_generator)\n    prediction.pop(\\\"audio_id\\\")\n    return prediction\n", "CodeBleu_score": 36.79121040105147}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tsure stereo.\n    waveform = to_stereo(waveform)\n    # Update data generator.\n    self._data_generator.update_data((waveform, audio_descriptor))\n    # Get prediction generator.\n    prediction_generator = self._get_prediction_generator()\n    # Get features.\n    features = self._get_features()\n    # Get estimator.\n    estimator = create_estimator(self._params, self._MWF)\n    # Get builder.\n    builder = self._get_builder()\n    # Get session.\n    session = self._get_session()\n    # Get input provider.\n    provider = self._get_input_provider()\n    # Get input dict.\n    input_dict = provider.get_input_dict(waveform)\n    # Get audio id.\n    audio_id = audio_descriptor.id\n    # Get prediction.\n    prediction = next(prediction_generator)\n    # Remove audio id from prediction.\n    prediction.pop(\\\"audio_id\\\")\n    # Get audio descriptor.\n    audio_descriptor = AudioDescriptor(\n        audio_descriptor.id,\n        audio_descriptor.name,\n        audio_descriptor.duration,\n        audio_descriptor.sample_rate,\n        audio_descriptor.bit_depth,\n    )\n    # Convert prediction to audio.\n    audio = builder.to_audio(prediction, features, audio_descriptor)\n    # Convert audio to stereo.\n    audio = to_stereo(audio)\n    # Return audio.\n    return {\\\"audio\\\": audio, \\\"audio_id\\\": audio_id}\n\n", "CodeBleu_score": 37.74397475754131}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\\\"resnet50\\\", \\\"resnet101\\\"]\n        if config.BACKBONE == \\\"resnet50\\\":\n            return np.array([\n                [int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))]\n                for stride in config.BACKBONE_STRIDES])\n        else:\n            return np.array([\n                [int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))]\n                for stride in config.BACKBONE_STRIDES])\n\n\n\n\n", "CodeBleu_score": 70.6754217517982}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\\\"resnet50\\\", \\\"resnet101\\\"]\n        return np.array([\n            int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride)),\n        ])\n\n", "CodeBleu_score": 62.1917767143075}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    elif config.BACKBONE == \\\"resnet50\\\":\n        return np.array([\n            [int(math.ceil(image_shape[0] / stride)),\n             int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES\n        ])\n    elif config.BACKBONE == \\\"resnet101\\\":\n        return np.array([\n            [int(math.ceil(image_shape[0] / stride)),\n             int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES\n        ])\n    else:\n        raise ValueError(\\\"Backbone name is invalid\\\")\n\n", "CodeBleu_score": 65.47701314327267}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split window (y1, x1, y2, x2) into y1, x1, y2, x2\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip to window by min and max y\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    # Clip to window by min and max x\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\\\"clip_boxes_out\\\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n\n\n\n", "CodeBleu_score": 55.55036761271924}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.cast(boxes, tf.float32)\n    window = tf.cast(window, tf.float32)\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\\\"clip_boxes_out\\\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 54.12113609120451}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split window (y1, x1, y2, x2) into y1, x1, y2, x2\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip to window by min and max y\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\\\"clip_boxes_out\\\")\n    return clipped\n\n\n\n\n", "CodeBleu_score": 48.50505496449225}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                    kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01),\n                    name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(anchors_per_location * 2, (1, 1), padding='valid',\n                  activation='linear', kernel_initializer='zero',\n                  name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2] and extract scores.\n    rpn_class_logits = KL.Lambda(lambda t: t[:, :, :, :anchors_per_location],\n                                   name='rpn_class_logits')(x)\n    rpn_probs = KL.Activation('softmax', name='rpn_class_prob_raw')(rpn_class_logits)\n    rpn_class = KL.Lambda(lambda t: t[:, :, :, :anchors_per_location],\n                           name='rpn_class')(rpn_probs)\n\n    # Bounding box refinement. [batch, height, width, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', kernel_initializer='zero',\n                  name='rpn_bbox_pred')(shared)\n    rpn_bbox = KL.Lambda(lambda t: t[:, :, :, :anchors_per_location * 4],\n                           name='rpn_bbox')(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_class, rpn_bbox]\n\n", "CodeBleu_score": 52.23955632606234}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                    kernel_initializer=keras.initializers.normal(stddev=0.01),\n                    kernel_regularizer=keras.regularizers.l2(0.0001),\n                    name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(anchors_per_location * 2, (1, 1), padding='valid',\n                  activation='linear', kernel_initializer=keras.initializers.normal(stddev=0.01),\n                  kernel_regularizer=keras.regularizers.l2(0.0001),\n                  name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2].\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', kernel_initializer=keras.initializers.normal(stddev=0.01),\n                  kernel_regularizer=keras.regularizers.l2(0.0001),\n                  name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox", "CodeBleu_score": 67.42659773031903}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                        kernel_initializer=keras.initializers.normal(stddev=0.01),\n                        kernel_regularizer=keras.regularizers.l2(0.0005),\n                        name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score.\n    x = KL.Conv2D(anchors_per_location * 2, (1, 1), padding='valid',\n                    activation='linear', kernel_initializer=keras.initializers.normal(stddev=0.01),\n                    kernel_regularizer=keras.regularizers.l2(0.0005),\n                    name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2] and then channelwise softmax.\n    x = KL.Lambda(lambda t: K.permute_dimensions(t, (0, 2, 3, 1)))(x)\n    x = KL.Activation('softmax', name='rpn_class_xxx')(x)\n    x = KL.Lambda(lambda t: K.permute_dimensions(t, (0, 3, 1, 2)))(x)\n\n    # Bounding box refinement.\n    x_refine = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                        activation='linear', kernel_initializer=keras.initializers.normal(stddev=0.01),\n                        kernel_regularizer=keras.regularizers.l2(0.0005),\n                        name='rpn_bbox_pred')(shared)\n\n    return [x, x_refine]\n\n", "CodeBleu_score": 50.14715301581325}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return K.mean(loss)\n\n", "CodeBleu_score": 57.53898374809818}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t    tf.reset_default_graph()\n    input_layer = keras.layers.Input(shape=x_train.shape[1:])\n    conv_layer = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    max_pool_layer = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv_layer)\n    conv_layer = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(max_pool_layer)\n    max_pool_layer = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv_layer)\n    flatten_layer = keras.layers.Flatten()(max_pool_layer)\n    dense_layer = keras.layers.Dense(units=128, activation='relu')(flatten_layer)\n    output_layer = keras.layers.Dense(units=num_classes, activation='softmax')(dense_layer)\n    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 26.978565137144138}, {"code": "def build_model(x_train, num_classes)\n\t    K.clear_session()\n    inputs = Input(shape=x_train.shape[1:])\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Flatten()(x)\n    x = Dense(512, activation='relu')(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 29.918152054868376}, {"code": "def build_model(x_train, num_classes)\n\t    tf.keras.backend.clear_session()\n    inputs = tf.keras.layers.Input(shape=x_train.shape[1:])\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 27.693842953220255}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        if m.sum() == 0:\n            boxes.append([0, 0, 0, 0])\n            continue\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    # convert all boxes to numpy array\n    boxes = np.array(boxes)\n    return boxes\n\n", "CodeBleu_score": 65.88325995492066}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # If mask is empty, no need to execute the rest of the code\n        if m.sum() == 0:\n            continue\n        # Get coordinates for bounding box\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # Add bounding box to list\n            boxes.append([x1, y1, x2, y2])\n    # Return bounding boxes array\n    return np.asarray(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 54.52150907227825}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # If mask is empty, no need to continue\n        if m.sum() == 0:\n            continue\n        # Get coordinates for bounding box\n        j, i = np.where(m)\n        if j.shape[0] == 0 or i.shape[0] == 0:\n            continue\n        # Calculate coordinates\n        x1, x2 = i.min(), i.max()\n        y1, y2 = j.min(), j.max()\n        # Store in array\n        boxes.append([x1, y1, x2, y2])\n    # Return array\n    return np.array(boxes)\n", "CodeBleu_score": 29.621704980560843}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    box_area = box[2] * box[3]\n    intersection_areas = np.dot(boxes_area, (np.minimum(box[2:], boxes[2:]) * np.minimum(box[:2], boxes[:2])).T)\n    union_areas = box_area + boxes_area - intersection_areas\n    iou = intersection_areas / union_areas\n    return iou\n\n\n\n\n", "CodeBleu_score": 21.516263054549515}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    box_area = box[2] * box[3]\n    intersection_areas = np.minimum(np.expand_dims(box[2:], 0), boxes[:, 2]) * np.minimum(np.expand_dims(box[3:], 0), boxes[:, 3])\n    intersection_areas = np.maximum(intersection_areas, 0)\n    union_areas = box_area + boxes_area - intersection_areas\n    iou = intersection_areas / union_areas\n    return iou\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.866077271376223}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    box_area = box[2] * box[3]\n    intersection_areas = np.zeros(boxes.shape[0])\n    for i, box_ in enumerate(boxes):\n        x1_inter = np.maximum(box[0], box_[0])\n        y1_inter = np.maximum(box[1], box_[1])\n        x2_inter = np.minimum(box[2], box_[2])\n        y2_inter = np.minimum(box[3], box_[3])\n        if x1_inter >= x2_inter or y1_inter >= y2_inter:\n            intersection_areas[i] = 0\n        else:\n            intersection_areas[i] = (x2_inter - x1_inter + 1) * (y2_inter - y1_inter + 1)\n    union_areas = box_area + boxes_area - intersection_areas\n    ious = intersection_areas / union_areas\n    return ious\n\n\n\n\n", "CodeBleu_score": 29.374629215772156}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If no boxes, return empty result\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n    # Calculate overlap area between each box in boxes1 and boxes2\n    b1_type = type(boxes1[0])\n    b2_type = type(boxes2[0])\n    if b1_type is list:\n        boxes1 = np.array(boxes1)\n    if b2_type is list:\n        boxes2 = np.array(boxes2)\n    if len(boxes1.shape) == 1:\n        boxes1 = boxes1.reshape(1, -1)\n    if len(boxes2.shape) == 1:\n        boxes2 = boxes2.reshape(1, -1)\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = boxes1[:, 0], boxes1[:, 1], boxes1[:, 2], boxes1[:, 3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = boxes2[:, 0], boxes2[:, 1], boxes2[:, 2], boxes2[:, 3]\n    # Intersection area\n    inter_rect_x1 = np.maximum(b1_x1, b2_x1)\n    inter_rect_y1 = np.maximum(b1_y1, b2_y1)\n    inter_rect_x2 = np.minimum(b1_x2, b2_x2)\n    inter_rect_y2 = np.minimum(b1_y2, b2_y2)\n    inter_area = np.maximum(inter_rect_x2 - inter_rect_x1, 0) * np.maximum(inter_rect_y2 - inter_rect_y1, 0)\n    # Union Area\n    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    union_area = b1_area", "CodeBleu_score": 26.481159282048882}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either set of boxes is empty, return an empty overlap matrix\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n    # Calculate intersection areas\n    b1_y1, b1_x1, b1_y2, b1_x2 = np.split(boxes1, 4, axis=1)\n    b2_y1, b2_x1, b2_y2, b2_x2 = np.split(boxes2, 4, axis=1)\n    y1 = np.maximum(b1_y1, b2_y1)\n    x1 = np.maximum(b1_x1, b2_x1)\n    y2 = np.minimum(b1_y2, b2_y2)\n    x2 = np.minimum(b1_x2, b2_x2)\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate union areas\n    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n    union = b1_area + b2_area - intersection\n    # Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 18.81357963278306}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If there are no boxes, return an empty matrix\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n    # Convert boxes to float\n    boxes1 = boxes1.astype(np.float32)\n    boxes2 = boxes2.astype(np.float32)\n    # Compute box areas\n    boxes1_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    boxes2_area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    overlaps = np.zeros((len(boxes1), len(boxes2)))\n    for i in range(len(boxes1)):\n        box1_area = boxes1_area[i]\n        for j in range(len(boxes2)):\n            box2_area = boxes2_area[j]\n            overlaps[i, j] = compute_iou(boxes1[i], boxes2[j], box1_area, box2_area)\n    return overlaps\n\n", "CodeBleu_score": 52.25640882312782}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info['path']\n    image = skimage.io.imread(path)\n    if image.ndim!= 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 77.60314328711083}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    path = image_info[\\\"path\\\"]\n    image = skimage.io.imread(path)\n    if image.ndim!= 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 75.38881082445434}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    path = image_info[\\\"path\\\"]\n    try:\n        image = skimage.io.imread(path)\n    except (IOError, ValueError) as e:\n        print(\\\"ERROR: Could not read image \\\" + path + \\\". {}\\\".format(e))\n        return None\n    if image.ndim!= 3:\n        if image.ndim == 4:\n            image = image[:, :, :, 0]\n        else:\n            print(\\\"ERROR: Image not found or not in RGB format \\\" + path)\n            return None\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 45.33194727320789}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\\\"\\\" for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis(\\\"off\\\")\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm,\n                   interpolation=interpolation)\n        i += 1\n\n\n\n\n", "CodeBleu_score": 68.84594370578628}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Calculate number of rows and columns\n\n    rows = len(images) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n    axes = np.array(axes).reshape(-1)\n    for i, (image, ax) in enumerate(zip(images, axes)):\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.axis('off')\n        if titles is not None:\n            ax.set_title(titles[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 43.223426908007035}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert len(images) <= cols ** 2, \\\"Too many images for specified number of columns and rows.\\\"\n    rows = int(np.ceil(len(images) / cols))\n    fig = plt.figure()\n    fig.set_size_inches(cols * 4, rows * 4)\n    for i, image in enumerate(images):\n        sub = fig.add_subplot(rows, cols, i + 1)\n        if image.ndim!= 2:\n            sub.axis(\\\"off\\\")\n        if titles is not None:\n            sub.set_title(titles[i], fontsize=16)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.show()\n\n", "CodeBleu_score": 38.826456812043965}]}
