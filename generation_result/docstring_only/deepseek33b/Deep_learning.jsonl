{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n    from .model_loading import load_model_weights_from_config, find_max_memory_format\n\n    if self.model_path is None:\n        raise ValueError(\"Model path not set\")\n\n    if self.model_config is None:\n        raise ValueError(\"Model config not set\")\n\n    if self.device is None:\n        raise ValueError(\"Device not set\")\n\n    if self.model_cache is not None:\n        try:\n            if os.path.exists(self.model_cache):\n                logger.info(f\"Loading model from cache: {self.model_cache}\")\n                model = torch.load(self.model_cache, map_location=self.device)\n                if isinstance(model, dict):\n                    model = model[\"state_dict\"]\n                if isinstance(model, torch.Tensor):\n                    model = model.to(self.device)\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, torch.nn.Module):\n                    model = model.to(self.device)\n                if isinstance(model, torch.nn.Module):\n                    model.eval()\n                    return {\"state_dict\": model.state_dict()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device) for k, v in model.items()}\n                if isinstance(model, dict):\n                    model = {k: v.to(self.device)", "CodeBleu_score": 32.39661567095122}, {"code": "def load_model_from_config(self, half_attention)\n    # Check if a cached model is available and valid\n    if self.use_cache and os.path.exists(self.model_cache_path):\n        try:\n            # Load the cached model\n            model = torch.load(self.model_cache_path)\n            # Check if the loaded model is valid\n            if isinstance(model, dict) and \"model\" in model:\n                model = model[\"model\"]\n                if isinstance(model, torch.nn.Module):\n                    # Ensure the model is in evaluation mode\n                    model.eval()\n                    # Move the model to the appropriate device\n                    model.to(self.device)\n                    # Return the loaded model\n                    return model\n        except Exception as e:\n            print(f\"Error loading cached model: {e}\")\n\n    # Load the model from the specified path\n    if self.model_path.endswith(\".safetensors\"):\n        # Load the model from a safetensors file\n        model = load_model_from_safetensors(self.model_path)\n    else:\n        # Load the model from a PyTorch-compatible file format\n        model = torch.load(self.model_path, map_location=self.device)\n\n    # Configure the model using settings from a YAML file\n    model = configure_model(model, self.model_config_path)\n\n    # Move the model to the appropriate device\n    model.to(self.device)\n\n    # Optionally cache the loaded model for future use\n    if self.use_cache:\n        # Ensure the cache directory exists\n        os.makedirs(os.path.dirname(self.model_cache_path), exist_ok=True)\n        # Cache the model\n        torch.save({\"model\": model}, self.model_cache_path)\n\n    # Optionally optimize the model for half-precision attention\n    if half_attention:\n        model = model.half()\n\n    # Optionally optimize the model for memory format\n    if self.optimize_memory_format:\n        # Convert the model to the appropriate memory format\n        model = model.to(memory_format=torch.", "CodeBleu_score": 39.749915216973776}, {"code": "def load_model_from_config(self, half_attention)\n    if self.config.get(\"cache_model\", False) and os.path.exists(self.config[\"cache_model_path\"]):\n        try:\n            model = torch.load(self.config[\"cache_model_path\"], map_location=self.device)\n            if model[\"config\"] == self.config:\n                print(\"Loaded model from cache.\")\n                self.model = model[\"model\"]\n                self.model.to(self.device)\n                self.model.eval()\n                return model\n        except:\n            pass\n\n    if self.config[\"model_path\"].endswith(\".safetensors\"):\n        model = load_file(self.config[\"model_path\"], device=self.device)\n    else:\n        model = torch.load(self.config[\"model_path\"], map_location=self.device)\n\n    if \"state_dict\" in model:\n        model = model[\"state_dict\"]\n\n    if half_attention:\n        model = half_attention_transformer_state_dict(model)\n\n    if self.config.get(\"optimize_memory\", False):\n        model = optimize_state_dict_memory(model)\n\n    self.model = build_model(self.config[\"model_config\"])\n    self.model.load_state_dict(model, strict=False)\n    self.model.to(self.device)\n    self.model.eval()\n\n    if self.config.get(\"cache_model\", False):\n        torch.save({\"config\": self.config, \"model\": self.model}, self.config[\"cache_model_path\"])\n\n    return {\"config\": self.config, \"model\": self.model}\n", "CodeBleu_score": 37.5079144830384}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    # make sure sane device and dtype\n    cond = cond.to(model.device, dtype=model.dtype)\n    shape = shape if isinstance(shape, list) else [shape, ]\n    # print the sampling parameters\n    print(f'Sampling with steps: {steps}')\n    t, *_ = utils.get_time_schedule(steps, model.num_timesteps)\n    print(f'using timesteps: {t}')\n    print(f'using eta: {eta}')\n    bs = shape[0]\n    # if we use DDIM, we need an extra step to do nothing\n    samples, intermediates = ddim_sampling.sample(model, steps, batch_size=bs, shape=shape,\n                                                 conditioning=cond, eta=eta, callback=callback,\n                                                 normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                                 quantize_x0=quantize_x0, temperature=temperature,\n                                                 score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                                 x_t=x_t)\n    return samples, intermediates\n\n", "CodeBleu_score": 40.95360567563542}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    from .ddim import DDIMSampler\n    sampler = DDIMSampler(model)\n    bs = shape[0]\n    print(f\"Beginning sampling with eta {eta} and {steps} steps\")\n    return sampler.sample(steps, bs, shape, cond, eta, callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 43.931882234474415}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    # make sure sane device and dtype.\n    device = next(model.parameters()).device\n    dtype = next(model.parameters()).dtype\n\n    # define shape and sample\n    sample_fn = diffusion.p_sample_ddim if not quantize_x0 else diffusion.p_sample_ddim_interp\n    normal_shape = (cond.shape[0], *shape[1:])\n    bs = normal_shape[0]\n    print(f'Begin sampling with eta {eta} and {steps} steps')\n    samples, intermediates = sample_fn(model, steps, normal_shape, cond, eta, callback=callback,\n                                       normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                       temperature=temperature, score_corrector=score_corrector,\n                                       corrector_kwargs=corrector_kwargs, x_T=x_t)\n    return samples, intermediates\n\n", "CodeBleu_score": 43.57793437038368}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if self.ema_weights is None:\n        return\n    if context is not None:\n        print(f\"{context}: Switching to EMA weights\")\n    self.model.set_weights(self.ema_weights)\n    if context is not None:\n        print(f\"{context}: Restoring original weights\")\n    self.model.set_weights(self.original_weights)\n\n", "CodeBleu_score": 53.87572916597711}, {"code": "def ema_scope(self, context=None)\n    if self.ema_weights is None:\n        self.ema_weights = self.model.get_weights()\n    if context is not None:\n        print(f\"{context} - Switching to EMA weights\")\n    self.model.set_weights(self.ema_weights)\n    if context is not None:\n        print(f\"{context} - Restoring original weights\")\n    self.model.set_weights(self.original_weights)\n", "CodeBleu_score": 52.92790880848821}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = {}\n    if self.ema_scope_enabled:\n        if not self.ema_scope_context:\n            self.ema_scope_context = context\n            self.ema_scope_enabled = False\n            self.ema_scope_context['ema_weights'] = self.ema_weights\n            self.ema_scope_context['model_weights'] = self.model_weights\n            self.ema_weights = self.model_weights\n            self.model_weights = self.ema_weights\n            if self.debug:\n                print('EMA scope enabled')\n        else:\n            self.ema_scope_enabled = False\n            self.ema_weights = self.ema_scope_context['ema_weights']\n            self.model_weights = self.ema_scope_context['model_weights']\n            self.ema_scope_context = None\n            if self.debug:\n                print('EMA scope disabled')\n    else:\n        self.ema_scope_enabled = True\n        self.ema_scope_context = context\n        self.ema_scope_context['ema_weights'] = self.ema_weights\n        self.ema_scope_context['model_weights'] = self.model_weights\n        self.ema_weights = self.model_weights\n        self.model_weights = self.ema_weights\n        if self.debug:\n            print('EMA scope enabled')", "CodeBleu_score": 22.23198690106726}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    state_dict = torch.load(path, map_location='cpu')\n    if ignore_keys:\n        for key in ignore_keys:\n            state_dict.pop(key)\n    self.load_state_dict(state_dict, strict=False)\n    print('Loaded from %s' % path)\n    if ignore_keys:\n        print('Ignore keys %s' % ignore_keys)", "CodeBleu_score": 17.59355720432313}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    ignore_keys = [] if ignore_keys is None else ignore_keys\n    state_dict = torch.load(path, map_location=\"cpu\")\n    for k in ignore_keys:\n        if k in state_dict:\n            state_dict.pop(k)\n    self.load_state_dict(state_dict, strict=False)\n    del state_dict\n    print(f\"weights loaded from {path}\")\n    self.print_trainable_parameters()\n\n", "CodeBleu_score": 26.15029134245405}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if \"state_dict\" in state_dict:\n        state_dict = state_dict[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in list(state_dict.keys()):\n            if k in ignore_keys:\n                print(f\"deleting key {k} from pretrained checkpoint\")\n                del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    print(f\"loaded model from {path}\")\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print(f\"missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"unexpected keys: {unexpected_keys}\")", "CodeBleu_score": 38.878341389667426}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n    if not isinstance(input, torch.Tensor):\n        input = torch.Tensor(input)\n    input = input.to(self.device)\n    encoded = self.encoder(input)\n    decoded = self.decoder(encoded)\n    if return_pred_indices:\n        return decoded, self.get_difference_metric(input, decoded), self.get_pred_indices(input, decoded)\n    return decoded, self.get_difference_metric(input, decoded)\n", "CodeBleu_score": 33.26704255970794}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode input\n    encoded_input = self.encoder(input)\n    # Decode input\n    decoded_output = self.decoder(encoded_input)\n    # Calculate difference between input and output\n    diff = torch.abs(input - decoded_output)\n    # Return decoded output and difference\n    if return_pred_indices:\n        return decoded_output, diff, encoded_input\n    else:\n        return decoded_output, diff", "CodeBleu_score": 23.664444121497947}, {"code": "def forward(self, input, return_pred_indices=False)\n    self.reset()\n    self.encode(input)\n    self.decode()\n    if return_pred_indices:\n        return self.pred_indices\n    else:\n        return self.decoded_output", "CodeBleu_score": 26.540493662184435}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n    if batch is None:\n        return None\n    x = batch[k]\n    if x is None:\n        return None\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if x.shape[1] != 3:\n        x = x.permute(0, 3, 1, 2)\n    x = x.to(memory_format=torch.contiguous_format).float()\n    if self.training and self.batch_size_range is not None:\n        n = int(np.interp(self.t, self.batch_size_range, self.batch_size_range_output))\n        if n != self.batch_size:\n            self.batch_size = n\n            self.t0 = self.t\n    if self.batch_size_range is not None:\n        x = F.interpolate(x, self.size, mode=\"bicubic\", align_corners=False)\n    return x.detach()\n\n", "CodeBleu_score": 54.498884056484485}, {"code": "def get_input(self, batch, k)\n    if self.batch_size_mode == 'sequential':\n        batch = batch[k].to(self.device)\n        if len(batch.shape) == 3:\n            batch = batch[None]\n        if batch.shape[2] != 3:\n            batch = batch.permute(0, 3, 1, 2)\n        batch = batch.contiguous().float()\n        if self.batch_resizing:\n            if self.training:\n                progress = self.step / self.steps\n                width = int(np.clip(self.batch_range[0] + progress * (self.batch_range[1] - self.batch_range[0]), *self.batch_range))\n                height = int(np.clip(self.batch_range[0] + progress * (self.batch_range[1] - self.batch_range[0]), *self.batch_range))\n                batch = torch.nn.functional.interpolate(batch, (height, width), mode='bicubic', align_corners=False)\n            else:\n                batch = torch.nn.functional.interpolate(batch, self.batch_range, mode='bicubic', align_corners=False)\n        return batch.detach()", "CodeBleu_score": 37.48755136005572}, {"code": "def get_input(self, batch, k)\n    if type(batch) == dict:\n        input = batch[k]\n    else:\n        input = batch\n    if len(input.shape) == 3:\n        input = input[..., None]\n    if input.shape[1] != 3:\n        input = input.permute(0, 3, 1, 2)\n    input = input.to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range > 0:\n        input = TF.resize(input, self.batch_resize_range, interpolation=InterpolationMode.BICUBIC)\n    return input.detach()\n\n", "CodeBleu_score": 34.84789245485683}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x = x.view(x.size(0), -1)\n    z = self.encoder(x)\n    x_hat = self.decoder(z)\n    loss = self.loss_fn(x_hat, x)\n    self.log(\"train_loss\", loss)\n    return loss\n\n", "CodeBleu_score": 5.1883400181998365}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    y_hat = self(x)\n    if optimizer_idx == 0:\n        loss = self.loss_fn(y_hat.view(-1), y.view(-1))\n        self.log(\"train_loss\", loss)\n    if optimizer_idx == 1:\n        y_hat = y_hat.view(-1)\n        y = y.view(-1)\n        d_loss = self.discriminator_loss(y_hat, y)\n        self.log(\"d_loss\", d_loss)\n    return loss", "CodeBleu_score": 12.862486938244519}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    y_hat = self(x)\n    loss = self.loss_fn(y_hat, y)\n    self.log(\"train_loss\", loss)\n    return loss\n\n", "CodeBleu_score": 4.650885069627061}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if only_inputs:\n        x = batch\n        x_recon = x\n    else:\n        x, x_recon = batch\n    x = x.to(self.device)\n    x_recon = x_recon.to(self.device)\n    if self.ema is not None and plot_ema:\n        x_recon_ema = self.ema(x).to(self.device)\n    else:\n        x_recon_ema = None\n    if self.colorize_input:\n        x = self.colorize(x)\n        x_recon = self.colorize(x_recon)\n        if x_recon_ema is not None:\n            x_recon_ema = self.colorize(x_recon_ema)\n    if x.shape[1] == 1:\n        x = x.repeat(1, 3, 1, 1)\n        x_recon = x_recon.repeat(1, 3, 1, 1)\n        if x_recon_ema is not None:\n            x_recon_ema = x_recon_ema.repeat(1, 3, 1, 1)\n    images = torch.cat([x, x_recon], dim=0)\n    if x_recon_ema is not None:\n        images = torch.cat([images, x_recon_ema], dim=0)\n    grid = torchvision.utils.make_grid(images, nrow=x.shape[0])\n    self.logger.experiment.add_image(\"reconstructions\", grid, self.current_epoch)\n", "CodeBleu_score": 34.042020764808164}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    inputs = batch[0]\n    if self.ema is not None and plot_ema:\n        with self.ema.average_parameters():\n            reconstructions = self(inputs)\n    else:\n        reconstructions = self(inputs)\n\n    if only_inputs:\n        return self.log_images_to_summary(inputs, name=\"inputs\", **kwargs)\n\n    self.log_images_to_summary(inputs, name=\"inputs\", **kwargs)\n    self.log_images_to_summary(reconstructions, name=\"reconstructions\", **kwargs)", "CodeBleu_score": 23.338958070537807}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if self.ema is not None and plot_ema:\n        self.ema.store(self.model.parameters())\n        self.ema.copy_to(self.model.parameters())\n\n    x, y = batch\n\n    # Input images\n    x = x.to(self.device)\n    log_dict = {\"Inputs\": self.get_input_grid(x, only_inputs=only_inputs)}\n\n    # Reconstructions and EMA\n    if not only_inputs:\n        x_recon = self.model(x)\n        log_dict[\"Reconstructions\"] = self.get_input_grid(x_recon, only_inputs=only_inputs)\n        if self.ema is not None and plot_ema:\n            self.ema.restore(self.model.parameters())\n            x_recon_ema = self.model(x)\n            log_dict[\"Reconstructions EMA\"] = self.get_input_grid(\n                x_recon_ema, only_inputs=only_inputs\n            )\n\n    self.logger.experiment.add_images(\"Validation\", log_dict, self.global_step, **kwargs)", "CodeBleu_score": 34.82977654149593}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(\n            'Loss type {} is not recognized.'.format(self.loss_type))\n\n    return loss\n\n", "CodeBleu_score": 57.15991922081077}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(self.loss_type)\n    return loss\n\n", "CodeBleu_score": 54.94034906420227}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l1':\n        loss = abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise Exception(\"unknown loss_type\")\n    return loss\n\n", "CodeBleu_score": 47.78836917011631}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n    if self.training:\n        if self.use_time_conditioning:\n            c_time = self.time_conditioning(c)\n            c = torch.cat([c, c_time], dim=1)\n        if self.use_conditional_noise:\n            c_noise = self.conditional_noise(c)\n            c = torch.cat([c, c_noise], dim=1)\n        if self.use_conditional_dropout:\n            c_dropout = self.conditional_dropout(c)\n            c = torch.cat([c, c_dropout], dim=1)\n    return self.model(x, c)\n\n", "CodeBleu_score": 31.901617874140353}, {"code": "def forward(self, x, c, *args, **kwargs)\n    if self.training:\n        t = th.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n        return self.p_losses(x, c, t, *args, **kwargs)\n    else:\n        return self.p_losses(x, c, *args, **kwargs)\n\n", "CodeBleu_score": 33.16324166673482}, {"code": "def forward(self, x, c, *args, **kwargs)\n    if self.conditional:\n        c = self.cond_net(c)\n    if self.time_steps is not None:\n        t = torch.randint(0, self.time_steps, (x.shape[0],)).to(x.device)\n    else:\n        t = None\n    return self.model(x, c, t, *args, **kwargs)", "CodeBleu_score": 33.52784024484472}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if indices.ndim == 2:\n        indices = indices[None]\n    shape = tuple(indices.shape[0:1]) + shape[1:]\n    latents = self.get_codebook_entry_inner(indices.long(), shape)\n    if latents.shape[0] == 1 and len(shape) == 3:\n        latents = latents[0, :, :, :]\n    return latents\n", "CodeBleu_score": 43.79807887059191}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if self.permuter is not None:\n        indices = self.permuter.inverse(indices)\n    bhwc = np.reshape(indices, (-1, 1))\n    quantized = self.codebook.get_codebook_entry(bhwc, shape)\n    if self.permuter is not None:\n        quantized = self.permuter.permute(quantized)\n    return quantized\n\n", "CodeBleu_score": 47.71082664689293}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_used:\n        indices = self.unmap_to_all(indices)\n    if self.permuter is not None:\n        indices = self.permuter.inverse(indices)\n    bhwc = np.reshape(indices, (-1, 1))\n    latents = self.codebook.get_codebook_entry(bhwc, 1)\n    latents = latents.view(shape)\n    latents = latents.permute(0, 3, 1, 2).contiguous()\n    return latents\n\n", "CodeBleu_score": 54.82133863422648}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        m, n = dimension, 1\n        for i in range(2, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                if i > dimension // i:\n                    break\n                if factor > 0 and i > factor:\n                    break\n                if dimension // i < m:\n                    m, n = i, dimension // i\n        return m, n\n", "CodeBleu_score": 37.90356119648294}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            return None\n    else:\n        for i in range(1, dimension + 1):\n            if dimension % i == 0:\n                m = i\n                n = dimension // i\n                if m <= n:\n                    return m, n\n\n", "CodeBleu_score": 31.755314101889308}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            return None, None\n    else:\n        min_sum = float('inf')\n        min_m = None\n        min_n = None\n        for m in range(1, int(dimension ** 0.5) + 1):\n            if dimension % m == 0:\n                n = dimension // m\n                if m <= n and m + n < min_sum:\n                    min_sum = m + n\n                    min_m = m\n                    min_n = n\n        return min_m, min_n\n\n", "CodeBleu_score": 37.880485296827324}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if updown:\n        # Apply the weight decomposition\n        weight = self.weight_decompose(orig_weight)\n    else:\n        # Apply the weight reconstruction\n        weight = self.weight_reconstruct(orig_weight)\n\n    # Normalize the weight\n    weight = self.normalize_weight(weight)\n\n    # Scale the weight\n    weight = self.scale_weight(weight)\n\n    # Ensure that the tensors are on the same device and have the same data type\n    weight = weight.to(orig_weight.device).to(orig_weight.dtype)\n\n    # Calculate the adjusted weight\n    adjusted_weight = weight - orig_weight\n\n    return adjusted_weight\n\n", "CodeBleu_score": 22.878822575453807}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if orig_weight.ndim == 1:\n        return updown\n    else:\n        if updown.ndim == 1:\n            updown = updown.unsqueeze(0)\n        if orig_weight.ndim == 2:\n            orig_weight = orig_weight.unsqueeze(0)\n        updown = updown.to(orig_weight.device)\n        updown = updown.to(orig_weight.dtype)\n        orig_weight = orig_weight.to(updown.device)\n        orig_weight = orig_weight.to(updown.dtype)\n        updown = (updown * self.scale) / torch.norm(updown, dim=1, keepdim=True)\n        updown = updown - orig_weight\n        updown = updown / torch.norm(updown, dim=1, keepdim=True)\n        return updown", "CodeBleu_score": 36.19095844682889}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    device = updown.weight.device\n    dtype = updown.weight.dtype\n    orig_weight = orig_weight.to(device).to(dtype)\n    updown.weight.data = (updown.weight + self.scale * (orig_weight - updown.weight.div(orig_weight.norm()).mul(orig_weight.norm())))\n\n", "CodeBleu_score": 11.635405856059364}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias is not None:\n        self.bias.data = self.bias.data * self.multiplier\n    if self.scaling_factor != 1:\n        updown = self.decompose_weight(updown)\n    return updown, self.bias", "CodeBleu_score": 45.01083447881133}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.updown_scale\n    if self.updown_scale != 1.0:\n        updown = updown * self.updown_scale\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    return updown, ex_bias", "CodeBleu_score": 35.60591822014491}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.updown_bias_multiplier\n    if updown.numel() == orig_weight.numel():\n        updown = updown.view(orig_weight.shape)\n    updown = updown.reshape(output_shape)\n    if self.updown_scaler is not None:\n        updown = self.updown_scaler(updown)\n    return updown, ex_bias", "CodeBleu_score": 38.99360866492802}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n    self.weight.data = orig_weight.data.to(self.weight.device)\n    if self.bias is not None:\n        self.bias.data = orig_weight.bias.data.to(self.bias.device)\n    return self.finalize_updown()\n", "CodeBleu_score": 23.46922087562953}, {"code": "def calc_updown(self, orig_weight)\n    if self.bias is not None:\n        self.bias.data = self.bias.data.to(device=orig_weight.device)\n    self.weight.data = self.weight.data.to(device=orig_weight.device)\n    return self._calc_updown(orig_weight)\n\n", "CodeBleu_score": 38.05483115453625}, {"code": "def calc_updown(self, orig_weight)\n    if self.weight.device != orig_weight.device:\n        self.weight.data = self.weight.to(orig_weight.device)\n        if self.bias is not None:\n            self.bias.data = self.bias.to(orig_weight.device)\n    self.weight.grad.data = self.weight.grad.to(orig_weight.device)\n    if self.bias is not None:\n        self.bias.grad.data = self.bias.grad.to(orig_weight.device)\n    return self.finalize_updown()\n\n", "CodeBleu_score": 41.537556651096786}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n    import gc\n    import sys\n    import os\n    import time\n    import psutil\n    import torch\n    from torch import nn\n    from torch.nn import functional as F\n    from torch.nn.utils import prune\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.nn.utils import clip_grad_norm_\n    from torch.nn.utils import clip_grad_value_\n    from torch.", "CodeBleu_score": 12.71524387502623}, {"code": "def purge_networks_from_memory()\n    from gc import collect\n    from tensorflow.keras.backend import clear_session\n    from tensorflow.keras.models import load_model\n    from os import listdir\n    from os import remove\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join\n    from os import listdir\n    from os.path import isfile, join", "CodeBleu_score": 12.64208867700543}, {"code": "def purge_networks_from_memory()\n    global networks\n    if len(networks) > max_networks:\n        for i in range(len(networks) - max_networks):\n            del networks[0]\n        gc.collect()\n\n", "CodeBleu_score": 29.878707371101342}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, \"backup_weights\"):\n        self.weight.data.copy_(self.backup_weights)\n    if hasattr(self, \"backup_bias\"):\n        self.bias.data.copy_(self.backup_bias)\n    if hasattr(self, \"backup_proj\"):\n        self.proj.weight.data.copy_(self.backup_proj)\n    if hasattr(self, \"backup_proj_bias\"):\n        self.proj.bias.data.copy_(self.backup_proj_bias)\n    if hasattr(self, \"backup_proj_bias\") is False:\n        self.bias = None\n\n", "CodeBleu_score": 26.88349897095259}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, 'weight_backup'):\n        self.weight = self.weight_backup\n    if hasattr(self, 'bias_backup'):\n        self.bias = self.bias_backup\n    if hasattr(self, 'projection_weight_backup'):\n        self.projection_weight = self.projection_weight_backup\n    if hasattr(self, 'projection_bias_backup'):\n        self.projection_bias = self.projection_bias_backup\n    if hasattr(self, 'bias_backup'):\n        self.bias = self.bias_backup\n    else:\n        self.bias = None\n", "CodeBleu_score": 29.155588906540736}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, \"_backup_weights\"):\n        self.weight = self._backup_weights\n        del self._backup_weights\n    if hasattr(self, \"_backup_bias\"):\n        self.bias = self._backup_bias\n        del self._backup_bias\n    if hasattr(self, \"_backup_proj_weights\"):\n        self.weight = self._backup_proj_weights\n        del self._backup_proj_weights\n    if hasattr(self, \"_backup_proj_bias\"):\n        self.bias = self._backup_proj_bias\n        del self._backup_proj_bias\n", "CodeBleu_score": 27.782857256248093}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n    if len(org_module.networks) == 0:\n        return original_forward(input)\n    device = input.device\n    for layer in org_module.layers:\n        layer.weight.data = layer.weight_backup.to(device)\n        layer.bias.data = layer.bias_backup.to(device)\n    output = original_forward(input)\n    for layer in org_module.layers:\n        layer.weight.data = layer.weight_backup.to(device)\n        layer.bias.data = layer.bias_backup.to(device)\n    for network in org_module.networks:\n        if network.layer == org_module.layer_index:\n            output = network.forward(output)\n    return output", "CodeBleu_score": 31.817552798448656}, {"code": "def network_forward(org_module, input, original_forward)\n    if len(org_module.loaded_networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(org_module.device)\n        org_module.load_state_dict(org_module.backup_state_dict)\n        output = original_forward(input)\n        for network in org_module.loaded_networks:\n            if network.layer == org_module.layer:\n                output = network(output)\n        return output", "CodeBleu_score": 31.344969854160297}, {"code": "def network_forward(org_module, input, original_forward)\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torch.utils.data\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision", "CodeBleu_score": 12.755342751517688}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n    trainable_params = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainable_params.append(layer.weight)\n            trainable_params.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainable_params.append(layer.weight)\n            trainable_params.append(layer.bias)\n    return trainable_params\n", "CodeBleu_score": 35.22706494146498}, {"code": "def trainables(self)\n    trainable_params = []\n    for layer in self.model.children():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainable_params.extend(list(layer.parameters()))\n    return trainable_params\n\n", "CodeBleu_score": 37.08173729251178}, {"code": "def trainables(self)\n    return [p for p in self.parameters() if p.requires_grad]\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    if len(names) != len(multipliers):\n        print(\"Error: The number of hypernetwork names and multipliers must be the same.\")\n        return\n    hypernetworks = []\n    for i, name in enumerate(names):\n        if name in loaded_hypernetworks:\n            hypernetwork = loaded_hypernetworks[name]\n        else:\n            hypernetwork = Hypernetwork(name)\n            loaded_hypernetworks[name] = hypernetwork\n        hypernetwork.multiplier = multipliers[i]\n        hypernetworks.append(hypernetwork)\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 37.40399786854589}, {"code": "def load_hypernetworks(names, multipliers=None)\n    if multipliers is None:\n        multipliers = [1] * len(names)\n    loaded_hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        if name in loaded_hypernetworks:\n            hypernetwork = loaded_hypernetworks[name]\n        else:\n            hypernetwork = Hypernetwork(name)\n            loaded_hypernetworks[name] = hypernetwork\n        hypernetwork.multiplier = multiplier\n        loaded_hypernetworks.append(hypernetwork)\n    return loaded_hypernetworks\n\n", "CodeBleu_score": 30.329481586085127}, {"code": "def load_hypernetworks(names, multipliers=None)\n    from . import hypernetworks\n    hypernetworks.loaded_hypernetworks.clear()\n    for i in range(len(names)):\n        if names[i] in hypernetworks.hypernetwork_index_dict:\n            hypernetworks.loaded_hypernetworks.append(hypernetworks.hypernetwork_index_dict[names[i]])\n        else:\n            hypernetworks.loaded_hypernetworks.append(hypernetworks.Hypernetwork(names[i]))\n        if multipliers is not None:\n            hypernetworks.loaded_hypernetworks[i].multiplier = multipliers[i]\n\n", "CodeBleu_score": 30.819497101955275}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 75.93453377925789}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n", "CodeBleu_score": 71.58115203601199}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n", "CodeBleu_score": 75.93453377925789}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        context = x\n    if self.use_hypernet:\n        context = self.hypernet(context)\n    if self.use_scalenet:\n        context = self.scalenet(context)\n    query = self.to_q(x)\n    key = self.to_k(context)\n    value = self.to_v(context)\n    (bs, nh, hs, isz, tsz) = query.shape\n    (bs, nh, hs, esz, tsz) = key.shape\n    query = query.reshape((bs * nh, hs, isz, tsz))\n    key = key.reshape((bs * nh, hs, esz, tsz))\n    value = value.reshape((bs * nh, hs, esz, tsz))\n    (out, _) = attention_scaled_dot_prod_attention(query, key, value, mask=mask, dropout_p=self.dropout)\n    out = out.reshape((bs, nh, hs, isz, tsz))\n    out = out.permute(0, 2, 1, 3, 4)\n    out = out.reshape((bs, hs, isz, tsz))\n    return self.to_out(out)", "CodeBleu_score": 30.80948146413166}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if self.use_hypernet:\n        context = self.hypernet(context)\n    (q, k, v) = (self.to_q(x), self.to_k(context), self.to_v(context))\n    (b, h, s, d) = q.shape\n    (q, k, v) = map(lambda t: t.reshape(b, h, s, self.heads, self.d_head).permute(0, 2, 1, 3, 4), (q, k, v))\n    q = q * self.scale\n    sim = torch.einsum('... i d, ... j d  -> ... i j', q, k)\n    if mask is not None:\n        mask = mask.unsqueeze(1).unsqueeze(1)\n        sim = sim * mask\n    attn = sim.softmax(dim=-1)\n    out = torch.einsum('... i j, ... j d -> ... i d', attn, v)\n    out = out.permute(0, 2, 1, 3).reshape(b, s, h * self.d_head)\n    return self.to_out(out)", "CodeBleu_score": 44.47571548749214}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    assert self.query_key_value.split(',') == ['q', 'k', 'v']\n    assert self.attention_type == 'cross'\n    assert self.causal_attention is False\n    assert self.cross_attention_dim is not None\n    assert self.cross_attention_shape is not None\n    assert self.cross_attention_shape in ['per_row', 'per_token']\n    assert self.cross_attention_shape == 'per_row'\n    assert self.num_heads is not None\n    assert self.head_dim is not None\n    assert self.scale is not None\n    assert self.upcast_attention is not None\n    assert self.upcast_softmax is not None\n    assert self.upcast_cross_attention is not None\n    assert self.flash_attention is not None\n    assert self.flash_attention_causal is not None\n    assert self.flash_attention_chunk_size is not None\n    assert self.flash_attention_memory_efficient is not None\n    assert self.flash_attention_causal_chunk_size is not None\n    assert self.flash_attention_causal_guidance_scale is not None\n    assert self.flash_attention_causal_use_mask is not None\n    assert self.flash_attention_causal_use_attention_bias is not None\n    assert self.flash_attention_causal_use_query_bias is not None\n    assert self.flash_attention_causal_use_key_bias is not None\n    assert self.flash_attention_causal_use_value_bias is not None\n    assert self.flash_attention_causal_use_out_proj_bias is not None\n    assert self.flash_attention_causal_use_rotary_emb is not None\n    assert self.flash_attention_causal_use_alibi is not None\n    assert self.flash_attention_causal_alibi_bias is not None\n    assert self.flash_attention_causal_alibi_value is not None\n    assert self.flash_attention_causal_al", "CodeBleu_score": 11.089974816421558}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    # Backup the original attributes\n    original_hypernetwork_name = hypernetwork.hypernetwork_name\n    original_filename = hypernetwork.filename\n\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    hypernetwork.filename = filename\n\n    try:\n        # Attempt to save the hypernetwork\n        hypernetwork.save_hypernetwork(checkpoint)\n    except Exception as e:\n        # Restore the original attributes before re-raising the exception\n        hypernetwork.hypernetwork_name = original_hypernetwork_name\n        hypernetwork.filename = original_filename\n        raise e\n\n    # Restore the original attributes\n    hypernetwork.hypernetwork_name = original_hypernetwork_name\n    hypernetwork.filename = original_filename\n\n", "CodeBleu_score": 32.559603337604266}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    hypernetwork_name = hypernetwork_name.replace(\" \", \"_\").lower()\n    hypernetwork_name = hypernetwork_name.replace(\".\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\",\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"-\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\":\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\";\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"(\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\")\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"[\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"]\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"{\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"}\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"|\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"\\\\\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"/\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"?\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"<\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\">\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"*\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"'\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace('\"', \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"`\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"~\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"@\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"#\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"$\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"%\", \"_\")\n    hypernetwork_name = hypernetwork_name.replace(\"^\", \"_\")\n    hypernetwork_name = hypernetwork_name.", "CodeBleu_score": 25.457526261620295}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    try:\n        # Backup the original attributes\n        original_state = hypernetwork.state\n        original_checkpoint = hypernetwork.checkpoint\n        original_hypernetwork_name = hypernetwork.hypernetwork_name\n        original_filename = hypernetwork.filename\n\n        # Update the hypernetwork with the new checkpoint data\n        hypernetwork.state = checkpoint\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.hypernetwork_name = hypernetwork_name\n        hypernetwork.filename = filename\n\n        # Save the hypernetwork\n        hypernetwork.save()\n\n    except Exception as e:\n        # Restore the original attributes if an error occurs\n        hypernetwork.state = original_state\n        hypernetwork.checkpoint = original_checkpoint\n        hypernetwork.hypernetwork_name = original_hypernetwork_name\n        hypernetwork.filename = original_filename\n\n        raise e\n\n", "CodeBleu_score": 40.50600126650682}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n    import copy\n    import torch\n    from torch.nn.parallel import DistributedDataParallel\n    if context is None:\n        context = locals()\n    if not self.ema_enabled:\n        yield\n        return\n    try:\n        if self.ema_weights is None:\n            self.ema_weights = copy.deepcopy(self.model.state_dict())\n        self.model.load_state_dict(self.ema_weights)\n        if self.ema_log_info:\n            print('Switched to EMA weights.')\n        yield\n    finally:\n        self.model.load_state_dict(self.model_weights)\n        if self.ema_log_info:\n            print('Restored model weights.')\n", "CodeBleu_score": 37.13316440980167}, {"code": "def ema_scope(self, context=None)\n    if self.ema_enabled:\n        self.ema_backup = self.ema_model.state_dict()\n        self.ema_model.load_state_dict(self.model.state_dict())\n        if context is not None:\n            print(f\"{context}: Switched to EMA weights\")\n    yield\n    if self.ema_enabled:\n        self.model.load_state_dict(self.ema_backup)\n        self.ema_model.load_state_dict(self.ema_backup)\n        if context is not None:\n            print(f\"{context}: Restored model parameters\")", "CodeBleu_score": 68.6658806523757}, {"code": "def ema_scope(self, context=None)\n    if self.ema_status:\n        if context is not None:\n            print(f\"{context} - EMA weights are used.\")\n        self.model_ema.update_attr(self.model, ema_only=True)\n        yield\n        self.model_ema.update_attr(self.model, ema_only=True)\n        if context is not None:\n            print(f\"{context} - EMA weights are restored.\")\n    else:\n        yield", "CodeBleu_score": 56.58943695175764}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n        B, C = t.shape\n        assert model_output.shape == (B, C * 2, 1, 1)\n        model_output, model_var_values = torch.split(model_output, C, dim=1)\n        if self.model_var_type == ModelVarType.LEARNED:\n            model_log_variance = model_var_values\n            model_variance = torch.exp(model_log_variance)\n        else:\n            min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t)\n            max_log = _extract_into_tensor(np.log(self.betas), t)\n            # The model_var_values is [-1, 1] for [min_var, max_var].\n            frac = (model_var_values + 1) / 2\n            model_log_variance = frac * max_log + (1 - frac) * min_log\n            model_variance = torch.exp(model_log_variance)\n    else:\n        model_variance, model_log_variance = {\n            # For fixedlarge, we set the initial (log-)variance to\n            # sigmas.\n            ModelVarType.FIXED_LARGE: (\n                torch.tensor(self.sigmas[t] ** 2).to(model_output.device),\n                torch.tensor(np.log(self.sigmas[t] ** 2)).to(model_output.device),\n            ),\n            # For fixedsmall, we set the initial (log-)variance to\n            # 0.\n            ModelVarType.FIXED_SMALL: (\n                self.sigmas[t] ** 2 * torch.ones_like(model_output),\n                torch.zeros_like(model_output),\n            ),\n        }[self.model_var_type]\n    pred_original_", "CodeBleu_score": 28.17066388260111}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n        model_output = self.model(x, t)\n        if self.model_var_type == ModelVarType.LEARNED:\n            assert self.model_output_type == \"epsilon\"\n            return model_output\n        else:\n            return th.cat([model_output, self.model_output_range.to(model_output.device)], dim=1)\n    else:\n        if self.use_clipped_model_output:\n            return self.clipped_model_mean_variance(x, t, clip_denoised)\n        else:\n            return self.model_mean_variance(x, t)\n\n", "CodeBleu_score": 29.169734732871312}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n        model_output = self.model(x, t)\n        if self.model_var_type == ModelVarType.LEARNED:\n            assert self.model_output_type == \"epsilon\"\n            return model_output\n        else:\n            assert self.model_output_type == \"epsilon\"\n            return th.cat([model_output, model_output], dim=1)\n    else:\n        if self.model_output_type == \"epsilon\":\n            return {\"sample\": self.predict_epsilon_from_x(x, t, clip_denoised=clip_denoised)}\n        elif self.model_output_type == \"v\":\n            return {\"sample\": self.predict_v_from_x(x, t, clip_denoised=clip_denoised)}\n        elif self.model_output_type == \"logit\":\n            assert self.model_var_type == ModelVarType.FIXED_SMALL\n            return {\"sample\": self.predict_logit_from_x(x, t, clip_denoised=clip_denoised)}\n        else:\n            raise NotImplementedError()\n\n", "CodeBleu_score": 27.2261621165485}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError(self.loss_type)\n    return loss\n", "CodeBleu_score": 53.86968263770287}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise Exception(\"Unknown loss_type\")\n    return loss\n", "CodeBleu_score": 55.42328048440235}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError(self.loss_type)\n    return loss\n", "CodeBleu_score": 53.86968263770287}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, log_variance = self.q_mean_log_variance(x_start=x_start, t=t)\n    return mean, log_variance\n\n", "CodeBleu_score": 35.56670199290848}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, dtype=torch.long, device=x_start.device)\n    qt_log_variance_clipped = self.clip_log_variance(x_start, t)\n    kl = utils.calc_kl(x_start, torch.zeros_like(x_start), torch.zeros_like(x_start), qt_log_variance_clipped, torch.zeros_like(qt_log_variance_clipped))\n    return torch.mean(kl) / math.log(2.0)\n\n", "CodeBleu_score": 40.20778541132819}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, log_var = self.q_mean_logvar(x_start, t)\n    return torch.mean(normal_kl(mean, log_var, mean_posterior, log_var_posterior)) / np.log(2.0)\n", "CodeBleu_score": 39.499249895807644}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        os.path.join(os.getcwd(), \"cache\", f\"{subsection}\"),\n        size_limit=4e9,\n        disk_min_file_size=256e3,\n    )\n\n", "CodeBleu_score": 49.50784247051432}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        os.path.join(CACHE_DIR, subsection),\n        size_limit=4e9,\n        disk_min_file_size=256e3,\n    )\n\n", "CodeBleu_score": 51.41610143619013}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        os.path.join(\n            os.path.expanduser(\"~\"),\n            \".cache\",\n            \"openai-cli\",\n            \"cache\",\n            subsection,\n        ),\n        size_limit=4 * 1024 ** 3,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 56.68091137632805}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n    global caches\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        if not os.path.isdir(cache_dir):\n            os.makedirs(cache_dir)\n        if os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 45.438579705662555}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        if not os.path.isdir(cache_directory):\n            os.makedirs(cache_directory)\n        if os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n", "CodeBleu_score": 44.49330902123897}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    cache_lock.acquire()\n    if subsection not in caches:\n        if not os.path.exists(cache_directory):\n            os.makedirs(cache_directory)\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n    cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 41.19341026505063}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title]['mtime'] >= mtime:\n            return cache[title]['data']\n        else:\n            cache[title] = {'mtime': mtime, 'data': None}\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    dump_cache(cache)\n    return data\n", "CodeBleu_score": 27.124207865375965}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title]['mtime'] < mtime:\n            cache[title] = None\n        else:\n            return cache[title]['data']\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 24.666954315309248}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title][\"mtime\"] >= mtime:\n        return cache[title][\"data\"]\n    data = func()\n    cache[title] = {\"data\": data, \"mtime\": mtime}\n    dump_cache(cache)\n    return data\n", "CodeBleu_score": 17.331012215808784}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 57.91195741057486}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 57.91195741057486}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception(exc_info)\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(format_exc().rstrip(), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 56.133304295667166}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = extract_style_text(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 54.23618178758871}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == '' or style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    try:\n        prompt = extract_style_text(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n", "CodeBleu_score": 52.55984189651588}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = extract_style_text(style.prompt, prompt)\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except Exception:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 48.9019683804369}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n    if self._X is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u, u.random_partial(n_frames)) for u in utterances]\n\n    X_batch, y_batch = zip(*[(x.data, x.label) for x in a])\n\n    return X_batch, y_batch\n\n", "CodeBleu_score": 68.64460578124442}, {"code": "def random_partial(self, count, n_frames)\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n    if self.utterances == None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u, u.random_partial(n_frames)) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 78.08975544795668}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    change_in_dBFS = target_dBFS - wav.dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(change_in_dBFS)", "CodeBleu_score": 55.59478144260163}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    change_in_dBFS = target_dBFS - wav.dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(change_in_dBFS)\n\n", "CodeBleu_score": 55.59478144260163}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    change_in_dBFS = target_dBFS - wav.dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(change_in_dBFS)\n\n", "CodeBleu_score": 55.59478144260163}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\n    global _model\n\n    if device is None:\n        _model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _model_device = torch.device(device)\n    else:\n        _model_device = device\n\n    _model = SpeakerEncoder(_model_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(f\"Loaded encoder \\\"{weights_fpath.name}\\\" trained to step {checkpoint['step']}\")\n\n", "CodeBleu_score": 67.83704619286101}, {"code": "def load_model(weights_fpath: Path, device=None)\n\n    global _model\n\n    if device is None:\n        _model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _model_device = torch.device(device)\n    else:\n        _model_device = device\n\n    _model = SpeakerEncoder(_model_device, _model_hidden_size).to(_model_device)\n\n    checkpoint = torch.load(weights_fpath, _model_device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 70.38380885148987}, {"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    else:\n        _device = device\n\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    _model.to(_device)\n\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 77.94885092554784}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 62.39161247866224}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception('Model not loaded')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n", "CodeBleu_score": 62.2898862640771}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"Model is not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n", "CodeBleu_score": 64.03502800452469}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n    pad_samples = int((partial_utterance_n_frames - overlap) * step)\n    mel_samples = partial_utterance_n_frames\n    hop_samples = int((1 - overlap) * step)\n    slices = []\n    wav_slices = []\n    mel_slices = []\n    steps = max(1, int(pad_samples / step))\n    wav_n = 0\n    while wav_n < n_samples:\n        wav_slice = slice(wav_n, wav_n + mel_samples)\n        mel_slice = slice(mel_n, mel_n + mel_samples)\n        mel_n += mel_samples\n        slices.append((wav_slice, mel_slice))\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n        wav_n += hop_samples\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    if last_wav_slice.stop - last_mel_slice.stop < min_pad_coverage * mel_samples:\n        i = len(mel_slices) - 1\n        while last_wav_slice.stop - last_mel_slice.stop < min_pad_coverage * mel_samples:\n            if i == 0:\n                mel_samples = last_wav_slice.stop - last_mel_slice.stop\n                break\n            mel_samples += mel_slices[i].stop - mel_slices[i].start\n            i -= 1\n            last_mel_slice = mel_slices[i]\n        last_mel_slice = slice(last_mel_slice.start, last_mel_slice.start + mel_samples)\n        mel_slices[-1] = last_mel_slice\n        wav_slices[-1] = last_wav_slice\n    return", "CodeBleu_score": 34.06622926498098}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n    pad_samples = int((partial_utterance_n_frames - overlap) * min_pad_coverage)\n    # Make sure that the last tap covers at least min_pad_coverage of the partial utterance\n    # By default, partial utterance of 0.75 covers at least 75% of the last tap.\n    last_step_samples = n_samples % step\n    if last_step_samples <= pad_samples:\n        step -= last_step_samples\n    elif last_step_samples > pad_samples:\n        step += partial_utterance_n_frames - last_step_samples\n    wav_slices, mel_slices = [], []\n    steps = gen_steps(n_samples, step, partial_utterance_n_frames)\n    for i in range(steps):\n        start = step * i\n        end = start + partial_utterance_n_frames\n        mel_slices.append((start, end))\n        wav_steps = mel_steps * params.audio_sampling_rate / params.mel_sampling_rate\n        wav_slices.append((start * wav_steps, end * wav_steps))\n    # Evaluate whether extra padding is warranted or not\n    last_wav_step = (n_samples - wav_steps) / (partial_utterance_n_frames - step)\n    if last_wav_step > pad_samples:\n        last_wav_step -= pad_samples\n    else:\n        last_wav_step = 0\n    last_wav_step = int(last_wav_step)\n    if last_wav_step:\n        start = n_samples - partial_utterance_n_frames + last_wav_step\n        wav_slices.append((start * wav_steps, n_samples))\n    return wav_slices, mel_slices", "CodeBleu_score": 34.264778376069806}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n    pad_samples = int((partial_utterance_n_frames - overlap) * step)\n    mel_samples = partial_utterance_n_frames\n    hop_samples = int((1 - overlap) * step)\n    slices = []\n    start = 0\n    while start < n_samples:\n        end = min(start + mel_samples, n_samples)\n        mel_start = start\n        mel_end = end\n        if ((mel_end - mel_start) < step):\n            start = end\n            continue\n        slices.append((mel_start, mel_end))\n        start += hop_samples\n    if start > n_samples:\n        mel_start = n_samples - mel_samples + 1\n        mel_end = n_samples\n        slices.append((mel_start, mel_end))\n    if (mel_samples > pad_samples) and (((mel_start - pad_samples) >= 0) or (mel_end + pad_samples) <= n_samples):\n        mel_start = max(0, mel_start - pad_samples)\n        mel_end = min(n_samples, mel_end + pad_samples)\n        start = mel_start\n        for (mel_start, mel_end) in slices:\n            if (mel_end - start < min_pad_coverage):\n                mel_end += pad_samples\n            start = mel_end\n    return slices\n", "CodeBleu_score": 25.800434970420387}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(height, -1)\n\n    cmap = plt.cm.get_cmap()\n    im = ax.imshow(embed, cmap=cmap)\n    plt.colorbar(im, ax=ax)\n    plt.title(title)\n\n    # turn off ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 56.66643564076582}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = plt.cm.get_cmap(\"bwr\")\n    im = ax.imshow(embed, cmap=cmap)\n    plt.colorbar(im, ax=ax)\n    plt.clim(color_range[0], color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 60.670891613154446}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = plt.cm.get_cmap(\"bwr\")\n    im = ax.imshow(embed, cmap=cmap)\n    plt.colorbar(im, ax=ax)\n    plt.clim(color_range[0], color_range[1])\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 60.670891613154446}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_mask = torch.eye(speakers_per_batch, dtype=torch.bool, device=device)\n    embeds_mask = embeds_mask.repeat([utterances_per_speaker, 1])\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds) / (speakers_per_batch - 1)\n    embeds = embeds.unsqueeze(0).expand(speakers_per_batch, -1, -1)\n    centroids_incl = centroids_incl.unsqueeze(1).expand(speakers_per_batch, utterances_per_speaker, -1)\n    centroids_excl = centroids_excl.unsqueeze(1).expand(speakers_per_batch, utterances_per_speaker, -1)\n    similarities = torch.where(embeds_mask, torch.zeros_like(embeds[0]), F.cosine_similarity(embeds, centroids_incl, dim=-1) - F.cosine_similarity(embeds, centroids_excl, dim=-1))\n    similarities = similarities * self.sim_matrix_scale + self.sim_matrix_bias\n    similarities = similarities.masked_fill(embeds_mask, float(\"-inf\"))\n    return similarities\n\n", "CodeBleu_score": 27.66646832023718}, {"code": "def similarity_matrix(self, embeds)\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_flat = embeds.view(speakers_per_batch * utterances_per_speaker, -1)\n    centroids_incl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_excl = torch.mean(embeds_flat.view(speakers_per_batch, utterances_per_speaker, -1), dim=1, keepdim=True)\n    centroids_incl = centroids_incl.expand(speakers_per_batch, utterances_per_speaker, -1)\n    centroids_excl = centroids_excl.expand(speakers_per_batch, utterances_per_speaker, -1)\n    centroids_incl = centroids_incl.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    centroids_excl = centroids_excl.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    similarities = torch.zeros(speakers_per_batch * utterances_per_speaker, speakers_per_batch).to(self.loss_device)\n    mask_matrix = torch.ones_like(similarities) * -100\n    mask_matrix[:, :speakers_per_batch] = 1\n    for i in range(speakers_per_batch):\n        centroid_i = centroids_incl[i]\n        centroid_i_excl = centroids_excl[i]\n        similarities_i = torch.matmul(embeds_flat, centroid_i) - torch.matmul(embeds_flat, centroid_i_excl)\n        similarities_i = similarities_i / self.temperature\n        similarities[i * utterances_per_speaker:(i + 1) * utterances_per_speaker] = similarities_i\n    similarities = similarities * mask_matrix\n    similarities = similarities / self.temperature\n    similarities", "CodeBleu_score": 25.660976409138485}, {"code": "def similarity_matrix(self, embeds)\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_mask = torch.eye(speakers_per_batch, dtype=torch.bool, device=self.device)\n    embeds_mask = embeds_mask.repeat(1, utterances_per_speaker)\n    embeds_mask = embeds_mask.reshape((speakers_per_batch * utterances_per_speaker, speakers_per_batch))\n    embeds_mask = torch.logical_not(embeds_mask)\n    centroids_incl = torch.mean(embeds, 1, keepdim=True)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, 2, 2, keepdim=True) + 1e-5)\n    centroids_excl = torch.sum(embeds, 1, keepdim=True) - embeds\n    centroids_excl /= utterances_per_speaker - 1\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, 2, 2, keepdim=True) + 1e-5)\n    sim_matrix = torch.zeros((speakers_per_batch, utterances_per_speaker, speakers_per_batch), device=self.device)\n    sim_matrix_mask = torch.zeros_like(sim_matrix)\n    for j in range(speakers_per_batch):\n        mask_j = torch.logical_and(embeds_mask[:, j], ~embeds_mask[:, j])\n        mask_j = mask_j.reshape((speakers_per_batch, utterances_per_speaker))\n        sim_matrix_mask[:, :, j] = mask_j\n        sim_matrix[:, :, j] = (embeds @ centroids_incl[j]).squeeze(2)\n        sim_matrix[:, :, j] = sim_matrix[:, :, j] / centroids_incl[j].norm(2, 1)", "CodeBleu_score": 34.0607784007176}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n    speakers, utterances, embed_dim = embeds.shape\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape similarities into labels\n    sim_matrix = sim_matrix.reshape((speakers * utterances, speakers))\n    labels = np.zeros(shape=(speakers * utterances))\n    mask = np.ones_like(labels)\n    for speaker_id in range(speakers):\n        labels[speaker_id * utterances : (speaker_id + 1) * utterances] = speaker_id\n        mask[speaker_id * utterances : (speaker_id + 1) * utterances] = 0\n    labels = np.delete(labels, np.where(mask == 1))\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, labels.long().to(self.device))\n    # Compute EER without backprop\n    with torch.no_grad():\n        # Compute the similarity matrix\n        sim_matrix = self.similarity_matrix(embeds)\n        # Reshape similarities into labels\n        sim_matrix = sim_matrix.reshape((speakers * utterances, speakers))\n        labels = np.zeros(shape=(speakers * utterances))\n        mask = np.ones_like(labels)\n        for speaker_id in range(speakers):\n            labels[speaker_id * utterances : (speaker_id + 1) * utterances] = speaker_id\n            mask[speaker_id * utterances : (speaker_id + 1) * utterances] = 0\n        labels = np.delete(labels, np.where(mask == 1))\n        # Compute EER\n        fpr, tpr, thresholds = roc_curve(labels, sim_matrix.detach().cpu().numpy()[:, 0])\n        eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n    return loss, eer\n\n", "CodeBleu_score": 47.851864779350045}, {"code": "def loss(self, embeds)\n    # Get the shape of the embeddings\n    shape = embeds.shape\n    # Determine the number of speakers and utterances per speaker\n    speakers_per_batch, utterances_per_speaker = shape[0], shape[1]\n\n    # Loop through the embeddings to compute the loss\n    loss = 0.0\n    # Generate the labels for the similarity matrix\n    labels = np.expand_dims(np.arange(speakers_per_batch), axis=1)\n    labels = np.repeat(labels, utterances_per_speaker, axis=1)\n    labels = labels.flatten()\n\n    # Compute the similarity matrix\n    embeds_flat = embeds.reshape(-1, embeds.shape[-1])\n    similarities_matrix = np.matmul(embeds_flat, embeds_flat.T)\n\n    # Generate the mask for the similarity matrix\n    mask = np.ones_like(similarities_matrix) - np.eye(len(similarities_matrix))\n    similarities_matrix = similarities_matrix[mask == 1]\n\n    # Compute the loss\n    loss = self.loss_fn(similarities_matrix, labels)\n\n    # Compute the EER without backpropagation\n    with torch.no_grad():\n        # Compute the similarity matrix for the labels\n        labels_matrix = np.equal(labels.reshape(-1, 1), labels.reshape(1, -1))\n        labels_matrix = labels_matrix.astype(np.float32)\n        labels_matrix = torch.from_numpy(labels_matrix)\n\n        # Compute the similarity matrix for the embeddings\n        similarities_matrix = torch.matmul(embeds_flat, embeds_flat.T)\n        similarities_matrix = similarities_matrix.reshape(-1)\n\n        # Compute the ROC curve\n        fpr, tpr, thresholds = metrics.roc_curve(labels_matrix.numpy().flatten(), similarities_matrix.numpy().flatten())\n\n        # Find the threshold where the false positive rate equals the false negative rate", "CodeBleu_score": 34.84740720712529}, {"code": "def loss(self, embeds)\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n\n    # Loss\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(\n        (speakers_per_batch * utterances_per_speaker, speakers_per_batch)\n    )\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, target)\n\n    # EER (not backpropagated)\n    with torch.no_grad():\n        preds = sim_matrix.detach().cpu().numpy()\n        del target, sim_matrix\n\n        preds = np.argmax(preds, axis=-1)\n        eer = compute_eer(preds, ground_truth)\n\n    return loss, eer\n\n", "CodeBleu_score": 38.893936889820054}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n    import yaml\n    import json\n    from IPython.display import display\n    from IPython.core.display import HTML\n    from IPython.core.display_functions import display_html\n    from IPython.core.display_html import HTML\n    from IPython.display import IFrame\n    from IPython.core.display_html import HTML\n    from IPython.display import display\n    from IPython.display import display_html\n    from IPython.display import display_markdown\n    from IPython.display import display_json\n    from IPython.display import display_latex\n    from IPython.display import display_svg\n    from IPython.display import display_html\n    from IPython.display import display_png\n    from IPython.display import display_jpeg\n    from IPython.display import display_gif\n    from IPython.display import display_tiff\n    from IPython.display import display_bmp\n    from IPython.display import display_pdf\n    from IPython.display import display_mp4\n    from IPython.display import display_ogg\n    from IPython.display import display_wav\n    from IPython.display import display_vlc\n    from IPython.display import display_html\n    from IPython.display import display_png\n    from IPython.display import display_jpeg\n    from IPython.display import display_gif\n    from IPython.display import display_tiff\n    from IPython.display import display_bmp\n    from IPython.display import display_pdf\n    from IPython.display import display_mp4\n    from IPython.display import display_ogg\n    from IPython.display import display_wav\n    from IPython.display import display_vlc\n    from IPython.display import display_html\n    from IPython.display import display_png\n    from IPython.display import display_jpeg\n    from IPython.display import display_gif\n    from IPython.display import display_tiff\n    from IPython.display import display_bmp\n    from IPython.display import display_pdf\n    from IPython.display import display_mp4\n    from IPython.display import display_ogg\n    from IPython.display import display", "CodeBleu_score": 10.546505272179383}, {"code": "def log_params(self)\n    if not self.disable_logs:\n        # Import parameter configurations\n        from ..config.parameters import model_params, data_params\n\n        # Initialize log string\n        log_str = \"\"\n\n        # Iterate through model parameters\n        for param_name in model_params:\n            param_value = getattr(self, param_name)\n            log_str += f\"{param_name}: {param_value}\\n\"\n\n        # Iterate through data parameters\n        for param_name in data_params:\n            param_value = getattr(self, param_name)\n            log_str += f\"{param_name}: {param_value}\\n\"\n\n        # Display log string\n        self.visualize.log(log_str, title=\"Parameters\")\n\n", "CodeBleu_score": 25.999235307471775}, {"code": "def log_params(self)\n    if not self.disable_viz_log:\n        self.viz.text(\n            \"<pre>{}</pre>\".format(\n                json.dumps(\n                    {\n                        **self.config,\n                        **self.datamodule.config,\n                        **self.model.config,\n                        **self.trainer.config,\n                    },\n                    indent=4,\n                )\n            ),\n            win=\"params\",\n            opts=dict(title=\"Parameters\"),\n        )\n\n", "CodeBleu_score": 11.897484745103633}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if not self.visualize:\n        return\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers, :]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    n_speakers = embeds.shape[0]\n    utterances_per_speaker = np.array([sum(utterances_per_speaker[:i + 1]) for i in range(len(utterances_per_speaker))])\n    speakers = []\n    for s, n_utts in enumerate(utterances_per_speaker):\n        for _ in range(n_utts):\n            speakers.append(s)\n    speakers = np.array(speakers)\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    plt.figure(figsize=(12, 12))\n    plt.scatter(projected[:, 0], projected[:, 1], c=speakers)\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.title(f'UMAP projection of the embeddings (step {step})')\n    plt.tight_layout()\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        plt.close()\n    else:\n        plt.show()\n    plt.pause(0.001)", "CodeBleu_score": 50.431455259361435}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if not self.visualize_projections:\n        return\n\n    embeds = embeds[:max_speakers * utterances_per_speaker]\n    labels = np.array([[i] * utterances_per_speaker for i in range(max_speakers)]).reshape(-1)\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    n_components = min(5, embeds.shape[1])\n    reducer = umap.UMAP(n_components=n_components, n_neighbors=5, min_dist=0.0, metric='cosine')\n    projections = reducer.fit_transform(embeds)\n    fig = plt.figure()\n    cmap = plt.get_cmap('tab20')\n    num_speakers = len(np.unique(labels))\n    colors = [cmap(i) for i in np.linspace(0, 1, num_speakers)]\n    for i, label in enumerate(np.unique(labels)):\n        indices = np.where(labels == label)[0]\n        plt.scatter(projections[indices, 0], projections[indices, 1], c=colors[i], label=f'Speaker {label}')\n    plt.title(f'UMAP Projections (Step {step})')\n    plt.legend()\n    plt.tight_layout()\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        plt.close(fig)\n    else:\n        plt.pause(0.001)\n        plt.show()", "CodeBleu_score": 47.71184966711378}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    embeds = embeds[:max_speakers*utterances_per_speaker]\n    embeds = embeds.reshape(embeds.shape[0], -1).cpu().detach().numpy()\n    speakers = np.repeat(range(max_speakers), utterances_per_speaker)\n    n_speakers = len(np.unique(speakers))\n    colors = [plt.cm.tab10(i / n_speakers) for i in speakers]\n\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n\n    if not self.no_visdom:\n        self.vis.scatter(\n            X=projected,\n            Y=np.array(speakers) + 1,\n            opts={\n                'legend': ['Speaker {}'.format(i) for i in range(max_speakers)],\n                'title': 'UMAP Projection of Speaker Embeddings (Step {})'.format(step)\n            },\n            win='umap'\n        )\n\n    if out_fpath is not None:\n        fig, ax = plt.subplots(figsize=(12, 10))\n        plt.scatter(projected[:, 0], projected[:, 1], c=colors)\n        plt.gca().set_aspect('equal', 'datalim')\n        plt.title('UMAP Projection of Speaker Embeddings (Step {})'.format(step))\n        plt.savefig(out_fpath)\n        plt.close()", "CodeBleu_score": 51.63332659887039}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    x_shape = tf.shape(x)\n    b = x_shape[0]\n    t = x_shape[1]\n    e = x_shape[2]\n    speaker_embedding = tf.reshape(speaker_embedding, [b, 1, e])\n    speaker_embedding = tf.tile(speaker_embedding, [1, t, 1])\n    return tf.concat([speaker_embedding, x], 2)\n\n", "CodeBleu_score": 14.061187427100268}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    batch_size, seq_len, _ = tf.shape(x)\n    _, embedding_dim = tf.shape(speaker_embedding)\n    speaker_embedding = tf.repeat(speaker_embedding, seq_len, axis=0)\n    speaker_embedding = tf.reshape(speaker_embedding, (batch_size, seq_len, embedding_dim))\n    speaker_embedding = tf.transpose(speaker_embedding, [2, 1, 0])\n    return tf.concat([x, speaker_embedding], axis=-1)\n\n", "CodeBleu_score": 17.865638315771548}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    batch_size, seq_len, _ = x.size()\n    seq_len = int(seq_len)\n    if self.use_speaker_embedding:\n        x = torch.cat((x, speaker_embedding.repeat(1, seq_len, 1)), dim=-1)\n    return x\n\n", "CodeBleu_score": 19.418871690995882}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(\n        [batch_size, self.num_layers, seq_len], device=device)\n    self.attention_weights = torch.zeros(\n        [batch_size, self.num_layers, seq_len], device=device)\n\n", "CodeBleu_score": 30.908445507093813}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n    self.cumulative = torch.zeros(batch_size, seq_len, dtype=torch.float, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, dtype=torch.float, device=device)\n", "CodeBleu_score": 29.32133648498956}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    self.cumulative = torch.zeros(encoder_seq_proj.size(), device=device)\n    self.attention = torch.zeros(encoder_seq_proj.size()[:2], device=device)\n", "CodeBleu_score": 24.51370699199212}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n    device = next(self.parameters()).device\n    ckpt = torch.load(path, map_location=device)\n    self.load_state_dict(ckpt[\"model_state\"])\n    if \"optimizer_state\" in ckpt and optimizer is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer_state\"])", "CodeBleu_score": 49.17581608404482}, {"code": "def load(self, path, optimizer=None)\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File doesn't exists {path}\")\n    params = torch.load(path, map_location=lambda storage, loc: storage)\n    self.model.load_state_dict(params[\"state_dict\"])\n    self.model.to(self.device)\n    if optimizer is not None and \"optimizer\" in params:\n        optimizer.load_state_dict(params[\"optimizer\"])", "CodeBleu_score": 42.502348118442804}, {"code": "def load(self, path, optimizer=None)\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model.to(self.device)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])", "CodeBleu_score": 47.21535372984669}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n    state = {\n        'model': self.state_dict()\n    }\n    if optimizer is not None:\n        state['optimizer'] = optimizer.state_dict()\n    torch.save(state, path)", "CodeBleu_score": 54.80081311267614}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n", "CodeBleu_score": 58.06990691898981}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n", "CodeBleu_score": 58.06990691898981}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 81.02329231965021}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check if the entire text is within curly braces\n    if text.startswith(\"{\"):\n        # Remove the curly braces and split the text into segments\n        segments = text[1:-1].split(\"|\")\n\n        # Process each segment separately\n        for segment in segments:\n            # Check if the segment starts with a number\n            if segment[0].isdigit():\n                # Split the segment into number and text\n                number, text = segment.split(maxsplit=1)\n                # Convert the number to an integer\n                number = int(number)\n                # Clean the text using specified cleaners\n                cleaned_text = clean_text(text, cleaner_names)\n                # Convert the cleaned text to symbol IDs\n                symbol_ids = text_to_sequence(cleaned_text, cleaner_names)\n                # Repeat the symbol IDs based on the number\n                sequence += symbol_ids * number\n            else:\n                # Clean the segment using specified cleaners\n                cleaned_text = clean_text(segment, cleaner_names)\n                # Convert the cleaned segment to symbol IDs\n                symbol_ids = text_to_sequence(cleaned_text, cleaner_names)\n                # Append the symbol IDs to the sequence\n                sequence += symbol_ids\n    else:\n        # Clean the text using specified cleaners\n        cleaned_text = clean_text(text, cleaner_names)\n        # Convert the cleaned text to symbol IDs\n        sequence = text_to_sequence(cleaned_text, cleaner_names)\n\n    # Append the end-of-sequence token to the sequence\n    sequence.append(symbols[\"eos\"])\n\n    return sequence\n\n", "CodeBleu_score": 26.738056099099623}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 83.22960902656705}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back with curly braces\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 76.72428123931681}, {"code": "def sequence_to_text(sequence)\n    arpabet = {\n        0: \" \",\n        1: \"AO\",\n        2: \"AA\",\n        3: \"IY\",\n        4: \"AY\",\n        5: \"EH\",\n        6: \"ER\",\n        7: \"AH\",\n        8: \"UH\",\n        9: \"UW\",\n        10: \"EY\",\n        11: \"IH\",\n        12: \"OW\",\n        13: \"AW\",\n        14: \"OY\",\n        15: \"CH\",\n        16: \"JH\",\n        17: \"Y\",\n        18: \"P\",\n        19: \"B\",\n        20: \"T\",\n        21: \"D\",\n        22: \"K\",\n        23: \"G\",\n        24: \"M\",\n        25: \"N\",\n        26: \"NG\",\n        27: \"L\",\n        28: \"R\",\n        29: \"W\",\n        30: \"YU\",\n        31: \"TH\",\n        32: \"S\",\n        33: \"Z\",\n        34: \"SH\",\n        35: \"ZH\",\n        36: \"V\",\n        37: \"DH\",\n        38: \"F\",\n        39: \"HH\",\n        40: \"sil\",\n        41: \"Z\",\n        42: \"S\",\n        43: \"Z\",\n        44: \"TH\",\n        45: \"ZH\",\n        46: \"V\",\n        47: \"DH\",\n        48: \"M\",\n        49: \"N\",\n        50: \"NG\",\n        51: \"L\",\n        52: \"R\",\n        53: \"ER\",\n        54: \"AX\",\n        55: \"S\",\n        56: \"SH\",\n        57: \"F\",\n        58: \"K\",\n        59: \"G\",\n        60: \"P\",\n        61: \"B\",\n        62: \"T", "CodeBleu_score": 2.543602101126054}, {"code": "def sequence_to_text(sequence)\n    result = \"\"\n    for i in sequence:\n        if i in symbols:\n            s = symbols[i]\n            if i > 1:\n                s = \"{\" + s + \"}\"\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 41.721479304847286}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 74.27358591886917}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath = embed_utterance[0]\n    wav = preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_utterance[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 56.43206398410341}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    if encoder_model_fpath is None:\n        raise Exception(\"encoder_model_fpath is None. Aborting.\")\n    if fpaths is None:\n        raise Exception(\"fpaths is None. Aborting.\")\n    if isinstance(fpaths, str):\n        fpaths = [fpaths]\n    # Load the speaker encoder model\n    encoder_model = load_model(encoder_model_fpath)\n    # Iterate over the files and compute the embeddings\n    for fpath in fpaths:\n        # Load the utterance\n        wav = load_preprocess_wav(fpath)\n        # Get the embedding.\n        embed = embed_utterance(wav, encoder_model)\n        # Create the path to the embedding.\n        embed_fpath = Path(fpath).with_suffix(\".npy\")\n        # Save the embedding.\n        np.save(embed_fpath, embed, allow_pickle=False)\n", "CodeBleu_score": 34.05089698173908}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Verify the existence of the required directories and files\n    if not encoder_model_fpath.exists():\n        print(\"Error: Could not find the encoder model. Expected at: %s\" % encoder_model_fpath)\n        exit(-1)\n    if not Path(synthesizer_root).exists():\n        print(\"Error: Could not find the synthesizer root directory %s\" % synthesizer_root)\n        exit(-1)\n    if not Path(synthesizer_root).is_dir():\n        print(\"Error: %s is not a directory.\" % synthesizer_root)\n        exit(-1)\n\n    # Create the speaker encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Preprocess the dataset\n    print(\"Preprocessing the dataset. This will take a while.\")\n    preprocess_dataset(synthesizer_root, encoder, synthesizer_root, n_processes)\n    print(\"Preprocessing complete.\")\n\n    # Compute the embeddings\n    print(\"Computing embeddings...\")\n    speaker_embeddings = compute_embeddings(synthesizer_root, encoder, n_processes)\n    print(\"Embeddings computed.\")\n\n    # Write the embeddings to disk\n    print(\"Writing embeddings to disk...\")\n    with open(os.path.join(synthesizer_root, \"speaker_embeddings.json\"), \"w\") as f:\n        json.dump(speaker_embeddings, f)\n    print(\"Embeddings written to disk.\")\n", "CodeBleu_score": 26.028682192581392}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    print(\"Creating embeddings...\")\n\n    # Verify that the required directories and files exist\n    if not encoder_model_fpath.exists():\n        raise Exception(\n            f\"Couldn't find encoder at {encoder_model_fpath}. Download it and place it in the synthesizer/ directory.\"\n        )\n    encoder.load_model(encoder_model_fpath)\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\"train.json\")\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = json.load(metadata_file)\n\n    # Prepare file paths for audio and embeddings\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    wav_fpaths = wav_dir.glob(\"**/*.wav\")\n    wav_fpaths = sorted(list(wav_fpaths))\n    embed_fpath = synthesizer_root.joinpath(\"embeds.npy\")\n\n    # Create embeddings\n    with tqdm(total=len(wav_fpaths), desc=\"Embedding\", unit=\"utterances\") as pbar:\n        with multiprocessing.Pool(processes=n_processes) as pool:\n            for _ in pool.imap_unordered(\n                functools.partial(process_utterance, wav_dir, embed_fpath), wav_fpaths\n            ):\n                pbar.update()\n\n", "CodeBleu_score": 43.44060157839117}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Verify that the required directories and files exist\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Couldn\\'t find encoder model at {encoder_model_fpath}\")\n    if not Path(synthesizer_root).exists():\n        raise Exception(f\"Couldn\\'t find synthesizer root directory at {synthesizer_root}\")\n\n    # Create the output directory if it doesn't exist\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Read the metadata file for voice IDs\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    print(\"Preparing dataset...\")\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        if len(metadata) == 0:\n            raise Exception(f\"Couldn\\'t find any .wav files in {synthesizer_root}\")\n\n        # Create a list of tuples containing the audio file path and the corresponding embedding file path\n        fpaths = [(synthesizer_root.joinpath(\"wavs\", m[0]), embed_dir.joinpath(m[2].strip() + \".npy\")) for m in metadata]\n\n    # Create and train the synthesizer by passing the list of file paths\n    train_embeds(fpaths, encoder_model_fpath, n_processes)\n\n", "CodeBleu_score": 55.3123459953398}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Save attention plot\n    plot_alignment(attention.data.cpu().numpy()[0].T, plot_dir + '/alignment_' + str(step) + '.png',\n                   info='step={}, loss={:.4f}'.format(step, loss))\n\n    # Save predicted mel spectrogram\n    mel_output_dir.save_spectrogram(mel_prediction.data.cpu().numpy()[0], 'mel-prediction_' + str(step) + '.png')\n\n    # Save reconstructed waveform\n    wav_dir.save_wav(reconstruct_waveform(mel_prediction, hparams), 'wav-reconstruction_' + str(step) + '.wav')\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    plot_spectrogram(target_spectrogram.data.cpu().numpy()[0],\n                     mel_prediction.data.cpu().numpy()[0],\n                     plot_dir + '/mel-spectrogram_' + str(step) + '.png',\n                     title='{}, time={}, step={}, loss={:.4f}'.format(hparams.model, time.strftime('%m/%d_%H:%M'), step, loss))\n\n    # Print input sequence at current step\n    print('Input sequence:', input_seq[sample_num])\n\n", "CodeBleu_score": 33.75286110377939}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    attention_plot = attention.detach().cpu().numpy()\n    plt.imshow(attention_plot, cmap='viridis')\n    plt.savefig(os.path.join(plot_dir, f'step_{step}.png'))\n    plt.close()\n\n    mel_output = mel_prediction.detach().cpu().numpy()[0]\n    np.save(os.path.join(mel_output_dir, f'step_{step}.npy'), mel_output)\n\n    reconstructed_wav = wav_reconstruction(mel_output, hparams)\n    wavfile.write(os.path.join(wav_dir, f'step_{step}.wav'), hparams.sample_rate, reconstructed_wav)\n\n    target_spectrogram = target_spectrogram.detach().cpu().numpy()[0]\n    plt.figure(figsize=(16, 10))\n    plt.imshow(target_spectrogram, origin='lower', aspect='auto', interpolation='none')\n    plt.colorbar()\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    plt.title(f'{hparams.model_type} at {datetime.now().strftime(\"%H:%M:%S\")} step {step} loss {loss:.4f}')\n    plt.savefig(os.path.join(plot_dir, f'step_{step}_target.png'))\n    plt.close()\n\n    plt.figure(figsize=(16, 10))\n    plt.imshow(mel_output, origin='lower', aspect='auto', interpolation='none')\n    plt.colorbar()\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    plt.title(f'{hparams.model_type} at {datetime.now().strftime(\"%H:%M:%S\")} step {step} loss {loss:.4f}')\n    plt.savefig(os.path", "CodeBleu_score": 28.312927594420138}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Save attention plot\n    attention_plot_path = os.path.join(plot_dir, 'attention_plot_{}.png'.format(step))\n    plot_attention(attention, attention_plot_path)\n\n    # Save predicted mel spectrogram\n    mel_output_path = os.path.join(mel_output_dir, 'mel_output_{}.png'.format(step))\n    plot_mel_spectrogram(mel_prediction, mel_output_path)\n\n    # Save reconstructed waveform\n    wav_output_path = os.path.join(wav_dir, 'wav_output_{}.wav'.format(step))\n    wav_output = audio.inv_mel_spectrogram(mel_prediction.transpose(0, 1))\n    audio.save_wav(wav_output, wav_output_path, hparams.sample_rate)\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    plot_path = os.path.join(plot_dir, 'mel_comparison_{}.png'.format(step))\n    plot_mel_spectrogram(target_spectrogram, plot_path, mel_prediction.transpose(0, 1))\n\n    # Print input sequence at current step\n    print('Input sequence at step {}: {}'.format(step, input_seq[sample_num]))\n\n    # Save loss to a text file\n    with open(os.path.join(plot_dir, 'loss.txt'), 'a') as f:\n        f.write('Step: {}, Loss: {}\\n'.format(step, loss))\n", "CodeBleu_score": 39.1850311885305}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n    model_params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_params])\n    self.num_params = params / 1000000\n    print(\"Number of trainable parameters: %.3fM\" % self.num_params)\n\n", "CodeBleu_score": 37.031228181871604}, {"code": "def num_params(self)\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(f\"Trainable parameters: {params/1000000:0.3f} million\")\n\n", "CodeBleu_score": 38.938669313136835}, {"code": "def num_params(self)\n    model_params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_params])\n    print(\"Total number of trainable parameters: %.3fM\" % (params / 1e6))\n\n", "CodeBleu_score": 37.68060624851673}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n    assert side in ['both', 'before', 'after']\n    if side == 'both':\n        new_size = (x.shape[0] + 2 * pad, *x.shape[1:])\n    elif side == 'before':\n        new_size = (x.shape[0] + pad, *x.shape[1:])\n    else:\n        new_size = (x.shape[0] + pad, *x.shape[1:])\n    if x.is_cuda:\n        new_x = torch.zeros(new_size).cuda()\n    else:\n        new_x = torch.zeros(new_size)\n    if side == 'both':\n        new_x[pad:-pad, ...] = x\n    elif side == 'before':\n        new_x[pad:, ...] = x\n    else:\n        new_x[:-pad, ...] = x\n    return new_x", "CodeBleu_score": 36.72014072171499}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if pad <= 0:\n        return x\n    if side == 'before':\n        new_size = (x.shape[0] + pad,) + x.shape[1:]\n        padded = torch.zeros(new_size, dtype=x.dtype, device=x.device)\n        padded[pad:] = x\n    elif side == 'after':\n        new_size = (x.shape[0] + pad,) + x.shape[1:]\n        padded = torch.zeros(new_size, dtype=x.dtype, device=x.device)\n        padded[:-pad] = x\n    else:\n        pad1 = pad // 2\n        pad2 = pad - pad1\n        new_size = (x.shape[0] + pad,) + x.shape[1:]\n        padded = torch.zeros(new_size, dtype=x.dtype, device=x.device)\n        padded[pad1:-pad2] = x\n    return padded", "CodeBleu_score": 34.54096523356106}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if x.size(0) >= pad:\n        return x\n    if side == 'after':\n        return torch.cat([x, torch.zeros(pad - x.size(0), *x.shape[1:], device=x.device)], 0)\n    elif side == 'before':\n        return torch.cat([torch.zeros(pad - x.size(0), *x.shape[1:], device=x.device), x], 0)\n    elif side == 'both':\n        each_side = (pad - x.size(0)) // 2\n        return torch.cat([torch.zeros(each_side, *x.shape[1:], device=x.device), x, torch.zeros(pad - x.size(0) - each_side, *x.shape[1:], device=x.device)], 0)\n    else:\n        raise ValueError(f\"Invalid side: {side}\")", "CodeBleu_score": 23.34971957046368}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)", "CodeBleu_score": 62.46286889331226}, {"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(attn, interpolation=\"nearest\", cmap=\"Greens\")\n    plt.savefig(path + \".png\")\n    plt.close()\n\n", "CodeBleu_score": 46.329947794340384}, {"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as ticker\n\n    # Create a figure and a subplot\n    fig, ax = plt.subplots()\n\n    # Visualize the attention matrix\n    cax = ax.matshow(attn, cmap='viridis')\n\n    # Set the tick labels for the x and y axes\n    ax.set_xticklabels([''] + list(range(1, attn.shape[1] + 1)), rotation=90)\n    ax.set_yticklabels([''] + list(range(1, attn.shape[0] + 1)))\n\n    # Set the tick positions for the x and y axes\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    # Add a colorbar to the plot\n    cbar = fig.colorbar(cax)\n\n    # Set the label for the colorbar\n    cbar.ax.set_ylabel('Attention weight', rotation=270)\n\n    # Save the plot as a PNG image\n    plt.savefig(path + '.png', bbox_inches='tight')\n\n    # Close the figure\n    plt.close(fig)", "CodeBleu_score": 43.47690767298752}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 6))\n\n    # Add a subplot\n    ax = fig.add_subplot(111)\n\n    # Customize the axis labels and tick parameters\n    ax.set_xlabel('X', color='blue', fontsize=14)\n    ax.set_ylabel('Y', color='blue', fontsize=14)\n    ax.tick_params(axis='x', colors='blue', labelsize=12)\n    ax.tick_params(axis='y', colors='blue', labelsize=12)\n\n    # Plot the array\n    ax.plot(array)\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 59.252950478614565}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 5))\n\n    # Add a subplot\n    ax = fig.add_subplot(111)\n\n    # Customize the axis labels and tick parameters\n    ax.set_xlabel(\"X-axis\", color=\"blue\", fontsize=14)\n    ax.set_ylabel(\"Y-axis\", color=\"blue\", fontsize=14)\n    ax.tick_params(axis=\"x\", colors=\"blue\", labelsize=12)\n    ax.tick_params(axis=\"y\", colors=\"blue\", labelsize=12)\n\n    # Plot the array on the customized subplot\n    ax.plot(array)\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 36.34299137493496}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(15, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('X', color='#000000', fontsize=20)\n    ax.set_ylabel('Y', color='#000000', fontsize=20)\n    ax.tick_params(axis='x', colors='#000000', labelsize=15)\n    ax.tick_params(axis='y', colors='#000000', labelsize=15)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 35.72289787839098}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as file:\n        # Create a CSV writer object\n        writer = csv.DictWriter(file, fieldnames=[\"Image Name\", \"Prediction\", \"Confidence\"])\n\n        # Write the header if the file is new\n        if file.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 63.00443231556315}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n    import pandas as pd\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.writer(csvfile)\n\n        # Check if the file is new\n        if os.stat('predictions.csv').st_size == 0:\n            # Write the header row\n            writer.writerow(['image_name', 'prediction', 'confidence'])\n\n        # Write the data as a new row\n        writer.writerow([data['image_name'], data['prediction'], data['confidence']])\n\n", "CodeBleu_score": 38.31808883515443}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n    import pandas as pd\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as csvfile:\n        # Create a CSV writer object\n        writer = csv.writer(csvfile)\n\n        # Check if the file is new\n        if os.stat(\"predictions.csv\").st_size == 0:\n            # Write the header row if the file is new\n            writer.writerow([\"image_name\", \"prediction\", \"confidence\"])\n\n        # Write the data as a new row\n        writer.writerow([data[\"image_name\"], data[\"prediction\"], data[\"confidence\"]])\n\n", "CodeBleu_score": 39.38049070674484}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        import torch\n\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file or tempfile.NamedTemporaryFile().name\n        ts = torch.jit.trace(model, im, strict=False)\n        (optimize or not torch.cuda.is_available()) and ts.optimize_for_inference()\n        ts.save(f)\n        LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        LOGGER.exception(f\"{prefix} export failure: {e}\")\n        return f if file else None, e\n\n", "CodeBleu_score": 47.05075920980926}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        import torch\n\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file or tempfile.NamedTemporaryFile().name\n        torch.jit.save(torch.jit.trace(model, im), f)\n        LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        LOGGER.exception(f\"{prefix} export failure: {e}\")\n        return \"\", e\n\n", "CodeBleu_score": 35.60125177242451}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        import torch\n\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file or tempfile.NamedTemporaryFile().name\n        ts = model.model.float() if prefix == \"TorchScript (FP32):\" else model.model\n        ts.fuse().eval()\n        y = model(im)\n        if optimize:\n            try:\n                LOGGER.info(f\"{prefix} optimizing with torch {torch.__version__}...\")\n                ts = torch.jit.optimize_for_mobile(ts)\n            except Exception as e:\n                LOGGER.info(f\"{prefix} optimization failure: {e}\")\n        with io.BytesIO() as buffer:\n            torch.jit.save(ts, buffer)\n            (optimize,), _ = buffer.seek(0), buffer.read()\n            if optimize != ts:\n                LOGGER.info(f\"{prefix} optimization applied\")\n        torch.save(dict(model=ts, results=y), f)\n        LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        LOGGER.warning(f\"{prefix} export failure: {e}\")\n        return f, e\n\n", "CodeBleu_score": 43.683932874966494}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # file name\n        model.eval()\n        for k, m in model.model[-1].named_modules():\n            if isinstance(m, Detect):\n                if not isinstance(m.anchor_grid, list):\n                    k = k.split(\".m.\")[1]  # detect head key\n                    m.anchor_grid = m.anchor_grid.cpu().float().view(\n                        -1\n                    )  # to avoid non-tensor error\n                    model.model[-1].state_dict()[\n                        f\"{k}.anchor_grid\"\n                    ] = m.anchor_grid  # assign anchor grid\n                    m.anchor_grid = m.anchor_grid.view(\n                        m.nl, -1\n                    )  # shape to list\n                    check_anchor_order(m)\n                    m.anchor_grid = m.anchor_grid.to(d)\n\n        model.model[-1].export = True  # set Detect() layer export=True\n        for _ in range(2):\n            y = model(im, augment=False, profile=False)  # feed-forward\n        y = (\n            y[0]\n            if isinstance(y, tuple)\n            else y\n        )  # unpack results\n        model.model[-1].export = False  # set Detect() layer export=False\n        y = y.float()  # FP32\n        a = [\n            y[0, :, :4],\n            y[0, :, 4:8],\n            y[0, :, 8:12],\n        ]  # anchor boxes for all 3 scales\n        if nms:  # append all detections to output list\n            z = []\n            for i in range(model.nl):\n                z.append(y[0, i, :, :4])\n                z.append(a[i])\n            y = torch.cat(z, 1)[0]\n        else:  # append", "CodeBleu_score": 33.6205704683077}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # file name\n        model.eval()  # CoreML export\n        for k, m in model.named_modules():\n            if isinstance(m, Detect):\n                m.export = True  # set Detect() layer export=True\n        model.model[-1].export = True  # set Detect() layer export=True\n        y = model(im)  # dry run\n        if int8:\n            LOGGER.info(f\"{prefix} CoreML INT8 Quantization enabled...\")\n            model.model[-1].concat = False  # use Concat instead of ConcatV2\n            ct.convert(\n                model,\n                inputs=[ct.TensorType(shape=im.shape, dtype=np.float32)],\n                outputs=[ct.TensorType(name=\"output\")],\n                minimum_deployment_target=ct.target.iOS14,\n                compute_units=ct.ComputeUnit.ALL,\n            )\n            mlmodel = ct.models.MLModel(str(f))\n            spec = mlmodel.get_spec()\n            ct.utils.rename_feature(spec, \"1144\", \"output\")\n            mlmodel = ct.models.MLModel(spec)\n            shutil.rmtree(f)  # remove original\n            mlmodel.save(str(f))  # save final\n        elif half:\n            LOGGER.info(f\"{prefix} CoreML FP16 Quantization enabled...\")\n            model.model[-1].concat = False  # use Concat instead of ConcatV2\n            ct.convert(\n                model,\n                inputs=[ct.TensorType(shape=im.shape, dtype=np.float32)],\n                outputs=[ct.TensorType(name=\"output\")],\n                compute_units=ct.ComputeUnit.ALL,\n            )\n            mlmodel = ct", "CodeBleu_score": 34.32379383291501}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # file name\n        model.eval()\n        for k, m in model.model[-1].named_modules():\n            if isinstance(m, Detect):\n                if not isinstance(m.anchor_grid, list):\n                    k = k.split(\".m.\")[1]  # strip detect() prefix\n                    setattr(model, k, DetectHead(m.anchor_grid, m.stride))\n        model.model[-1].export = True  # set Detect() layer export=True\n        for _ in range(2):\n            y = model(im, augment=False, visualize=False)  # dry runs\n        model.model[-1].export = False  # set Detect() layer export=False\n        if int8:\n            LOGGER.info(f\"{prefix} CoreML quantization enabled (full-int8 inputs and outputs)\")\n        if half:\n            LOGGER.info(f\"{prefix} CoreML FP16 enabled (half-float inputs and outputs)\")\n        if nms:\n            LOGGER.info(f\"{prefix} CoreML NMS enabled (non-maximum suppression)\")\n        mlmodel = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"images\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n            outputs=[ct.ImageType(name=\"output\", shape=im.shape[:2])],\n            minimum_deployment_target=ct.target.iOS15,\n            compute_units=ct.ComputeUnit.ALL,\n            convert_to=\"mlprogram\",\n            debug=False,\n            compute_precision=ct.precision.FLOAT16 if half else ct.precision.FLOAT32,\n            compute_units=ct.ComputeUnit.CPU_AND_GPU,\n        )\n        mlmodel =", "CodeBleu_score": 41.412405857409176}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n    import random\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 48.609071015262195}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule == \"discrete\":\n        return self.log_alpha_t[t]\n    elif self.schedule == \"linear\":\n        return self.log_alpha_t[t]\n    elif self.schedule == \"cosine\":\n        return self.log_alpha_t[t]\n\n", "CodeBleu_score": 19.99432814882346}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == \"discrete\":\n        return self.log_alpha_t[t]\n    elif self.schedule_type == \"linear\":\n        return self.log_alpha_t[t]\n    elif self.schedule_type == \"cosine\":\n        return self.log_alpha_t[t]\n    else:\n        raise ValueError(\n            \"Invalid schedule type. Choose from 'discrete', 'linear', or 'cosine'.\"\n        )", "CodeBleu_score": 25.23214840368533}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == \"discrete\":\n        return self.interpolate(t)\n    elif self.schedule_type == \"linear\":\n        return self.linear_coeff(t)\n    elif self.schedule_type == \"cosine\":\n        return self.cosine_coeff(t)\n    else:\n        raise ValueError(f\"Unknown schedule type: {self.schedule_type}\")", "CodeBleu_score": 21.922382159310683}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n        # For a linear schedule, compute t using beta values and a temporary variable\n        beta_t = self.beta_0 + self.beta_1 * lamb\n        t = (torch.sqrt(beta_t ** 2 + 4 * self.beta_0 * beta_t) - beta_t) / (2 * self.beta_0)\n    elif self.schedule == \"quadratic\":\n        # For a quadratic schedule, compute t using beta values and a temporary variable\n        beta_t = self.beta_0 + self.beta_1 * lamb + self.beta_2 * lamb ** 2\n        t = (torch.sqrt(beta_t ** 2 + 4 * self.beta_0 * beta_t) - beta_t) / (2 * self.beta_0)\n    elif self.schedule == \"sigmoid\":\n        # For a sigmoid schedule, compute t using beta values and a temporary variable\n        beta_t = self.beta_0 + self.beta_1 * lamb\n        t = (torch.log(beta_t) - torch.log(self.beta_0 - beta_t)) / self.beta_1\n    elif self.schedule == \"discrete\":\n        # For a discrete schedule, interpolate log_alpha to find t\n        log_alpha_t = torch.log(self.alpha_0) - lamb * self.log_alpha_0\n        t = torch.interpolate(log_alpha_t, self.log_alpha, self.t)\n    else:\n        # For other schedules, compute log_alpha and use an arccos function to derive t\n        log_alpha_t = torch.log(self.alpha_0) - lamb * self.log_alpha_0\n        t = torch.arccos(torch.exp(log_alpha_t - self.log_alpha)) / self.alpha_0\n\n    return t\n", "CodeBleu_score": 34.138769326718574}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n        return (self.T - self.beta_0) * lamb + self.beta_0\n    elif self.schedule == 'discrete':\n        return self.T - self.beta_0 - np.argmin(np.abs(lamb - self.log_alpha))\n    else:\n        log_alpha = self.log_alpha(lamb)\n        return np.arccos(np.clip(1 - 2 * log_alpha, -1, 1)) / np.pi * self.T\n\n", "CodeBleu_score": 18.240321419860216}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n        # beta values\n        beta_0 = 0.1\n        beta_1 = 20\n        # temp variable\n        temp = (self.T * (beta_1 - beta_0) / (self.logSNR_max - self.logSNR_min))\n        # calculate t\n        t = (lamb - self.logSNR_min) * temp\n    elif self.schedule == \"discrete\":\n        # interpolate log_alpha\n        log_alpha = np.interp(lamb, self.logSNR_list, self.log_alpha_list)\n        # calculate t\n        t = (log_alpha - self.log_alpha_min) * self.T / (self.log_alpha_max - self.log_alpha_min)\n    else:\n        # calculate log_alpha\n        log_alpha = self.log_alpha_min + (self.log_alpha_max - self.log_alpha_min) * (np.arccos(np.exp(-lamb)) / np.pi)\n        # calculate t\n        t = (log_alpha - self.log_alpha_min) * self.T / (self.log_alpha_max - self.log_alpha_min)\n    return t\n", "CodeBleu_score": 27.64358924486463}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if cond is None:\n        cond = torch.zeros_like(x)\n    t_expanded = expand_to_planes(t_continuous, x.shape)\n    if model_type == 'continuous':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2_1':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2_2':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2_3':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2_4':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type == 'unet_condition_time_attn_2_5':\n        x = x + t_expanded * model(x, t_expanded, cond=cond)\n    elif model_type", "CodeBleu_score": 22.62495736503584}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    t = t_continuous\n    if t.ndim == 0:\n        t = tf.expand_dims(t, 0)\n    if cond is not None:\n        cond = tf.cast(cond, tf.float32)\n    x_in = tf.cast(x, tf.float32)\n    if t.shape[0] != x_in.shape[0]:\n        t = tf.repeat(t, x_in.shape[0] // t.shape[0], axis=0)\n    if self.model_type == 'ddpm':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'ddim':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2_conditional':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2_conditional_from_vqvae':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2_conditional_from_vqvae_and_discriminator':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2_conditional_from_vqvae_and_discriminator_and_discriminator_2':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif self.model_type == 'p2_conditional_from_vqvae_and_discriminator_and_discriminator_2_and_discriminator_3':\n        x_out = self.diffusion_model(x_in, t, cond=cond)\n    elif", "CodeBleu_score": 25.418878594195437}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    batch_size = x.shape[0]\n    t = t_continuous.expand(batch_size, 1)\n    if cond is not None:\n        cond = cond.expand(batch_size, 1)\n    if model_type == 'mlp':\n        model_output = model(t, cond)\n    elif model_type == 'unet':\n        model_output = model(x, t, cond)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n    if model_type == 'mlp':\n        model_output = model_output.view(batch_size, 1, 1, 1)\n\n    if model_type == 'mlp':\n        alpha = alpha_mlp(t)\n        beta = beta_mlp(t)\n    elif model_type == 'unet':\n        alpha = alpha_unet(t)\n        beta = beta_unet(t)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n    if model_type == 'mlp':\n        epsilon_theta = model_output\n    elif model_type == 'unet':\n        epsilon_theta = (model_output - x) / (torch.sqrt(alpha))\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n    if model_type == 'mlp':\n        pred_noise = alpha * epsilon_theta + beta * torch.randn_like(epsilon_theta)\n    elif model_type == 'unet':\n        pred_noise = alpha * epsilon_theta + beta * torch.randn_like(epsilon_theta)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n    return pred_noise\n", "CodeBleu_score": 30.398785011010116}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    x0 = x0.reshape(-1)\n    if t is None:\n        t = self.t\n    if t == 'mean':\n        t = x0.abs().mean()\n    elif t == 'median':\n        t = x0.abs().median()\n    elif t == 'max':\n        t = x0.abs().max()\n    elif t == 'min':\n        t = x0.abs().min()\n    s = (1.0 / t).clamp_(min=0.0, max=1.0)\n    return x0.clamp_(min=-t, max=t) * s\n", "CodeBleu_score": 26.511650740972414}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    t = int(t * len(x0))\n    x0 = x0.abs()\n    x0 = x0.sort()[0]\n    thresh = x0[t]\n    if thresh == 0:\n        thresh += 1e-12\n    scale = 1.0 / thresh\n    x0 *= scale\n    x0 = x0.clamp(-1.0, 1.0)\n    x0 *= thresh\n    return x0", "CodeBleu_score": 32.35778164795724}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    x0 = x0[x0 != 0]\n    x0 = torch.abs(x0)\n    x0 = x0.sort().values\n    x0 = x0[int(t * x0.shape[0])]\n    x0 = torch.clamp(self.x0, -x0, x0)\n    x0 = x0 / x0.abs().max()\n    return x0", "CodeBleu_score": 36.82613995415994}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n    x0 = x.clone()\n    if self.dynamic_thresholding:\n        max_x0 = torch.max(x0)\n        threshold = self.dynamic_thresholding_ratio * max_x0\n        x0 = torch.clamp(x0, -threshold, threshold)\n        x0 /= threshold\n    return x0", "CodeBleu_score": 16.1803268151856}, {"code": "def data_prediction_fn(self, x, t)\n    x0 = x.clone()\n    x0 = x0 - x0.min()\n    x0 = x0 / x0.max()\n    x0 = x0 * self.max_threshold\n    x0 = torch.clamp(x0, max=self.max_threshold)\n    x0 = x0 / self.max_threshold\n    return x0\n\n", "CodeBleu_score": 13.076049801725997}, {"code": "def data_prediction_fn(self, x, t)\n    # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n    threshold = self.dynamic_threshold_ratio * t\n    # Clamp the input tensor values within the threshold\n    x0 = torch.clamp(x, -threshold, threshold)\n    # Normalize the clamped values\n    x0 = x0 / (2 * threshold) + 0.5\n    return x0", "CodeBleu_score": 9.448304836681931}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    eps = model(input, **kwargs)\n    sigma_in = sigma_in.reshape(input.shape[0], *([1] * (len(input.shape) - 1)))\n    adjusted_sigma = sigma_in * 255\n    adjusted_eps = eps * adjusted_sigma\n    adjusted_pred = input + adjusted_eps\n    return adjusted_pred\n\n", "CodeBleu_score": 23.092971013005517}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(input.shape)\n    input = input * sigma_in\n    pred = model(input, **kwargs)\n    pred = pred * sigma_in\n    return pred", "CodeBleu_score": 26.544122390610585}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1)\n    input_scaled = input / sigma_in\n    output = model(input_scaled, sigma_in, **kwargs)\n    eps = output - sigma_in * input_scaled\n    return eps", "CodeBleu_score": 27.82915197979743}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n    with tf.GradientTape() as tape:\n        tape.watch(taus)\n        alpha = tf.math.exp(beta_0 + beta_1 * taus)\n        log_alpha = tf.math.log(alpha)\n    dlog_alpha_dtaus = tape.gradient(log_alpha, taus)\n    integrand = tf.math.square(dlog_alpha_dtaus) * alpha\n    return integrand\n\n", "CodeBleu_score": 27.013913120165544}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = tf.exp(beta_0) * tf.exp(beta_1 * taus)\n    log_alpha = tf.math.log(alpha)\n    with tf.GradientTape() as tape:\n        tape.watch(taus)\n        tape.watch(log_alpha)\n        grad = tape.gradient(log_alpha, taus)\n    return grad * alpha\n\n", "CodeBleu_score": 26.829935265486803}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = beta_0 * taus.exp() + beta_1\n    log_alpha = alpha.log()\n    log_alpha.backward()\n    integrand = log_alpha.grad * alpha\n    return integrand\n", "CodeBleu_score": 21.303052540972338}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = get_noise_sampler(model, x, sigmas, extra_args)\n    for c, sigma in enumerate(sigmas):\n        eps = noise_sampler(x, sigma, extra_args)\n        eps1 = (sigma / model.sigma_data) * x + eps * s_noise\n        denoised = model.denoise(x, sigma, eps=eps1, s_in=s_in, **extra_args)\n        d = (x - denoised) / sigma\n        dt = sigma / model.sigma_data * s_noise\n        x = x + eta * d * dt\n        if callback is not None:\n            callback({'x': x, 'i': c, 'sigma': sigma, 'sigma_hat': sigma, 'denoised': denoised})\n        if disable is not None:\n            disable.update()\n    x = x + s_noise * noise_sampler(x, sigma, extra_args)\n    return x\n\n", "CodeBleu_score": 43.23831487969229}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = get_noise_sampler(x)\n    for sigma in sigmas:\n        eps = noise_sampler(x, sigma, extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': 0, 'sigma': sigma, 'sigma_hat': sigma, 'denoised': x})\n        t_enc = sigma.mul(s_in).sqrt()\n        x_prev = x\n        x = x + eps * (t_enc - sigma)\n        if sigma.max() != 0:\n            x_prev = x_prev + eps * (t_enc - sigma)\n        if eta:\n            x, x_prev = model.get_ancestral_step(x_prev, t_enc, sigma, eta=eta)\n        if s_noise > 0:\n            x = x + torch.randn_like(x) * s_noise\n    return x\n", "CodeBleu_score": 44.294923244157644}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x, t, sigma, **kwargs: torch.randn_like(x)\n    for sigma in sigmas:\n        x_np = x.detach().cpu().numpy()\n        extra_args['sigma'] = sigma.item()\n        extra_args['sigma_hat'] = sigma.item()\n        extra_args['t'] = sigma.item()\n        extra_args['t_next'] = sigma.item()\n        extra_args['sigma_next'] = sigma.item()\n        extra_args['sigma_hat_next'] = sigma.item()\n        extra_args['H_t'] = sigma.item()\n        extra_args['H_t_next'] = sigma.item()\n        extra_args['H_t_prev'] = sigma.item()\n        extra_args['H_t_s'] = sigma.item()\n        extra_args['H_t_next_s'] = sigma.item()\n        extra_args['H_t_prev_s'] = sigma.item()\n        extra_args['eta'] = eta\n        extra_args['s_noise'] = s_noise\n        extra_args['x_np'] = x_np\n        extra_args['x_np_0'] = x_np\n        extra_args['x_np_1'] = x_np\n        extra_args['x_np_prev'] = x_np\n        extra_args['x_np_next'] = x_np\n        extra_args['x_np_euler'] = x_np\n        extra_args['x_np_euler_0'] = x_np\n        extra_args['x_np_euler_1'] = x_np\n        extra_args['x_np_euler_prev'] = x_np\n        extra_args['x_np_euler_next'] = x_np\n        extra_args['x_np_euler_prev_s'] = x_np\n        extra_args['x_np_", "CodeBleu_score": 20.210877704010382}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n    def product_term(s):\n        product = 1\n        for k in range(order):\n            if k != j:\n                product *= (s - t[i+k]) / (t[i+j] - t[i+k])\n        return product\n    if order < j:\n        raise ValueError(\"Order is too high for the step index.\")\n    return integrate(product_term, t[i], t[i+1])\n\n", "CodeBleu_score": 44.363618606842984}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\n    if j >= i:\n        raise ValueError(\"The order of the multistep method must be lower than the step index.\")\n\n    def product_term(s):\n        result = 1\n        for m in range(i - j):\n            result *= (s - t[i - m]) / (t[i - j] - t[i - m])\n        return result\n\n    integral, _ = quad(product_term, t[i], t[i + 1])\n    return integral\n\n", "CodeBleu_score": 41.96757956234429}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if j > i:\n        raise ValueError(\"j must be less than or equal to i\")\n    def product_term(s):\n        result = 1\n        for k in range(j, i + 1):\n            if k != j:\n                result *= (s - t[k]) / (t[j] - t[k])\n        return result\n    integral, _ = quad(product_term, t[i], t[i + 1])\n    return integral\n\n", "CodeBleu_score": 30.64786204546488}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    sigma_hat = x.new_zeros([x.shape[0]])\n    for sigma in sigmas:\n        extra_args['sigma'] = sigma\n        sigma_hat = sigma_hat + sigma\n        model_output = model(x, sigma_hat, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigma_hat, 'model_output': model_output})\n        denoised_x = x - noise_sampler(x, sigma_hat)\n        if sigma > 0:\n            denoised_x = denoised_x / ((sigma_hat ** 2 + 1) ** 0.5)\n        x = denoised_x + (sigma_hat ** 2 - sigma ** 2) ** 0.5 * model_output\n    return x", "CodeBleu_score": 41.7423323173333}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    extra_args = extra_args or {}\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = get_noise_sampler(x.device)\n    for sigma in sigmas:\n        e_t = model(x, sigma * s_in, **extra_args)\n        e_t = noise_sampler(x, sigma, e_t)\n        x = x + e_t * (sigma > 0)\n        if callback is not None:\n            callback({'x': x, 'i': 0, 'sigma': sigma, 'sigma_hat': sigma, 'denoised': e_t})\n        s_in = sigma\n    return x\n\n", "CodeBleu_score": 36.20123977381525}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    extra_args = extra_args or {}\n    s_in = x.new_ones([x.shape[0]])\n    noise_sampler = noise_sampler or (lambda x, sigma, extra_args: x)\n    with torch.enable_grad():\n        for sigma in sigmas:\n            if sigma > 0:\n                x = x + noise_sampler(x, sigma, extra_args) * s_in\n            x = model(x, sigma, **extra_args)\n            if callback:\n                callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigma, 'denoising': model.denoising})\n            s_in = s_in * (sigma / sigmas[-1])\n    return x\n\n", "CodeBleu_score": 35.440547773763655}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n    n_dims_to_append = target_dims - x.ndim\n    if n_dims_to_append < 0:\n        raise ValueError(\n            f\"Invalid target dimensions: {target_dims} for tensor with {x.ndim} dimensions.\")\n    for _ in range(n_dims_to_append):\n        x = x[..., None]\n    if x.device.type == 'mps':\n        x = x.detach().clone()\n    return x\n", "CodeBleu_score": 45.20141159315343}, {"code": "def append_dims(x, target_dims)\n    ndims_to_append = target_dims - x.ndim\n    if ndims_to_append < 0:\n        raise ValueError(\n            f\"Target dimensions ({target_dims}) cannot be fewer than the current dimensions ({x.ndim}).\"\n        )\n    for _ in range(ndims_to_append):\n        x = x.unsqueeze(-1)\n    if x.device.type == \"mps\":\n        return x.detach().clone()\n    else:\n        return x\n\n", "CodeBleu_score": 39.63712359516114}, {"code": "def append_dims(x, target_dims)\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(\n            f\"Target dimensions ({target_dims}) cannot be fewer than current dimensions ({x.ndim}).\"\n        )\n    for _ in range(dims_to_append):\n        x = x[..., None]\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n    return x", "CodeBleu_score": 47.49894428022513}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n    import os\n    import requests\n    import hashlib\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    if not os.path.exists(path):\n        with requests.get(url, stream=True) as r:\n            r.raise_for_status()\n            with open(path, 'wb') as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    f.write(chunk)\n    if digest:\n        with open(path, 'rb') as f:\n            sha256 = hashlib.sha256()\n            while True:\n                buf = f.read(1048576)\n                if not buf:\n                    break\n                sha256.update(buf)\n        if sha256.hexdigest() != digest:\n            raise ValueError('Hash mismatch for downloaded file')\n    return path\n\n", "CodeBleu_score": 41.80077921466726}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import requests\n    import hashlib\n\n    # Create the target directory if it doesn't exist\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    # Check if the file already exists\n    if os.path.exists(path):\n        # If the file already exists, check its hash against the provided digest\n        if digest is not None:\n            with open(path, \"rb\") as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n            if file_hash != digest:\n                raise ValueError(\n                    f\"File {path} has incorrect hash {file_hash} (expected {digest})\"\n                )\n        # If the file already exists and no digest was provided, return the path to the existing file\n        return path\n\n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()\n\n    # Write the file to the specified path\n    with open(path, \"wb\") as f:\n        f.write(response.content)\n\n    # Check the file's hash against the provided digest, if one was provided\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise ValueError(\n                f\"File {path} has incorrect hash {file_hash} (expected {digest})\"\n            )\n\n    # Return the path to the downloaded file\n    return path\n\n", "CodeBleu_score": 46.31662567165763}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import hashlib\n    import requests\n\n    def sha256sum(filename):\n        \"\"\"Compute the SHA-256 hash of a file.\"\"\"\n        h = hashlib.sha256()\n        b = bytearray(128 * 1024)\n        mv = memoryview(b)\n        with open(filename, 'rb', buffering=0) as f:\n            for n in iter(lambda: f.readinto(mv), 0):\n                h.update(mv[:n])\n        return h.hexdigest()\n\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n\n    if not os.path.exists(path):\n        print(f\"Downloading {url} to {path}\")\n        r = requests.get(url, stream=True)\n        with open(path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n\n    if digest is not None:\n        if sha256sum(path) != digest:\n            raise ValueError(f\"SHA-256 hash mismatch for {path}\")\n\n    return path\n", "CodeBleu_score": 42.27285899163055}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n    modes = [module.training for module in model.modules()]\n    try:\n        model.train(mode)\n        yield model\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode", "CodeBleu_score": 50.08017349765329}, {"code": "def train_mode(model, mode=True)\n    modes = [module.training for module in model.modules()]\n    try:\n        model.train(mode)\n        yield model\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode\n\n", "CodeBleu_score": 50.08017349765329}, {"code": "def train_mode(model, mode=True)\n    modes = [module.training for module in model.modules()]\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode\n\n", "CodeBleu_score": 45.17133808214328}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n    # Update averaged model parameters\n    for p_ema, p_model in zip(averaged_model.parameters(), model.parameters()):\n        p_ema.data.mul_(decay).add_(p_model.data, alpha=1 - decay)\n\n    # Update averaged model buffers\n    for b_ema, b_model in zip(averaged_model.buffers(), model.buffers()):\n        b_ema.data.copy_(b_model.data)", "CodeBleu_score": 26.22153836372717}, {"code": "def ema_update(model, averaged_model, decay)\n    with torch.no_grad():\n        for ema_v, model_v in zip(averaged_model.parameters(), model.parameters()):\n            if ema_v.requires_grad:\n                ema_v.data.mul_(decay).add_((1 - decay) * model_v.detach())\n\n        for ema_v, model_v in zip(averaged_model.buffers(), model.buffers()):\n            if ema_v.requires_grad:\n                ema_v.data.copy_(model_v.data)\n\n", "CodeBleu_score": 30.819882439572545}, {"code": "def ema_update(model, averaged_model, decay)\n    with torch.no_grad():\n        for current_params, avg_params in zip(model.parameters(), averaged_model.parameters()):\n            avg_params.mul_(decay).add_(current_params, alpha=1 - decay)\n        for current_buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n            avg_buffer.copy_(current_buffer)\n\n", "CodeBleu_score": 30.7368474432282}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    if min_value == 0:\n        min_value = torch.finfo(dtype).tiny\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_value = torch.log(min_value) - loc\n    max_value = torch.log(max_value) - loc\n    min_cdf = torch.sigmoid(min_value / scale)\n    max_cdf = torch.sigmoid(max_value / scale)\n    u = torch.rand(shape, dtype=dtype, device=device) * (max_cdf - min_cdf) + min_cdf\n    x = torch.log(u / (1. - u)) * scale + loc\n    x = torch.exp(x)\n    return x\n", "CodeBleu_score": 46.39138495967582}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    min_cdf = logistic.cdf(min_value, loc, scale)\n    max_cdf = logistic.cdf(max_value, loc, scale)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    u = (max_cdf - min_cdf) * u + min_cdf\n    x = logistic.icdf(u, loc, scale)\n    return x\n\n", "CodeBleu_score": 33.67285581763666}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    min_cdf = torch.log(min_value / scale) - loc\n    max_cdf = torch.log(max_value / scale) - loc\n    u = torch.rand(shape, dtype=dtype, device=device)\n    return scale * torch.exp(torch.logit(u * (max_cdf - min_cdf) + min_cdf) + loc)\n", "CodeBleu_score": 39.28205072457004}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n", "CodeBleu_score": 39.31155010226621}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not implemented\")\n\n", "CodeBleu_score": 39.1577377281018}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        assert channels is not None\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n", "CodeBleu_score": 40.31289302422579}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if freqs.shape[-1] % 2:\n        freqs = freqs[..., :-1]\n    t, freqs = map(lambda t: t.to(t.device, dtype=torch.float32), (t, freqs))\n    freqs = freqs * scale\n    shape = t.shape\n    t = t.reshape(*shape[:-1], shape[-1] // 2, 2)\n    t = torch.view_as_complex(t)\n    t = torch.view_as_real(t * freqs)\n    t = t.reshape(*shape)\n    return torch.cat((t_pass, t_rot), dim=-1)\n\n", "CodeBleu_score": 27.69618619045513}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    import torch\n    if freqs.ndim == 1:\n        freqs = freqs[:, None]\n    t = torch.as_tensor(t, dtype = torch.float32)\n    freqs = torch.as_tensor(freqs, dtype = torch.float32)\n    rot_dim = freqs.shape[-1]\n    t_left, t = t[..., :rot_dim], t[..., rot_dim:]\n    t = t * freqs.cos() + t_left * freqs.sin()\n    t_right, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n    t = torch.cat((t_right, t_pass), dim = -1)\n    return t\n", "CodeBleu_score": 39.31471073824774}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if len(freqs.shape) == 1:\n        freqs = tf.repeat(freqs, 2)\n    t, freqs = map(tf.cast, (t, freqs))\n    t_shape = tf.shape(t)\n    freqs_shape = tf.shape(freqs)\n    seq_len, n_channels = t_shape[1], t_shape[2]\n    freqs = tf.reshape(freqs, (1, 1, -1, 2))\n    freqs = tf.repeat(freqs, seq_len, axis=1)\n    freqs = tf.repeat(freqs, n_channels, axis=2)\n    t = tf.cast(t, tf.complex64)\n    t_rotated = tf.multiply(t, tf.complex(tf.cos(freqs), tf.sin(freqs)))\n    t_rotated = tf.cast(t_rotated, tf.float32)\n    t_unrotated = t[..., n_channels // 2:]\n    t_out = tf.concat([t_rotated, t_unrotated], axis=-1)\n    return t_out * scale\n", "CodeBleu_score": 36.857929184833594}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n    embeddings = self.codebook.embeddings(idx)\n    if dim != -1:\n        embeddings = embeddings.moveaxis(dim, -1)\n    return embeddings", "CodeBleu_score": 30.71407254824461}, {"code": "def idx2vq(self, idx, dim=-1)\n    return self.idx2vq(idx, dim)", "CodeBleu_score": 14.70169372595011}, {"code": "def idx2vq(self, idx, dim=-1)\n    return self.codebook.embedding(idx).moveaxis(-1, dim)", "CodeBleu_score": 20.1571199580062}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n    encoded = x\n    for block in self.blocks:\n        encoded = block(encoded)\n    if quantize:\n        quantized, indices, loss = self.quantizer(encoded)\n        return quantized, encoded, indices, loss\n    return encoded\n\n", "CodeBleu_score": 28.320468258172117}, {"code": "def encode(self, x, quantize=False)\n    if quantize:\n        return self.encode_with_quantization(x)\n    else:\n        return self.encode_without_quantization(x)\n", "CodeBleu_score": 17.36295917658052}, {"code": "def encode(self, x, quantize=False)\n    if quantize:\n        return self.quantize(x)\n    else:\n        return self.encode(x)", "CodeBleu_score": 14.717600497598019}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = self.ema_scope_context_counter\n        self.ema_scope_context_counter += 1\n    self._ema_scope_context_counter_stack.append(context)\n    if self._ema_scope_context_counter_stack.count(context) > 1:\n        yield\n    else:\n        try:\n            if self.ema_scope_context_counter == 0:\n                self._ema_scope_context_counter_stack.pop()\n                yield\n            else:\n                self.swap_ema_weights()\n                if self.ema_scope_context_counter == 1:\n                    self._ema_scope_context_counter_stack.pop()\n                yield\n        finally:\n            if self._ema_scope_context_counter_stack.count(context) > 1:\n                self._ema_scope_context_counter_stack.pop()\n            else:\n                self.swap_ema_weights()\n                self._ema_scope_context_counter_stack.pop()", "CodeBleu_score": 39.73388094516783}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = {}\n    if self.ema_weights:\n        self.ema_weights.store(context)\n        self.ema_weights.apply_ema_weights()\n        if context:\n            context['ema_applied'] = True\n    try:\n        yield\n    finally:\n        if self.ema_weights:\n            self.ema_weights.restore(context)\n            if context:\n                context['ema_applied'] = False", "CodeBleu_score": 41.67294543522622}, {"code": "def ema_scope(self, context=None)\n    if self.ema_enabled:\n        self.ema_backup = self.get_ema_params()\n        self.set_ema_params(self.ema_params)\n        if context is not None:\n            self.logger.info(f\"{context} - Switching to EMA weights\")\n    yield\n    if self.ema_enabled:\n        self.set_ema_params(self.ema_backup)\n        if context is not None:\n            self.logger.info(f\"{context} - Restoring original weights\")", "CodeBleu_score": 51.67703275793099}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm_mlp(x)\n    x = x * scale_mlp + shift_mlp\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 34.43901548849141}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm_1(x)\n    x = x * scale_mlp + shift_mlp\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 34.300451694887336}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm_msa(x)\n    x = self.mlp(self.norm_mlp(x), shift_mlp, scale_mlp) * gate_mlp\n    return x\n\n", "CodeBleu_score": 38.38771440708732}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = jnp.concatenate([qkv_context, qkv_x], axis=-1)\n    q, k, v, intermediates = _attention(qkv, c)\n    qkv_context, qkv_x = qkv.split([c, c], axis=-1)\n    context = context_block.post_attention(qkv_context, intermediates)\n    x = x_block.post_attention(qkv_x, intermediates)\n    return context, x\n\n", "CodeBleu_score": 14.3720542118012}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Compute qkv tensors\n    qkv = context_block.pre_attention(context, c)\n    qkv = x_block.pre_attention(x, c)\n\n    # Compute intermediates\n    intermediates = context_block.intermediate(context, c)\n    intermediates = x_block.intermediate(x, c)\n\n    # Concatenate qkv tensors\n    qkv = torch.cat([qkv, intermediates], dim=-1)\n\n    # Apply optimized attention\n    qkv = attention(qkv, c)\n\n    # Split attention results back into context and x parts\n    context, x = torch.split(qkv, [context.shape[-1], x.shape[-1]], dim=-1)\n\n    # Process context and x through their respective post-attention blocks\n    context = context_block.post_attention(context, c)\n    x = x_block.post_attention(x, c)\n\n    return context, x\n\n", "CodeBleu_score": 21.722311906658938}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = jnp.concatenate([qkv_context, qkv_x], axis=2)\n    qkv_shape = qkv.shape\n    qkv = jnp.reshape(qkv, (qkv_shape[0], qkv_shape[1], 3, c, qkv_shape[2] // c))\n    qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n    q, k, v = qkv[0], qkv[1], qkv[2]\n    q = q * (c ** (-0.5))\n    dot_product = jnp.einsum(\"bhci,bhcj->bhij\", q, k)\n    attention = jax.nn.softmax(dot_product, axis=-1)\n    attention = attention @ v\n    attention_shape = attention.shape\n    attention = jnp.transpose(attention, (0, 2, 1, 3))\n    attention = jnp.reshape(attention, (attention_shape[0], attention_shape[1], attention_shape[2] * attention_shape[3]))\n    context_intermediate, x_intermediate = context_block.post_attention(attention[:, :, :context.shape[2]], attention[:, :, context.shape[2]:])\n    context = context + context_intermediate\n    x = x + x_intermediate\n    return context, x\n\n", "CodeBleu_score": 31.583035229273204}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 66.60053342302585}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.21369170165848}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1), mode=\"constant\")\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 72.1741376928431}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n    import torch\n    import torch.nn.functional as F\n    import math\n\n    # Reshape the tensors to prepare for attention computation\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n\n    # Compute attention using scaled dot-product attention\n    try:\n        attn_output = F.softmax(torch.bmm(q, k.transpose(1, 2)) / math.sqrt(q.size(-1)), dim=-1)\n        attn_output = torch.bmm(attn_output, v)\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            # If out of memory, switch to slice attention\n            attn_output = slice_attention(q, k, v)\n        else:\n            raise e\n\n    # Reshape the output back to the original dimensions\n    attn_output = attn_output.transpose(1, 2).reshape(q.shape[0], q.shape[1], q.shape[2], q.shape[3])\n    return attn_output\n", "CodeBleu_score": 32.413267939972066}, {"code": "def pytorch_attention(q, k, v)\n    # Reshape tensors to 2D\n    q = q.reshape(q.shape[0], q.shape[1], -1)\n    k = k.reshape(k.shape[0], k.shape[1], -1)\n    v = v.reshape(v.shape[0], v.shape[1], -1)\n\n    # Transpose tensors to prepare for attention computation\n    q = q.transpose(1, 2)\n    k = k.transpose(1, 2)\n    v = v.transpose(1, 2)\n\n    # Compute attention using scaled dot-product attention\n    try:\n        attn_output = torch.bmm(q, k.transpose(1, 2))\n        attn_output = attn_output / np.sqrt(k.shape[-1])\n        attn_output = torch.softmax(attn_output, dim=-1)\n        attn_output = torch.bmm(attn_output, v)\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            # Fallback to slice attention if OOM exception occurs\n            attn_output = torch.zeros_like(v)\n            for i in range(v.shape[0]):\n                attn_output[i] = torch.bmm(q[i].unsqueeze(0), k[i].transpose(0, 1))\n                attn_output[i] = attn_output[i] / np.sqrt(k.shape[-1])\n                attn_output[i] = torch.softmax(attn_output[i], dim=-1)\n                attn_output[i] = torch.bmm(attn_output[i].unsqueeze(0), v[i].unsqueeze(2))\n        else:\n            raise e\n\n    # Reshape attention output back to original dimensions\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(attn_output.shape[0],", "CodeBleu_score": 27.646440841744145}, {"code": "def pytorch_attention(q, k, v)\n    # Reshape the tensors to prepare for attention computation\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n\n    # Compute attention using scaled dot-product attention\n    try:\n        attention = torch.nn.functional.softmax(\n            q @ k.transpose(1, 2) / np.sqrt(k.shape[2]), dim=-1)\n    except RuntimeError:\n        # If OOM exception occurs, switch to slice attention as a fallback\n        attention = torch.zeros_like(q @ k.transpose(1, 2))\n        for i in range(0, q.shape[1], 100):\n            attention[:, i:i+100] = torch.nn.functional.softmax(\n                q[:, i:i+100] @ k.transpose(1, 2) / np.sqrt(k.shape[2]), dim=-1)\n\n    # Compute the attention output\n    attention_output = attention @ v\n\n    # Reshape the attention output back to the original dimensions\n    attention_output = attention_output.transpose(1, 2).reshape(\n        q.shape[0], q.shape[1], q.shape[2], q.shape[3])\n\n    return attention_output\n\n", "CodeBleu_score": 30.59361044782908}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n    if control and name:\n        control_value = control.pop(name)\n        try:\n            h.add_control(name, control_value)\n        except:\n            logger.warning(f\"Failed to add control '{name}' with value '{control_value}' to header.\")\n    return h", "CodeBleu_score": 32.88737461561987}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        control_name = control.pop(0)\n        try:\n            h.add_control(control_name, name)\n        except Exception as e:\n            print(\"Warning: Could not add control {} to {}: {}\".format(control_name, name, e))\n    return h\n\n", "CodeBleu_score": 37.87138209657102}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            control_value = control.pop(0)\n        except IndexError:\n            control_value = None\n        if control_value:\n            try:\n                h[name] = control_value\n            except Exception as e:\n                print(f\"Warning: Failed to apply control {name} with value {control_value}: {e}\")\n    return h\n\n", "CodeBleu_score": 36.59486438233748}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    if indices.shape[1] != self.permutation_dim:\n        raise RuntimeError(\"Indices have the wrong shape: {} vs {}\".format(indices.shape[1], self.permutation_dim))\n    if self.quantize_interface:\n        # shape specifying the group size to compute the neighbors\n        if shape is not None:\n            if indices.dtype != torch.int64:\n                raise ValueError(\"Input indices should be a 64-bit integer tensor, \"\n                                 \"but got {}\".format(indices.dtype))\n            # indices shape: (batch_size, group_size, num_groups)\n            half_indices = indices[:, :, None, :].expand(-1, -1, 2, -1)\n            # half_indices shape: (batch_size, group_size, 2, num_groups)\n            half_indices = half_indices.reshape(-1, self.permutation_dim)\n            # half_indices shape: (batch_size * group_size * 2, num_groups)\n            neighbors = self.codebook.embedding(half_indices)\n            # neighbors shape: (batch_size * group_size * 2, num_groups, embedding_dim)\n            neighbors = neighbors.view(indices.shape[0], indices.shape[1], 2, -1)\n            # neighbors shape: (batch_size, group_size, 2, num_groups, embedding_dim)\n            quantized = torch.sum(neighbors, dim=2)\n            # quantized shape: (batch_size, group_size, num_groups, embedding_dim)\n        else:\n            quantized = self.codebook.embedding(indices)\n            # quantized shape: (batch_size, group_size, num_groups, embedding_dim)\n        if self.use_same_embedding_for_transform:\n            quantized = quantized / self.stop_gradient_for_codebook_iteration\n            # quantized shape: (batch_size, group_size, num_groups, embedding_dim)\n        if shape is not None:\n            quant", "CodeBleu_score": 38.514550896476365}, {"code": "def get_codebook_entry(self, indices, shape)\n    if not isinstance(indices, tf.Tensor):\n        indices = tf.constant(indices)\n    one_hot_indices = tf.one_hot(indices, self.num_embeddings)\n    shape = shape if shape is not None else one_hot_indices.shape\n    one_hot_indices = tf.reshape(one_hot_indices, (-1, self.num_embeddings))\n    embeddings = tf.matmul(one_hot_indices, self.embeddings)\n    embeddings = tf.reshape(embeddings, shape)\n    embeddings = tf.transpose(embeddings, [0, 3, 1, 2])\n    return embeddings\n\n", "CodeBleu_score": 41.444672468799624}, {"code": "def get_codebook_entry(self, indices, shape)\n    one_hot = F.one_hot(indices, self.num_embeddings).float()\n    zq = torch.matmul(one_hot, self.embedding.weight)\n\n    if shape is not None:\n        zq = zq.view(shape)\n        # [B, C, H, W] -> [B, H, W, C]\n        zq = zq.permute(0, 3, 1, 2).contiguous()\n\n    return zq\n\n", "CodeBleu_score": 43.52413111510499}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n    noise_list = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 1):\n        size = 2 ** i\n        noise_list.append(torch.randn(size, size, device=self.device))\n        noise_list.append(torch.randn(size, size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 35.5130550161217}, {"code": "def make_noise(self)\n    noise_list = [torch.randn(4, 4, device=self.device)]\n    for size_log in range(3, int(math.log(self.size, 2)) + 1):\n        size = 2 ** size_log\n        noise_list.append(torch.randn(size, size, device=self.device))\n        noise_list.append(torch.randn(size, size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 24.928955141638834}, {"code": "def make_noise(self)\n    noise_list = [torch.randn(4, 4, device=self.device)]\n    for size_log in range(3, int(self.log_size) + 1):\n        size = 2 ** size_log\n        noise_list.append(torch.randn(size, size, device=self.device))\n        noise_list.append(torch.randn(size, size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 29.027285743304375}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n    latent_in = tf.random.normal(shape=(num_latent, self.latent_dim))\n    latent_out = self.style_mlp(latent_in)\n    latent_mean = tf.reduce_mean(latent_out, axis=0)\n    return latent_mean\n\n", "CodeBleu_score": 28.98613962728177}, {"code": "def mean_latent(self, num_latent)\n    z_latent = torch.randn(num_latent, self.latent_dim).to(self.device)\n    z_latent = self.style(z_latent)\n    z_latent = z_latent.mean(0)\n    return z_latent\n\n", "CodeBleu_score": 18.54660134372497}, {"code": "def mean_latent(self, num_latent)\n    latent_in = torch.randn(num_latent, self.style_dim, device=self.device)\n    latent = self.style(latent_in)\n    latent = latent.mean(0)\n    return latent", "CodeBleu_score": 30.62442195141787}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n    jitter = tf.random.uniform(shape=[], minval=-shift, maxval=shift)\n    img = img + jitter\n    img = tf.clip_by_value(img, 0, 1)\n    return img\n", "CodeBleu_score": 48.05473232980888}, {"code": "def color_jitter(img, shift)\n    jitter = tf.random.uniform([3], -shift, shift)\n    img = img + jitter\n    img = tf.clip_by_value(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 47.76768563505824}, {"code": "def color_jitter(img, shift)\n    img = img.copy()\n    jitter = np.random.randint(-shift, shift, size=img.shape)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n", "CodeBleu_score": 51.85827263422544}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n    self.lq = data[\"lq\"]\n    if \"gt\" in data:\n        self.gt = data[\"gt\"]\n    if \"facial_components\" in data:\n        self.left_eye = data[\"facial_components\"][\"left_eye\"]\n        self.right_eye = data[\"facial_components\"][\"right_eye\"]\n        self.mouth = data[\"facial_components\"][\"mouth\"]\n\n", "CodeBleu_score": 41.324697189010315}, {"code": "def feed_data(self, data)\n    if self.device == \"cpu\":\n        self.lq = data[\"lq\"].cpu()\n    else:\n        self.lq = data[\"lq\"].cuda()\n\n    if \"gt\" in data.keys():\n        if self.device == \"cpu\":\n            self.gt = data[\"gt\"].cpu()\n        else:\n            self.gt = data[\"gt\"].cuda()\n\n    if \"facial_components\" in data.keys():\n        self.left_eye_loc = data[\"facial_components\"][\"left_eye_loc\"]\n        self.right_eye_loc = data[\"facial_components\"][\"right_eye_loc\"]\n        self.mouth_loc = data[\"facial_components\"][\"mouth_loc\"]\n", "CodeBleu_score": 33.35954389586504}, {"code": "def feed_data(self, data)\n    self.lq = data['lq']\n    if 'gt' in data.keys():\n        self.gt = data['gt']\n    if 'facial_components' in data.keys():\n        self.left_eye_loc = data['facial_components']['left_eye_loc']\n        self.right_eye_loc = data['facial_components']['right_eye_loc']\n        self.mouth_loc = data['facial_components']['mouth_loc']", "CodeBleu_score": 37.8742188893219}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n    pyramid = [self.gt_image]\n    for i in range(self.pyramid_levels):\n        pyramid.insert(0, self.downscale_image(pyramid[0]))\n    return pyramid\n\n", "CodeBleu_score": 15.507169596791378}, {"code": "def construct_img_pyramid(self)\n    pyramid = [self.gt_img]\n    for i in range(1, self.num_downscales):\n        img = cv2.resize(pyramid[0], None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n        pyramid.insert(0, img)\n    return pyramid", "CodeBleu_score": 31.946352354335705}, {"code": "def construct_img_pyramid(self)\n    pyramid = [self.gt_img]\n    for i in range(self.pyramid_levels - 1):\n        pyramid.insert(0, self.downscale_img(pyramid[0]))\n    return pyramid\n", "CodeBleu_score": 15.681956141892842}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n    save_filename = \"%s_iter_%d.pth\" % (epoch, current_iter)\n    save_path = os.path.join(self.save_dir, save_filename)\n    torch.save(\n        {\n            \"gen_A2B\": self.gen_A2B.state_dict(),\n            \"gen_B2A\": self.gen_B2A.state_dict(),\n            \"gen_A2B_ema\": self.gen_A2B_ema.state_dict(),\n            \"gen_B2A_ema\": self.gen_B2A_ema.state_dict(),\n            \"disc_A\": self.disc_A.state_dict(),\n            \"disc_B\": self.disc_B.state_dict(),\n            \"disc_A_ema\": self.disc_A_ema.state_dict(),\n            \"disc_B_ema\": self.disc_B_ema.state_dict(),\n            \"disc_l\": self.disc_l.state_dict(),\n            \"disc_r\": self.disc_r.state_dict(),\n            \"disc_m\": self.disc_m.state_dict(),\n            \"optimizer_G\": self.optimizer_G.state_dict(),\n            \"optimizer_D\": self.optimizer_D.state_dict(),\n            \"epoch\": epoch,\n            \"current_iter\": current_iter,\n        },\n        save_path,\n    )\n    if self.use_wandb:\n        wandb.save(save_path)\n\n", "CodeBleu_score": 27.82162884306932}, {"code": "def save(self, epoch, current_iter)\n    state = {\n        \"epoch\": epoch,\n        \"iter\": current_iter,\n        \"generator\": self.generator.state_dict(),\n        \"generator_ema\": self.generator_ema.state_dict(),\n        \"discriminator\": self.discriminator.state_dict(),\n    }\n    if self.use_facial_discriminators:\n        state[\"discriminator_left_eye\"] = self.discriminator_left_eye.state_dict()\n        state[\"discriminator_right_eye\"] = self.discriminator_right_eye.state_dict()\n        state[\"discriminator_mouth\"] = self.discriminator_mouth.state_dict()\n    torch.save(state, os.path.join(self.checkpoint_dir, \"checkpoint.pt\"))\n    torch.save(self.optimizer.state_dict(), os.path.join(self.checkpoint_dir, \"optimizer.pt\"))\n    torch.save(self.scheduler.state_dict(), os.path.join(self.checkpoint_dir, \"scheduler.pt\"))\n\n", "CodeBleu_score": 30.49038595455346}, {"code": "def save(self, epoch, current_iter)\n    save_path = os.path.join(self.save_dir, \"models\")\n    os.makedirs(save_path, exist_ok=True)\n    self.gen.save(os.path.join(save_path, \"gen_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n    self.gen_ema.save(os.path.join(save_path, \"gen_ema_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n    self.disc.save(os.path.join(save_path, \"disc_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n    if self.use_facial_discriminators:\n        self.disc_left_eye.save(os.path.join(save_path, \"disc_left_eye_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n        self.disc_right_eye.save(os.path.join(save_path, \"disc_right_eye_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n        self.disc_mouth.save(os.path.join(save_path, \"disc_mouth_epoch_{}_iter_{}.h5\".format(epoch, current_iter)))\n    self.save_training_state(epoch, current_iter)\n", "CodeBleu_score": 42.17635064979274}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n    import os\n    import shutil\n\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n\n", "CodeBleu_score": 75.22246111197535}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n", "CodeBleu_score": 52.02132431049928}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n", "CodeBleu_score": 52.02132431049928}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n    if len(self.experiences) < sample_size:\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for sample in batch:\n        self.experiences.remove(sample)\n    return batch", "CodeBleu_score": 45.23721283201068}, {"code": "def next_batch(self, sample_size=1)\n    batch = []\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for i in batch:\n        self.memory.remove(i)\n    return batch\n\n", "CodeBleu_score": 50.65519714697415}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for i in batch:\n        self.memory.remove(i)\n    return batch\n", "CodeBleu_score": 48.48618631661154}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n    flattened_configs = [flatten_config(config) for config in self.all_configs]\n    X = np.array(flattened_configs)\n    y_pred = self.cost_model.predict(X)\n    if self.metric == \"latency\":\n        order = np.argsort(y_pred)\n    elif self.metric == \"accuracy\":\n        order = np.argsort(-y_pred)\n    else:\n        raise ValueError(\"Invalid metric specified.\")\n    return order[:self.num_configs]", "CodeBleu_score": 21.817439812671623}, {"code": "def find_estimated_top_configs(self)\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_config = []\n        for key, value in config.items():\n            if isinstance(value, (int, float)):\n                flattened_config.append(value)\n            elif isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if isinstance(sub_value, (int, float)):\n                        flattened_config.append(sub_value)\n        flattened_configs.append(flattened_config)\n\n    X = np.array(flattened_configs)\n    y_pred = self.cost_model.predict(X)\n\n    if self.metric == \"accuracy\":\n        sorted_indices = np.argsort(y_pred)[::-1]\n    elif self.metric == \"latency\":\n        sorted_indices = np.argsort(y_pred)\n    elif self.metric == \"energy\":\n        sorted_indices = np.argsort(y_pred)\n    else:\n        raise ValueError(\"Invalid metric specified.\")\n\n    return sorted_indices\n\n", "CodeBleu_score": 36.54589746561302}, {"code": "def find_estimated_top_configs(self)\n    all_configs = self.config_space.get_all_configs()\n    all_configs = [self.config_space.flatten_config(config) for config in all_configs]\n    all_configs = np.array(all_configs)\n\n    if self.metric == \"time\":\n        all_configs_values = all_configs[:, 0]\n    elif self.metric == \"energy\":\n        all_configs_values = all_configs[:, 1]\n    elif self.metric == \"accuracy\":\n        all_configs_values = all_configs[:, 2]\n    elif self.metric == \"latency\":\n        all_configs_values = all_configs[:, 3]\n\n    if self.metric == \"time\":\n        estimated_top_configs = self.cost_model.predict(all_configs_values.reshape(-1, 1))\n    elif self.metric == \"energy\":\n        estimated_top_configs = self.cost_model.predict(all_configs_values.reshape(-1, 1))\n    elif self.metric == \"accuracy\":\n        estimated_top_configs = self.cost_model.predict(all_configs_values.reshape(-1, 1))\n    elif self.metric == \"latency\":\n        estimated_top_configs = self.cost_model.predict(all_configs_values.reshape(-1, 1))\n\n    if self.metric == \"time\":\n        estimated_top_configs_indices = np.argsort(estimated_top_configs)\n    elif self.metric == \"energy\":\n        estimated_top_configs_indices = np.argsort(estimated_top_configs)\n    elif self.metric == \"accuracy\":\n        estimated_top_configs_indices = np.argsort(estimated_top_configs)[::-1]\n    elif self.metric == \"latency\":\n        estimated_top_configs_indices = np.argsort(estimated_top_configs)\n\n    return estimated_top_configs_indices", "CodeBleu_score": 23.31273212722885}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 73.40656824096638}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 73.40656824096638}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 74.55414743122569}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n    if self.optimal_cmd:\n        cmd = self.optimal_cmd\n        proc = subprocess.Popen(cmd, shell=True)\n        proc.wait()\n        self.logger.info(\"Successfully executed the optimal command: %s\" % cmd)\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 47.12670496276956}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        self.logger.info(\"Autotuning succeeded. The optimal command is: %s\", self.optimal_cmd)\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 49.847815235812206}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n        cmd = self.optimal_cmd.split()\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        process.wait()\n        self.logger.info(f\"Successfully executed the command: {self.optimal_cmd}\")\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 47.22909553447225}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n    exp_name = exp[\"name\"]\n    exp_args = exp[\"args\"]\n    exp_script = exp[\"script\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_master_port = exp[\"master_port\"]\n    exp_user_args = exp[\"user_args\"]\n    exp_user_args_mappings = exp[\"user_args_mappings\"]\n    exp_user_args_mapping_type = exp[\"user_args_mapping_type\"]\n    exp_user_args_mapping_type_args = exp[\"user_args_mapping_type_args\"]\n\n    # Set the experiment's master port and result directory\n    exp_args[\"master_port\"] = exp_master_port\n    exp_args[\"result_dir\"] = exp_result_dir\n\n    # Update user arguments based on argument mappings\n    if exp_user_args_mappings:\n        if exp_user_args_mapping_type == \"grid\":\n            exp_user_args = self.update_user_args_with_grid_mappings(\n                exp_user_args, exp_user_args_mappings, exp_user_args_mapping_type_args\n            )\n        elif exp_user_args_mapping_type == \"random\":\n            exp_user_args = self.update_user_args_with_random_mappings(\n                exp_user_args, exp_user_args_mappings, exp_user_args_mapping_type_args\n            )\n        else:\n            raise ValueError(\n                f\"Invalid user_args_mapping_type: {exp_user_args_mapping_type}\"\n            )\n\n    # Create a new thread to run the experiment\n    t = threading.Thread(\n        target=self.run_experiment,\n        args=(\n            exp_name,\n            exp_script,\n            exp_args,\n            exp_user_args,\n            reservations,\n        ),\n    )\n\n    # Start the thread\n    t.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[exp_name] = {\n        \"timestamp\": time.time(),", "CodeBleu_score": 23.6904320392896}, {"code": "def run_job(self, exp: dict, reservations)\n    try:\n        # Set the experiment's master port and result directory\n        exp[\"master_port\"] = self.master_port\n        exp[\"result_dir\"] = self.result_dir\n\n        # Update user arguments based on argument mappings\n        for arg in exp[\"args\"]:\n            if arg in self.arg_mappings:\n                arg_value = self.arg_mappings[arg]\n                if isinstance(arg_value, list):\n                    exp[\"args\"][arg] = \" \".join(arg_value)\n                else:\n                    exp[\"args\"][arg] = arg_value\n\n        # Start the experiment in a new thread\n        t = threading.Thread(target=run_experiment, args=(exp,))\n        t.start()\n\n        # Store the running experiment details with a timestamp\n        self.running_experiments[exp[\"name\"]] = {\n            \"exp\": exp,\n            \"start_time\": time.time(),\n        }\n        return True\n    except Exception as e:\n        print(f\"Error running job: {e}\")\n        return False\n\n", "CodeBleu_score": 30.732662526925342}, {"code": "def run_job(self, exp: dict, reservations)\n    exp[\"master_port\"] = self.get_free_port()\n    exp[\"result_dir\"] = self.result_dir\n    self.update_user_args(exp)\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n    self.running_experiments[exp[\"timestamp\"]] = exp\n", "CodeBleu_score": 10.830879711116086}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed:\n        if self.rank == 0:\n            self.log_permissions = True\n    if self.log_permissions:\n        message = f\"Rank {self.rank} - {message}\"\n        if ranks is not None:\n            message = f\"{message} - Ranks: {ranks}\"\n        with open(path, \"a\") as f:\n            json.dump(message, f)\n            f.write(\"\\n\")\n\n", "CodeBleu_score": 24.519433778592436}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed_training_initialized:\n        if self.rank == 0:\n            self.log_permissions = True\n    else:\n        self.log_permissions = True\n\n    if self.log_permissions:\n        if ranks is not None:\n            message[\"ranks\"] = ranks\n        if path is None:\n            path = self.log_file_path\n        with open(path, \"a\") as f:\n            json.dump(message, f)\n            f.write(\"\\n\")\n", "CodeBleu_score": 27.381497324960762}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed_training_initialized:\n        if ranks is None:\n            ranks = [0]\n        if path is None:\n            path = self.experiment_results_path\n        if self.rank in ranks:\n            message = f\"rank {self.rank}: \" + message\n            with open(path, \"a\") as f:\n                json.dump(message, f)\n                f.write(\"\\n\")\n    else:\n        if self.rank == 0:\n            if ranks is None:\n                ranks = [0]\n            if path is None:\n                path = self.experiment_results_path\n            if self.rank in ranks:\n                message = f\"rank {self.rank}: \" + message\n                with open(path, \"a\") as f:\n                    json.dump(message, f)\n                    f.write(\"\\n\")\n\n", "CodeBleu_score": 25.059849817289937}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n        raise ValueError(f\"No layer files found for layer index {layer_index}.\")\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Layer index {layer_index} out of range.\")\n    prefix = f\"layer_{layer_index}_\"\n    tp_degree_to_files = defaultdict(list)\n    for file in layer_files:\n        if not file.startswith(prefix):\n            continue\n        tp_degree = int(file.split(\"_\")[-1].split(\".\")[0])\n        tp_degree_to_files[tp_degree].append(file)\n    return tp_degree_to_files\n", "CodeBleu_score": 29.720338277992255}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self.get_files_by_prefix(f\"{layer_index}_\")\n    assert layer_index < self.num_layers, \"Layer index out of range\"\n    tp_degree = self.tp_degrees[layer_index]\n    partition_map = {}\n    for i in range(tp_degree):\n        partition_map[i] = [f for f in layer_files if f.startswith(f\"{layer_index}_{i}_\")]\n    return partition_map\n", "CodeBleu_score": 34.65893745043726}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    if len(self.layer_files) == 0:\n        raise ValueError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index.\")\n    prefix = self.layer_files[layer_index]\n    files = [f for f in self.files if f.startswith(prefix)]\n    partitions = {}\n    for f in files:\n        tp_degree = int(f.split(\"_\")[-1].split(\".\")[0])\n        if tp_degree not in partitions:\n            partitions[tp_degree] = []\n        partitions[tp_degree].append(f)\n    return partitions\n\n", "CodeBleu_score": 38.54701037634006}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n    import platform\n    import psutil\n\n    # Get system information\n    os_info = platform.system()\n    os_release = platform.release()\n    os_version = platform.version()\n    os_architecture = platform.machine()\n    processor = platform.processor()\n    python_version = platform.python_version()\n\n    # Get memory information\n    memory_info = psutil.virtual_memory()\n    total_memory = memory_info.total / (1024 ** 3)  # Convert to GB\n    available_memory = memory_info.available / (1024 ** 3)  # Convert to GB\n\n    # Get disk information\n    disk_info = psutil.disk_usage('/')\n    total_disk_space = disk_info.total / (1024 ** 3)  # Convert to GB\n    free_disk_space = disk_info.free / (1024 ** 3)  # Convert to GB\n\n    # Create a dictionary with the system information\n    system_info_dict = {\n        'Operating System': os_info,\n        'Release': os_release,\n        'Version': os_version,\n        'Architecture': os_architecture,\n        'Processor': processor,\n        'Python Version': python_version,\n        'Total Memory (GB)': total_memory,\n        'Available Memory (GB)': available_memory,\n        'Total Disk Space (GB)': total_disk_space,\n        'Free Disk Space (GB)': free_disk_space\n    }\n\n    return system_info_dict\n", "CodeBleu_score": 46.778262733762766}, {"code": "def system_info()\n    import platform\n    import sys\n    import psutil\n\n    # Get system information\n    system = platform.system()\n    architecture = platform.architecture()[0]\n    version = platform.version()\n    processor = platform.processor()\n    python_version = sys.version\n\n    # Get memory information\n    memory = psutil.virtual_memory()\n    total_memory = memory.total\n    available_memory = memory.available\n\n    # Get disk information\n    disk = psutil.disk_usage('/')\n    total_disk_space = disk.total\n    free_disk_space = disk.free\n\n    # Get CPU information\n    cpu_count = psutil.cpu_count()\n    cpu_freq = psutil.cpu_freq()\n    cpu_percent = psutil.cpu_percent()\n\n    # Create dictionary with system information\n    system_info = {\n        \"System\": system,\n        \"Architecture\": architecture,\n        \"Version\": version,\n        \"Processor\": processor,\n        \"Python Version\": python_version,\n        \"Total Memory\": total_memory,\n        \"Available Memory\": available_memory,\n        \"Total Disk Space\": total_disk_space,\n        \"Free Disk Space\": free_disk_space,\n        \"CPU Count\": cpu_count,\n        \"CPU Frequency\": cpu_freq,\n        \"CPU Percent\": cpu_percent\n    }\n\n    return system_info\n\n", "CodeBleu_score": 42.33929303335838}, {"code": "def system_info()\n    import platform\n    import sys\n\n    # Get system information\n    system = platform.system()\n    architecture = platform.architecture()[0]\n    version = platform.version()\n    processor = platform.processor()\n    python_version = sys.version\n\n    # Create a dictionary with the system information\n    system_info = {\n        \"System\": system,\n        \"Architecture\": architecture,\n        \"Version\": version,\n        \"Processor\": processor,\n        \"Python Version\": python_version\n    }\n\n    return system_info\n", "CodeBleu_score": 49.171423837931826}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n    import torch\n    import torch.cuda\n    cuda_is_available = torch.cuda.is_available()\n    if cuda_is_available:\n        gpu_count = torch.cuda.device_count()\n        gpu_names = [torch.cuda.get_device_name(i) for i in range(gpu_count)]\n        cuda_version = torch.version.cuda\n        return f\"CUDA is available with {gpu_count} GPUs: {gpu_names}.\\nCUDA version: {cuda_version}\"\n    else:\n        return \"CUDA is not available.\"\n", "CodeBleu_score": 50.08804395220927}, {"code": "def cuda_info()\n    import torch\n    if torch.cuda.is_available():\n        print(f\"CUDA version: {torch.version.cuda}\")\n        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    else:\n        print(\"CUDA is not available.\")\n", "CodeBleu_score": 44.291919998112775}, {"code": "def cuda_info()\n    import torch\n\n    device_count = torch.cuda.device_count()\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    device_names = []\n    for i in range(device_count):\n        device_names.append(torch.cuda.get_device_name(i))\n    return device_names, cuda_available, cuda_version\n\n", "CodeBleu_score": 35.698587686106684}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_to_class_name_mapping = encoder_manager.config.get(\"class_id_to_class_name_mapping\", None)\n    class_name_to_class_id_mapping = {v: k for k, v in class_id_to_class_name_mapping.items()} if class_id_to_class_name_mapping is not None else None\n\n    class_accuracy = {class_name: 0 for class_name in class_id_to_class_name_mapping.values()}\n    total_accuracy = 0\n    total_items = 0\n\n    for item in dataset_items:\n        audio_file = item[\"audio_file\"]\n        true_class_name = item[class_name_key]\n        true_class_id = class_name_to_class_id_mapping[true_class_name] if class_name_to_class_id_mapping is not None else None\n\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class(embedding, true_class_id) if true_class_id is not None else encoder_manager.predict_class(embedding)\n        predicted_class_name = class_id_to_class_name_mapping[predicted_class_id] if class_id_to_class_name_mapping is not None else None\n\n        if predicted_class_name == true_class_name:\n            class_accuracy[true_class_name] += 1\n            total_accuracy += 1\n\n        total_items += 1\n\n    for class_name, accuracy in class_accuracy.items():\n        print(f\"Accuracy for class {class_name}: {accuracy / total_items:.2f}\")\n\n    print(f\"Average accuracy: {total_accuracy / total_items:.2f}\")\n", "CodeBleu_score": 32.63434454651429}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", {})\n\n    # Initialize variables to store the accuracy for each class and the total number of items\n    class_accuracy = {}\n    total_items = 0\n\n    # Loop through each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file from the item\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Get the true class name from the item\n        true_class_name = item[class_name_key]\n\n        # Get the predicted class label from the embedding\n        predicted_class_label = encoder_manager.predict_class_label(embedding)\n\n        # If a mapping from class ID to class name is available, use it to get the true class name\n        if class_id_to_class_name:\n            true_class_name = class_id_to_class_name[true_class_name]\n\n        # If a predicted class label is available, increment the accuracy for the true class name\n        if predicted_class_label is not None:\n            if true_class_name in class_accuracy:\n                class_accuracy[true_class_name] += int(\n                    predicted_class_label == true_class_name\n                )\n            else:\n                class_accuracy[true_class_name] = int(\n                    predicted_class_label == true_class_name\n                )\n\n        # Increment the total number of items\n        total_items += 1\n\n    # Calculate the accuracy for each class\n    for class_name, correct_predictions in class_accuracy.items():\n        class_accuracy[class_name] = correct_predictions / total_items\n\n    # Calculate the average accuracy across all classes\n    average_accuracy = sum(class_accuracy.values()) / len(class_accuracy)\n\n    # Print", "CodeBleu_score": 32.892919167448575}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n    total_accuracy = 0\n    accuracy_per_class = {}\n\n    for item in dataset_items:\n        audio_file = item[encoder_manager.config.audio_file_key]\n        true_class_name = item[class_name_key]\n        true_class_id = item.get(encoder_manager.config.class_id_key)\n        true_class_name_from_id = class_id_to_class_name.get(true_class_id)\n        if true_class_name_from_id:\n            true_class_name = true_class_name_from_id\n\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_name = encoder_manager.predict_class(embedding)\n\n        if true_class_name == predicted_class_name:\n            total_accuracy += 1\n\n        if true_class_name not in accuracy_per_class:\n            accuracy_per_class[true_class_name] = 0\n\n        if true_class_name == predicted_class_name:\n            accuracy_per_class[true_class_name] += 1\n\n    for class_name, accuracy in accuracy_per_class.items():\n        print(f\"Accuracy for class {class_name}: {accuracy}\")\n\n    average_accuracy = total_accuracy / len(dataset_items)\n    print(f\"Average accuracy: {average_accuracy}\")", "CodeBleu_score": 29.675329188421685}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n    import os\n    import shutil\n    from pydub import AudioSegment\n    from pydub.silence import split_on_silence\n    from pydub.silence import detect_silence\n\n    # Set the input and output directories\n    input_dir = 'input'\n    output_dir = 'output'\n\n    # Replace the input directory with the output directory in the audio path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the output file already exists\n    if os.path.exists(output_path):\n        # If the file already exists, return the output path and a False flag\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Define the silence threshold (in dBFS)\n    silence_threshold = -50\n\n    # Define the minimum silence duration (in milliseconds)\n    min_silence_len = 500\n\n    # Split the audio into chunks based on silence\n    chunks = split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_threshold)\n\n    # Check if any speech was detected\n    if len(chunks) == 0:\n        # If no speech was detected, return the output path and a False flag\n        return output_path, False\n\n    # Combine the chunks into a single audio segment\n    combined = sum(chunks)\n\n    # Export the processed audio to the output path\n    combined.export(output_path, format='wav')\n\n    # Return the output path and a flag indicating that speech was detected\n    return output_path, True\n\n", "CodeBleu_score": 36.800806567349134}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    import os\n    import subprocess\n    import re\n    import shutil\n    from pydub import AudioSegment\n    from pydub.silence import split_on_silence\n\n    # Set the input and output directories\n    input_dir = \"/content/drive/MyDrive/Colab Notebooks/Audio_files\"\n    output_dir = \"/content/drive/MyDrive/Colab Notebooks/Audio_files/output\"\n\n    # Replace the input directory path with the output directory path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the file already exists\n    if os.path.exists(output_path):\n        print(f\"File already exists: {output_path}\")\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Remove silence from the audio\n    chunks = split_on_silence(audio, min_silence_len=100, silence_thresh=-40, keep_silence=200)\n    processed_audio = sum(chunks)\n\n    # Save the processed audio\n    processed_audio.export(output_path, format=\"wav\")\n\n    # Return the output path and a flag indicating if speech was detected\n    return output_path, len(chunks) > 0\n", "CodeBleu_score": 36.061012439906534}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    import os\n    import shutil\n    from pydub import AudioSegment\n    from pydub.silence import split_on_silence\n\n    # Replace the input directory path with the output directory path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the output file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Split the audio into chunks based on silence\n    chunks = split_on_silence(\n        audio,\n        min_silence_len=min_silence_len,\n        silence_thresh=silence_thresh,\n        keep_silence=keep_silence\n    )\n\n    # Combine the chunks into a single audio segment\n    combined_audio = sum(chunks)\n\n    # Export the combined audio to the output file\n    combined_audio.export(output_path, format=\"wav\")\n\n    # Check if speech was detected\n    if combined_audio.duration_seconds < min_duration:\n        os.remove(output_path)\n        return output_path, False\n\n    return output_path, True\n\n", "CodeBleu_score": 41.21041763199086}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import librosa\n    import soundfile as sf\n    from joblib import Parallel, delayed\n    from tqdm import tqdm\n    from shutil import copytree\n\n    # If output directory is provided, copy input directory to output directory\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n\n    # Recursively find all audio files with the specified extension\n    audio_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    # Resample audio files in parallel using specified number of jobs\n    def resample_file(file):\n        y, sr = librosa.load(file, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        sf.write(file, y_resampled, output_sr)\n\n    with tqdm(total=len(audio_files), desc=\"Resampling audio files\") as pbar:\n        Parallel(n_jobs=n_jobs)(delayed(resample_file)(file) for file in audio_files)\n        pbar.update(1)\n\n    print(\"Resampling complete.\")\n", "CodeBleu_score": 40.096192187173294}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import shutil\n    from tqdm import tqdm\n    from joblib import Parallel, delayed\n    from pydub import AudioSegment\n\n    if output_dir:\n        shutil.copytree(input_dir, output_dir)\n        input_dir = output_dir\n\n    def resample_file(file_path):\n        audio = AudioSegment.from_file(file_path)\n        audio = audio.set_frame_rate(output_sr)\n        audio.export(file_path, format=file_ext)\n\n    file_paths = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(f\".{file_ext}\"):\n                file_paths.append(os.path.join(root, file))\n\n    with tqdm(total=len(file_paths), desc=\"Resampling audio files\") as pbar:\n        Parallel(n_jobs=n_jobs)(delayed(resample_file)(file_path) for file_path in file_paths)\n        pbar.update(1)\n\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 38.947489290167866}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import glob\n    import librosa\n    import soundfile as sf\n    from tqdm import tqdm\n    from joblib import Parallel, delayed\n    from shutil import copytree\n\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n\n    files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    def resample_file(file):\n        y, sr = librosa.load(file, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        if output_dir is not None:\n            output_file = os.path.join(output_dir, os.path.relpath(file, input_dir))\n            sf.write(output_file, y_resampled, output_sr)\n        else:\n            sf.write(file, y_resampled, output_sr)\n\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(file) for file in tqdm(files))\n\n    print(f\"Resampled {len(files)} files to {output_sr} Hz\")\n", "CodeBleu_score": 39.75806686197853}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    eval_loss = 0.0\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, labels = batch['inputs'].to(device), batch['labels'].to(device)\n            inputs = inputs.reshape(inputs.shape[0] * inputs.shape[1], inputs.shape[2])\n            labels = labels.reshape(labels.shape[0] * labels.shape[1])\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    writer.add_scalar('Loss/eval', eval_loss, global_step)\n    writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], global_step)\n    writer.add_scalar('Accuracy/eval', accuracy(outputs, labels), global_step)\n    writer.add_scalar('F1/eval', f1(outputs, labels), global_step)\n    writer.add_scalar('Precision/eval', precision(outputs, labels), global_step)\n    writer.add_scalar('Recall/eval', recall(outputs, labels), global_step)\n    writer.add_scalar('AUC/eval', auc(outputs, labels), global_step)\n    writer.add_scalar('MCC/eval', mcc(outputs, labels), global_step)\n    writer.add_scalar('Confusion_Matrix/eval', confusion_matrix(outputs, labels), global_step)\n    writer.add_scalar('ROC_AUC/eval', roc_auc(outputs, labels), global_step)\n    writer.add_scalar('PR_AUC/eval', pr_auc(outputs, labels), global_step)\n    writer.add_scalar('KL_Divergence/eval', kl_divergence(outputs, labels), global_step)\n    writer.add_scalar('JS_Divergence/eval', js_divergence(outputs,", "CodeBleu_score": 28.208764436477395}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    eval_loss = 0.0\n    for i, (inputs, labels) in enumerate(data_loader):\n        with torch.no_grad():\n            inputs = inputs.reshape(-1, inputs.shape[-1])\n            labels = labels.reshape(-1, 1)\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.squeeze())\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    writer.add_scalar('Loss/eval', eval_loss, global_step)\n    writer.add_scalar('Accuracy/eval', accuracy(outputs, labels), global_step)\n    writer.add_scalar('F1_Score/eval', f1_score(outputs, labels), global_step)\n    writer.add_scalar('Precision/eval', precision(outputs, labels), global_step)\n    writer.add_scalar('Recall/eval', recall(outputs, labels), global_step)\n    writer.add_scalar('ROC_AUC/eval', roc_auc(outputs, labels), global_step)\n    writer.add_scalar('Confusion_Matrix/eval', confusion_matrix(outputs, labels), global_step)\n    writer.add_scalar('Mean_Absolute_Error/eval', mean_absolute_error(outputs, labels), global_step)\n    writer.add_scalar('Mean_Squared_Error/eval', mean_squared_error(outputs, labels), global_step)\n    writer.add_scalar('Root_Mean_Squared_Error/eval', root_mean_squared_error(outputs, labels), global_step)\n    writer.add_scalar('R2_Score/eval', r2_score(outputs, labels), global_step)\n    writer.add_scalar('Explained_Variance_Score/eval', explained_variance_score(", "CodeBleu_score": 29.122672923949466}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            data = data.view(-1, 3, 32, 32)\n            target = target.view(-1)\n            output = model(data)\n            loss = criterion(output, target)\n            total_loss += loss.item() * data.size(0)\n\n    avg_loss = total_loss / len(data_loader.dataset)\n    writer.add_scalar(\"Loss/eval\", avg_loss, global_step)\n\n    print(\n        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n            avg_loss,\n            correct,\n            len(data_loader.dataset),\n            100.0 * correct / len(data_loader.dataset),\n        )\n    )\n\n    return avg_loss\n\n", "CodeBleu_score": 25.555448113662134}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n    config_class_name = f\"{model_name.replace('_', '')}_config\"\n    if model_name == \"xtts\":\n        from .xtts_config import XTTSConfig as config_class\n    else:\n        for path in [\n            f\"tts.models.{model_name}.config\",\n            f\"tts.models.{model_name}.configs\",\n        ]:\n            try:\n                config_module = importlib.import_module(path)\n                break\n            except ModuleNotFoundError:\n                config_module = None\n        if config_module is None:\n            raise ModuleNotFoundError(\n                f\"Config for {model_name} cannot be found.\"\n            )\n        config_class = getattr(config_module, config_class_name)\n    return config_class", "CodeBleu_score": 42.78644575620867}, {"code": "def register_config(model_name: str) -> Coqpit\n    if model_name == \"xtts\":\n        from xtts.configs import XTTSConfig\n\n        return XTTSConfig\n    else:\n        config_class_name = f\"{model_name.title()}Config\"\n        config_class = None\n        try:\n            config_class = import_class(f\"xtts.configs.{config_class_name}\")\n        except ModuleNotFoundError:\n            pass\n        if config_class is None:\n            try:\n                config_class = import_class(f\"TTS.tts.configs.{config_class_name}\")\n            except ModuleNotFoundError:\n                pass\n        if config_class is None:\n            try:\n                config_class = import_class(f\"TTS.configs.{config_class_name}\")\n            except ModuleNotFoundError:\n                pass\n        if config_class is None:\n            raise ModuleNotFoundError(f\"Config for '{model_name}' cannot be found.\")\n        return config_class\n\n", "CodeBleu_score": 44.07583594629566}, {"code": "def register_config(model_name: str) -> Coqpit\n    if model_name == \"xtts\":\n        from xtts.configs.xtts_config import XTTSConfig\n\n        return XTTSConfig\n    else:\n        config_class_name = f\"{model_name.title()}Config\"\n        try:\n            return import_class(f\"xtts.configs.{model_name}_config.{config_class_name}\")\n        except ModuleNotFoundError:\n            pass\n        try:\n            return import_class(f\"xtts.configs.{model_name}.{config_class_name}\")\n        except ModuleNotFoundError:\n            pass\n        try:\n            return import_class(f\"xtts.configs.{model_name}.configs.{config_class_name}\")\n        except ModuleNotFoundError:\n            pass\n        raise ModuleNotFoundError(f\"Could not find {config_class_name} configuration class\")\n\n", "CodeBleu_score": 33.31150459864023}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n    config = Coqpit()\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            data = yaml.safe_load(yaml_file)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            try:\n                data = json.load(json_file)\n            except json.decoder.JSONDecodeError:\n                # try to read jsonl format\n                data = [json.loads(line) for line in json_file]\n    else:\n        raise Exception(f\"Unsupported config: {config_path}\")\n    model_name = data[\"model\"]\n    config_class = register_config(model_name.lower())\n    config = config_class()\n    config.from_dict(data)\n    return config\n\n", "CodeBleu_score": 65.9292405164254}, {"code": "def load_config(config_path: str) -> Coqpit\n    config = Coqpit()\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = yaml.safe_load(f)\n    elif ext == \".json\":\n        try:\n            import orjson\n\n            with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n                data = orjson.loads(f.read())\n        except ModuleNotFoundError:\n            with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.loads(f.read())\n    else:\n        raise Exception(f\"Only .yml, .yaml and .json are supported. Received {config_path}\")\n    model = data[\"model\"]\n    model_name = model.replace(\"_\", \"\")\n    config_class = _get_class_from_name(\n        model_name,\n        {\n            \"Vits\": VitsConfig,\n            \"Synthesizer\": SynthesizerConfig,\n            \"MultiBandMelgan\": MultiBandMelganConfig,\n            \"HiFiGAN\": HifiganConfig,\n            \"FastPitch\": FastPitchConfig,\n            \"FastSpeech2\": FastSpeech2Config,\n            \"GlowTTS\": GlowTTSConfig,\n            \"Tacotron2\": Tacotron2Config,\n            \"Melgan\": MelganConfig,\n            \"AlignTTS\": AlignTTSConfig,\n        },\n    )\n    config = config_class(**data)\n    return config\n\n", "CodeBleu_score": 59.72410718205764}, {"code": "def load_config(config_path: str) -> Coqpit\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict.update(yaml.safe_load(yaml_file))\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict.update(json.load(json_file))\n    else:\n        raise Exception(\n            f\"Unknown config file type. Only yaml and json are supported. {config_path}\"\n        )\n\n    model_name = _process_config_dict(config_dict)\n    config_class = register_config(model_name.lower())\n    config = config_class()\n    config.from_dict(config_dict)\n    return config", "CodeBleu_score": 59.78880125918866}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.47690953282769}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath", "CodeBleu_score": 77.47690953282769}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n", "CodeBleu_score": 77.47690953282769}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    torch.cuda.empty_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide checkpoint, config, and vocab paths.\"\n    config = load_config(xtts_config)\n    model = XTTSModel(config)\n    model.load_checkpoint(xtts_checkpoint, map_location=\"cpu\")\n    model.load_vocabulary(xtts_vocab)\n    if torch.cuda.is_available():\n        model.to(\"cuda\")\n    print(\"XTTS model loaded successfully.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 44.63520463922755}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    import torch\n    import gc\n    import os\n    import json\n    import sys\n    import logging\n    from xtts.XTTSModel import XTTSModel\n    from xtts.XTTSConfig import XTTSConfig\n    from xtts.XTTSVocab import XTTSVocab\n    from xtts.XTTSTokenizer import XTTSTokenizer\n\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if the required paths are provided\n    if not xtts_checkpoint:\n        logging.error(\"Error: XTTS checkpoint path is not provided.\")\n        sys.exit(1)\n    if not xtts_config:\n        logging.error(\"Error: XTTS config path is not provided.\")\n        sys.exit(1)\n    if not xtts_vocab:\n        logging.error(\"Error: XTTS vocab path is not provided.\")\n        sys.exit(1)\n\n    # Load the configuration from the provided path\n    with open(xtts_config, \"r\") as f:\n        config = json.load(f)\n    config = XTTSConfig.from_dict(config)\n\n    # Initialize the XTTS model\n    model = XTTSModel(config)\n\n    # Load the model checkpoint and vocabulary\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    vocab = XTTSVocab(xtts_vocab)\n    tokenizer = XTTSTokenizer(vocab)\n\n    # Move the model to the GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n\n    # Print a success message\n    logging.info(\"XTTS model loaded successfully.\")\n\n    return model, vocab, tokenizer\n\n", "CodeBleu_score": 44.71676439511364}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    import torch\n    from transformers import AutoConfig, AutoModelForCausalLM\n    import gc\n    gc.collect()\n    torch.cuda.empty_cache()\n    if xtts_checkpoint is None:\n        return \"Error: Please provide a checkpoint path.\"\n    if xtts_config is None:\n        return \"Error: Please provide a config path.\"\n    if xtts_vocab is None:\n        return \"Error: Please provide a vocab path.\"\n    print(\"Loading configuration...\")\n    config = AutoConfig.from_pretrained(xtts_config)\n    print(\"Initializing model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        xtts_checkpoint,\n        from_tf=bool(\".ckpt\" in xtts_checkpoint),\n        config=config,\n    )\n    print(\"Loading model checkpoint and vocabulary...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\")))\n    model.eval()\n    if torch.cuda.is_available():\n        model.to(\"cuda\")\n    print(\"Model loaded successfully.\")\n    return \"Model loaded successfully.\"\n\n", "CodeBleu_score": 44.570569420418906}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.use_torch:\n        num_frames = int(num_frames * self.hop_length)\n    num_frames = min(num_frames, x.shape[1])\n    if x.shape[1] < num_frames:\n        raise ValueError(\n            \"Input signal length ({}) is shorter than the specified number of frames ({}).\".format(\n                x.shape[1], num_frames\n            )\n        )\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval)\n    frames = []\n    for offset in offsets:\n        offset = int(offset)\n        frames.append(x[:, offset : offset + num_frames])\n    frames = np.vstack(frames)\n    embeddings = self.model(frames)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n    if l2_norm:\n        embeddings = embeddings / np.linalg.norm(embeddings, 2)\n    return embeddings\n\n", "CodeBleu_score": 46.721966168120545}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.use_torch_spec:\n        num_frames = num_frames * self.hop_length\n    num_frames = min(num_frames, x.shape[1])\n    offsets = np.linspace(0, x.shape[1] - num_frames, num=num_eval)\n    frames = []\n    for offset in offsets:\n        offset = int(offset)\n        frames.append(x[:, offset:offset + num_frames])\n    frames = np.vstack(frames)\n    embeddings = self.model.predict(frames)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings, 2)\n    return embeddings\n", "CodeBleu_score": 52.52653387351314}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if isinstance(x, torch.Tensor):\n        x = x.cpu().numpy()\n    if x.ndim == 1:\n        x = x[None, :]\n    if self.sample_rate != self.model_sample_rate:\n        num_frames = int(num_frames * self.model_sample_rate / self.sample_rate)\n    hop_length = self.n_fft // 4\n    if num_frames * hop_length > x.shape[1]:\n        num_frames = x.shape[1] // hop_length\n    if num_frames == 0:\n        num_frames = 1\n    offsets = np.linspace(0, x.shape[1] - num_frames * hop_length, num=num_eval, endpoint=True, dtype=int)\n    feats = []\n    for offset in offsets:\n        feats.append(self.get_scene_embeddings(x[:, offset * hop_length:offset * hop_length + num_frames * hop_length], self.n_fft, self.hop_length, self.n_mels, self.f_min, self.f_max, self.htk, self.norm, self.log))\n    feats = np.concatenate(feats, axis=0)\n    if return_mean:\n        feats = feats.mean(axis=0)\n    if l2_norm:\n        feats = feats / np.linalg.norm(feats, 2)\n    return feats", "CodeBleu_score": 37.666926163923755}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import umap\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        np.sum(num_classes_in_batch == i) for i in range(len(np.unique(num_classes_in_batch)))\n    ]\n\n    # Limit the number of classes to 10\n    num_classes_to_plot = min(10, len(np.unique(num_classes_in_batch)))\n\n    # Calculate the indices of the classes to plot\n    class_indices = np.argsort(num_utterances_per_class)[-num_classes_to_plot:]\n\n    # Filter the embeddings and labels to include only the classes to plot\n    embeddings_to_plot = embeddings[np.isin(num_classes_in_batch, class_indices)]\n    labels_to_plot = num_classes_in_batch[np.isin(num_classes_in_batch, class_indices)]\n\n    # Perform dimensionality reduction using UMAP\n    reducer = umap.UMAP()\n    embeddings_reduced = reducer.fit_transform(embeddings_to_plot)\n\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(\n        embeddings_reduced[:, 0],\n        embeddings_reduced[:, 1],\n        c=labels_to_plot,\n        cmap=\"tab10\",\n        s=5,\n        alpha=0.5,\n    )\n    plt.title(\"UMAP projection\", fontsize=16)\n    plt.axis(\"equal\")\n    plt.savefig(\"umap\")\n    plt.close()\n\n    # Return the figure\n    return scatter.get_figure()\n\n", "CodeBleu_score": 41.70181250857109}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import matplotlib.pyplot as plt\n    from umap import UMAP\n    import numpy as np\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [np.sum(num_classes_in_batch == i) for i in range(num_classes_in_batch.max() + 1)]\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the cumulative sum of utterances per class\n    cumulative_sum = np.cumsum(num_utterances_per_class)\n\n    # Get the indices of the classes to keep\n    indices_to_keep = np.concatenate([np.arange(sum) for sum in cumulative_sum])\n\n    # Reduce the embeddings and labels to the selected classes\n    embeddings_reduced = embeddings[indices_to_keep]\n    labels_reduced = num_classes_in_batch[indices_to_keep]\n\n    # Perform dimensionality reduction using UMAP\n    reducer = UMAP()\n    embeddings_transformed = reducer.fit_transform(embeddings_reduced)\n\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(embeddings_transformed[:, 0], embeddings_transformed[:, 1], c=labels_reduced, cmap='tab10')\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.colorbar(scatter)\n    plt.title('UMAP projection')\n    plt.savefig(\"umap\")\n    plt.close()\n\n    # Return the figure\n    return plt.gcf()\n", "CodeBleu_score": 44.29878335268757}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import umap\n    import matplotlib.pyplot as plt\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [sum(num_classes_in_batch == i) for i in range(max(num_classes_in_batch) + 1)]\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = embeddings[:sum(num_utterances_per_class), :]\n        num_classes_in_batch = num_classes_in_batch[:sum(num_utterances_per_class)]\n\n    # Perform dimensionality reduction with UMAP\n    reducer = umap.UMAP()\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 10))\n    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=num_classes_in_batch, cmap='tab10')\n    plt.title(\"UMAP projection\")\n    plt.axis('equal')\n    plt.savefig('umap')\n\n    # Return the figure\n    return scatter.figure\n", "CodeBleu_score": 45.30787822828785}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim_mat = []\n    for spk, utts in self.spk_to_utts.items():\n        spk_dvecs = dvecs[utts]\n        spk_centroids = centroids[utts]\n        for idx, (dvec, centroid) in enumerate(zip(spk_dvecs, spk_centroids)):\n            new_centroids = np.vstack((spk_centroids[:idx], spk_centroids[idx+1:]))\n            cos_sim = np.dot(dvec, new_centroids.T) / (np.linalg.norm(dvec) * np.linalg.norm(new_centroids, axis=1))\n            cos_sim = np.clip(cos_sim, -1, 1)\n            cos_sim_mat.append(cos_sim)\n    return np.vstack(cos_sim_mat)\n", "CodeBleu_score": 28.821778799278754}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim = np.zeros((len(dvecs), len(centroids)))\n    for i, dvec in enumerate(dvecs):\n        new_centroids = np.zeros((len(centroids), dvec.shape[-1]))\n        for j, centroid in enumerate(centroids):\n            new_centroids[j] = np.mean(\n                np.concatenate(\n                    [dvecs[k] for k in range(len(dvecs)) if k != i], axis=0\n                ),\n                axis=0,\n            )\n        cos_sim[i] = np.clip(\n            np.dot(dvec, new_centroids.T)\n            / (np.linalg.norm(dvec) * np.linalg.norm(new_centroids, axis=1)),\n            -1,\n            1,\n        )\n    return cos_sim\n\n", "CodeBleu_score": 31.57869955443548}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim_mat = []\n    for spkr_idx, dvecs_spkr in enumerate(dvecs):\n        centroids_spkr = np.delete(centroids, spkr_idx, axis=0)\n        centroid_spkr = np.mean(centroids_spkr, axis=0)\n        centroid_spkr = centroid_spkr / np.linalg.norm(centroid_spkr)\n        centroid_spkr = np.expand_dims(centroid_spkr, axis=0)\n        centroids_spkr = centroids_spkr / np.linalg.norm(centroids_spkr, axis=1, keepdims=True)\n        cos_sim = np.clip(np.dot(dvecs_spkr, centroids_spkr.T), -1, 1)\n        cos_sim_spkr = np.clip(np.dot(dvecs_spkr, centroid_spkr.T), -1, 1)\n        cos_sim_mat.append(cos_sim)\n        cos_sim_mat.append(cos_sim_spkr)\n    return np.hstack(cos_sim_mat)\n\n", "CodeBleu_score": 34.15974226498826}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)    N = dvecs.size(0)\n    L = []\n    for j in range(N):\n        L_row = - F.log_softmax(cos_sim_matrix[j], 0).diag()\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 39.8387049735233}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)    N = dvecs.size(0)\n    main_loss = []\n    for j in range(N):\n        dvec_j = dvecs[j].repeat(N, 1)\n        input_ = torch.sqrt(2.0) * torch.abs(dvec_j - dvecs)\n        nce_weights = Variable(\n            torch.cat(\n                [\n                    torch.diag(self.nce_weights[0:j]),\n                    torch.diag(self.nce_weights[j : N - 1]),\n                ]\n            )\n        ).cuda()\n        nce_biases = Variable(torch.cat([self.nce_biases[0:j], self.nce_biases[j : N - 1]])).cuda()\n        loss_a = torch.mean(nce_weights * input_ + nce_biases)\n        loss_b = torch.log(torch.sum(torch.exp(input_)))\n        loss = -loss_a + loss_b\n        main_loss.append(loss)\n    return torch.stack(main_loss)\n\n", "CodeBleu_score": 28.370417391873637}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)    N = dvecs.size(0)\n    main_loss = []\n    for j in range(N):\n        dvec_j = dvecs[j].unsqueeze(0)\n        inner_prod_mat = torch.mm(dvec_j, dvecs.t())\n        log_probs = F.log_softmax(inner_prod_mat, dim=1)\n        loss = -log_probs[:, j]\n        main_loss.append(loss)\n    return torch.stack(main_loss)\n", "CodeBleu_score": 33.86460466508267}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    N, M = dvecs.shape\n    loss_row = []\n    for j in range(N):\n        # exclude the current centroid\n        mask = torch.ones(cos_sim_matrix.shape).to(self.device)\n        mask[j, j] = 0\n        # compute the contrast loss\n        loss_row_j = torch.sum(-F.logsigmoid(cos_sim_matrix[j] * mask), dim=0)\n        loss_row.append(loss_row_j)\n    # stack the losses for each row into tensors\n    loss = torch.stack(loss_row)\n    return loss\n", "CodeBleu_score": 30.10655206866297}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss_rows = []\n    for i in range(self.num_embed):\n        excl_centroid_idx = [x for x in range(self.num_embed) if x != i]\n        loss_row = torch.cat([(1-torch.sigmoid(cos_sim_matrix[i][excl_centroid_idx])).unsqueeze(0)])\n        loss_rows.append(loss_row)\n    loss = torch.cat(loss_rows, dim=0)\n    return loss\n", "CodeBleu_score": 27.22438413122813}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss_rows = []\n    for i, dvec in enumerate(dvecs):\n        centroids_sigmoids = torch.sigmoid(cos_sim_matrix[i])\n        excl_centroids_sigmoids = torch.cat(\n            [centroids_sigmoids[:i], centroids_sigmoids[i + 1 :]], dim=0\n        )\n        loss_row = torch.log(\n            torch.sum(excl_centroids_sigmoids)\n        ) - torch.log(torch.sigmoid(cos_sim_matrix[i, i]))\n        loss_rows.append(loss_row)\n    return torch.stack(loss_rows)\n\n", "CodeBleu_score": 38.8415137741458}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n    (x, _label) = self.check_input(x, _label)\n    num_speakers = x.shape[0]\n    num_utts = x.shape[1]\n    if num_utts < 2:\n        raise RuntimeError(\n            f'Number of utterances per speaker must be at least 2, received {num_utts}')\n    centroids = torch.mean(x, dim=1)\n    centroids_expand = centroids.view(\n        num_speakers, 1, -1).expand(num_speakers, num_utts, -1)\n    cos_sim_matrix = F.cosine_similarity(x, centroids_expand, dim=2)\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    cos_sim_matrix = cos_sim_matrix * (1 - torch.eye(num_speakers))\n    loss = self.loss_fn(cos_sim_matrix, _label)\n    loss = loss.mean()\n    return loss", "CodeBleu_score": 50.36159732109904}, {"code": "def forward(self, x, _label=None)\n    if _label is None:\n        _label = torch.arange(x.size()[0])\n        _label = _label.repeat(x.size()[1])\n    else:\n        _label = _label.view(-1)\n    step_size = x.size()[1]\n\n    # get loss\n    centroids_batch = get_centroids(x)\n    cossim = get_cossim(x, centroids_batch)\n    sim_matrix = self.w * cossim.view(\n        -1, step_size, step_size) + self.b\n    loss = calc_loss(sim_matrix, _label)\n\n    return loss, cossim", "CodeBleu_score": 38.22085030690049}, {"code": "def forward(self, x, _label=None)\n    (num_speakers, num_utts_per_speaker, dvec_feats) = x.shape\n    num_utts_per_speaker_sqrt = np.sqrt(num_utts_per_speaker)\n    x = x.reshape((num_speakers * num_utts_per_speaker, dvec_feats))\n    centroids = self.get_centroids(x)\n    centroids = centroids.repeat(num_utts_per_speaker, axis=0)\n    cos_sim = F.linear(x, centroids)\n    cos_sim = cos_sim * num_utts_per_speaker_sqrt\n    label = np.repeat(np.arange(num_speakers), num_utts_per_speaker)\n    if (_label is None):\n        label = label.astype(np.int64)\n    else:\n        label = _label\n    loss = F.softmax_cross_entropy(cos_sim, label)\n    return loss.mean()\n\n", "CodeBleu_score": 37.220508448321986}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    extra_voice_dirs.append(\"./voices\")\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if subdir not in voices:\n                        voices[subdir] = []\n                    voices[subdir].append(os.path.join(subdir, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    if subdir not in voices:\n                        voices[subdir] = []\n                    voices[subdir].append(os.path.join(subdir, file))\n    return voices\n\n", "CodeBleu_score": 27.680849639003192}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith('.npz'):\n                    subdir_name = os.path.basename(subdir)\n                    if subdir_name not in voices:\n                        voices[subdir_name] = []\n                    voices[subdir_name].append(os.path.join(subdir, file))\n                elif file.endswith('.wav') or file.endswith('.mp3'):\n                    subdir_name = os.path.basename(subdir)\n                    if subdir_name not in voices:\n                        voices[subdir_name] = []\n                    voices[subdir_name].append(os.path.join(subdir, file))\n    return voices\n", "CodeBleu_score": 27.60241031817477}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith('.npz'):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n        for root, dirs, files in os.walk(extra_voice_dir):\n            if not voices.get(root):\n                voices[root] = []\n            for file in files:\n                if file.endswith('.wav') or file.endswith('.mp3'):\n                    voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 26.737070955518778}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voice_paths(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found at {voice_path}\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"mel\"], voice_data[\"pitches\"], voice_data[\"speaker_id\"]\n    else:\n        mel, pitches, speaker_id = model.generate_voice(voice_path)\n        np.savez(voice_path.replace(\".wav\", \".npz\"), mel=mel, pitches=pitches, speaker_id=speaker_id)\n        return mel, pitches, speaker_id\n\n", "CodeBleu_score": 46.268258130815695}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    if voice == \"random\":\n        return None, None, None\n\n    voice_paths = get_voice_paths(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise Exception(f\"Multiple voices found for {voice}: {voice_paths}\")\n\n    voice_path = voice_paths[0]\n\n    if not os.path.exists(voice_path):\n        raise Exception(f\"Voice {voice} not found for model {model}\")\n\n    if voice_path.endswith(\".npz\"):\n        return load_voice_npz(voice_path)\n    else:\n        return load_voice_audio(voice_path)\n\n", "CodeBleu_score": 40.94074425392672}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found at {voice_path}\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"mel\"], voice_data[\"pitches\"], voice_data[\"speaker\"]\n    if voice_path.endswith(\".npy\"):\n        voice_data = np.load(voice_path)\n        return voice_data, None, None\n    voice_npz_path = voice_path.replace(\".wav\", \".npz\")\n    if not os.path.exists(voice_npz_path):\n        mel, pitches, speaker = generate_voice(model, voice_path)\n        np.savez_compressed(voice_npz_path, mel=mel, pitches=pitches, speaker=speaker)\n    else:\n        voice_data = np.load(voice_npz_path)\n        mel, pitches, speaker = voice_data[\"mel\"], voice_data[\"pitches\"], voice_data[\"speaker\"]\n    return mel, pitches, speaker", "CodeBleu_score": 48.26204854453753}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, seq_len, num_heads, head_dim = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_len, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_len + 1, seq_len + 1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 46.63951300908694}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, seq_len, num_heads, head_dim = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, num_heads, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_len, num_heads * (head_dim + 1))\n    pos_score = padded_pos_score[:, :, head_dim:]\n    return pos_score\n\n", "CodeBleu_score": 41.32749245954937}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, num_heads, seq_len1, seq_len2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_len1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_len2 + 1, seq_len1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n", "CodeBleu_score": 48.695460860076935}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n    # Extract the context up to the specified length\n    contexts = [context[:length] for length in lens]\n    # Apply the convolutional layers\n    for layer in self.layers:\n        contexts = layer(contexts)\n    # Pad the processed contexts to create a consistent batch size\n    contexts = pad_sequence(contexts, batch_first=True)\n    return contexts\n\n", "CodeBleu_score": 15.041222152592976}, {"code": "def run_padded_sequence(self, context, lens)\n    max_len = max(lens)\n    contexts = []\n    for i in range(len(context)):\n        context_i = context[i][:lens[i]]\n        context_i = context_i.unsqueeze(0)\n        context_i = self.conv_dropout(self.relu(self.conv(context_i)))\n        context_i = context_i.squeeze(0)\n        contexts.append(context_i)\n    contexts = pad_sequence(contexts, batch_first=True)\n    return contexts", "CodeBleu_score": 23.1297206928214}, {"code": "def run_padded_sequence(self, context, lens)\n    contexts = []\n    for i in range(len(lens)):\n        context_i = context[i, :lens[i], :]\n        context_i = self.conv_layers(context_i.unsqueeze(0))\n        contexts.append(context_i)\n    contexts = torch.cat(contexts, dim=0)\n    return contexts\n\n", "CodeBleu_score": 23.527424930815656}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # sort the lengths and indices\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    # pack the context sequences\n    packed_context = torch.nn.utils.rnn.pack_sequence(context, enforce_sorted=False)\n    # apply the function\n    packed_output = fn(packed_context)\n    # unpack the sequences\n    output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    # map the results back to the original indices\n    unsorted_output = output[sorted_indices]\n    return unsorted_output", "CodeBleu_score": 13.372377445301561}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    lens, indices = lens.sort(descending=True)\n    packed_context = pack_sequence(context)\n    packed_result = fn(packed_context)\n    result = unpack_sequence(packed_result)\n    return [result[i] for i in indices]", "CodeBleu_score": 6.201691364658328}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    if lens is None:\n        lens = [len(c) for c in context]\n    sort_lens, sort_idxs, rev_idxs = self.sort_key_val(lens, rev=True)\n    sort_context = [context[i] for i in sort_idxs]\n    sort_context = self.pack_seqs(sort_context, rev_idxs)\n    sort_context = fn(sort_context)\n    sort_context = self.unpack_seqs(sort_context, rev_idxs)\n    return [sort_context[i] for i in rev_idxs]\n", "CodeBleu_score": 24.612023770503516}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.remove_weight_norm(self.convt_pre)\n    for l in self.conv_blocks:\n        l.remove_weight_norm()\n\n", "CodeBleu_score": 26.441091666306736}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.remove_weight_norm(self.convt_pre)\n    for l in self.conv_blocks:\n        nn.utils.remove_weight_norm(l)\n\n", "CodeBleu_score": 34.271430735519374}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    remove_weight_norm(self.convt_pre)\n    for block in self.conv_blocks:\n        block.remove_weight_norm()", "CodeBleu_score": 38.364630147612274}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_resnext\"):\n        return \"gluon_se_\" + mx_name\n    if mx_name.startswith(\"senet\"):\n        return \"gluon_senet_\" + mx_name[len(\"senet\") :]\n    if mx_name.startswith(\"inceptionv3\"):\n        return \"gluon_inception_v3\"\n    return \"gluon_\" + mx_name\n\n", "CodeBleu_score": 26.050603860304054}, {"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        return 'gluon_se_' + mx_name[3:]\n    if mx_name.startswith('senet_'):\n        return 'gluon_senet_' + mx_name[6:]\n    if mx_name.startswith('inceptionv3'):\n        return 'gluon_inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 27.784079470641586}, {"code": "def map_mx_to_torch_model(mx_name)\n    name = mx_name.lower()\n    name = name.replace(\"se_\", \"se\")\n    name = name.replace(\"senet_\", \"senet\")\n    name = name.replace(\"inceptionv3\", \"inception_v3\")\n    name = \"gluon_\" + name\n    return name\n\n", "CodeBleu_score": 31.819474232462554}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n    num_samples_per_worker = self._num_samples_per_replica\n    if self.training:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.replicas_per_worker))\n    if self.batch_size:\n        num_samples_per_worker = ((num_samples_per_worker + self.batch_size - 1) // self.batch_size) * self.batch_size\n    return num_samples_per_worker", "CodeBleu_score": 35.58015383077776}, {"code": "def _num_samples_per_worker(self)\n    if self.repeats:\n        return int(math.ceil(self.repeats * self.num_samples / self.num_workers))\n    elif self.total_samples:\n        return int(math.ceil(self.total_samples / self.num_workers))\n    else:\n        return int(math.ceil(self.num_samples / self.num_workers))\n\n", "CodeBleu_score": 19.477018401429742}, {"code": "def _num_samples_per_worker(self)\n    num_samples = self.repeats * self.total_samples\n    num_samples_per_worker = num_samples // self.num_replicas\n    if self.training:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size)\n    return num_samples_per_worker\n\n", "CodeBleu_score": 28.717489426857068}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\n    if types is None:\n        types = IMG_EXTENSIONS\n\n    labels = []\n    filenames = []\n\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets)\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 70.0152793364505}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if class_to_idx is None:\n        # build class_to_idx based on the directory structure\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return make_dataset(directory=folder, class_to_idx=class_to_idx, extensions=types, is_valid_file=None, leaf_name_only=leaf_name_only, sort=sort)\n\n", "CodeBleu_score": 17.273791538548455}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\n    if types is None:\n        types = ['jpg', 'jpeg', 'png']\n    else:\n        types = [t.lower() for t in types]\n    images = []\n    labels = []\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            if file.lower().endswith(tuple(types)):\n                file_path = os.path.join(root, file)\n                if leaf_name_only:\n                    label = os.path.basename(os.path.normpath(root))  # Use the leaf directory name as label\n                else:\n                    label = os.path.basename(root)  # Use the full directory path as label\n                images.append(file_path)\n                labels.append(label)\n\n    # Build class_to_idx mapping if not provided\n    if class_to_idx is None:\n        unique_labels = sorted(set(labels))\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(unique_labels)}\n\n    # Map labels to indices\n    target = [class_to_idx[label] for label in labels]\n\n    if sort:\n        # Sort the images and targets by the labels\n        sorted_indices = np.argsort(labels)\n        images = [images[i] for i in sorted_indices]\n        target = [target[i] for i in sorted_indices]\n\n    return images, target, class_to_idx\n\n", "CodeBleu_score": 43.59860378602609}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n    import torch\n    import random\n    import os\n    seed = int(os.getenv(\"PL_GLOBAL_SEED\", \"1234\"))\n    try:\n        if torch.utils.data.get_worker_info() is not None:\n            seed = int(torch.utils.data.get_worker_info().seed) % (2 ** 32 - 1)\n    except Exception:\n        seed = int(os.getenv(\"RANK\", \"1\"))\n    random.seed(seed)\n    return seed", "CodeBleu_score": 32.55976993923926}, {"code": "def pytorch_worker_seed()\n    import os\n    import random\n    import time\n    seed = int(os.environ.get(\"PYTORCH_DATALOADER_SEED\", 0))\n    if seed == 0:\n        try:\n            import torch\n            seed = torch.utils.data.get_worker_info().seed\n        except Exception:\n            seed = time.time() * 256\n    if seed is None:\n        seed = time.time() * 256\n    random.seed(seed)\n    return int(seed)", "CodeBleu_score": 30.74076511775521}, {"code": "def pytorch_worker_seed()\n    import os\n    import random\n    import torch\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return int(os.environ.get('WDS_RANK', '0')) + random.randint(0, 1000000)\n", "CodeBleu_score": 53.45298619502079}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lambda x: x + add if x < thresh else x)\n    return img\n\n", "CodeBleu_score": 17.325234420078097}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lambda x: x + add if x < thresh else x)\n    return img\n\n", "CodeBleu_score": 17.325234420078097}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lambda x: x + add if x < thresh else x)\n    else:\n        return img\n\n", "CodeBleu_score": 18.601931810494637}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names, probs = zip(*transforms.items())\n    probs = np.array(probs) / np.sum(probs)\n    return transform_names, probs\n\n", "CodeBleu_score": 37.303148423557154}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    transform_probs = np.array(list(transforms.values()))\n    transform_probs = transform_probs / transform_probs.sum()\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 22.969458327848137}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    transform_probs = [transforms[transform_name] for transform_name in transform_names]\n    transform_probs = np.array(transform_probs) / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 19.19904315022798}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        img_aug = img.copy()\n        for op in self.augmentations:\n            img_aug = op.apply(img_aug, m)\n        mixed += weight * img_aug\n    mixed = (1 - m) * img + m * mixed\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return mixed\n\n", "CodeBleu_score": 22.186683668899548}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed_img += weight * self._apply_sequence(img, m)\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    return cv2.addWeighted(img, 1 - self.mixed_prob, mixed_img, self.mixed_prob, 0)\n", "CodeBleu_score": 18.520790810376546}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self._apply_augment_sequence(img.copy(), m)\n    mixed = np.clip((1 - self.m) * img + mixed, 0, 255).astype(np.uint8)\n    return mixed\n", "CodeBleu_score": 18.382713442371884}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = (sum(x) / n,) * n\n        print(\"Warning: x is not a tuple or list of length n. Adapted to match n.\")\n    else:\n        assert len(x) == n, \"x is not a tuple or list of length n.\"\n    return x", "CodeBleu_score": 41.987024756379235}, {"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        logging.warning(\n            f\"The length of x is {len(x)}, but it should be {n}. Repeating the mean of x.\"\n        )\n        x = (sum(x) / len(x),) * n\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 43.12565245043188}, {"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        warnings.warn(\n            f\"Expected {n} channels, but got {len(x)} channels. \"\n            f\"Repeating the mean of the channels {n} times.\")\n        x = tuple(mean(x) for _ in range(n))\n    else:\n        assert len(x) == n\n    return x", "CodeBleu_score": 46.64735681657332}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            # Use modulus to generate different seeds for data loader workers\n            numpy.random.seed(worker_info.seed % (2 ** 32 - 1))\n        else:\n            raise ValueError(f\"Expected worker_seeding to be 'all' or a callable function, but got {worker_seeding}\")\n    elif callable(worker_seeding):\n        # Use worker_seeding for worker-specific seeding\n        worker_seeding(worker_info.seed)\n    else:\n        raise ValueError(f\"Expected worker_seeding to be 'all' or a callable function, but got {worker_seeding}\")\n\n", "CodeBleu_score": 40.3769366701764}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    global worker_info\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding != 'all':\n            return\n        seed_all()\n        return\n    if not callable(worker_seeding):\n        return\n    worker_seeding(worker_info.seed % (2 ** 32 - 1))\n\n", "CodeBleu_score": 35.16002362714553}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    global worker_info\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            np.random.seed(seed)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            return\n        raise ValueError(f'Invalid worker_seeding option: {worker_seeding}')\n    if isinstance(worker_seeding, Callable):\n        seed = worker_info.seed % (2 ** 32 - 1)\n        worker_seeding(seed)\n        return\n    raise TypeError(f'Invalid worker_seeding type: {type(worker_seeding)}')\n\n", "CodeBleu_score": 53.04397549608421}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)), tf.int32\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 58.188209703000204}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 59.10215344895222}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 59.10215344895222}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image.set_shape((image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 31.09446553684803}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if image_bytes.shape[-1] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.dtype != tf.uint8:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[0] != 2:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[1] != 1:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[2] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[3] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[4] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[5] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[6] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[7] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[8] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[9] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[10] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[11] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[12] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[13] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[14] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[15] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[16] != 3:\n        raise ValueError('Invalid image.')\n    if image_bytes.shape[17] != 3:\n        raise ValueError('Invalid image.')", "CodeBleu_score": 12.363652938435841}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if image_bytes.shape[-1] != 3:\n        raise ValueError('Invalid image.')\n    image = tf.io.decode_jpeg(image_bytes, channels=3, fancy_upscaling=False,\n                             dct_method='INTEGER_FAST')\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=interpolation)\n    image = tf.image.random_crop(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n    dtype = tf.bfloat16 if use_bfloat16 else tf.float32\n    image = tf.cast(image, dtype)\n    return image\n\n", "CodeBleu_score": 45.26361741016757}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    image = _central_crop(image, image_size, image_size)\n    image.set_shape([image_size, image_size, 3])\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.60596368040571}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    image = tf.image.central_crop(image, 0.875)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 38.579342987164914}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    with tf.io.gfile.GFile(image_bytes, 'rb') as f:\n        img = Image.open(f)\n        img = img.convert('RGB')\n    crop_size = tuple(image_size)\n    img = _normalize_image(img)\n    img = _center_crop(img, crop_size)\n    img = _resize_and_convert_image(img, image_size, interpolation, image_bytes)\n    if use_bfloat16:\n        img = img.astype(_BFLOAT16_DTYPE.as_numpy_dtype)\n    else:\n        img = img.astype(_FLOAT32_DTYPE.as_numpy_dtype)\n    return img\n", "CodeBleu_score": 36.62010173411772}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"Eval dataloader is not provided and no default dataloader is available.\")\n    self.model.eval()\n    self.model.to(self.device)\n    all_preds = []\n    all_labels = []\n    all_losses = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = self.model(**batch)\n        logits = outputs.logits\n        loss = outputs.loss\n        all_losses.append(loss.detach().cpu().item())\n        preds = torch.argmax(logits, dim=-1)\n        all_preds.extend(preds.detach().cpu().tolist())\n        all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n    eval_loss = sum(all_losses) / len(all_losses)\n    eval_acc = accuracy_score(all_labels, all_preds)\n    eval_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    eval_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n    eval_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n    eval_results = {\n        \"loss\": eval_loss,\n        \"accuracy\": eval_acc,\n        \"f1\": eval_f1,\n        \"precision\": eval_precision,\n        \"recall\": eval_recall,\n    }\n    return eval_results\n\n", "CodeBleu_score": 44.22697892985141}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Create a new file for the evaluation results\n    with open(f\"{output_file}.txt\", \"w\") as f:\n        pass\n\n    # Initialize the evaluation metrics\n    metrics = {\n        \"bleu\": 0,\n        \"rouge\": 0,\n        \"bertscore\": 0,\n        \"meteor\": 0,\n        \"chrf\": 0,\n        \"bertscore_precision\": 0,\n        \"bertscore_recall\": 0,\n        \"bertscore_f1\": 0,\n    }\n\n    # Initialize the number of samples and the total loss\n    num_samples = 0\n    total_loss = 0\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Loop over the evaluation dataset\n    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n        # Get the inputs and labels from the batch\n        input_ids = batch[\"input_ids\"].to(self.device)\n        attention_mask = batch[\"attention_mask\"].to(self.device)\n        labels = batch[\"labels\"].to(self.device)\n\n        # Generate a summary using the model\n        summary = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=self.max_length,\n            num_beams=self.num_beams,\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            repetition_penalty=self.repetition_penalty,\n            do_sample=self.do_sample,\n            num_return_sequences=self.num_return_sequences,\n        )\n\n        # Decode the summary and the labels\n        summary = self.tokenizer.batch_decode(summary, skip_special_tokens=True)\n        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True", "CodeBleu_score": 40.193413464949565}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    model.eval()\n\n    total_loss = 0\n\n    predictions = None\n    labels = None\n\n    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n        batch = tuple(t.to(self.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            outputs = model(**inputs)\n\n        loss = outputs[0]\n        logits = outputs[1]\n        total_loss += loss.item()\n\n        if predictions is None:\n            predictions = logits.detach().cpu().numpy()\n            labels = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            predictions = np.append(predictions, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(\n                labels, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n\n    eval_loss = total_loss / len(eval_dataloader)\n\n    predictions = np.argmax(predictions, axis=1)\n\n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='macro')\n    precision = precision_score(labels, predictions, average='macro')\n    recall = recall_score(labels, predictions, average='macro')\n\n    results = {\n        \"eval_loss\": eval_loss,\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall\n    }\n\n    return results\n\n", "CodeBleu_score": 40.70751410991335}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(f\"Rank-1: {rank1:.4f}\")\n    print(f\"Rank-5: {rank5:.4f}\")\n    print(f\"mIoU: {miou:.4f}\")", "CodeBleu_score": 32.76446957899689}, {"code": " def print_computed_metrics(self, metrics)    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print(f\"Rank-1: {rank1:.4f}\")\n    print(f\"Rank-5: {rank5:.4f}\")\n    print(f\"mIoU: {miou:.4f}\")", "CodeBleu_score": 23.56720190995324}, {"code": " def print_computed_metrics(self, metrics)    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(f\"Rank-1: {rank1:.4f}\")\n    print(f\"Rank-5: {rank5:.4f}\")\n    print(f\"mIoU: {miou:.4f}\")", "CodeBleu_score": 32.76446957899689}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n    ious = []\n    ranks = []\n    for segment, datum in zip(segments, data):\n        iou = self.calculate_iou(segment, datum)\n        rank = self.calculate_rank(segment, datum)\n        ious.append(iou)\n        ranks.append(rank)\n    ious = np.array(ious)\n    ranks = np.array(ranks)\n    rank1 = np.mean(ranks == 1)\n    rank5 = np.mean(ranks <= 5)\n    mean_iou = np.mean(ious)\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 26.32588219014772}, {"code": "def _eval_predictions(self, segments, data)\n    # Initialize variables to store rank1, rank5, and mean IoU\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n\n    # Loop over all segment-data pairs\n    for segment, datum in zip(segments, data):\n        # Get the ground truth data\n        gt_data = datum['gt']\n        # Get the predicted data\n        pred_data = datum['pred']\n        # Get the image size\n        size = datum['size']\n\n        # Initialize variables to store the best IoU and rank\n        best_iou = -float('inf')\n        best_rank = float('inf')\n\n        # Loop over all predicted data\n        for rank, pred in enumerate(pred_data):\n            # Compute the IoU between the segment and the predicted data\n            iou = self._compute_iou(segment, pred, size)\n            # If the IoU is better than the current best, update the best IoU and rank\n            if iou > best_iou:\n                best_iou = iou\n                best_rank = rank\n\n        # If the best rank is 1, increment rank1\n        if best_rank == 1:\n            rank1 += 1\n        # If the best rank is 5, increment rank5\n        elif best_rank == 5:\n            rank5 += 1\n\n        # Add the best IoU to the mean IoU\n        mean_iou += best_iou\n\n    # Compute the percentage of segments with rank 1 and rank 5\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    # Compute the mean IoU\n    mean_iou /= len(segments)\n\n    # Return the rank1, rank5, and mean IoU\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 27.932865410879376}, {"code": "def _eval_predictions(self, segments, data)\n    # Initialize variables to store the sum of ranks and IoUs for the top predictions\n    sum_rank1 = 0\n    sum_rank5 = 0\n    sum_iou = 0\n\n    # Iterate over each segment in the list of segments\n    for segment in segments:\n        # Initialize variables to store the maximum IoU and rank for the current segment\n        max_iou = 0\n        max_rank = 0\n\n        # Iterate over each data point in the list of data\n        for i, d in enumerate(data):\n            # Calculate the IoU between the current segment and the current data point\n            iou = self._intersection_over_union(segment, d)\n\n            # If the IoU is greater than the current maximum IoU, update the maximum IoU and rank\n            if iou > max_iou:\n                max_iou = iou\n                max_rank = i\n\n        # Add the rank and IoU of the top prediction to the sum of ranks and IoUs\n        sum_rank1 += int(max_rank == 0)\n        sum_rank5 += int(max_rank < 5)\n        sum_iou += max_iou\n\n    # Calculate the percentage of segments with rank 1 and rank 5, and the mean IoU across all segments\n    rank1 = sum_rank1 / len(segments)\n    rank5 = sum_rank5 / len(segments)\n    iou = sum_iou / len(segments)\n\n    # Return the rank1, rank5, and mean IoU\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 26.19394898735548}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    self.logger.info(\"***** Running Prediction *****\")\n    self.logger.info(\"  Num examples = %d\", len(eval_dataloader.dataset))\n    self.logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n    preds = []\n    model.eval()\n    model.to(self.args.device)\n    for inputs in tqdm(eval_dataloader):\n        inputs = self._prepare_inputs(inputs)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]\n            outputs = outputs.detach().cpu().numpy()\n            preds.extend(outputs)\n    results = self.post_processing_function(eval_dataloader.dataset, preds, output_file)\n    return results\n\n", "CodeBleu_score": 41.07016793184166}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n                \"labels\": batch[3],\n            }\n            outputs = model(**inputs)\n\n        tmp_eval_loss, logits = outputs[:2]\n        if self.n_gpu > 1:\n            logits = logits.mean()  # mean() to average on multi-gpu parallel training\n        if self.args.output_mode == \"classification\":\n            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n        elif self.args.output_mode == \"regression\":\n            preds = logits.detach().cpu().numpy()\n        elif self.args.output_mode == \"multilabel_classification\":\n            preds = (logits.sigmoid().detach().cpu().numpy() > self.args.threshold).astype(\n                np.int32\n            )\n        labels = inputs[\"labels\"].detach().cpu().numpy()\n        scores.append(\n            {\n                \"pred\": preds,\n                \"label\": labels,\n                \"logit\": logits.detach().cpu().numpy(),\n            }\n        )\n    results = self.prepare_results(scores, output_file)\n    return results", "CodeBleu_score": 28.674587451814094}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    self.logger.info(\"Predicting\")\n    self.logger.info(\"***** Running Prediction *****\")\n    self.logger.info(\"  Num examples = %d\", len(eval_dataloader.dataset))\n    self.logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n    preds = []\n    model.eval()\n    model = model.to(self.args.device)\n    for inputs in tqdm(eval_dataloader):\n        inputs = self._prepare_inputs(inputs)\n        with torch.no_grad():\n            inputs = {k: v.to(self.args.device) for k, v in inputs.items()}\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            loss = logits.mean().item()\n            preds.append(loss)\n    if output_file is not None:\n        with open(output_file, \"w\") as writer:\n            logger.info(\"***** Prediction results *****\")\n            for pred in preds:\n                writer.write(str(pred) + \"\\n\")\n    return preds", "CodeBleu_score": 41.65321042184454}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    full_scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return torch.dot(full_scores[0], full_scores[1])\n\n", "CodeBleu_score": 33.1395665394932}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    scores = [scores[0], scores[1]]\n    scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return scores.dot(self.text_embeddings.weight.t())\n\n", "CodeBleu_score": 38.02294148499993}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    full_scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return torch.sum(full_scores[:, 0] * full_scores[:, 1])\n\n", "CodeBleu_score": 31.30364355450515}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    Y_pred = np.argmax(Y_pred, axis=1)\n    Y_true = np.argmax(Y_true, axis=1)\n    errors = Y_pred != Y_true\n    if np.any(errors):\n        print(\"Prediction errors:\")\n        print(Y_pred[errors])\n        print(Y_true[errors])\n    result = {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(result, f)\n    return result", "CodeBleu_score": 35.064067730346736}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    # Concatenate predictions and true labels\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n\n    # Check for prediction errors\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        print(f\"Prediction errors: {len(error_indices)}\")\n        print(f\"Samples of errors: {error_indices}\")\n\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    # Return predictions and true labels\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}", "CodeBleu_score": 29.88068345770469}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError(\n            \"Predictions and true labels have different shapes: {} and {}\".format(Y_pred.shape, Y_true.shape)\n        )\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump((Y_pred, Y_true), f)\n    return {\"pred\": Y_pred, \"true\": Y_true}", "CodeBleu_score": 29.634985179646822}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n\n", "CodeBleu_score": 57.00304501759299}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([torch.zeros_like(cmasks), torch.zeros_like(vmasks)], dim=-1)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids", "CodeBleu_score": 27.266055029793897}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([torch.zeros_like(cmasks), torch.zeros_like(vmasks)], dim=-1)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 27.266055029793897}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=-1)\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 26.483934797735564}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # Combine input_ids and input_video_embeds\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    # Adjust attention_mask and token_type_ids to match the new sequence length\n    attention_mask = torch.cat([attention_mask, torch.ones_like(input_video_embeds)], dim=1)\n    token_type_ids = torch.cat([token_type_ids, torch.zeros_like(input_video_embeds)], dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n", "CodeBleu_score": 37.72514533313931}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # Combine input_ids and input_video_embeds\n    combined_input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    # Adjust sequence length\n    combined_attention_mask = torch.cat([attention_mask, torch.ones_like(input_video_embeds)], dim=1)\n    combined_token_type_ids = torch.cat([token_type_ids, torch.zeros_like(input_video_embeds)], dim=1)\n\n    # Update input_ids, input_video_embeds, attention_mask, and token_type_ids\n    model_inputs = {\n        \"input_ids\": combined_input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": combined_attention_mask,\n        \"token_type_ids\": combined_token_type_ids,\n    }\n\n    return model_inputs", "CodeBleu_score": 36.76770649770927}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    # combine text and video inputs\n    combined_input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    # adjust the sequence length to match the combined length\n    combined_input_ids = combined_input_ids[:, :self.config.max_length]\n    attention_mask = attention_mask[:, :self.config.max_length]\n    token_type_ids = token_type_ids[:, :self.config.max_length]\n\n    # update the input_ids, input_video_embeds, attention_mask, and token_type_ids\n    model_kwargs[\"input_ids\"] = combined_input_ids\n    model_kwargs[\"input_video_embeds\"] = input_video_embeds\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n\n    return model_kwargs\n", "CodeBleu_score": 23.17734517250723}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    cfg.data.test.pipeline[1].meta_keys.remove('img_id')\n    idx = 0\n    for i, p in enumerate(cfg.data.test.pipeline):\n        if p['type'] == 'LoadImageFromFile':\n            idx = i\n            break\n    else:\n        raise Exception('Cannot find LoadImageFromFile pipeline')\n    cfg.data.test.pipeline[idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.pipeline)\n", "CodeBleu_score": 25.236427639235202}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    cfg.data.test.pipeline[1].meta_keys.remove('img_id')\n    for i, pipeline in enumerate(cfg.data.test.pipeline):\n        if pipeline['type'] == 'LoadImageFromFile':\n            transform_idx = i\n            break\n    else:\n        raise RuntimeError('Cannot find `LoadImageFromFile` in pipeline')\n    cfg.data.test.pipeline[transform_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 27.01799250371284}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    cfg = cfg.deepcopy()\n    if 'meta_keys' in cfg.data.test:\n        cfg.data.test.meta_keys = [\n            k for k in cfg.data.test.meta_keys if k != 'img_id']\n    if 'img_id' in cfg.data.test.meta_keys:\n        cfg.data.test.meta_keys.remove('img_id')\n    for transform in cfg.data.test.pipeline:\n        if transform['type'] == 'LoadImageFromFile':\n            transform.type = 'mmdet.InferencerLoader'\n            break\n    else:\n        raise ValueError('Cannot find `LoadImageFromFile` in the test pipeline.')\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 32.13839138953708}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.238418972850305}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif'))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 44.48693016562773}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 44.61752859785201}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk_data = []\n    for idx, input in enumerate(inputs):\n        if isinstance(input, dict):\n            if \"img\" in input:\n                data = self.pipeline(input[\"img\"])\n            elif \"img_path\" in input:\n                data = self.pipeline(input[\"img_path\"])\n            else:\n                raise ValueError(\"Invalid input format. Input must contain 'img' or 'img_path' key.\")\n        else:\n            data = self.pipeline(input)\n        chunk_data.append(data)\n        if (idx + 1) % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data", "CodeBleu_score": 38.283418159721336}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk_data = []\n    for i, input in enumerate(inputs):\n        if isinstance(input, dict):\n            if 'img' in input:\n                chunk_data.append(self.pipeline(input['img']))\n            elif 'img_path' in input:\n                chunk_data.append(self.pipeline(input['img_path']))\n        else:\n            chunk_data.append(self.pipeline(input))\n        if (i + 1) % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data", "CodeBleu_score": 36.60379978289476}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if \"img\" in input:\n                input = self.pipeline(input[\"img\"])\n            elif \"img_path\" in input:\n                input = self.pipeline(input[\"img_path\"])\n        chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 35.35022699945541}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if visualization is None:\n        visualization = []\n    if not no_save_pred:\n        self.results_dir = pred_out_dir\n        self.print_log(f'Saving prediction results to {pred_out_dir}')\n        os.makedirs(pred_out_dir, exist_ok=True)\n    else:\n        self.results_dir = None\n    result = dict(preds=preds)\n    if return_datasamples:\n        if pred_out_dir:\n            self.print_log(\n                'Saving datasamples is not supported when return_datasamples is True.',\n                logger='current',\n                level=logging.WARNING)\n        result['datasamples'] = self.data_preprocessor.instantiate_data_samples(\n            preds)\n    if len(visualization) > 0:\n        result['visualization'] = visualization\n    if print_result:\n        self.print_log(result)\n    return result\n\n", "CodeBleu_score": 43.15326859734606}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if self.test_mode:\n        assert 'do_evaluation' not in kwargs, (\n            'do_evaluation has been deprecated, please use evaluation=\"default\" '\n            'in your test config.')\n        if 'evaluation' not in kwargs:\n            kwargs['evaluation'] = 'default'\n\n    if visualization is None:\n        visualization = []\n\n    result = {}\n    result['preds'] = preds\n    result['visualization'] = visualization\n\n    if not self.test_mode:\n        assert 'do_evaluation' in kwargs, (\n            'do_evaluation is not found in kwargs, this may be caused '\n            'by passing wrong inputs to EvalHook, i.e., with '\n            'predictions=xxx, labels=xxx in metrics_kwargs.')\n        do_evaluation = kwargs.pop('do_evaluation')\n        if do_evaluation:\n            eval_res = self.evaluate(preds, **kwargs)\n            log_msg = []\n            for name, val in eval_res.items():\n                log_msg.append(f'\\n{name}: {val:.04f}')\n                result[name] = val\n            result['eval_results'] = eval_res\n            if print_result:\n                print(''.join(log_msg))\n    else:\n        if pred_out_dir and not no_save_pred:\n            print_log(\n                f'write results to {pred_out_dir}', logger='mmdet')\n            mmengine.dump(result, pred_out_dir)\n\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn('return_datasamples does not support '\n                          'prediction output directory, '\n                          'ignoring it.')\n        return self.data_preprocessor.reverse(preds)\n    else:\n        return result\n\n", "CodeBleu_score": 39.563111611055135}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if no_save_pred:\n        pred_out_dir = ''\n    if pred_out_dir:\n        os.makedirs(pred_out_dir, exist_ok=True)\n    results = []\n    if not return_datasamples:\n        preds = self.postprocess_predictions(preds)\n        for pred in preds:\n            results.append(pred)\n    if pred_out_dir:\n        if return_datasamples:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.')\n        else:\n            for idx, pred in enumerate(preds):\n                pred.dump(os.path.join(pred_out_dir, f'pred_{idx}.pkl'))\n    if visualization is not None:\n        results.append(visualization)\n    if print_result:\n        print(results)\n    return results", "CodeBleu_score": 49.30004346594923}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0] = dict(type=\"Resize\", img_scale=(1333, 800), keep_ratio=True)\n    test_pipeline[1] = dict(type=\"RandomFlip\", flip_ratio=0.0)\n    test_pipeline.append(dict(type=\"DefaultFormatBundle\"))\n    test_pipeline.append(dict(type=\"Collect\", keys=[\"img\"]))\n    return test_pipeline\n\n", "CodeBleu_score": 19.81473067323154}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = cfg.data.test.pipeline.copy()\n    test_pipeline[0].transforms[0].type = 'Resize'\n    test_pipeline[0].transforms[0].keep_ratio = True\n    test_pipeline[0].transforms[0].scale = (256, 256)\n    test_pipeline[0].transforms[0].backend = 'pillow'\n    test_pipeline[0].transforms[0].interpolation = 'bicubic'\n    test_pipeline[-1].type = 'Collect'\n    test_pipeline[-1].keys = ['img']\n    test_pipeline[-1].meta_keys = ['filename', 'ori_filename', 'ori_shape', 'img_shape', 'pad_shape', 'scale_factor', 'flip', 'flip_direction', 'img_norm_cfg', 'border']\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 19.351669530474858}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0] = cfg.data.test.pipeline[0].replace(type='Resize')\n    test_pipeline[-1] = cfg.data.test.pipeline[-1]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n", "CodeBleu_score": 21.979021324150466}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    assert model.training is False\n    assert len(img.shape) == 3\n    assert img.shape[2] == 3\n    assert isinstance(frame_id, int)\n    assert isinstance(video_len, int)\n\n    cfg = model.cfg\n\n    device = next(model.parameters()).device  # model device\n\n    # construct a data sample\n    data = {\n        'img': img,\n        'frame_id': frame_id,\n        'imgs_shape': (img.shape[0], img.shape[1]),\n        'ori_shape': (img.shape[0], img.shape[1]),\n        'img_id': frame_id,\n        'video_len': video_len\n    }\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # this is a workaround to avoid the bug of MMDataParallel\n        data['img_metas'] = data['img_metas'][0].data\n        data['img'] = data['img'][0].data\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result", "CodeBleu_score": 40.610113635783804}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_id=0, img_shape=img.shape,\n                ori_shape=img.shape, img_path=None,\n                video_len=video_len)\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if device.type == 'cpu':\n        # prevent unsupported modules\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 53.22784622641957}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_shape=img.shape,\n                img_id=0, ori_shape=img.shape, img_id=0,\n                video_len=video_len)\n\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 53.6891467507791}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            return tokens_a, tokens_b\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 70.19363570061576}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            return tokens_a, tokens_b\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if len(trunc_tokens) == 1:\n            return [], []\n        trunc_tokens = trunc_tokens[int(rng.integers(0, len(trunc_tokens) - 1)) :]", "CodeBleu_score": 54.982469236640306}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == '':\n        return None\n    if activation_string == 'relu':\n        return tf.keras.layers.ReLU()\n    if activation_string == 'softmax':\n        return tf.keras.layers.Softmax()\n    if activation_string == 'sigmoid':\n        return tf.keras.layers.Activation('sigmoid')\n    if activation_string == 'tanh':\n        return tf.keras.layers.Activation('tanh')\n    if activation_string == 'linear':\n        return tf.keras.layers.Activation('linear')\n    raise ValueError(f\"Unknown activation: {activation_string}\")\n\n", "CodeBleu_score": 35.476428046677675}, {"code": "def get_activation(activation_string)\n    if isinstance(activation_string, str):\n        if activation_string == \"relu\":\n            return tf.nn.relu\n        elif activation_string == \"tanh\":\n            return tf.nn.tanh\n        elif activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        elif activation_string == \"softmax\":\n            return tf.nn.softmax\n        elif activation_string == \"linear\":\n            return None\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation_string}\")\n    else:\n        return activation_string\n", "CodeBleu_score": 30.68670832566941}, {"code": "def get_activation(activation_string)\n    if activation_string is None:\n        return None\n    if isinstance(activation_string, str):\n        if activation_string == \"relu\":\n            return tf.keras.activations.relu\n        elif activation_string == \"sigmoid\":\n            return tf.keras.activations.sigmoid\n        elif activation_string == \"tanh\":\n            return tf.keras.activations.tanh\n        elif activation_string == \"softmax\":\n            return tf.keras.activations.softmax\n        elif activation_string == \"linear\":\n            return tf.keras.activations.linear\n        elif activation_string == \"swish\":\n            return tf.keras.activations.swish\n        elif activation_string == \"gelu\":\n            return tf.keras.activations.gelu\n        elif activation_string == \"selu\":\n            return tf.keras.activations.selu\n        elif activation_string == \"elu\":\n            return tf.keras.activations.elu\n        else:\n            raise ValueError(f\"Unknown activation function: {activation_string}\")\n    else:\n        return activation_string\n\n", "CodeBleu_score": 24.99397655016209}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n", "CodeBleu_score": 53.03103381915149}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.size()\n    batch_size, from_seq_length = from_shape\n    to_shape = to_mask.size()\n    to_seq_length = to_shape[1]\n    to_mask = torch.reshape(to_mask, [batch_size, 1, to_seq_length])\n    to_mask = to_mask.float()\n    broadcast_ones = torch.ones(shape=[batch_size, from_seq_length, 1], dtype=torch.float32)\n    mask = broadcast_ones * to_mask\n    return mask", "CodeBleu_score": 34.95485800709653}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.size()\n    batch_size, from_seq_length = from_shape\n    to_shape = to_mask.size()\n    to_seq_length = to_shape[1]\n    to_mask = torch.reshape(to_mask, [batch_size, 1, to_seq_length])\n    to_mask = to_mask.float()\n    broadcast_ones = torch.ones(shape=[batch_size, from_seq_length, 1], dtype=torch.float)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 34.73981985293416}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.size()\n    batch_size, from_seq_length = from_shape\n    to_shape = to_mask.size()\n    to_seq_length = to_shape[1]\n    to_mask = torch.reshape(to_mask, [batch_size, 1, to_seq_length])\n    to_mask = to_mask.to(torch.float32)\n    broadcast_ones = torch.ones(\n        size=[batch_size, from_seq_length, 1], dtype=torch.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 35.562547002676276}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = input_tensor.view(batch_size, seq_length,\n                                     num_attention_heads, width).transpose(1, 2)\n    return output_tensor\n\n", "CodeBleu_score": 26.985449289921338}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = input_tensor.view(batch_size, seq_length,\n                                     num_attention_heads, width).permute(0, 2, 1, 3)\n    return output_tensor\n\n", "CodeBleu_score": 28.511048981254643}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = input_tensor.reshape(\n        batch_size, seq_length, num_attention_heads, width)\n    output_tensor = output_tensor.transpose([0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 39.97595862562899}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return input_tensor.reshape([-1, width])", "CodeBleu_score": 45.6117142698909}, {"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\n            \"Input tensor must have at least 2 dimensions, but got shape {}\".format(input_tensor.shape))\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor.shape[-1]\n        return tf.reshape(input_tensor, [-1, width])", "CodeBleu_score": 46.950662915677675}, {"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor.shape[-1]\n        return input_tensor.reshape([-1, width])", "CodeBleu_score": 45.157895448039056}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 5.998047197537955}, {"code": "def get_train_examples(self, data_dir)\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 5.998047197537955}, {"code": "def get_train_examples(self, data_dir)\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 5.998047197537955}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.logical_and(is_real_example, predictions == label_ids), tf.float32)\n    )\n    loss = tf.reduce_mean(per_example_loss)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 31.342494100735465}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.logical_and(tf.equal(predictions, label_ids), is_real_example), tf.float32)\n    )\n    loss = tf.reduce_mean(tf.cast(per_example_loss, tf.float32) * tf.cast(is_real_example, tf.float32))\n    return {\"accuracy\": accuracy, \"loss\": loss}\n\n", "CodeBleu_score": 33.28572755975691}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.metrics.accuracy(\n        label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 41.84367264182095}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        features.append(\n            convert_single_example(ex_index, example, label_list,\n                                   max_seq_length, tokenizer))\n    return features\n\n", "CodeBleu_score": 65.31315978838998}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        features.append(convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer))\n    return features\n", "CodeBleu_score": 62.55827638638258}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.75900525694976}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    # Get the output weights and bias\n    output_weights = bert_config.output_weights\n    output_bias = bert_config.output_bias\n\n    # Compute logits\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Reshape labels and convert to one-hot encoding\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Compute per-example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * logits, axis=-1)\n\n    # Compute mean loss\n    loss = tf.reduce_mean(per_example_loss)\n\n    # Compute log probabilities\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 37.14600420541592}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, 2],\n        initializer=tf.truncated_normal_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return loss, per_example_loss, log_probs\n", "CodeBleu_score": 47.24262518580641}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, 2],\n        initializer=tf.truncated_normal_initializer(stddev=bert_config.initializer_range))\n\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n", "CodeBleu_score": 50.905329207502994}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_shape = sequence_tensor.size()\n    sequence_flat_offsets = torch.arange(0, sequence_shape[0] * sequence_shape[1], sequence_shape[1])\n    positions_flat = positions.view(-1) + sequence_flat_offsets.type_as(positions)\n    flattened_sequence_tensor = sequence_tensor.view(-1, sequence_shape[-1])\n    output_tensor = torch.index_select(flattened_sequence_tensor, 0, positions_flat)\n    return output_tensor\n", "CodeBleu_score": 28.871852367661567}, {"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_shape = tf.shape(sequence_tensor)\n    batch_size = sequence_shape[0]\n    sequence_length = sequence_shape[1]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * sequence_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * sequence_length, -1])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 41.14179977422843}, {"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_shape = tf.shape(sequence_tensor)\n    batch_size = sequence_shape[0]\n    sequence_length = sequence_shape[1]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * sequence_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * sequence_length, -1])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 41.14179977422843}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 81.12883787321546}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 81.12883787321546}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_text = \" \".join(doc_tokens)\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = tok_text[new_start:(new_end + 1)]\n            if text_span == \"\":\n                continue\n            tokenized_text = tokenizer.tokenize(text_span)\n            tokenized_answer_text = tokenizer.tokenize(orig_answer_text)\n            if len(tokenized_text) != len(tokenized_answer_text):\n                continue\n            if tokenized_text == tokenized_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 57.913097908862134}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.62181028303381}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.62181028303381}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.62181028303381}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n    tokens = self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens)\n    _tokens = []\n    for token in tokens:\n        _tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return _tokens\n\n", "CodeBleu_score": 42.12866935446645}, {"code": "def tokenize(self, text)\n    tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            tokens.append(sub_token)\n    return tokens\n\n", "CodeBleu_score": 80.44897730916172}, {"code": "def tokenize(self, text)\n    tokens = self.basic_tokenizer.tokenize(text)\n    for token in tokens:\n        token = self.wordpiece_tokenizer.tokenize(token)\n        tokens.extend(token)\n    return tokens", "CodeBleu_score": 27.99783883968912}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio, sr = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        if audio.shape[0] < duration * sr:\n            audio = np.pad(audio, (0, int(duration * sr) - audio.shape[0]), mode=\"constant\")\n        return audio, False\n    except Exception as e:\n        logging.error(f\"Error loading audio file {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 34.020133158825224}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        import librosa\n\n        audio, _ = librosa.load(\n            path,\n            sr=sample_rate,\n            mono=True,\n            res_type=\"kaiser_fast\",\n            offset=offset,\n            duration=duration,\n            dtype=dtype,\n        )\n        return audio, False\n    except Exception as e:\n        logging.error(f\"Error loading audio file {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 37.174066257353886}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    import librosa\n    import logging\n\n    logging.info(f\"Loading audio data from {path}...\")\n    try:\n        audio_data, _ = librosa.load(\n            path,\n            offset=offset,\n            duration=duration,\n            sr=sample_rate,\n            dtype=dtype,\n        )\n        logging.info(f\"Audio data loaded successfully from {path}.\")\n        return audio_data, False\n    except Exception as e:\n        logging.error(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 40.62894178658711}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        n_tiles = n_channels // waveform.shape[-1] + 1\n        return tf.tile(waveform, [1, n_tiles])[..., :n_channels]\n\n", "CodeBleu_score": 49.97553708789834}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[-1] + 1])[..., :n_channels]\n\n", "CodeBleu_score": 38.10655171826537}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape.ndims == 2:\n        waveform = tf.expand_dims(waveform, axis=-1)\n    waveform = tf.repeat(waveform, n_channels, axis=-1)\n    waveform = waveform[:, :n_channels]\n    return waveform\n\n", "CodeBleu_score": 43.7996530003247}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    # Convert the spectrogram to dB\n    spectrogram_db = tf.math.log(spectrogram + 1e-6) * 20 / math.log(10)\n\n    # Find the maximum dB value\n    max_db = tf.reduce_max(spectrogram_db)\n\n    # Clip the dB spectrogram to a specified range below the maximum\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, max_db - db_range, max_db\n    )\n\n    # Convert the clipped dB spectrogram from float32 to uint8 format\n    spectrogram_db_uint8 = tf.cast(\n        (spectrogram_db_clipped - max_db + db_range)\n        * 255\n        / db_range,\n        tf.uint8,\n    )\n\n    return spectrogram_db_uint8\n\n", "CodeBleu_score": 39.42986267917517}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, max_db - db_range, max_db\n    )\n    return tf.cast(\n        (spectrogram_db_clipped + db_range) * 255.0 / db_range, tf.uint8\n    )\n\n", "CodeBleu_score": 44.21853797346764}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    # Convert spectrogram to dB\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n\n    # Find the maximum dB value\n    max_db = tf.reduce_max(spectrogram_db)\n\n    # Clip the dB spectrogram to a specified range below the maximum\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, max_db - db_range, max_db\n    )\n\n    # Convert the clipped dB spectrogram to uint8 format\n    spectrogram_db_uint8 = tf.cast(\n        (spectrogram_db_clipped - max_db + db_range)\n        / db_range\n        * 255.0,\n        tf.uint8,\n    )\n\n    return spectrogram_db_uint8\n\n", "CodeBleu_score": 42.332988843998685}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Apply the Hann window function raised to a specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with defined frame length and step\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n\n    # Transpose the STFT tensor\n    spectrogram = tf.transpose(tf.abs(stft) ** spec_exponent, [1, 2, 0])\n\n    return spectrogram\n\n", "CodeBleu_score": 41.005650540380124}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Apply the Hann window function raised to a specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with defined frame length and step\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n\n    # Transpose the spectrogram tensor and raise its absolute value to a specified exponent\n    spectrogram = tf.abs(tf.transpose(stft)) ** spec_exponent\n\n    return spectrogram\n\n", "CodeBleu_score": 39.06379537596752}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Apply a Hann window raised to a power.\n    window = tf.signal.hann_window(frame_length, periodic=True)\n    window = tf.math.pow(window, window_exponent)\n    # Extract the STFT.\n    spectrogram = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n    # Obtain the magnitude of the STFT.\n    spectrogram = tf.abs(spectrogram)\n    # Raise the magnitude to the power specified by the input \"spec_exponent\".\n    spectrogram = tf.math.pow(spectrogram, spec_exponent)\n    # Transpose the spectrogram.\n    spectrogram = tf.transpose(spectrogram, [1, 0])\n    # Return the spectrogram.\n    return spectrogram\n\n", "CodeBleu_score": 37.78109001973712}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    time_dimension = tf.shape(spectrogram)[1]\n    new_time_dimension = tf.cast(tf.cast(time_dimension, tf.float32) * factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dimension], method=method\n    )\n    if factor < 1.0:\n        # Crop the spectrogram to the original time dimension\n        cropped_spectrogram = tf.image.resize_with_crop_or_pad(\n            resized_spectrogram,\n            tf.shape(spectrogram)[0],\n            time_dimension,\n        )\n        return cropped_spectrogram\n    else:\n        # Pad the spectrogram with zeros to the original time dimension\n        padded_spectrogram = tf.image.resize_with_crop_or_pad(\n            resized_spectrogram,\n            tf.shape(spectrogram)[0],\n            time_dimension,\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 39.23212187817253}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(original_shape[1], tf.float32) / factor, tf.int32\n    )\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [original_shape[0], new_time_dim], method=method\n    )\n\n    # Calculate the amount of padding or cropping needed\n    padding_amount = original_shape[1] - new_time_dim\n\n    # Pad or crop the spectrogram to match the original dimensions\n    if padding_amount > 0:\n        padded_spectrogram = tf.pad(\n            resized_spectrogram, [[0, 0], [0, padding_amount]]\n        )\n    else:\n        padded_spectrogram = resized_spectrogram[:, : original_shape[1]]\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 35.93694696693306}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension\n    new_time_dim = int(spectrogram.shape[1] * factor)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], new_time_dim],\n        method=method,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if factor > 1.0:\n        # Crop the resized spectrogram to the original dimensions\n        cropped_spectrogram = resized_spectrogram[:, : spectrogram.shape[1]]\n        return cropped_spectrogram\n    elif factor < 1.0:\n        # Pad the resized spectrogram to the original dimensions\n        padding = tf.zeros(\n            [spectrogram.shape[0], spectrogram.shape[1] - new_time_dim],\n            dtype=spectrogram.dtype,\n        )\n        padded_spectrogram = tf.concat([resized_spectrogram, padding], axis=1)\n        return padded_spectrogram\n    else:\n        # No time stretching needed\n        return spectrogram\n\n", "CodeBleu_score": 36.332160734296245}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[1] * resizing_factor), spectrogram.shape[2]],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[1],\n        spectrogram.shape[2],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 27.692115688729142}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    factor = 2 ** (semitone_shift / 12)\n    new_height = int(spectrogram.shape[1] * factor)\n    spectrogram = tf.image.resize(spectrogram, [new_height, spectrogram.shape[2]], method=method)\n    if factor > 1.0:\n        spectrogram = tf.image.resize_with_crop_or_pad(\n            spectrogram, spectrogram.shape[1], spectrogram.shape[2]\n        )\n    elif factor < 1.0:\n        spectrogram = tf.image.resize_with_crop_or_pad(\n            spectrogram, spectrogram.shape[1], spectrogram.shape[2]\n        )\n    return spectrogram", "CodeBleu_score": 30.090347725547815}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor based on the semitone shift\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[1] * factor), spectrogram.shape[2]],\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[1],\n        spectrogram.shape[2],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 28.505428609762234}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 33.21724837244303}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 33.21724837244303}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return nn.ReLU()\n    elif conv_activation == \"elu\":\n        return nn.ELU()\n    elif conv_activation == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid conv_activation: {conv_activation}\")\n", "CodeBleu_score": 33.88294928168856}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    # Initialize an empty dictionary to store the logit masks\n    logit_masks = {}\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for the current instrument\n        logit_mask = unet(input_tensor, instrument, params)\n        # Add the logit mask to the dictionary\n        logit_masks[instrument] = logit_mask\n    # Combine the logit masks using a softmax function along the specified axis\n    combined_masks = tf.nn.softmax(\n        tf.stack(list(logit_masks.values()), axis=-1), axis=-1\n    )\n    # Create an output dictionary to store the instrument spectrograms\n    instrument_spectrograms = {}\n    # Iterate over the instruments\n    for i, instrument in enumerate(instruments):\n        # Multiply the corresponding mask with the input tensor to obtain the spectrogram\n        instrument_spectrograms[instrument] = input_tensor * combined_masks[..., i : i + 1]\n    # Return the dictionary of instrument spectrograms\n    return instrument_spectrograms\n\n", "CodeBleu_score": 40.064897962814435}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    logits = {}\n    for instrument in instruments:\n        logits[instrument] = unet(input_tensor, instrument, params)\n    logits_combined = tf.stack(list(logits.values()), axis=-1)\n    logits_combined = tf.nn.softmax(logits_combined, axis=-1)\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = input_tensor * logits_combined[..., i : i + 1]\n    return spectrograms\n\n", "CodeBleu_score": 36.684469433687966}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = unet_model(input_tensor, instrument, params)\n\n    combined_masks = tf.stack(list(logit_masks.values()), axis=-1)\n    combined_masks = tf.nn.softmax(combined_masks, axis=-1)\n\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = input_tensor * combined_masks[..., i : i + 1]\n\n    return output_dict\n\n", "CodeBleu_score": 38.49319823272623}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.430555747644036}, {"code": "def compute_file_checksum(path)\n    import hashlib\n\n    # Initialize a SHA-256 hash object\n    sha256_hash = hashlib.sha256()\n\n    # Read the file in chunks and update the hash\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.430555747644036}, {"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 51.074864338551386}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n", "CodeBleu_score": 41.59539529728399}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/checksums.json\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index: {response.text}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model '{name}' not found in checksum index\")\n    return index[name]", "CodeBleu_score": 39.12970391613043}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index: {response.status_code}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n", "CodeBleu_score": 39.43177986784139}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n    url = f\"{self.base_url}/{name}\"\n    self.logger.info(f\"Downloading {url}\")\n    with requests.get(url, stream=True, headers={\"Accept-Encoding\": \"identity\"}) as r:\n        r.raise_for_status()\n        with tempfile.NamedTemporaryFile() as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n            f.flush()\n            self.logger.info(f\"Validating checksum for {name}\")\n            if self._checksum(f.name) != self._checksum_url(name):\n                raise ValueError(\"Checksum validation failed\")\n            self.logger.info(f\"Extracting {name} to {path}\")\n            with tarfile.open(f.name) as tar:\n                tar.extractall(path=path)\n\n", "CodeBleu_score": 31.625789060982946}, {"code": "def download(self, name: str, path: str) -> None:\n    url = self.url(name)\n    log.info(f\"Downloading model from {url}\")\n    with self.http.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        content = response.iter_bytes()\n        if self.checksum(name) != self.checksum(content):\n            raise RuntimeError(\"Downloaded model is corrupted\")\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with path.open(\"wb\") as f:\n            for chunk in content:\n                f.write(chunk)\n    log.info(f\"Extracting model to {path}\")", "CodeBleu_score": 29.238749264190368}, {"code": "def download(self, name: str, path: str) -> None:\n    url = self.get_url(name)\n    logger.info(f\"Downloading {name} model ({url}) to {path}\")\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n    if not self.validate_checksum(path):\n        raise ValueError(\"Checksum validation failed.\")\n    logger.info(f\"Extracting {name} model to {path}\")\n    with zipfile.ZipFile(path, \"r\") as zip_ref:\n        zip_ref.extractall(path)\n    os.remove(path)\n    logger.info(f\"{name} model downloaded and extracted to {path}\")", "CodeBleu_score": 30.231100756172815}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded://\"):\n        resource_name = descriptor.split(\"//\")[1]\n        resource_path = f\"resources/{resource_name}.json\"\n        try:\n            with open(resource_path, \"r\") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{resource_name}' not found.\")\n    else:\n        try:\n            with open(descriptor, \"r\") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n    return config\n\n", "CodeBleu_score": 46.706153690815796}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded://\"):\n        # Extract the embedded configuration name from the descriptor\n        config_name = descriptor[len(\"embedded://\"):]\n\n        # Read the JSON resource corresponding to the configuration name\n        config_json = pkgutil.get_data(\n            __name__, f\"resources/configurations/{config_name}.json\")\n        if config_json is None:\n            raise ValueError(\n                f\"Embedded configuration '{config_name}' not found.\")\n\n        # Parse the JSON content and return the configuration dictionary\n        config = json.loads(config_json)\n        return config\n    else:\n        # Check if the file exists\n        if not os.path.isfile(descriptor):\n            raise ValueError(f\"File '{descriptor}' not found.\")\n\n        # Read the JSON content from the file\n        with open(descriptor, \"r\") as f:\n            config = json.load(f)\n\n        return config\n\n", "CodeBleu_score": 45.82413752052403}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded:\"):\n        config_name = descriptor.split(\":\", 1)[1]\n        config_json = pkgutil.get_data(__name__, f\"config/{config_name}.json\")\n        if config_json is None:\n            raise ValueError(f\"Embedded configuration '{config_name}' not found.\")\n        return json.loads(config_json)\n    elif os.path.isfile(descriptor):\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n    else:\n        raise ValueError(f\"Invalid configuration descriptor: {descriptor}\")\n\n", "CodeBleu_score": 47.408630113813075}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"Invalid concat_axis value: {concat_axis}. Expected 0 or 1.\"\n        )\n\n    # Concatenate tensors along the specified axis\n    concatenated_tensor = tf.concat(\n        list(tensor_dict.values()), axis=concat_axis\n    )\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Split the processed tensor back into the original dictionary format\n    processed_tensors = tf.split(\n        processed_tensor, len(tensor_dict), axis=concat_axis\n    )\n\n    # Create a new dictionary with the processed tensors\n    processed_tensor_dict = {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), processed_tensors)\n    }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 40.92241562347613}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    if concat_axis == 0:\n        concatenated_tensor = tf.concat(\n            [tensor for tensor in tensor_dict.values()], axis=concat_axis\n        )\n        processed_tensor = func(concatenated_tensor)\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        concatenated_tensor = tf.concat(\n            [tensor for tensor in tensor_dict.values()], axis=concat_axis\n        )\n        processed_tensor = func(concatenated_tensor)\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=1)\n\n    return {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), processed_tensors)\n    }\n\n", "CodeBleu_score": 45.21599402023858}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n    tensors = list(tensor_dict.values())\n    if concat_axis == 0:\n        concatenated_tensor = tf.concat(tensors, axis=concat_axis)\n    else:\n        concatenated_tensor = tf.stack(tensors, axis=concat_axis)\n    processed_tensor = func(concatenated_tensor)\n    if concat_axis == 0:\n        processed_tensors = tf.split(processed_tensor, len(tensors), axis=concat_axis)\n    else:\n        processed_tensors = tf.unstack(processed_tensor, axis=concat_axis)\n    processed_tensor_dict = {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), processed_tensors)\n    }\n    return processed_tensor_dict\n\n", "CodeBleu_score": 45.42148756408949}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\n    # Compute the minimum and maximum values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range\n    scaled_tensor = tf.image.convert_image_dtype(tensor, tf.uint8, saturate=True)\n\n    # Return the scaled tensor, the original minimum value, and the original maximum value\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 39.42217984656959}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 49.21164030304975}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value) * 255\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 46.58315187870857}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    padding_len = (-tensor.shape[0]) % segment_len\n    tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(tensor, [-1, segment_len])\n", "CodeBleu_score": 13.863254283014188}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_length = tf.shape(tensor)[0]\n    padding_amount = (-tensor_length) % segment_len\n    tensor = tf.pad(tensor, [[0, padding_amount]])\n    return tf.reshape(tensor, (-1, segment_len))\n\n", "CodeBleu_score": 17.95200236639007}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    padding_len = (-len(tensor)) % segment_len\n    padded = tf.pad(tensor, [[0, padding_len]])\n    reshaped = tf.reshape(padded, (-1, segment_len))\n    return reshaped\n\n", "CodeBleu_score": 17.454812521608517}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    n_extra_rows = int((frame_length - 1) / 2)\n    extension = torch.zeros((n_extra_rows, F))\n    extended_spec = torch.cat((extension, instr_spec, extension), dim=0)\n    return extended_spec.reshape(extended_spec.shape[0] * extended_spec.shape[1], extended_spec.shape[2])\n\n", "CodeBleu_score": 14.829071753135665}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    rows_to_pad = frame_length - instr_spec.shape[0]\n    padding_shape = (rows_to_pad, instr_spec.shape[1])\n    padding = np.zeros(padding_shape)\n    padded_instr_spec = np.concatenate((instr_spec, padding), axis=0)\n    reshaped_instr_spec = padded_instr_spec.reshape((1, padded_instr_spec.shape[0], padded_instr_spec.shape[1]))\n    return reshaped_instr_spec\n\n", "CodeBleu_score": 17.8135390129535}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    if instr_spec.shape[1] < frame_length:\n        pad_length = frame_length - instr_spec.shape[1]\n        extension = np.zeros((instr_spec.shape[0], pad_length, instr_spec.shape[2]))\n        extended_instr_spec = np.concatenate((instr_spec, extension), axis=1)\n    else:\n        extended_instr_spec = instr_spec\n    extended_instr_spec = extended_instr_spec.reshape((-1, F))\n    return extended_instr_spec\n", "CodeBleu_score": 27.473508769848298}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 27.127655353502654}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n    from typing import Any\n\n    df = pd.read_csv(csv_path, **kwargs)\n    df_tensor = tf.convert_to_tensor(df)\n    ds = tf.data.Dataset.from_tensor_slices(df_tensor)\n    return ds\n", "CodeBleu_score": 33.063191431906525}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 31.40320167404485}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result &= tensor_tf.shape[i] == target_length\n    return result\n\n", "CodeBleu_score": 39.64250569807448}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result &= tensor_tf.shape[i] == target_length\n    return result\n\n", "CodeBleu_score": 39.64250569807448}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, dim_length in enumerate(target_shape):\n        if dim_length is not None:\n            result = result and tensor_tf.shape[i] == dim_length\n    return result\n\n", "CodeBleu_score": 32.220437492414014}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        train_csv_path=audio_path,\n        cache_dir=CACHE_DIR,\n        batch_size=BATCH_SIZE,\n        num_chunks_per_song=NUM_CHUNKS_PER_SONG,\n        options={\n            \"data_augmentation\": True,\n            \"cache\": True,\n        },\n    )\n    return builder.build(\n        chunk_duration=CHUNK_DURATION,\n        random_seed=RANDOM_SEED,\n    )\n\n", "CodeBleu_score": 29.005562461651884}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    train_dataset = builder.build(\n        train_csv_path=\"train.csv\",\n        cache_dir=\"cache\",\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache=audio_params[\"cache\"],\n    )\n    return train_dataset\n\n", "CodeBleu_score": 42.749407834278884}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        dataset_path=audio_path,\n        chunk_duration=CHUNK_DURATION,\n        random_seed=RANDOM_SEED,\n    )\n\n    dataset = builder.build(\n        train_csv_path=\"train.csv\",\n        cache_dir=\"cache\",\n        batch_size=BATCH_SIZE,\n        num_chunks_per_song=NUM_CHUNKS_PER_SONG,\n        data_augmentation=DATA_AUGMENTATION,\n        cache=CACHE,\n    )\n\n    return dataset\n\n", "CodeBleu_score": 30.009357495255063}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n    if self.instrument_builders is None:\n        self.instrument_builders = []\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(self.client, instrument))\n    for builder in self.instrument_builders:\n        yield builder", "CodeBleu_score": 53.91616107668327}, {"code": "def instruments(self) -> Any\n    if not self._instrument_builders:\n        for instrument in self.instruments:\n            self._instrument_builders[instrument] = InstrumentDatasetBuilder(instrument, self.instruments_data_path)\n    for instrument_builder in self._instrument_builders.values():\n        yield instrument_builder", "CodeBleu_score": 36.29158866935273}, {"code": "def instruments(self) -> Any\n    for instrument in self.instruments:\n        if instrument not in self.instrument_builders:\n            self.instrument_builders[instrument] = InstrumentDatasetBuilder(instrument=instrument)\n        yield self.instrument_builders[instrument]\n", "CodeBleu_score": 31.460691722734456}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache:\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        if wait:\n            while not os.path.exists(cache):\n                logger.info(f\"Waiting for cache {cache} to be created...\")\n                time.sleep(1)\n        else:\n            dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 47.78468369540087}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    if not os.path.exists(cache):\n        os.makedirs(cache, exist_ok=True)\n    cache_index_file = os.path.join(cache, \"index.json\")\n    while wait and not os.path.exists(cache_index_file):\n        logging.info(\"Waiting for cache index file to appear at {}\".format(cache_index_file))\n        time.sleep(1)\n    logging.info(\"Caching dataset to {}\".format(cache))\n    dataset.cache(cache)\n    return dataset", "CodeBleu_score": 45.63285280238213}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache:\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        if wait:\n            while not os.path.exists(cache):\n                logging.info(f\"Waiting for cache {cache} to be created...\")\n                time.sleep(1)\n        else:\n            dataset.cache(cache)\n    else:\n        cache = None\n    return dataset", "CodeBleu_score": 48.55149579746022}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\n    model_dir = ModelProvider.get_model_dir(params[\"model_name\"])\n    params.update({\"model_dir\": model_dir})\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n\n    config = tf.estimator.RunConfig(\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n\n    model_fn = get_model_fn(params[\"model_name\"], MWF)\n    return tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, params=params, config=config\n    )\n\n", "CodeBleu_score": 32.31951849591658}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_dir = ModelProvider.get_model_dir(params)\n    params.update(model_dir=model_dir)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n    model_fn = model_fn_estimator if not MWF else model_fn_estimator_mwf\n    return tf.estimator.Estimator(model_fn=model_fn, params=params, config=config)\n\n", "CodeBleu_score": 27.62816499444852}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_dir = ModelProvider.get_model_dir(params)\n    params = ModelProvider.update_params(params, model_dir)\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    config = tf.estimator.RunConfig(session_config=session_config)\n\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=config,\n    )\n\n", "CodeBleu_score": 30.978058144427155}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    if waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    prediction_generator = self.model.separate(waveform)\n    prediction_generator.update(waveform=waveform, audio_descriptor=audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 58.71174088258031}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.shape[1] != 2:\n        waveform = self._make_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    prediction_generator.data_generator.update_data(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction", "CodeBleu_score": 63.94433984041527}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    if waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    prediction_generator = self.prediction_setup.get_prediction_generator()\n    prediction_generator.data_generator.update_data(waveform, audio_descriptor)\n    prediction = next(prediction_generator.predict())\n    prediction.pop(\"audio_id\")\n    return prediction", "CodeBleu_score": 50.23986977592996}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    if \"resnet\" in config.BACKBONE:\n        return np.array(\n            [[image_shape[0] // stride, image_shape[1] // stride]\n             for stride in config.BACKBONE_STRIDES])\n    else:\n        raise NotImplementedError(config.BACKBONE)\n", "CodeBleu_score": 61.14667611599147}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[image_shape[0] // stride, image_shape[1] // stride]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 66.41798505940051}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    backbone = config.BACKBONE\n    assert backbone in [\"resnet50\", \"resnet101\"]\n    return np.array([int(image_shape[0] / stride), int(image_shape[1] / stride)] for stride in config.BACKBONE_STRIDES)\n\n", "CodeBleu_score": 59.827014073394494}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n    with tf.name_scope('clip_boxes'):\n        boxes = tf.transpose(boxes)\n        window = tf.transpose(window)\n        # Split coordinates\n        ymin, xmin, ymax, xmax = tf.split(boxes, 4, axis=0)\n        w_y_min, w_x_min, w_y_max, w_x_max = tf.split(window, 4, axis=0)\n        # Clip coordinates\n        ymin = tf.maximum(tf.minimum(ymin, w_y_max), w_y_min)\n        xmin = tf.maximum(tf.minimum(xmin, w_x_max), w_x_min)\n        ymax = tf.maximum(tf.minimum(ymax, w_y_max), w_y_min)\n        xmax = tf.maximum(tf.minimum(xmax, w_x_max), w_x_min)\n        clipped = tf.concat([ymin, xmin, ymax, xmax], axis=0)\n        clipped = tf.transpose(clipped)\n        return clipped\n", "CodeBleu_score": 41.59763889735507}, {"code": "def clip_boxes_graph(boxes, window)\n    with tf.name_scope('clip_boxes'):\n        # Split coordinates of both boxes and window\n        ymin_box, xmin_box, ymax_box, xmax_box = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n        ymin_win, xmin_win, ymax_win, xmax_win = tf.split(value=window, num_or_size_splits=4, axis=0)\n        # Clip the boxes\n        ymin = tf.maximum(tf.minimum(ymin_box, ymax_win), ymin_win)\n        xmin = tf.maximum(tf.minimum(xmin_box, xmax_win), xmin_win)\n        ymax = tf.minimum(tf.maximum(ymax_box, ymin_win), ymax_win)\n        xmax = tf.minimum(tf.maximum(xmax_box, xmin_win), xmax_win)\n        clipped = tf.concat([ymin, xmin, ymax, xmax], axis=1, name='clipped_boxes')\n        clipped.set_shape((clipped.shape[0], 4))\n        return clipped\n", "CodeBleu_score": 45.72889502834198}, {"code": "def clip_boxes_graph(boxes, window)\n    with tf.name_scope('clip_boxes'):\n        ymin = tf.maximum(tf.minimum(boxes[:, 0], window[0][1]), window[0][0])\n        xmin = tf.maximum(tf.minimum(boxes[:, 1], window[1][1]), window[1][0])\n        ymax = tf.maximum(tf.minimum(boxes[:, 2], window[0][1]), window[0][0])\n        xmax = tf.maximum(tf.minimum(boxes[:, 3], window[1][1]), window[1][0])\n        clipped = tf.stack([ymin, xmin, ymax, xmax], axis=1, name='clipped_boxes')\n        clipped.set_shape((clipped.shape[0], 4))\n        return clipped\n\n", "CodeBleu_score": 38.72784791574955}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Shared convolutional base of the RPN\n    shared = K.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                             kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = K.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                         kernel_initializer='uniform', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = K.layers.Reshape((-1, 2), name=\"rpn_class_reshape\")(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = K.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = K.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                         kernel_initializer='uniform',\n                         name=\"rpn_bbox_pred\")(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = K.layers.Reshape((-1, 4), name=\"rpn_bbox_reshape\")(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 50.2454929148137}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    shared = tf.keras.layers.Conv2D(512, kernel_size=3, strides=anchor_stride, padding=\"same\", activation=\"relu\", name=\"rpn_conv_shared\")(feature_map)\n\n    x = tf.keras.layers.Conv2D(2 * anchors_per_location, kernel_size=1, padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    rpn_class_logits = tf.keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = tf.keras.layers.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    x = tf.keras.layers.Conv2D(anchors_per_location * 4, kernel_size=1, padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    rpn_bbox = tf.keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 54.11446838566064}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    shared = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n\n    rpn_class_logits = Reshape((-1, 2), name=\"rpn_class_xxx\")(x)\n\n    rpn_probs = Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n\n    rpn_bbox = Reshape((-1, 4), name=\"rpn_bbox_xxx\")(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 39.13090156243604}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n    abs_diff = tf.abs(y_true - y_pred)\n    mask = tf.cast(abs_diff < 1, tf.float32)\n    smooth_l1_loss = (abs_diff**2) * mask + (abs_diff - 0.5) * (1 - mask)\n    return tf.reduce_mean(smooth_l1_loss)\n", "CodeBleu_score": 23.79190442961324}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    abs_diff = tf.abs(y_true - y_pred)\n    mask = tf.cast(abs_diff < 1, tf.float32)\n    return (abs_diff**2) * mask + (abs_diff - 0.5) * (1 - mask)\n\n", "CodeBleu_score": 22.145685622420295}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    abs_diff = tf.abs(y_true - y_pred)\n    mask = tf.cast(abs_diff < 1, tf.float32)\n    return (mask * 0.5 * tf.square(abs_diff)) + ((1 - mask) * (abs_diff - 0.5))\n", "CodeBleu_score": 25.871019455352055}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n    import tensorflow as tf\n    tf.keras.backend.clear_session()\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=x_train.shape[1:]),\n        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n        tf.keras.layers.MaxPooling2D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 26.613581160936047}, {"code": "def build_model(x_train, num_classes)\n    import tensorflow as tf\n    tf.keras.backend.clear_session()\n    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 27.06216001958561}, {"code": "def build_model(x_train, num_classes)\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n    # Reset the TensorFlow graph\n    tf.keras.backend.clear_session()\n\n    # Define the input shape based on the shape of the training data\n    input_shape = x_train.shape[1:]\n\n    # Define the model\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n", "CodeBleu_score": 28.733742743136997}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n    bboxes = []\n    for m in mask:\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            bboxes.append([x1, y1, x2, y2])\n        else:\n            bboxes.append([0, 0, 0, 0])\n    return np.array(bboxes, dtype=np.int)\n\n", "CodeBleu_score": 46.45983885125111}, {"code": "def extract_bboxes(mask)\n    boxes = []\n    for m in mask:\n        if m.sum() == 0:\n            boxes.append([0, 0, 0, 0])\n            continue\n        a = np.where(m.cpu() == True)\n        x1, x2 = np.min(a[1]), np.max(a[1])\n        y1, y2 = np.min(a[0]), np.max(a[0])\n        boxes.append([x1, y1, x2, y2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 19.326499995847595}, {"code": "def extract_bboxes(mask)\n    bboxes = []\n    for m in mask:\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            bboxes.append([x1, y1, x2, y2])\n        else:\n            bboxes.append([0, 0, 0, 0])\n    return np.asarray(bboxes, dtype=np.int32)\n\n", "CodeBleu_score": 46.68248751173715}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n    # compute the area of each box\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        # compute the intersection coordinates\n        x1 = np.maximum(boxes1[:, 0], boxes2[i, 0])\n        y1 = np.maximum(boxes1[:, 1], boxes2[i, 1])\n        x2 = np.minimum(boxes1[:, 2], boxes2[i, 2])\n        y2 = np.minimum(boxes1[:, 3], boxes2[i, 3])\n\n        # compute the intersection area\n        intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n\n        # compute the union area\n        union = area1 + area2[i] - intersection\n\n        # compute the IoU\n        overlaps[:, i] = intersection / union\n\n    return overlaps\n\n", "CodeBleu_score": 45.42869122747358}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # boxes1: (N, 4)\n    # boxes2: (M, 4)\n    # overlaps: (N, M)\n    N = boxes1.shape[0]\n    M = boxes2.shape[0]\n    overlaps = np.zeros((N, M), dtype=np.float32)\n    for n in range(N):\n        for m in range(M):\n            box1 = boxes1[n]\n            box2 = boxes2[m]\n            iw = min(box1[2], box2[2]) - max(box1[0], box2[0]) + 1\n            if iw > 0:\n                ih = min(box1[3], box2[3]) - max(box1[1], box2[1]) + 1\n                if ih > 0:\n                    ua = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1) + (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1) - iw * ih\n                    overlaps[n, m] = iw * ih / ua\n    return overlaps\n", "CodeBleu_score": 22.02679093939335}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # compute the area of each box in both sets\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        # compute the coordinates of the intersection rectangle\n        x1 = np.maximum(boxes1[:, 0], boxes2[i, 0])\n        y1 = np.maximum(boxes1[:, 1], boxes2[i, 1])\n        x2 = np.minimum(boxes1[:, 2], boxes2[i, 2])\n        y2 = np.minimum(boxes1[:, 3], boxes2[i, 3])\n\n        # compute the area of the intersection rectangle\n        w = np.maximum(0, x2 - x1)\n        h = np.maximum(0, y2 - y1)\n        intersection = w * h\n\n        # compute the IoU\n        union = area1 + area2[i] - intersection\n        overlaps[:, i] = intersection / union\n\n    return overlaps\n\n", "CodeBleu_score": 45.99231778537245}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n    image_info = self.image_info[image_id]\n    image = imageio.imread(image_info['path'])\n    if image.ndim != 3:\n        image = color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :-1]\n    return image", "CodeBleu_score": 67.74245906534819}, {"code": "def load_image(self, image_id):\n    image_info = self.image_info[image_id]\n    image = imageio.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :-1]\n    return image", "CodeBleu_score": 67.26920073364836}, {"code": "def load_image(self, image_id):\n    info = self.image_info[image_id]\n    image = Image.open(info['path'])\n    image = np.array(image)\n    if image.ndim == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 51.92528663037401}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    import matplotlib.pyplot as plt\n    import math\n\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, math.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm, interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n", "CodeBleu_score": 35.8558054005623}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    import math\n    import matplotlib.pyplot as plt\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, math.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm, interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n", "CodeBleu_score": 35.8558054005623}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    import math\n    import matplotlib.pyplot as plt\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, math.ceil(n_images / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 36.152030079588926}]}
