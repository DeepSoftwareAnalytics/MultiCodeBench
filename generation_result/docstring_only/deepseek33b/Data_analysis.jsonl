{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        train_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 67.31491587948683}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.28958649903562}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.30740775035834}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=100, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.988730693271556}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.92483101817302}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.92483101817302}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    label, means, weights, covariances, num_samples = ndarrays\n    return GMMParameters(label, means, weights, covariances, num_samples)\n\n", "CodeBleu_score": 38.69306689370285}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n", "CodeBleu_score": 40.08951228295815}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n    # Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    # Load the remaining data files and concatenate the features and labels\n    for i in range(1, len(data_paths)):\n        X_new, Y_new = load_svmlight_file(data_paths[i])\n        X_new = X_new.toarray()\n        assert X_new.shape[1] == X.shape[1]\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 43.68354795510007}, {"code": "def datafiles_fusion(data_paths):\n    # Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    # Iterate over the remaining data files\n    for i in range(1, len(data_paths)):\n        # Load the next data file\n        X_new, Y_new = load_svmlight_file(data_paths[i])\n        X_new = X_new.toarray()\n        # Ensure the number of features matches the first file\n        assert X_new.shape[1] == X.shape[1]\n        # Concatenate the new features and labels to the existing arrays\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 43.68354795510007}, {"code": "def datafiles_fusion(data_paths):\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for path in data_paths[1:]:\n        X_i, Y_i = load_svmlight_file(path)\n        assert X_i.shape[1] == X.shape[1]\n        X = np.concatenate((X, X_i.toarray()), axis=0)\n        Y = np.concatenate((Y, Y_i), axis=0)\n    return X, Y\n", "CodeBleu_score": 35.58133940730791}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Calculate recall\n    recall = recall_score(y_test, y_pred, average='micro')\n\n    # Calculate precision\n    precision = precision_score(y_test, y_pred, average='micro')\n\n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 33.42910286151195}, {"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 54.6058043693212}, {"code": "def eval_learning(y_test, y_pred)\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Calculate recall\n    recall = recall_score(y_test, y_pred, average='micro')\n\n    # Calculate precision\n    precision = precision_score(y_test, y_pred, average='micro')\n\n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 33.42910286151195}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n    # Set the model to training mode\n    model.train()\n\n    # Define the loss function and optimizer\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model for the specified number of epochs\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Forward pass\n            output = model(data)\n\n            # Compute the loss\n            loss = criterion(output, target)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Print the loss every 100 batches\n            if batch_idx % 100 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n", "CodeBleu_score": 49.46932052893549}, {"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 100 == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader), loss.item()))\n\n", "CodeBleu_score": 48.6515579329151}, {"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n", "CodeBleu_score": 54.62493948787468}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.dataset.type == \"LIBSVM\"\n    path = cfg.dataset.path\n    name = cfg.dataset.name\n    data, labels = load_svmlight_file(os.path.join(path, name))\n    data = data.todense().astype(np.float32)\n    labels = labels.astype(np.int32)\n    print(np.unique(labels, return_counts=True))\n    print(data.shape)\n    if name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(\"Unknown dataset\")\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 31.17801316901197}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.dataset.type == \"LIBSVM\"\n    path = cfg.dataset.path\n    name = cfg.dataset.name\n\n    data, labels = load_svmlight_file(os.path.join(path, name))\n    data = data.todense().astype(np.float32)\n    labels = labels.astype(np.float32)\n\n    print(f\"unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"data shape: {data.shape}\")\n\n    if name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(f\"Unknown dataset name: {name}\")\n\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 31.402567974874447}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(\n            f\"Expected dataset type LIBSVM, but got {cfg.dataset.type}\"\n        )\n    data_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n    data, labels = load_svmlight_file(data_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.float32)\n    print(f\"Unique labels: {np.unique(labels)}\")\n    print(f\"Label counts: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    if dataset_name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(f\"Dataset name {dataset_name} not supported\")\n    dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n    return dataset\n\n", "CodeBleu_score": 47.07341203449756}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    X = check_array(X, accept_sparse=\"csc\", copy=copy, dtype=FLOAT_DTYPES)\n    if issparse(X):\n        if selected == \"all\":\n            return X.toarray()\n        else:\n            X = X.tocsc()\n\n    if selected == \"all\":\n        return transform(X)\n\n    X_sel = X[:, selected]\n    X_not_sel = X[:, np.logical_not(selected)]\n    X_sel = transform(X_sel)\n\n    if issparse(X):\n        return hstack((X_sel, X_not_sel))\n    else:\n        return np.hstack((X_sel, X_not_sel))\n\n", "CodeBleu_score": 35.295882367515915}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    X = check_array(X, accept_sparse=\"csc\", copy=copy, ensure_2d=False,\n                     force_all_finite=False)\n    if issparse(X):\n        if transform == \"log1p\":\n            X.data = np.log1p(X.data)\n        else:\n            X = transform(X)\n    else:\n        if transform == \"log1p\":\n            np.log1p(X, out=X)\n        else:\n            X = transform(X)\n    return X", "CodeBleu_score": 20.796661261671908}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    X = check_array(X, accept_sparse=\"csc\", copy=copy, force_all_finite=False)\n    if issparse(X):\n        X = _transform_selected_sparse(X, transform, selected)\n    else:\n        X = _transform_selected_dense(X, transform, selected)\n    return X\n\n", "CodeBleu_score": 8.187057891854236}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n    X = check_array(X, dtype=None)\n    n_samples, n_features = X.shape\n    X_int, n_missing_values = self._encode(X, uniques=self.categories_)\n    mask = _get_mask(X_int, X_int != self.categories_[0][0])\n    X_masked = np.ma.MaskedArray(X_int, mask=mask, dtype=self.categories_[0].dtype)\n    cat_lens = [len(cat) for cat in self.categories_]\n    X_int = np.cumsum(cat_lens) - 1\n    indices = np.repeat(np.arange(n_samples, dtype=np.int32), n_features)\n    indptr = np.arange(n_features + 1, dtype=np.int32) * n_samples\n    data = (X_masked.ravel() - X_int.ravel()[X_masked.ravel()]).astype(np.int8)\n    out = sparse.csr_matrix((data, indices, indptr), shape=(n_samples, n_features), dtype=np.int8).T\n    if not self.sparse:\n        out = out.toarray()\n    return out\n\n", "CodeBleu_score": 29.698728851456067}, {"code": "def _transform_new(self, X)\n    X = self._validate_data(\n        X,\n        reset=False,\n        copy=self.copy,\n        force_all_finite=\"allow-nan\",\n        dtype=None,\n    )\n    n_samples, n_features = X.shape\n    X_int, n_missing_values = self._transform(X, handle_unknown=\"ignore\")\n    if n_missing_values > 0:\n        if self.handle_unknown == \"error\":\n            raise ValueError(\n                \"Found unknown categories {}\"\n                \" in column {} during transform\".format(\n                    np.unique(X[np.isnan(X_int)]).categories,\n                    np.where(np.isnan(X_int))[1],\n                )\n            )\n        else:\n            X_int[np.isnan(X_int)] = self.categories_[np.nan].codes[0]\n    X_mask = X_int < 0\n    if (X_int.dtype == np.int64 and X_int.max() < np.iinfo(np.int32).max) and (\n        not self.sparse\n    ):\n        X_int = X_int.astype(np.int32)\n    X_cat = _transform_selected(\n        X_int, self._transformers, mask=X_mask, transform_all=True\n    )\n    return X_cat\n\n", "CodeBleu_score": 27.79130331700601}, {"code": "def _transform_new(self, X)\n    X = self._validate_data(X, dtype=None, reset=False)\n    n_samples, n_features = X.shape\n    X_int, mask = self._encode(X, uniques=self.categories_)\n    if not mask.all():\n        msg = \"Found unknown categories {0} in column {1}\"\n        msg = msg.format(\n            {val: idx for idx, val in enumerate(self.categories_[0]) if not mask[:, idx].all()},\n            [\n                idx\n                for idx, mask_col in enumerate(mask.T)\n                if not mask_col.all()\n            ],\n        )\n        raise ValueError(msg)\n    X_mask = X_int.copy()\n    X_mask[~mask] = self.categories_[0][0]\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values[:-1])\n    indices = ((X_int + n_values[1:, None]).ravel(), X_mask.ravel())\n    indptr = np.hstack((0, np.cumsum(mask * n_samples)))\n    data = np.empty(indices[0].shape[0], dtype=bool)\n    data[:: n_samples + 1] = 1\n    return sparse.csr_matrix(\n        (data, indices, indptr), shape=(n_samples, mask.shape[1] * n_samples), dtype=bool\n    )\n", "CodeBleu_score": 44.87768200692421}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n    from scipy.stats import uniform, loguniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)\n", "CodeBleu_score": 34.62120361072451}, {"code": "def convert_to_sklearn(self)\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)", "CodeBleu_score": 23.057045198751105}, {"code": "def convert_to_sklearn(self)\n    if self.log:\n        return scipy.stats.loguniform(self.lower, self.upper)\n    else:\n        return scipy.stats.uniform(self.lower, self.upper)\n", "CodeBleu_score": 23.814079087046558}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = state.additional_args\n    figure_args = state.figure_args\n    features = state.features\n    if state.headers:\n        self._render_header(state, \"Partial Dependence Plots\")\n    fig, ax = plt.subplots(**figure_args)\n    kwargs = {**additional_args, **self.kw_args}\n    if state.two_way_interaction:\n        data = state.data.dropna()\n    else:\n        data = state.data\n    display = PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features=features,\n        **kwargs,\n    )\n    display.plot(ax=ax)\n    plt.tight_layout()\n    plt.show()", "CodeBleu_score": 47.451878640848946}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = self._get_additional_args(state)\n    figure_args = self._get_figure_args(state)\n    features = self._get_features(state)\n    if self.header:\n        self._render_header(state)\n    fig, axes = plt.subplots(1, len(features), **figure_args)\n    kwargs = {**additional_args, **self._predefined_kw_args}\n    if self.two_way_interaction and self.model is not None:\n        data = state.data[features].dropna()\n    else:\n        data = state.data[features]\n    display = PartialDependenceDisplay.from_estimator(self.model, data, features, ax=axes, **kwargs)\n    fig.tight_layout()\n    plt.show()", "CodeBleu_score": 43.98326641151639}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = state.additional_args\n    fig_args = state.fig_args\n    features = state.features\n    if state.headers:\n        self._render_header(state, \"Partial Dependence Plots\")\n    fig, axes = plt.subplots(\n        len(features), len(features), figsize=fig_args.get(\"figsize\", (15, 15)))\n    kwargs = {**additional_args, **self._prepare_pdp_kwargs(state)}\n    if state.two_way_interaction_plot:\n        data = state.data.dropna(subset=features)\n    else:\n        data = state.data\n    display = PartialDependenceDisplay.from_estimator(\n        state.model, data, features, ax=axes, **kwargs)\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 47.17148957514452}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.identified_features:\n        X = X.copy()\n        X[self.feature_generator.identified_features] = self.feature_generator.transform(X[self.feature_generator.identified_features])\n    X = fill_missing(X)\n    X = X.astype('float32')\n    return X", "CodeBleu_score": 47.373784634514735}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.identified_features:\n        X_ = X.copy()\n        X_[self.feature_generator.identified_features] = self.feature_generator.transform(X[self.feature_generator.identified_features])\n    else:\n        X_ = X\n    X_ = X_.fillna(0)\n    return X_.astype('float32')", "CodeBleu_score": 45.391869029696934}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.identified_features:\n        X = X.copy()\n        for feature in self.feature_generator.identified_features:\n            X[feature] = self.feature_generator.transform(X[feature])\n    X = X.fillna(0)\n    return X.astype(\"float32\")", "CodeBleu_score": 46.71917920981942}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n    params = self.get_params()\n    params[\"n_estimators\"] = self._get_final_n_estimators(\n        params, X, **kwargs\n    )\n    params[\"n_estimators\"] = max(params[\"n_estimators\"], 40)\n    n_trees_per_estimator = params[\"n_estimators\"] // params[\"n_jobs\"]\n    bytes_per_estimator = self._get_bytes_per_estimator(X, **params)\n    expected_min_memory_usage = (\n        n_trees_per_estimator * bytes_per_estimator * 1.2\n    )\n    return expected_min_memory_usage\n\n", "CodeBleu_score": 22.940346862435803}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    params = self.get_params(deep=False)\n    if params.get(\"n_estimators\"):\n        n_estimators = params[\"n_estimators\"]\n    else:\n        n_estimators = 40\n    if params.get(\"search_spaces\"):\n        n_estimators = 40\n    n_trees_per_estimator = self.get_n_trees_per_estimator(n_estimators)\n    bytes_per_estimator = self.get_bytes_per_estimator(X, **kwargs)\n    adjustment_factor = 1.0\n    if params.get(\"n_jobs\") is not None:\n        adjustment_factor = 1.0 / params[\"n_jobs\"]\n    expected_min_mem_usage = bytes_per_estimator * n_estimators * adjustment_factor\n    return expected_min_mem_usage\n\n", "CodeBleu_score": 40.76288275536155}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    params = self.get_params()\n    n_jobs = params.get(\"n_jobs\", 1)\n    n_estimators = params.get(\"n_estimators\", 100)\n    if n_estimators < 40 or params.get(\"search_spaces\"):\n        n_estimators = 40\n    n_trees_per_estimator = params.get(\"n_trees_per_estimator\", 1)\n    if n_estimators > 1:\n        n_estimators = int(n_estimators / n_trees_per_estimator)\n    bytes_per_estimator = (\n        X.shape[0] * X.shape[1] * 8 * 2.5\n    )  # 8 bytes per float64, 2.5 adjustment factor\n    expected_min_mem_usage = bytes_per_estimator * n_estimators\n    return expected_min_mem_usage", "CodeBleu_score": 40.42986381223703}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    from collections import Counter\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = ngram_counter.most_common(vocab_size)\n    top_ngrams = [ngram[0] for ngram in top_ngrams]\n    top_ngrams.sort()\n    vocab_dict = {ngram: idx for idx, ngram in enumerate(top_ngrams)}\n    vectorizer.vocabulary_ = vocab_dict\n\n", "CodeBleu_score": 48.51724184041746}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    from collections import Counter\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams\n    top_ngrams = ngram_counter.most_common(vocab_size)\n    # Sort the names of the top n n-grams\n    sorted_ngrams = sorted(ngram_freq.keys(), key=lambda x: ngram_freq[x], reverse=True)[:vocab_size]\n    # Create a new vocabulary dictionary mapping each n-gram to a unique index\n    new_vocab = {ngram: i for i, ngram in enumerate(sorted_ngrams)}\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer", "CodeBleu_score": 50.383520152457216}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    import collections\n    import numpy as np\n    import pandas as pd\n    import re\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    # Create a Counter object from the n-gram frequency data\n    ngram_freq_counter = collections.Counter(ngram_freq)\n\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = ngram_freq_counter.most_common(vocab_size)\n\n    # Sort the names of these top n n-grams\n    top_ngrams_sorted = sorted(top_ngrams, key=lambda x: x[0])\n\n    # Create a new vocabulary dictionary mapping each n-gram to a unique index\n    vocab_dict = {ngram: index for index, (ngram, _) in enumerate(top_ngrams_sorted)}\n\n    # Update the vectorizer's vocabulary with this new, downsized vocabulary\n    vectorizer.vocabulary_ = vocab_dict\n\n    # Return the updated vectorizer\n    return vectorizer\n", "CodeBleu_score": 34.75677108616674}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)    feature_types = {}\n    skewness_threshold = self.parameters[\"skewness_threshold\"]\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            feature_types[feature] = \"skewed\"\n        else:\n            feature_types[feature] = \"continuous\"\n    return feature_types", "CodeBleu_score": 40.82341820080841}, {"code": " def _select_continuous(self, df, features)    feature_types = {}\n    skew_threshold = self.parameters['skew_threshold']\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skew_threshold:\n            feature_types[feature] = 'skewed'\n        else:\n            feature_types[feature] = 'continuous'\n    return feature_types\n", "CodeBleu_score": 40.587385792414956}, {"code": " def _select_continuous(self, df, features)    continuous_features = []\n    skewed_features = []\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > self.skewness_threshold:\n            skewed_features.append(feature)\n        else:\n            continuous_features.append(feature)\n    return {'continuous': continuous_features, 'skewed': skewed_features}", "CodeBleu_score": 42.43312426267808}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        with np.errstate(divide=\"ignore\"):\n            weights = 1.0 / dist\n        weights[~np.isfinite(weights)] = 0.0\n        if dist.ndim == 1:\n            weights = weights / weights.sum()\n        else:\n            weights = weights / weights.sum(axis=1)[:, np.newaxis]\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights should be 'uniform', 'distance', or a callable function\"\n        )\n\n", "CodeBleu_score": 54.648429668992385}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        with np.errstate(divide=\"ignore\"):\n            weights = 1.0 / dist\n        weights[~np.isfinite(weights)] = 0.0\n        if dist.ndim == 1:\n            weights = weights / weights.sum()\n        else:\n            weights = weights / weights.sum(axis=1)[:, np.newaxis]\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights should be 'uniform', 'distance', or a callable function\"\n        )", "CodeBleu_score": 54.648429668992385}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # if user attempts to classify a point that was zero distance from one\n        # or more training points, those training points are weighted as 1.0\n        # and the other points as 0.0\n        if dist.dtype is np.dtype(object):\n            dist = dist.astype(np.float64)\n        inv_dist = 1.0 / dist\n        inf_mask = np.isinf(dist)\n        inf_row = np.any(inf_mask, axis=1)\n        weights = np.ones(dist.shape, dtype=np.float64)\n        weights[inf_row] = inf_mask[inf_row]\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights not recognized: should be 'uniform', \"\n            \"'distance', or a callable function\"\n        )", "CodeBleu_score": 63.30380482590693}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n    distances, indices = self.kneighbors()\n    weights = self._get_weights(distances)\n    if len(self._y.shape) == 1:\n        y = self._y.reshape(-1, 1)\n    else:\n        y = self._y\n    if self.weights == 'uniform':\n        predictions = np.mean(y[indices], axis=1)\n    else:\n        predictions = np.sum(y[indices] * weights, axis=1) / np.sum(weights, axis=1)\n    if len(self._y.shape) == 1:\n        predictions = predictions.flatten()\n    return predictions", "CodeBleu_score": 33.995601967250764}, {"code": "def predict_loo(self)\n    distances, indices = self.kneighbors(self._X)\n    weights = self._get_weights(distances)\n    if len(self._y.shape) == 1:\n        y = self._y.reshape(-1, 1)\n    else:\n        y = self._y\n    if self.weights == \"uniform\":\n        predictions = np.mean(y[indices], axis=1)\n    else:\n        predictions = np.sum(y[indices] * weights, axis=1) / np.sum(weights, axis=1)\n    if len(self._y.shape) == 1:\n        predictions = predictions.flatten()\n    return predictions\n\n", "CodeBleu_score": 32.6741949569905}, {"code": "def predict_loo(self)\n    distances, indices = self.kneighbors(self._X)\n    weights = self._get_weights(distances)\n    if self._y.ndim == 1:\n        self._y = self._y.reshape(-1, 1)\n    predictions = np.zeros_like(self._y)\n    if self.weights == \"uniform\":\n        for i in range(self._X.shape[0]):\n            predictions[i] = np.mean(self._y[indices[i]], axis=0)\n    else:\n        for i in range(self._X.shape[0]):\n            predictions[i] = np.sum(\n                self._y[indices[i]] * weights[i].reshape(-1, 1), axis=0\n            ) / np.sum(weights[i])\n    if self._y.ndim == 1:\n        predictions = predictions.flatten()\n    return predictions\n\n", "CodeBleu_score": 35.9781094257898}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n\n    # Reshape the quantile list into a one-dimensional array\n    quantile_list = np.array(quantile_list).reshape(-1)\n\n    # Determine the batch size from the input data's first dimension\n    batch_size = input_data.shape[0]\n\n    # Create an empty list to store the transformed data\n    transformed_data = []\n\n    # Iterate over each row in the input data\n    for i in range(batch_size):\n        # Fit and transform the data using IsotonicRegression with the given quantile list\n        ir = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds='clip', increasing=True)\n        ir.fit(quantile_list, input_data[i, :])\n        transformed_row = ir.transform(quantile_list)\n\n        # Append the transformed row to the list\n        transformed_data.append(transformed_row)\n\n    # Stack the transformed data into a new array and return it\n    return np.stack(transformed_data)\n\n", "CodeBleu_score": 53.89493075519547}, {"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n\n    # Reshape the quantile list into a 1D array\n    quantiles = np.array(quantile_list).reshape(-1)\n\n    # Determine the batch size from the first dimension of the input data\n    batch_size = input_data.shape[0]\n\n    # Create an empty array to store the transformed data\n    transformed_data = np.zeros_like(input_data)\n\n    # Loop through each row in the input data\n    for i in range(batch_size):\n        # Fit and transform the data using IsotonicRegression\n        ir = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n        ir.fit(quantiles, input_data[i])\n        transformed_data[i] = ir.transform(quantiles)\n\n    return transformed_data\n", "CodeBleu_score": 50.92721733384272}, {"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n\n    # Reshape the quantile list into a one-dimensional array\n    quantile_list = np.array(quantile_list).reshape(-1)\n\n    # Determine the batch size from the input data's first dimension\n    batch_size = input_data.shape[0]\n\n    # Initialize an empty list to store the transformed data\n    transformed_data = []\n\n    # Iterate over each row in the input data\n    for i in range(batch_size):\n        # Fit and transform the data using IsotonicRegression with the given quantile list\n        ir = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds='clip')\n        ir.fit(quantile_list, input_data[i])\n        transformed_row = ir.transform(quantile_list)\n\n        # Add the transformed row to the list\n        transformed_data.append(transformed_row)\n\n    # Stack the transformed data into a new array\n    transformed_data = np.stack(transformed_data)\n\n    return transformed_data\n", "CodeBleu_score": 55.08546574437537}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    transformed_matrix = vectorizer.transform(text_data)\n    return vectorizer, transformed_matrix\n\n", "CodeBleu_score": 48.306812645474515}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    # Remove rows with missing entries marked by \"--\"\n    df = df[df.ne('--').all(axis=1)]\n\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.drop([col for col in df.columns if df[col].nunique() <= 1 or 'id' in col.lower()], axis=1)\n\n    # Drop shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.drop([col for col in df.columns if 'days_to_death' in col.lower() or 'year_of_death' in col.lower()], axis=1)\n\n    # Split the dataframe into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return train_df, test_df\n\n", "CodeBleu_score": 41.27619801595815}, {"code": "def preprocess(df, test_size, shuffle)\n    # Drop rows with missing entries\n    df = df.replace('--', pd.NA)\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to 1\n    for col in df.columns:\n        if len(df[col].unique()) <= 1:\n            df = df.drop(col, axis=1)\n\n    # Drop columns with \"id\" in their name\n    df = df.drop([col for col in df.columns if 'id' in col], axis=1)\n\n    # Drop shortcut columns\n    df = df.drop(['days_to_death', 'year_of_death'], axis=1)\n\n    # Split the data into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return train_df, test_df\n\n", "CodeBleu_score": 36.68855281075347}, {"code": "def preprocess(df, test_size, shuffle)\n    # Remove rows with missing entries marked by \"--\"\n    df = df[df.ne('--').all(axis=1)]\n    # Drop columns with unique values less than or equal to 1\n    df = df.drop(columns=[col for col in df.columns if df[col].nunique() <= 1])\n    # Drop columns with \"id\" in their name\n    df = df.drop(columns=[col for col in df.columns if \"id\" in col])\n    # Drop shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.drop(columns=[col for col in df.columns if \"days_to_death\" in col or \"year_of_death\" in col])\n    # Split the data into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train_df, test_df\n", "CodeBleu_score": 32.602651093594474}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    X, y = _load_dataset(dataset, sparse)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    search_space = classifier.get_hyperparameter_search_space()\n    default_config = search_space.get_default_configuration()\n    classifier = classifier(configuration=default_config, random_state=42)\n    classifier.iterative_fit(X_train, y_train, n_iter=2, refit=True)\n    n_iter = 4\n    while not classifier.configuration_fully_fitted:\n        classifier.iterative_fit(X_train, y_train, n_iter=n_iter, refit=False)\n        n_iter *= 2\n    y_pred = classifier.predict(X_test)\n    return y_pred, y_test, classifier\n\n", "CodeBleu_score": 35.286136307677786}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import check_random_state\n    from sklearn.utils.sparsefuncs import csc_median_axis_0\n    from sklearn.utils.fixes import sparse_min_max\n    from sklearn.utils.fixes import sparse_min_max_axis\n    from sklearn.utils.fixes import sparse_min_max_axis_0\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min_max_axis_1\n    from sklearn.utils.fixes import sparse_min", "CodeBleu_score": 7.041697659043214}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import check_random_state\n\n    # Load the dataset\n    if dataset == \"iris\":\n        X, y = load_iris(return_X_y=True)\n    else:\n        raise ValueError(f\"Unknown dataset: {dataset}\")\n\n    # Make the data sparse\n    if sparse:\n        X = csr_matrix(X)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0\n    )\n\n    # Retrieve the classifier's hyperparameter search space\n    cs = classifier.get_hyperparameter_search_space()\n\n    # Obtain the default configuration\n    default_config = cs.get_default_configuration()\n\n    # Initialize the classifier with the default configuration and a random state\n    clf = classifier(random_state=check_random_state(0), **default_config)\n\n    # Fit the classifier iteratively to the training data for an initial two iterations with refitting enabled\n    clf.iterative_fit(X_train, y_train, n_iter=2, refit=True)\n\n    # Continue iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted\n    while not clf.configuration_fully_fitted:\n        clf.iterative_fit(X_train, y_train, n_iter=clf.iterative_fit_iterations * 2)\n\n    # Make predictions on the test data\n    y_pred = clf.predict(X_test)\n\n    # Return the predictions, true test labels, and the fitted classifier\n    return y_pred, y_test, clf\n\n", "CodeBleu_score": 38.69686604003427}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self.ensemble = self.backend.load_ensemble(self.ensemble_class, self.seed)\n    if self.ensemble is None and self.evaluator_output_conditions and self.resampling_strategy in ['cv', 'holdout']:\n        self.ensemble = self.backend.load_best_individual_model(self.evaluator_output_conditions, self.seed)\n    if self.ensemble is not None:\n        self.model_ids = self.backend.get_model_ids(self.ensemble)\n        self.models = [self.backend.load_model(model_id, self.seed) for model_id in self.model_ids]\n        if self.resampling_strategy == 'cv':\n            self.cross_validated_models = [self.backend.load_cross_validated_models(model_id, self.seed) for model_id in self.model_ids]\n        else:\n            self.cross_validated_models = None\n    else:\n        self.models = []\n        self.cross_validated_models = None\n", "CodeBleu_score": 41.473253636664225}, {"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self.ensemble = self.backend.load_ensemble(self.ensemble_class, self.seed)\n    if self.ensemble is None:\n        if self.evaluator_output_conditions is not None and self.resampling_strategy is not None:\n            self.ensemble = self.backend.load_best_individual_model(self.evaluator_output_conditions, self.resampling_strategy, self.seed)\n    if self.ensemble is not None:\n        self.model_identifiers = self.backend.get_model_identifiers(self.ensemble)\n        self.models = self.backend.load_models(self.ensemble, self.model_identifiers)\n        if self.resampling_strategy in ['cv', 'cv-holdout']:\n            self.cross_validated_models = self.backend.load_cross_validated_models(self.ensemble, self.model_identifiers)\n        else:\n            self.cross_validated_models = None\n    else:\n        self.model_identifiers = []\n        self.models = []\n        self.cross_validated_models = None", "CodeBleu_score": 44.56949475271132}, {"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self.ensemble = self.backend.load_ensemble(self.ensemble_class, self.seed)\n    if self.ensemble is None and self.evaluator_output_condition is not None and self.evaluator_output_condition.resampling_strategy is not None:\n        self.ensemble = self.backend.load_best_individual_model(self.evaluator_output_condition.resampling_strategy, self.seed)\n    if self.ensemble is not None:\n        self.model_ids = self.ensemble.get_model_identifiers()\n        self.models = [self.backend.load_model(model_id) for model_id in self.model_ids]\n        if self.evaluator_output_condition.resampling_strategy == \"cross-validation\":\n            self.cross_validated_models = [self.backend.load_cross_validated_model(model_id) for model_id in self.model_ids]\n        else:\n            self.cross_validated_models = None\n    else:\n        self.models = []\n        self.cross_validated_models = []\n\n", "CodeBleu_score": 44.021890876078835}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n    import openml\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = task.get_dataset()\n    is_categorical = dataset.features.select_by_index(X_train.columns.values).is_categorical()\n    mapping = {label: i for i, label in enumerate(y_train.unique())}\n    y_train_int = y_train.map(mapping)\n    y_test_int = y_test.map(mapping)\n    return X_train, y_train_int, X_test, y_test_int, is_categorical\n", "CodeBleu_score": 26.87393278474725}, {"code": "def load_task(task_id)\n    from openml.tasks import get_task\n    from openml.datasets import get_dataset\n    from sklearn.model_selection import train_test_split\n    import numpy as np\n\n    task = get_task(task_id)\n    X, y = task.get_X_and_y()\n\n    train_indices, test_indices = task.get_train_test_split_indices()\n\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n\n    dataset = get_dataset(task.dataset_id)\n\n    # Categorical variables have to be type 'str'\n    X_train = X_train.astype({i: 'str' for i, typ in zip(\n        range(X_train.shape[1]), dataset.features.values()) if typ == 'nominal'})\n    X_test = X_test.astype({i: 'str' for i, typ in zip(\n        range(X_test.shape[1]), dataset.features.values()) if typ == 'nominal'})\n\n    # Missing values have to be marked as np.nan\n    X_train = X_train.replace('?', np.nan)\n    X_test = X_test.replace('?', np.nan)\n\n    # Numerical variables have to be type 'float'\n    X_train = X_train.astype(\n        {i: 'float' for i, typ in zip(range(X_train.shape[1]), dataset.features.values()) if typ == 'numeric'})\n    X_test = X_test.astype(\n        {i: 'float' for i, typ in zip(range(X_test.shape[1]), dataset.features.values()) if typ == 'numeric'})\n\n    # Map unique label values to integers\n    unique_labels = np.unique(y_train)\n    label_map = {label: i for i, label in enumerate(unique_labels)}\n    y_train", "CodeBleu_score": 48.87772191374178}, {"code": "def load_task(task_id)\n    import openml\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = task.get_dataset()\n    _, categorical = dataset.get_features_by_type()\n    is_categorical = [feat.name in categorical for feat in dataset.features.values()]\n    y_train = map_labels_to_integers(y_train)\n    y_test = map_labels_to_integers(y_test)\n    return X_train, y_train, X_test, y_test, is_categorical\n", "CodeBleu_score": 31.286229748215522}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if isinstance(y, (pd.DataFrame, pd.Series)):\n        return y.values\n    elif issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n        return y\n    else:\n        return y\n\n", "CodeBleu_score": 51.17317636699092}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if sp.issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 51.99553313693461}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if issparse(y):\n        y = y.toarray()\n        if y.shape[1] == 1:\n            y = y.flatten()\n        elif y.shape[1] > 1:\n            y = pd.DataFrame(y)\n    return y\n\n", "CodeBleu_score": 55.43359831003008}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.ensemble_members)\n\n    # Initialize a weights array with zeros\n    weights = np.zeros(self.n_models)\n\n    # Calculate the weight for each ensemble member\n    for i in range(self.n_models):\n        weights[i] = counts[i] / self.n_models\n\n    # Normalize the weights if their sum is less than 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n\n    # Store the calculated weights\n    self.weights_ = weights\n", "CodeBleu_score": 31.636745797190223}, {"code": "def _calculate_weights(self) -> None\n    counts = np.bincount(self.ensemble_indices_)\n    self.weights_ = np.zeros(self.n_input_models_)\n    for i in range(self.n_input_models_):\n        self.weights_[i] = counts[i] / self.n_input_models_\n    if np.sum(self.weights_) < 1:\n        self.weights_ /= np.sum(self.weights_)\n", "CodeBleu_score": 22.90251404290605}, {"code": "def _calculate_weights(self) -> None\n    counts = np.bincount(self.ensemble_members)\n    self.weights_ = np.zeros(len(self.input_models_))\n    for i in range(len(self.input_models_)):\n        self.weights_[i] = counts[i] / len(self.ensemble_members)\n    if np.sum(self.weights_) < 1:\n        self.weights_ = self.weights_ / np.sum(self.weights_)\n", "CodeBleu_score": 25.92406469222327}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    y_pred = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.target_type_ != \"multilabel-indicator\":\n        assert np.allclose(np.sum(y_pred, axis=1), 1)\n    self.assert_proba(y_pred)\n    return y_pred\n\n", "CodeBleu_score": 26.784945703092962}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    y_pred = super(OneVsRestClassifier, self).predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.multilabel_:\n        return y_pred\n    else:\n        assert np.allclose(np.sum(y_pred, axis=1), 1)\n        assert np.all(y_pred >= 0)\n        assert np.all(y_pred <= 1)\n        return y_pred", "CodeBleu_score": 29.75622368698565}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    y_pred = super(OneVsRestClassifier, self).predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.multilabel_:\n        return y_pred\n    else:\n        assert_almost_equal(np.sum(y_pred, axis=1), np.ones(y_pred.shape[0]))\n        assert_array_less_equal(np.zeros(y_pred.shape[0]), y_pred)\n        assert_array_less_equal(y_pred, np.ones(y_pred.shape[0]))\n        return y_pred\n\n", "CodeBleu_score": 30.24995497057823}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return Ybin.dot(np.arange(Ybin.shape[1]))\n\n", "CodeBleu_score": 27.13397993791789}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return Ybin.dot(np.arange(Ybin.shape[1]))\n", "CodeBleu_score": 27.13397993791789}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return Ybin.dot(np.arange(Ybin.shape[1]))\n\n", "CodeBleu_score": 27.13397993791789}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config.classifier, self.config.feature_preprocessor)\n        classifier_weights, feature_preprocessor_weights = balancing.get_weights()\n        self.init_params[\"classifier__weights\"] = classifier_weights\n        self.init_params[\"feature_preprocessor__weights\"] = feature_preprocessor_weights\n    self.set_hyperparameters(self.config, self.init_params)\n    fit_params = {**self.init_params, **fit_params}\n    return super().fit_transformer(X, y, fit_params=fit_params)", "CodeBleu_score": 47.885892758694915}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.config[\"balancing\"] == \"weighting\":\n        balancing = Balancing(self.config, self.init_params)\n        fit_params[\"class_weight\"] = balancing.get_class_weight(y)\n        fit_params[\"sample_weight\"] = balancing.get_sample_weight(X, y)\n    self.update_init_params(self.config)\n    self.set_hyperparameters(self.config, self.init_params)\n    fit_params = {**fit_params, **self.get_additional_fit_params()}\n    return super().fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 45.91310042313371}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.config[\"balancing\"] == \"weighting\":\n        balancing = Balancing(X, y, self.config, self.init_params)\n        fit_params[\"classifier_weight\"] = balancing.get_classifier_weight()\n        fit_params[\"feature_preprocessor_weight\"] = balancing.get_feature_preprocessor_weight()\n    self.init_params.update(self.config)\n    self.hyperparameters = {**self.config, **self.init_params}\n    fit_params = {**fit_params, **self.hyperparameters}\n    return super().fit_transformer(X, y, fit_params)", "CodeBleu_score": 45.71465899968182}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n    import numpy as np\n    import pandas as pd\n    from sklearn.datasets import load_boston\n    from sklearn.linear_model import TweedieRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import Ridge\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import Lasso\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import ElasticNet\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LassoLars\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LassoLarsIC\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LassoCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import ElasticNetCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LassoLarsCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LassoLarsICCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeClassifierCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeClassifier\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeClassifierCV\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeClassifier\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeClassifierCV\n    from sklearn.preprocessing import PolynomialFeatures", "CodeBleu_score": 4.343046789727426}, {"code": "def test_glm_identity_link_func()\n    from pyspark.ml.linalg import Vectors\n    from pyspark.sql import SparkSession\n    from pyspark.sql.types import DoubleType, StructField, StructType\n    from pyspark.sql.functions import col, lit\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark_models.ml.glm import GLM\n    from pyspark_models.ml.glm import IdentityLink\n    from pyspark_models.ml.glm import TweedieDistribution\n    from pyspark_models.ml.glm import PowerLink\n    from pyspark_models.ml.glm import PowerLinkFunc\n    import statsmodels.api as sm\n    import numpy as np\n\n    # Create a SparkSession\n    spark = SparkSession.builder.appName(\"TestGLM\").getOrCreate()\n\n    # Create a sample dataset\n    data = [\n        (1.0, 2.0, 3.0, 1.0),\n        (2.0, 4.0, 6.0, 1.0),\n        (3.0, 6.0, 9.0, 1.0),\n        (4.0, 8.0, 12.0, 1.0),\n        (5.0, 10.0, 15.0, 1.0),\n    ]\n    schema = StructType(\n        [\n            StructField(\"feature1\", DoubleType(), True),\n            StructField(\"feature2\", DoubleType(), True),\n            StructField(\"feature3\", DoubleType(), True),\n            StructField(\"label\", DoubleType(), True),\n        ]\n    )\n    df = spark.createDataFrame(data, schema)\n\n    # Define the input columns\n    input_cols = [\"feature1\", \"feature2\", \"feature3\"]\n\n    # Define the output column\n    output_col = \"label\"\n\n    # Define the family and link\n    family = TweedieDistribution()\n    link = PowerLink()\n\n    # Create an instance of the GLM wrapper\n    glm = GLM(\n        input", "CodeBleu_score": 28.43465235338903}, {"code": "def test_glm_identity_link_func()\n    import numpy as np\n    import pandas as pd\n    import pytest\n    from sklearn.datasets import load_iris\n    from featuregen.glm import GLM\n    from featuregen.assembler import Assembler\n\n    # Load the iris dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Create a GLM model with an identity link function\n    model = GLM(family=\"Tweedie\", link=\"identity\")\n\n    # Fit the model to the data\n    model.fit(X, y)\n\n    # Create an assembler\n    assembler = Assembler(model)\n\n    # Assemble the model into an expression\n    expression = assembler.assemble()\n\n    # Define the expected expression\n    expected_expression = \"Tweedie(identity(Intercept + sepal length (cm) + sepal width (cm) + petal length (cm) + petal width (cm)), power=1, var_power=1, link_power=1)\"\n\n    # Assert that the assembled expression matches the expected expression\n    assert expression == expected_expression\n", "CodeBleu_score": 23.111659271551588}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n\n    new_col_name = f\"{col.name}_{agg_name}\"\n    return self.df.groupby(self.group_cols)[col.name].agg(agg_func).reset_index(name=new_col_name)\n\n", "CodeBleu_score": 58.7158407163698}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg.__name__ if isinstance(agg, Callable) else agg\n        agg_func = agg\n\n    new_col_name = f\"{col}__{agg_name}\"\n    return self.groupby(col, as_index=False).agg(**{new_col_name: agg_func})\n\n", "CodeBleu_score": 52.386982551548066}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n    new_col_name = f\"{col}__{agg_name}\"\n    return self.groupby(col).agg(agg_func).reset_index(name=new_col_name)[new_col_name]\n\n", "CodeBleu_score": 56.91505781343269}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n    # Calculate correlation coefficient\n    r = np.corrcoef(x, y)[0][1]\n    # Handle hue groups\n    if 'hue' in kws:\n        # Get unique hue groups\n        hue_groups = x.groupby(kws['hue']).groups\n        # Initialize index for hue group annotation\n        hue_index = 0\n        # Loop over hue groups\n        for hue_group, hue_rows in hue_groups.items():\n            # Exclude rows where x or y is NaN\n            x_group = x[hue_rows].dropna()\n            y_group = y[hue_rows].dropna()\n            # Calculate correlation coefficient for hue group\n            r_group = np.corrcoef(x_group, y_group)[0][1]\n            # Adjust font size based on number of hue groups and correlation value\n            fontsize = 12 - 0.5 * len(hue_groups) + abs(r_group) * 2\n            # Get appropriate axis\n            if 'ax' in kws:\n                ax = kws['ax']\n            elif 'axes' in kws:\n                ax = kws['axes'][hue_index]\n            else:\n                ax = plt.gca()\n            # Annotate axis with correlation coefficient, adjusting position based on hue group index\n            ax.annotate(\"r = {:.2f}\".format(r_group), xy=(0.05, 0.9 - 0.1 * hue_index), xycoords='axes fraction', fontsize=fontsize)\n            # Increment hue group index\n            hue_index += 1\n    else:\n        # Adjust font size based on correlation value\n        fontsize = 12 + abs(r) * 2\n        # Get appropriate axis\n        if 'ax' in kws:\n            ax = kws['ax']\n        elif 'axes' in kws:\n            ax = kws['axes']\n        else:\n            ax = plt.gca()\n        # Annotate axis with correlation", "CodeBleu_score": 31.177907880283897}, {"code": "def _corrfunc(self, x, y, **kws)\n    import seaborn as sns\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from scipy.stats import pearsonr\n    import matplotlib.font_manager as fm\n\n    # Set the default font size\n    font_size = 12\n\n    # Get the number of hue groups\n    n_hue = len(self.hue_names) if self.hue_names is not None else 1\n\n    # Get the current axis\n    ax = plt.gca()\n\n    # Get the x and y data\n    x_data = x.get_data()\n    y_data = y.get_data()\n\n    # Compute the correlation coefficient\n    r, _ = pearsonr(x_data, y_data)\n\n    # Adjust the font size based on the number of hue groups and the correlation value\n    if n_hue > 1:\n        font_size = 10\n    elif r > 0.8:\n        font_size = 14\n    elif r > 0.6:\n        font_size = 12\n    elif r > 0.4:\n        font_size = 10\n    else:\n        font_size = 8\n\n    # Get the font properties\n    font_prop = fm.FontProperties(size=font_size)\n\n    # Get the hue group index\n    hue_idx = self.hue_names.index(y.name) if self.hue_names is not None else 0\n\n    # Annotate the plot with the correlation coefficient\n    if sns.__version__.startswith('0.11'):\n        ax.annotate(\"r = {:.2f}\".format(r), xy=(0.5, 0.5), xycoords='axes fraction', ha='center', va='center', fontproperties=font_prop)\n    else:\n        ax.annotate(\"r = {:.2f}\".format(r), xy=(0.5, 0.5), xycoords='axes fraction', ha='center', va='center', fontproperties=font", "CodeBleu_score": 30.532175673735317}, {"code": "def _corrfunc(self, x, y, **kws)\n    hue = kws.get(\"hue\")\n    if hue is not None:\n        groups = x.groupby(hue)\n    else:\n        groups = [(\"\", x)]\n    ax = plt.gca()\n    for i, (key, xi) in enumerate(groups):\n        if hue is not None:\n            mask = y[hue].notna() & x[hue].notna()\n            corr = np.corrcoef(y[mask], x[mask])[0][1]\n        else:\n            corr = np.corrcoef(y, x)[0][1]\n        fontsize = len(groups) * 12\n        if len(groups) > 1:\n            ax.annotate(\n                \"r = {:.2f}\".format(corr),\n                xy=(0.05, 0.65 + (i * 0.1)),\n                xycoords=ax.transAxes,\n                fontsize=fontsize,\n            )\n        else:\n            ax.annotate(\n                \"r = {:.2f}\".format(corr),\n                xy=(0.05, 0.75),\n                xycoords=ax.transAxes,\n                fontsize=fontsize,\n            )\n\n", "CodeBleu_score": 34.062357262760024}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        import decimal\n        return float(decimal.Decimal(src).quantize(decimal.Decimal(f'1E-{rounddigit}'), rounding=decimal.ROUND_HALF_UP))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError('method must be one of \"decimal\", \"sig\", or \"format\"')\n\n", "CodeBleu_score": 56.5306608062807}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        from decimal import Decimal\n        return float(Decimal(src).quantize(Decimal(f'1.{\"0\" * rounddigit}'), rounding='ROUND_HALF_UP'))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError('Invalid method. Choose either \"decimal\", \"sig\", or \"format\".')\n\n", "CodeBleu_score": 56.37105356984284}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        import decimal\n        with decimal.localcontext() as ctx:\n            ctx.rounding = decimal.ROUND_HALF_UP\n            ctx.prec = rounddigit\n            return ctx.create_decimal_from_float(src)\n    elif method == 'format':\n        return float(f'%.{rounddigit}g' % src)\n    else:\n        raise ValueError(\"Invalid rounding method. Choose 'decimal', 'sig', or 'format'.\")\n", "CodeBleu_score": 68.94558888825875}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    dstdict = dict()\n    for k, v in srcdict.items():\n        if isinstance(v, float):\n            if method == 'decimal':\n                dstdict[k] = cls._round_digits(v, rounddigit)\n            elif method == 'sig':\n                dstdict[k] = cls._round_sigdigits(v, rounddigit)\n            elif method == 'format':\n                dstdict[k] = cls._round_formatdigits(v, rounddigit)\n            else:\n                raise ValueError(f\"Invalid rounding method: {method}\")\n        else:\n            dstdict[k] = v\n    return dstdict\n", "CodeBleu_score": 42.35658636571307}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    if method == 'decimal':\n        return {k: _round_digits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    elif method == 'sig':\n        return {k: _round_sigdigits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    elif method == 'format':\n        return {k: _round_format(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    else:\n        raise ValueError(f\"Invalid method: {method}\")\n\n", "CodeBleu_score": 31.99075703291929}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    else:\n        return {k: cls._round_digits(v, rounddigit, method) if isinstance(v, float) else v for k, v in srcdict.items()}\n\n", "CodeBleu_score": 31.910233957459468}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n\n    # Create a DataFrame from the x and y values\n    data = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue_data is provided, add a hue field to the DataFrame\n    if hue_data is not None:\n        data[hue_name] = hue_data\n\n    # Plot the scatter plot using Seaborn's scatterplot function\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # Set the title of the legend if not specified\n    if legend_kws is None or 'title' not in legend_kws:\n        legend_kws = {'title': hue_name}\n\n    # Add a legend to the plot\n    ax.legend(**legend_kws)\n", "CodeBleu_score": 41.09082129345738}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    # Combine x and y values into a DataFrame with specified column names\n    data = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue_data is provided, add a hue field to the DataFrame for color-coding the points\n    if hue_data is not None:\n        data[hue_name] = hue_data\n\n    # Plot the scatter plot using Seaborn's scatterplot function, applying any additional keyword arguments for the plot and legend\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # If no title is specified for the legend, set the title to the hue field name\n    if legend_kws is None or 'title' not in legend_kws:\n        legend_kws = {'title': hue_name}\n\n    # Add a legend to the plot, with any additional keyword arguments for the legend\n    ax.legend(**legend_kws)\n\n    return ax", "CodeBleu_score": 35.70323168755968}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[hue_name] = hue_data\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    if ax.get_legend() is not None:\n        if legend_kws.get('title') is None:\n            ax.get_legend().set_title(hue_name)\n        ax.legend(**legend_kws)\n    return ax\n\n", "CodeBleu_score": 42.629044976075214}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    main_data = self.get_main_data()\n    exog = self.get_exog_data()\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n        model = self.model_class(main_data, exog=exog, **fit_args)\n        model = model.fit()\n    return model\n", "CodeBleu_score": 37.20519413781349}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n        self.model = self.model_class(\n            self.main_time_series.data,\n            exog=self.exogenous_data,\n            **fit_args,\n        )\n    return self", "CodeBleu_score": 33.24876124982935}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n        if self.exogenous_data is not None:\n            model = self.model(\n                self.data, exog=self.exogenous_data, **fit_args\n            )\n        else:\n            model = self.model(self.data, **fit_args)\n        return model", "CodeBleu_score": 34.58378236358855}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n    if self.model_type == \"holt_winters\":\n        transformed_data = self.transformed_data\n        model = ExponentialSmoothing(\n            transformed_data,\n            trend=kwargs.get(\"trend\", \"add\"),\n            damped=kwargs.get(\"damped\", False),\n            seasonal=kwargs.get(\"seasonal\", \"add\"),\n            seasonal_periods=kwargs.get(\"seasonal_periods\", 12),\n        )\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logging.error(\n                \"pmdarima is not installed. Please install it using 'pip install pmdarima'.\"\n            )\n            sys.exit(1)\n        transformed_data = self.transformed_data\n        model = AutoARIMA(**kwargs)\n        model.fit(transformed_data)\n    elif self.model_type == \"sarimax\":\n        transformed_data = self.transformed_data\n        exogenous_vars = kwargs.get(\"exogenous_vars\", None)\n        if exogenous_vars is not None:\n            model = SARIMAX(\n                transformed_data,\n                exog=exogenous_vars,\n                order=kwargs.get(\"order\", (1, 1, 1)),\n                seasonal_order=kwargs.get(\"seasonal_order\", (0, 0, 0, 0)),\n            )\n        else:\n            model = SARIMAX(\n                transformed_data,\n                order=kwargs.get(\"order\", (1, 1, 1)),\n                seasonal_order=kwargs.get(\"seasonal_order\", (0, 0, 0, 0)),\n            )\n    else:\n        raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    return model\n\n", "CodeBleu_score": 39.85823281674444}, {"code": "def create_model(self, **kwargs)\n    if self.model_type == \"holt_winters\":\n        data = self.transformed_data\n        model = ExponentialSmoothing(\n            data,\n            trend=kwargs.get(\"trend\", \"add\"),\n            seasonal=kwargs.get(\"seasonal\", \"add\"),\n            seasonal_periods=kwargs.get(\"seasonal_periods\", 12),\n        )\n        return model\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logging.error(\n                \"pmdarima is not installed. Please install it with `pip install pmdarima`\"\n            )\n            sys.exit(1)\n        data = self.transformed_data\n        model = AutoARIMA(**kwargs)\n        return model\n    elif self.model_type == \"sarimax\":\n        data = self.transformed_data\n        exog = kwargs.get(\"exog\", None)\n        if exog is not None:\n            model = SARIMAX(data, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(data, **kwargs)\n        return model\n    else:\n        raise ValueError(f\"Unrecognized model type: {self.model_type}\")\n\n", "CodeBleu_score": 45.98824727593415}, {"code": "def create_model(self, **kwargs)\n    if self.model_type == \"holt_winters\":\n        data = self.transformed_data\n        model = ExponentialSmoothing(data, **kwargs)\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            self.logger.error(\"pmdarima is not installed. Please install it to use AutoARIMA.\")\n            sys.exit(1)\n        model = AutoARIMA(**kwargs)\n    elif self.model_type == \"sarimax\":\n        data = self.transformed_data\n        exog = self.exog\n        if exog is not None:\n            model = SARIMAX(data, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(data, **kwargs)\n    else:\n        raise ValueError(f\"Invalid model type: {self.model_type}\")\n    return model\n\n", "CodeBleu_score": 41.78970768464215}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n    import numpy\n    import chainerx\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, chainerx.ndarray):\n        return array\n    else:\n        return array\n\n", "CodeBleu_score": 60.28340415902021}, {"code": "def _to_chx(array)\n    if isinstance(array, np.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, cp.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 65.66390979550852}, {"code": "def _to_chx(array)\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, cupy.ndarray):\n        if cupy is None:\n            raise RuntimeError(\n                'CuPy is not available. Please install CuPy.\\n'\n                'See also https://docs-cupy.chainer.org/en/stable/install.html\\n'\n                'If you want to use CuPy with ChainerX, please install it with the same version of CuPy as follows:\\n'\n                '  pip install cupy-cudaXXX\\n'\n                'where ``XXX`` is the CUDA version used in your environment.\\n'\n                'See also https://docs-cupy.chainer.org/en/stable/install.html#install-cupy-with-cuda-support')\n        return _from_cupy(array)\n    else:\n        return array", "CodeBleu_score": 59.45984461200721}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if not all((y.ndim == x.ndim for y in ys)):\n        split_sizes = np.array(indices_or_sections)\n        if split_sizes.size == 1:\n            split_sizes = np.array([split_sizes[0], x.shape[axis] - split_sizes[0]])\n        split_sizes = np.cumsum(split_sizes[:-1])\n        expected_shapes = np.array_split(np.zeros_like(x), split_sizes, axis=axis)\n        ys = [y.reshape(expected_shape) if y.ndim != x.ndim else y for (y, expected_shape) in zip(ys, expected_shapes)]\n    return ys\n", "CodeBleu_score": 38.09823855983735}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if not all((y.ndim == x.ndim for y in ys)):\n        split_sizes = np.split(np.zeros_like(x), indices_or_sections, axis=axis)\n        ys = [y.reshape([s.shape[0] if i == axis else s for (i, s) in enumerate(split_sizes)]) for y in ys]\n    return ys", "CodeBleu_score": 35.21664480465175}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if not all((y.ndim == x.ndim for y in ys)):\n        split_sizes = np.split(np.zeros_like(x), indices_or_sections, axis=axis)\n        ys = [y.reshape(split_sizes[i].shape) if y.ndim != x.ndim else y for i, y in enumerate(ys)]\n    return ys\n", "CodeBleu_score": 37.352586170135844}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    xp = cuda.get_array_module(*batch)\n    concat = xp.concatenate(batch, axis=0)\n    sections = np.cumsum([len(x) for x in batch[:-1]], dtype=np.int32)\n    concat_dev = cuda.to_gpu(concat)\n    batch_dev = cuda.cupy.split(concat_dev, sections)\n    return batch_dev\n\n", "CodeBleu_score": 67.08117713123079}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    xp = cuda.get_array_module(*batch)\n    concat_example = xp.concatenate(batch, axis=0)\n    sections = np.cumsum([len(x) for x in batch[:-1]], dtype=np.int32)\n    concat_dev = cuda.to_gpu(concat_example)\n    batch_dev = cuda.cupy.split(concat_dev, sections)\n    return batch_dev\n\n", "CodeBleu_score": 66.66994065393041}, {"code": "def to_device_batch(batch):\n    device = config.device\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    else:\n        xp = cuda.cupy if device >= 0 else numpy\n        concat_example = []\n        for example in batch:\n            concat_example.append(xp.array(example))\n        concat_example_x = xp.concatenate(concat_example, axis=0)\n        sections = numpy.cumsum([len(x)\n                                 for x in concat_example[:-1]], dtype=numpy.int32)\n        concat_example_x = cuda.to_gpu(concat_example_x)\n        batch = cuda.cupy.split(concat_example_x, sections)\n        concat_example = []\n        for example in batch:\n            concat_example.append(cuda.to_cpu(example))\n        return concat_example\n\n", "CodeBleu_score": 55.10268287731328}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n    import torch\n    import os\n    from torchvision.utils import save_image\n    from torchvision.utils import make_grid\n    from torch.utils.data import DataLoader\n    from torchvision.datasets import CIFAR10\n    from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n    from torchvision.transforms import RandomHorizontalFlip, RandomResizedCrop\n    from torchvision.transforms import ColorJitter, RandomGrayscale, RandomApply\n    from torchvision.transforms import GaussianBlur\n    from torchvision.transforms import InterpolationMode\n    from torchvision.transforms import RandomRotation, RandomVerticalFlip\n    from torchvision.transforms import RandomErasing\n    from torchvision.transforms import RandomAffine\n    from torchvision.transforms import RandomPerspective\n    from torchvision.transforms import RandomInvert\n    from torchvision.transforms import RandomAdjustSharpness\n    from torchvision.transforms import RandomAutocontrast\n    from torchvision.transforms import RandomEqualize\n    from torchvision.transforms import RandomPosterize\n    from torchvision.transforms import RandomSolarize\n    from torchvision.transforms import RandomAdjustSharpness\n    from torchvision.transforms import RandomAutocontrast\n    from torchvision.transforms import RandomEqualize\n    from torchvision.transforms import RandomPosterize\n    from torchvision.transforms import RandomSolarize\n    from torchvision.transforms import RandomAdjustSharpness\n    from torchvision.transforms import RandomAutocontrast\n    from torchvision.transforms import RandomEqualize\n    from torchvision.transforms import RandomPosterize\n    from torchvision.transforms import RandomSolarize\n    from torchvision.transforms import RandomAdjustSharpness\n    from torchvision.transforms import RandomAutocontrast\n    from torchvision.transforms import RandomEqualize\n    from torchvision.transforms import RandomPosterize\n    from torchvision.transforms import", "CodeBleu_score": 5.475710913178133}, {"code": "def make_image(trainer)\n    with torch.no_grad():\n        # Set fixed random number seed\n        torch.manual_seed(0)\n        trainer.model.eval()\n\n        # Sample noise\n        z = torch.randn(64, 100).to(trainer.device)\n\n        # Generate and save the images\n        trainer.save_generated_images(\n            z,\n            f\"outputs/images/{str(trainer.curr_epoch).zfill(3)}.png\",\n            nrow=8,\n            normalize=True,\n        )\n\n", "CodeBleu_score": 12.921582353341146}, {"code": "def make_image(trainer)\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional as F\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import matplotlib.gridspec as gridspec\n    import random\n    import time\n    import math\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional as F\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import matplotlib.gridspec as gridspec\n    import random\n    import time\n    import math\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional as F\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import matplotlib.gridspec as gridspec\n    import random\n    import time\n    import math\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional as F\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import matplotlib.gridspec as gridspec\n    import random\n    import time\n    import math\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional as F\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import matplotlib.gridspec as gridspec\n    import random\n    import time\n    import math\n    import os\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    import torch.nn.functional", "CodeBleu_score": 6.903357463301127}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"?bhilqpBHILQPefdFD\"\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 47.370231997805654}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"?bhilqpBHILQPefdFD\"\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 47.370231997805654}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"?bhilqpBHILQPefdFD\"\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)", "CodeBleu_score": 47.370231997805654}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n\n    if g is None:\n        g = plt.gca()\n    elif isinstance(g, plt.Axes):\n        pass\n    elif hasattr(g, \"axes\"):\n        g = g.axes\n    else:\n        raise ValueError(\"g must be a Visualizer, matplotlib Axes object, or None\")\n\n    if len(labels) != len(colors):\n        raise ValueError(\"labels and colors must have the same length\")\n\n    legend_handles = [\n        patches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n    ]\n    return g.legend(handles=legend_handles, **legend_kwargs)", "CodeBleu_score": 66.0646698258533}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.patches as mpatches\n    if g is not None:\n        if isinstance(g, Visualizer):\n            ax = g.ax\n        else:\n            ax = g\n    else:\n        ax = plt.gca()\n\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels and colors must match\")\n\n    handles = [mpatches.Patch(color=colors[i], label=labels[i]) for i in range(len(labels))]\n\n    return ax.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 56.85423076850083}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n\n    if g is None:\n        g = plt.gca()\n    elif isinstance(g, plt.Axes):\n        pass\n    else:\n        g = g.axes\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels and colors must match.\")\n    handles = [patches.Patch(color=colors[i], label=labels[i]) for i in range(len(labels))]\n    return g.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 57.40426009568013}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n    import time\n    import matplotlib.pyplot as plt\n    from yellowbrick.features import ParallelCoordinates\n\n    # Create a plot axis and initialize the visualizer\n    fig, ax = plt.subplots()\n    visualizer = ParallelCoordinates(fast=fast, ax=ax)\n\n    # Record the start time\n    start_time = time.time()\n\n    # Fit and transform the data\n    visualizer.fit_transform(X, y)\n\n    # Calculate the elapsed time\n    elapsed_time = time.time() - start_time\n\n    # Clear and close the plot to free up resources\n    plt.close()\n\n    return elapsed_time\n", "CodeBleu_score": 61.215282442376065}, {"code": "def pcoords_time(X, y, fast=True)\n    import time\n    import matplotlib.pyplot as plt\n    from yellowbrick.features.pcoords import ParallelCoordinates\n    _, ax = plt.subplots()\n    visualizer = ParallelCoordinates(ax=ax, fast=fast)\n    start = time.time()\n    visualizer.fit_transform(X, y)\n    end = time.time()\n    plt.clf()\n    plt.close()\n    return end - start\n\n", "CodeBleu_score": 56.143983501081294}, {"code": "def pcoords_time(X, y, fast=True)\n    import time\n    import matplotlib.pyplot as plt\n    from yellowbrick.features.pcoords import ParallelCoordinates\n\n    # Create a plot axis\n    _, ax = plt.subplots()\n\n    # Initialize the ParallelCoordinates visualizer with the specified speed setting\n    visualizer = ParallelCoordinates(ax=ax, fast=fast)\n\n    # Record the start time\n    start_time = time.time()\n\n    # Fit and transform the input data X and y with the visualizer\n    visualizer.fit_transform(X, y)\n\n    # Calculate the elapsed time\n    elapsed_time = time.time() - start_time\n\n    # Clear and close the current plot to free up resources\n    plt.clf()\n    plt.close()\n\n    # Return the elapsed time\n    return elapsed_time\n\n", "CodeBleu_score": 56.36659732506504}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = self.alpha\n    if \"alpha\" in kwargs:\n        alpha = kwargs[\"alpha\"]\n    for i in range(len(X)):\n        if y[i] == 0:\n            color = \"red\"\n        else:\n            color = \"blue\"\n        self.ax.plot(X[i], np.arange(0, len(X[i])), color=color, alpha=alpha)\n    return self.ax\n", "CodeBleu_score": 34.16407411603782}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    for i in range(X.shape[0]):\n        self.ax.plot(X[i, :], np.arange(X.shape[1]) + i, color=self.colors[y[i]], alpha=alpha)\n    return self.ax\n", "CodeBleu_score": 29.1495150335236}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = self.alpha if self.alpha else 0.25\n    for i in range(X.shape[0]):\n        instance = X[i, :]\n        label = y[i]\n        color = self.colors[label]\n        self.ax.plot(self.increment_sequence, instance, color=color, alpha=alpha, **kwargs)\n    return self.ax\n\n", "CodeBleu_score": 51.85134860355575}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    X = np.hstack([X, np.ones((X.shape[0], 1))])\n    increments = [None]\n    for c in np.unique(y):\n        color = self.colors[c]\n        instances = X[y == c]\n        plt.plot(np.arange(instances.shape[0]), instances, color, alpha=alpha)\n        increments.append(instances.shape[0])\n    plt.plot(np.cumsum(increments), np.ones(len(increments)), 'k-', linewidth=2)\n    return plt.gca()\n", "CodeBleu_score": 37.01654903163353}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    X = np.c_[X, np.ones(X.shape[0])]\n    increments = [None]\n    for i, label in enumerate(np.unique(y)):\n        color = self.colors[i]\n        instances = X[y == label]\n        plt.plot(instances.T, c=color, alpha=alpha)\n        increments.append(instances.shape[0])\n    plt.vlines(increments, ymin=X.min(), ymax=X.max(), linestyles='dashed', colors='gray')\n    return plt.gca()\n", "CodeBleu_score": 30.238384332441914}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    X = np.c_[X, np.ones(X.shape[0])]\n    increments = [None]\n    for label in np.unique(y):\n        color = self.colors[label]\n        instances = X[y == label]\n        plt.plot(instances.T, c=color, alpha=alpha)\n        increments.extend(list(range(instances.shape[0])))\n        increments.append(None)\n    plt.xticks(increments)\n    return plt.gca()\n\n", "CodeBleu_score": 29.238954919249093}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    import matplotlib.pyplot as plt\n    from yellowbrick.features import RadViz, Rank2D\n\n    # Load the occupancy data\n    occupancy = pd.read_csv(\n        \"https://raw.githubusercontent.com/nicknochnack/YellowbrickUsage/main/data/occupancy.csv\"\n    )\n\n    # Load the concrete data\n    concrete = pd.read_excel(\n        \"https://raw.githubusercontent.com/nicknochnack/YellowbrickUsage/main/data/Concrete_Data.xls\"\n    )\n\n    # Set up the side-by-side axes grid\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\n    # Perform RadViz analysis on the occupancy data\n    features = [\n        \"temperature\",\n        \"relative_humidity\",\n        \"light\",\n        \"C02\",\n        \"humidity\",\n    ]\n    radviz = RadViz(features=features, classes=[\"unoccupied\", \"occupied\"], size=(500, 350))\n    radviz.fit(occupancy[features], occupancy[\"occupancy\"])\n    radviz.transform(occupancy[features])\n    radviz.finalize()\n\n    # Plot the RadViz analysis on the left subplot\n    radviz.ax.set_title(\"RadViz - Occupancy Data\")\n    for a in ax:\n        a.set_aspect(\"equal\")\n        a.grid(False)\n        a.axes.xaxis.set_ticks([])\n        a.axes.yaxis.set_ticks([])\n    radviz.ax.set_xticklabels([])\n    radviz.ax.set_yticklabels([])\n    radviz.ax.set_xticks([])\n    radviz.ax.set_yticks([])\n    radviz.ax.set_xlim(auto=True)\n    radviz.", "CodeBleu_score": 32.628030464535094}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from yellowbrick.features import RadViz, Rank2D\n\n    # Load the occupancy data\n    occupancy_data = pd.read_csv(\"data/occupancy_data/datatraining.txt\")\n\n    # Perform RadViz analysis\n    radviz = RadViz(\n        classes=[\"unoccupied\", \"occupied\"],\n        features=[\n            \"temperature\",\n            \"relative humidity\",\n            \"light\",\n            \"C02\",\n            \"humidity\",\n        ],\n    )\n    radviz.fit(occupancy_data.iloc[:, :5], occupancy_data.iloc[:, 5])\n    radviz.transform(occupancy_data.iloc[:, :5])\n    radviz.finalize()\n\n    # Load the concrete data\n    concrete_data = pd.read_excel(\"data/concrete_data.xls\")\n\n    # Perform Rank2D analysis\n    rank2d = Rank2D(algorithm=\"pearson\")\n    rank2d.fit(concrete_data.iloc[:, :8], concrete_data.iloc[:, -1])\n    rank2d.transform(concrete_data.iloc[:, :8])\n\n    # Set up the figure and axes grid\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\n    # Plot the RadViz analysis on the left subplot\n    radviz.poof(ax=axes[0])\n\n    # Plot the Rank2D analysis on the right subplot\n    rank2d.poof(ax=axes[1])\n\n    # Save the figure\n    fig.savefig(fname)\n\n", "CodeBleu_score": 39.43322312030579}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    from yellowbrick.datasets import load_concrete\n    from yellowbrick.datasets import load_occupancy\n    from yellowbrick.features import RadViz\n    from yellowbrick.features import Rank2D\n    import matplotlib.pyplot as plt\n\n    # Load the classification data set\n    X, y = load_occupancy()\n\n    # Specify the target classes\n    classes = [\"unoccupied\", \"occupied\"]\n\n    # Instantiate the visualizer\n    visualizer = RadViz(classes=classes, size=(1080, 720))\n\n    visualizer.fit(X, y)  # Fit the data to the visualizer\n    visualizer.transform(X)  # Transform the data\n    visualizer.finalize()  # Finalize and render the figure\n\n    # Load the regression data set\n    X, y = load_concrete()\n\n    # Instantiate the visualizer\n    visualizer = Rank2D(size=(1080, 720))\n\n    visualizer.fit(X, y)  # Fit the data to the visualizer\n    visualizer.transform(X)  # Transform the data\n    visualizer.finalize()  # Finalize and render the figure\n\n    # Save the figure to the specified file path\n    plt.savefig(fname)\n\n", "CodeBleu_score": 30.605501958152935}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.datasets import load_diabetes\n    from sklearn.model_selection import train_test_split\n\n    # Load the dataset\n    X, y = load_diabetes(return_X_y=True)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n    # Define the range of alpha values for regularization\n    alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n    # Plot the prediction error using LassoCV\n    lasso = LassoCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    axes[0].plot(alphas, lasso.mse_path_.mean(axis=-1), label=\"LassoCV\")\n    axes[0].set_xscale(\"log\")\n    axes[0].set_xlabel(\"alpha\")\n    axes[0].set_ylabel(\"Mean squared error\")\n    axes[0].set_title(\"LassoCV\")\n    axes[0].legend()\n\n    # Plot the residuals using RidgeCV\n    ridge = RidgeCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    axes[1].plot(alphas, ridge.cv_values_.mean(axis=0), label=\"RidgeCV\")\n    axes[1].set_xscale(\"log\")\n    axes[1].set_xlabel(\"alpha\")\n    axes[1].set_ylabel(\"Residual sum of squares\")\n    ax", "CodeBleu_score": 36.79564749072765}, {"code": "def regression(fname=\"regression.png\")\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import load_diabetes\n\n    # Create a figure with two subplots of size 18x6 inches\n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-6, 6, 200)\n\n    # Load the concrete dataset\n    X, y = load_diabetes(return_X_y=True)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Plot the prediction error using LassoCV\n    lasso = LassoCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    mse = mean_squared_error(y_test, lasso.predict(X_test))\n    ax[0].plot(lasso.alphas_, lasso.mse_path_.mean(axis=-1), label='Mean squared error')\n    ax[0].axvline(lasso.alpha_, linestyle='dashed', color='k', label='alpha: CV estimate')\n    ax[0].hlines(mse, xmin=ax[0].xaxis.get_data_interval()[0], xmax=lasso.alpha_, linestyle='dashed', color='k', label='alpha: MSE estimate')\n    ax[0].set_xlabel(r'$\\alpha$')\n    ax[0].set_ylabel('Mean squared error')\n    ax[0].set_xscale('log')\n    ax[0].set_title('LassoCV')\n    ax", "CodeBleu_score": 37.12237599756705}, {"code": "def regression(fname=\"regression.png\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.model_selection import train_test_split\n\n    X, y = load_diabetes(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    alphas = np.logspace(-10, 10, 21)\n\n    model = LassoCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    ax1.plot(model.alphas_, model.mse_path_.mean(axis=-1))\n    ax1.set_xlabel(\"alpha\")\n    ax1.set_ylabel(\"Mean Squared Error\")\n    ax1.set_xscale(\"log\")\n    ax1.set_title(\"LassoCV\")\n\n    model = RidgeCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    ax2.plot(model.alphas_, model.cv_values_.mean(axis=0))\n    ax2.set_xlabel(\"alpha\")\n    ax2.set_ylabel(\"Residual Sum of Squares\")\n    ax2.set_xscale(\"log\")\n    ax2.set_title(\"RidgeCV\")\n\n    fig.tight_layout()\n    fig.savefig(fname)\n    return fname\n\n", "CodeBleu_score": 39.55663802336434}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n    # Import necessary libraries\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.datasets import load_digits\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import plot_det_curve\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision", "CodeBleu_score": 5.923177362984343}, {"code": "def classification(fname=\"classification.png\")\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\n    # Load the spam dataset\n    spam = pd.read_csv(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/spam.csv\")\n\n    # Split the dataset into train and test sets\n    X = spam.drop(columns=[\"type\"])\n    y = spam[\"type\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a MultinomialNB classifier and fit it to the training data\n    nb = MultinomialNB()\n    nb.fit(X_train, y_train)\n\n    # Create a ClassificationReport visualizer and fit it to the training data\n    nb_visualizer = ClassificationReport(nb, support=True, cmap=\"Blues\")\n    nb_visualizer.fit(X_train, y_train)\n\n    # Create a LogisticRegression model and fit it to the entire dataset\n    lr = LogisticRegression()\n    lr.fit(X, y)\n\n    # Create a DiscriminationThreshold visualizer and fit it to the entire dataset\n    lr_visualizer = DiscriminationThreshold(lr)\n    lr_visualizer.fit(X, y)\n\n    # Finalize and show the visualizations\n    nb_visualizer.finalize()\n    lr_visualizer.finalize()\n\n    # Adjust the layout and save the combined figure to a file\n    fig, axes = plt.subplots(nrows=", "CodeBleu_score": 34.21497244730172}, {"code": "def classification(fname=\"classification.png\")\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\n    # Load the spam dataset\n    data = load_breast_cancer()\n    X = data.data\n    y = data.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create the classification report visualization\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Create the first visualization\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    visualizer = ClassificationReport(model, classes=['spam', 'ham'], ax=ax[0])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Create the second visualization\n    model = LogisticRegression()\n    model.fit(X, y)\n    visualizer = DiscriminationThreshold(model, ax=ax[1])\n    visualizer.fit(X, y)\n    visualizer.finalize()\n\n    # Adjust the layout\n    plt.tight_layout()\n\n    # Save the figure\n    plt.savefig(fname)\n", "CodeBleu_score": 38.8851255605666}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n    try:\n        import matplotlib as mpl\n        if mpl.__version__ >= '1.5.0':\n            colors = [prop['color'] for prop in mpl.rcParams['axes.prop_cycle']]\n        else:\n            colors = mpl.rcParams['axes.color_cycle']\n    except KeyError:\n        colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    return colors\n", "CodeBleu_score": 41.30486266272055}, {"code": "def get_color_cycle()\n    try:\n        import matplotlib\n        if matplotlib.__version__ >= '1.5.0':\n            return [color['color'] for color in matplotlib.rcParams['axes.prop_cycle']]\n        else:\n            return matplotlib.rcParams['axes.color_cycle']\n    except KeyError:\n        return matplotlib.rcParams['axes.color_cycle']\n", "CodeBleu_score": 39.88427409798535}, {"code": "def get_color_cycle()\n    try:\n        import matplotlib as mpl\n        if mpl.__version__ >= \"1.5.0\":\n            return [c[\"color\"] for c in mpl.rcParams[\"axes.prop_cycle\"]]\n        else:\n            return mpl.rcParams[\"axes.color_cycle\"]\n    except KeyError:\n        return mpl.rcParams[\"axes.color_cycle\"]\n\n", "CodeBleu_score": 43.31487206838083}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n\n    plt.plot(self.x_normalized, self.y_normalized, color='blue', linewidth=2)\n\n    plt.plot(self.x_normalized, self.y_distance, color='red', linewidth=2)\n\n    plt.vlines(self.knee_point, 0, 1, linestyles='dashed', colors='black', label='Knee point')\n\n    plt.xlabel('Normalized X')\n\n    plt.ylabel('Normalized Y')\n\n    plt.xticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n\n    plt.yticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n\n    plt.legend()\n\n    plt.show()\n", "CodeBleu_score": 45.819530418120195}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized, label='Normalized curve')\n    plt.plot(self.x_normalized, self.distance, c='red', label='Distance curve')\n    plt.vlines(self.knee_x, 0, 1, linestyles='dashed', colors='k', label='Knee point')\n    plt.xlabel('Normalized x')\n    plt.ylabel('Normalized y')\n    plt.xticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n    plt.yticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n    plt.legend()\n    plt.show()\n\n", "CodeBleu_score": 45.87540834881293}, {"code": "def plot_knee_normalized(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized)\n    plt.plot(self.x_normalized, self.distance_normalized, 'r')\n    plt.xlabel('Normalized x')\n    plt.ylabel('Normalized y')\n    plt.xticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n    plt.yticks(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))\n    if self.knee_point:\n        plt.axvline(x=self.knee_point[0], color='k', linestyle='--')\n    plt.show()\n\n", "CodeBleu_score": 46.335271779954056}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y, 'bx-')\n    plt.plot([self.knee, self.knee], [np.min(self.y), np.max(self.y)], 'k--')\n    plt.xlabel('K')\n    plt.ylabel('Sum of squared distances')\n    plt.title('Elbow Method For Optimal k')\n    plt.show()\n", "CodeBleu_score": 54.35051257002391}, {"code": "def plot_knee(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8,8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee_point, ymin=min(self.y), ymax=max(self.y), colors='r', linestyles='dashed')\n    plt.show()", "CodeBleu_score": 44.55935611716223}, {"code": "def plot_knee(\n        self,\n    )\n\n    # Create a figure with a specified size\n    plt.figure(figsize=(8, 8))\n\n    # Plot the data points\n    plt.plot(self.x, self.y, 'bo-', label='Points')\n\n    # Draw a vertical line at the knee point\n    if self.knee_point:\n        plt.axvline(x=self.knee_point, color='r', linestyle='--', label='Knee Point')\n\n    # Set the plot title and axis labels\n    plt.title('Knee Point Detection')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    # Display the plot\n    plt.legend()\n    plt.show()\n\n", "CodeBleu_score": 42.483432857417306}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_digits([data[i]], images_per_row=12)\n        plot_digits([pca.inverse_transform(Xproj[i])], images_per_row=12)\n        plt.subplot(1, 2, 1, frameon=False, xticks=[], yticks=[])\n        plt.title(\"Original Image\", fontsize=16)\n        plt.subplot(1, 2, 2, frameon=False, xticks=[], yticks=[])\n        plt.title(\"Reconstructed Image\", fontsize=16)\n\n    interact(show_decomp, i=(0, data.shape[0] - 1))\n\n", "CodeBleu_score": 68.2280376847909}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_digits([data[i]])\n        bar = plt.gca()\n        bar.axes.get_xaxis().set_ticklabels([])\n        bar.axes.get_yaxis().set_ticklabels([])\n\n        plot_digits([pca.inverse_transform(Xproj[i])])\n        bar = plt.gca()\n        bar.axes.get_xaxis().set_ticklabels([])\n        bar.axes.get_yaxis().set_ticklabels([])\n\n    interact(show_decomp, i=(0, len(data) - 1))\n\n", "CodeBleu_score": 62.54115339534479}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n    def show_decomp(i=0):\n        plot_digit(data[i])\n        fig = plt.figure(figsize=(16, 8))\n        Xproj = pca.inverse_transform(Xproj[i])\n        for i in range(10):\n            ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n            ax.imshow(Xproj[i].reshape(8, 8), cmap=plt.cm.binary,\n                      interpolation='nearest')\n    interact(show_decomp, i=(0, len(data) - 1))\n\n", "CodeBleu_score": 64.07576774096485}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import load_iris\n    from sklearn.neighbors import KNeighborsClassifier\n    iris = load_iris()\n    X = iris.data[:, :2]  # extract the first two features\n    y = iris.target\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('kNN decision boundaries on Iris dataset')\n    plt.show()\n\n", "CodeBleu_score": 60.57489696577936}, {"code": "def plot_iris_knn()\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import load_iris\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    import numpy as np\n    iris = load_iris()\n    X = iris.data[:, :2]  # Use only the first two features\n    y = iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train, y_train)\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('kNN Decision Boundaries on Iris Dataset')\n    plt.show()\n\n", "CodeBleu_score": 55.57543043343327}, {"code": "def plot_iris_knn()\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # Use the first two features\n    y = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Define the mesh grid over the feature space\n    h = 0.02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()\n\n", "CodeBleu_score": 57.0240747585139}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n    # we'll use 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,\n               edgecolor='black', s=20)\n\n    ax.axis('tight')\n\n", "CodeBleu_score": 67.23679917274943}, {"code": "def plot_sgd_separator()\n    # we'll use 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200,\n                        fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,\n               edgecolor='black', s=20)\n\n    ax.axis('tight')\n\n", "CodeBleu_score": 66.84391211048816}, {"code": "def plot_sgd_separator()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.datasets import make_blobs\n\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = [\"dashed\", \"solid\", \"dashed\"]\n    plt.contour(\n        X1, X2, Z, levels, colors=\"k\", linestyles=linestyles\n    )\n    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\n    plt.axis(\"tight\")\n\n", "CodeBleu_score": 71.32000693798926}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n    X, y = self.data\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 17.751443043732994}, {"code": "def dump_svmlight_file(self, file)\n    X = self.data.iloc[:, :-1].to_numpy()\n    y = self.data.iloc[:, -1].to_numpy()\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 41.34729373251776}, {"code": "def dump_svmlight_file(self, file)\n    X = np.array(self.data)\n    y = X[:, 2]\n    X = X[:, :2]\n    dump_svmlight_file(X, y, file)", "CodeBleu_score": 30.29354181628772}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n    print(\"Fitting model...\")\n    X, y = self.data.X, self.data.y\n    self.clf = svm.SVC(\n        kernel=self.kernel,\n        C=self.C,\n        gamma=self.gamma,\n        coef0=self.coef0,\n        degree=self.degree,\n    )\n    if len(np.unique(y)) == 1:\n        self.clf = OneClassSVM(\n            kernel=self.kernel,\n            C=self.C,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            degree=self.degree,\n        )\n    self.clf.fit(X, y)\n    if hasattr(self.clf, \"score\"):\n        print(\"Accuracy:\", self.clf.score(X, y))\n    self.surface = self.decision_surface(X)\n    self.surface_type = self.surface.dtype\n    self.fitted = True\n    self.trigger(\"surface_changed\")", "CodeBleu_score": 30.3319179793673}, {"code": "def fit(self)\n    print(\"fitting...\")\n    X, y = self.data[:, :-1], self.data[:, -1]\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if np.all(y == y[0]):\n        self.model = OneClassSVM(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, nu=0.5, shrinking=True)\n    else:\n        self.model = SVC(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, C=C, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)\n    self.model.fit(X, y)\n    if hasattr(self.model, \"score\"):\n        print(\"Accuracy:\", self.model.score(X, y))\n    self.surface = self.calculate_surface()\n    self.surface_type = \"svc\"\n    self.fitted = True\n    self.trigger(\"surface_changed\")", "CodeBleu_score": 37.324934408738656}, {"code": "def fit(self)\n    print(\"Fitting...\")\n    X, y = self.data.X, self.data.y\n    C = self.C\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if all(y == y[0]):\n        self.model = OneClassSVM(C=C, kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    else:\n        self.model = SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    self.model.fit(X, y)\n    if hasattr(self.model, \"score\"):\n        print(\"Accuracy:\", self.model.score(X, y))\n    self.surface = self.model.decision_function(X)\n    self.surface_type = \"decision\"\n    self.fitted = True\n    self.trigger(\"surface_changed\")\n\n", "CodeBleu_score": 29.27058453747583}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n    import importlib\n    import sys\n    import traceback\n\n    # Split the source code into its module and object components\n    module_name, object_name = sourcecode.split(\".\")\n\n    # If the source code starts with \"tpot.\", remove the \"tpot.\" prefix\n    if module_name.startswith(\"tpot\"):\n        module_name = module_name[5:]\n\n    # Try to import the object from the module\n    try:\n        module = importlib.import_module(module_name)\n        obj = getattr(module, object_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import object '{}' from module '{}'\".format(object_name, module_name))\n            return None, None, None\n\n    # Return the import string, object string, and the imported object\n    return module_name, object_name, obj\n", "CodeBleu_score": 41.615799210268435}, {"code": "def source_decode(sourcecode, verbose=0)\n    import sys\n    import importlib\n    import traceback\n    importlib.invalidate_caches()\n    if sourcecode.startswith(\"tpot.\"):\n        module_name, object_name = sourcecode.split(\".\")\n        import_string = module_name\n        object_string = object_name\n    else:\n        module_name, object_name = sourcecode.rsplit(\".\", 1)\n        import_string = module_name\n        object_string = object_name\n    try:\n        module = importlib.import_module(import_string)\n        object_ = getattr(module, object_string)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: {}\".format(e))\n            print(\"Failed to import {} from {}\".format(object_string, module_name))\n            print(\"Traceback:\")\n            print(traceback.format_exc())\n            print(\"Continuing without importing {}\".format(sourcecode))\n            object_ = None\n    return import_string, object_string, object_", "CodeBleu_score": 44.5938232627936}, {"code": "def source_decode(sourcecode, verbose=0)\n    import importlib\n    import sys\n    import traceback\n    import warnings\n    importlib.invalidate_caches()\n    sourcecode_split = sourcecode.split(\".\")\n    if len(sourcecode_split) == 1:\n        object_name = sourcecode_split[0]\n        module_name = sourcecode_split[0]\n        import_string = module_name\n    else:\n        object_name = sourcecode_split[-1]\n        module_name = \".\".join(sourcecode_split[:-1])\n        import_string = \".\".join(sourcecode_split)\n    try:\n        imported_object = getattr(importlib.import_module(module_name), object_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            warnings.warn(\"Failed to import {}.{}\".format(module_name, object_name))\n            return None\n    except Exception as e:\n        if verbose > 2:\n            raise e\n        else:\n            warnings.warn(\"Failed to import {}.{}\".format(module_name, object_name))\n            return None\n    return import_string, object_name, imported_object", "CodeBleu_score": 42.880811628544436}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for name, step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                if \"sample_weight\" in inspect.signature(step.fit).parameters:\n                    sample_weight_dict[f\"{name}_sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 51.227389569125826}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step_name, step in pipeline_steps:\n            if \"fit\" in dir(step):\n                fit_method = getattr(step, \"fit\")\n                if \"sample_weight\" in fit_method.__code__.co_varnames:\n                    key = f\"{step_name}__sample_weight\"\n                    sample_weight_dict[key] = sample_weight\n    return sample_weight_dict if sample_weight_dict else None\n", "CodeBleu_score": 36.35493526526588}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step_name, step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                if \"sample_weight\" in inspect.signature(step.fit).parameters:\n                    key = step_name + \"__sample_weight\"\n                    sample_weight_dict[key] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 55.78091675540274}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.pareto_front_ is None:\n        raise RuntimeError(\"You must call `fit()` before calling `summary()`.\")\n    if self.pareto_front_.shape[0] == 0:\n        raise RuntimeError(\n            \"No optimized pipeline was found. \"\n            \"You may need to call `fit()` with a larger population size.\"\n        )\n    best_pipeline_string = self.pareto_front_[0][0]\n    if self.verbosity >= 1:\n        print(\"Best pipeline:\", best_pipeline_string)\n        if self.verbosity >= 2:\n            print()\n    self.fitted_pipeline_ = self._compile_and_fit_pipeline(\n        best_pipeline_string, features, target, ignore_warnings=True\n    )\n    self.pareto_front_models_ = []\n    for pipeline_string in self.pareto_front_[:, 0]:\n        self.pareto_front_models_.append(\n            self._compile_and_fit_pipeline(pipeline_string, features, target)\n        )\n\n", "CodeBleu_score": 30.484677982347897}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.pareto_front_ is None:\n        raise RuntimeError(\n            \"Please call `fit` first to create a pareto front of pipelines.\"\n        )\n    if self.pareto_front_.best_pipeline is None:\n        raise RuntimeError(\n            \"Please call `fit` first to create a pareto front of pipelines.\"\n        )\n    self.best_pipeline_ = self.pareto_front_.best_pipeline\n    self.best_pipeline_.fit(features, target, suppress_warnings=True)\n    if self.verbosity >= 1:\n        print(f\"\\nBest pipeline:\\n{self.best_pipeline_}\")\n    if self.verbosity >= 2:\n        print(\"\\n\")\n    self.fitted_pipelines_ = [\n        pipeline.compile(self.operators, self.individual_selection, self.tournsize)\n        for pipeline in self.pareto_front_.pipelines\n    ]\n    for pipeline in self.fitted_pipelines_:\n        pipeline.fit(features, target, suppress_warnings=True)\n\n", "CodeBleu_score": 28.769151728911535}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.pareto_front_ is None:\n        raise RuntimeError(\"You must call fit() before calling summary()\")\n    best_pipeline_str = self.pareto_front_.iloc[0][\"pipeline\"]\n    best_pipeline = self._compile_pipeline(best_pipeline_str)\n    best_pipeline.fit(features, target, **self._get_fit_kwargs())\n    self.fitted_pipeline_ = best_pipeline\n    if self.verbosity > 0:\n        print(f\"Best pipeline: {best_pipeline_str}\")\n        if self.verbosity > 1:\n            print()\n    self.pareto_front_pipelines_ = [\n        self._compile_and_fit_pipeline(p, features, target)\n        for p in self.pareto_front_[\"pipeline\"]\n    ]\n\n", "CodeBleu_score": 24.19584883564434}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n        raise ValueError(\"Pipeline has not been optimized yet.\")\n    testing_features, testing_target = self._validate_data(testing_features, testing_target)\n    if self.scoring_function is None:\n        scorer = SCORERS[self.scoring]\n    else:\n        scorer = self.scoring_function\n    return scorer(self.fitted_pipeline, testing_features, testing_target)\n", "CodeBleu_score": 31.703795821538723}, {"code": "def score(self, testing_features, testing_target)\n    if self.optimized is False:\n        raise ValueError(\"Pipeline has not been optimized yet.\")\n    self.validate_testing_data(testing_features, testing_target)\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n    return scorer(self.fitted_pipeline, testing_features, testing_target)\n\n", "CodeBleu_score": 28.888420232666235}, {"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n        raise Exception(\"Pipeline has not been optimized.\")\n\n    # Validate testing features and target\n    self._validate_data(testing_features, testing_target)\n\n    # Get scorer\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n\n    # Calculate score\n    score = scorer(self.pipeline, testing_features, testing_target)\n\n    return score", "CodeBleu_score": 30.420986228929493}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)", "CodeBleu_score": 73.82620092731361}, {"code": " def _impute_values(self, features)    if self.verbosity > 1:\n        print('Imputing values...')\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy='median')\n        self.imputer.fit(features)\n    return self.imputer.transform(features)", "CodeBleu_score": 68.5721788227744}, {"code": " def _impute_values(self, features)    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)\n", "CodeBleu_score": 73.82620092731361}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n    code = self._generate_pipeline_code(expr)\n    context = {\n        \"self\": self,\n        \"np\": np,\n        \"pd\": pd,\n        \"sklearn\": sklearn,\n        \"sklearn_transform\": sklearn_transform,\n        \"sklearn_classification\": sklearn_classification,\n        \"sklearn_feature_selection\": sklearn_feature_selection,\n        \"sklearn_feature_extraction\": sklearn_feature_extraction,\n        \"sklearn_decomposition\": sklearn_decomposition,\n        \"sklearn_ensemble\": sklearn_ensemble,\n        \"sklearn_linear_model\": sklearn_linear_model,\n        \"sklearn_svm\": sklearn_svm,\n        \"sklearn_neural_network\": sklearn_neural_network,\n        \"sklearn_cluster\": sklearn_cluster,\n        \"sklearn_mixture\": sklearn_mixture,\n        \"sklearn_isotonic\": sklearn_isotonic,\n        \"sklearn_kernel_approximation\": sklearn_kernel_approximation,\n        \"sklearn_kernel_ridge\": sklearn_kernel_ridge,\n        \"sklearn_gaussian_process\": sklearn_gaussian_process,\n        \"sklearn_cross_decomposition\": sklearn_cross_decomposition,\n        \"sklearn_multioutput\": sklearn_multioutput,\n        \"sklearn_dummy\": sklearn_dummy,\n        \"sklearn_pipeline\": sklearn_pipeline,\n        \"sklearn_preprocessing\": sklearn_preprocessing,\n        \"sklearn_impute\": sklearn_impute,\n        \"sklearn_compose\": sklearn_compose,\n        \"sklearn_covariance\": sklearn_covariance,\n        \"sklearn_manifold\": sklearn_manifold,\n        \"sklearn_feature_selection\": sklearn_feature_selection,\n        \"sklearn_semi_supervised\": sklearn_semi_supervised,\n        \"sklearn_calibration\": sklearn_calibration,\n        \"sklearn_neural_network\": sklearn_", "CodeBleu_score": 24.336447190772013}, {"code": "def _compile_to_sklearn(self, expr)\n    code_str = self._generate_pipeline_code(expr, self._operators)\n    # pylint: disable=eval-used\n    pipeline = eval(code_str, self._eval_context)\n    pipeline.memory = self.memory\n    if self.random_state is not None:\n        for _, step in pipeline.steps:\n            if hasattr(step, \"random_state\"):\n                step.random_state = self.random_state\n    return pipeline\n\n", "CodeBleu_score": 37.183894509828605}, {"code": "def _compile_to_sklearn(self, expr)\n\n    # Generate the pipeline code string from the expression and a predefined set of operators\n    code_str = self._generate_pipeline_code(expr, self._operators)\n\n    # Evaluate the pipeline code string within a specific context to obtain the scikit-learn pipeline object\n    pipeline = eval(code_str, self._context)\n\n    # Set the pipeline's memory attribute\n    pipeline.memory = self.memory\n\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        for _, step in pipeline.steps:\n            if hasattr(step, \"random_state\"):\n                step.random_state = self.random_state\n\n    # Return the configured scikit-learn pipeline\n    return pipeline", "CodeBleu_score": 37.08041701134084}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n    import random\n    from deap import gp\n\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(ind) for ind in population]\n\n    # Convert each individual to a string representation\n    population_str = [str(ind) for ind in population]\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i+1, len(population)):\n            if not population_str[i] == population_str[j] and not primitive_sets[i].isdisjoint(primitive_sets[j]):\n                eligible_pairs.append((i, j))\n                eligible_pairs.append((j, i))\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs\n    i, j = random.choice(eligible_pairs)\n\n    # Return the corresponding individuals from the population\n    return population[i], population[j]\n\n", "CodeBleu_score": 36.74771915545207}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    # Create sets of primitive nodes for each individual\n    primitive_nodes_sets = [set(individual.get_primitive_nodes()) for individual in population]\n    # Convert individuals to string representations\n    individuals_str = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i, set1 in enumerate(primitive_nodes_sets):\n        for j, set2 in enumerate(primitive_nodes_sets[i+1:]):\n            if set1.intersection(set2) and individuals_str[i] != individuals_str[i+j+1]:\n                eligible_pairs.append((i, i+j+1))\n                eligible_pairs.append((i+j+1, i))\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    # Randomly select one of the eligible pairs\n    i, j = random.choice(eligible_pairs)\n    # Return the corresponding individuals from the population\n    return population[i], population[j]", "CodeBleu_score": 28.687044587451755}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    import random\n    import numpy as np\n\n    # Create sets of primitive nodes for each individual\n    primitives_sets = [set(individual.get_primitive_nodes()) for individual in population]\n\n    # Convert individuals to strings\n    individuals_str = [str(individual) for individual in population]\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if primitives_sets[i].intersection(primitives_sets[j]) and individuals_str[i] != individuals_str[j]:\n                eligible_pairs.append((i, j))\n                eligible_pairs.append((j, i))\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs\n    pair_index = random.randint(0, len(eligible_pairs) - 1)\n    individual1_index, individual2_index = eligible_pairs[pair_index]\n\n    # Return the corresponding individuals from the population\n    return population[individual1_index], population[individual2_index]\n\n", "CodeBleu_score": 39.37791320650139}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    steps = []\n    for node in pipeline_tree:\n        operator = operators[node.operator]\n        if operator.type == \"operator\":\n            steps.append(f\"{operator.name}()\")\n        elif operator.type == \"function\":\n            steps.append(f\"{operator.name}()\")\n    num_steps = len(steps)\n    if num_steps > 1:\n        code = f\"make_pipeline(\\n\"\n        for i, step in enumerate(steps):\n            if i == num_steps - 1:\n                code += f\"    {step}\\n\"\n            else:\n                code += f\"    {step},\\n\"\n        code += \")\"\n    else:\n        code = steps[0]\n    return code\n\n", "CodeBleu_score": 26.465795408004706}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    # Initialize an empty list to store the steps of the pipeline\n    steps = []\n    # Iterate over the operators in the pipeline tree\n    for operator in pipeline_tree:\n        # Check if the operator is a Pipeline object\n        if isinstance(operator, Pipeline):\n            # If it is, recursively call the function to generate the code for the sub-pipeline\n            code = generate_export_pipeline_code(operator.steps, operators)\n            # Append the generated code to the steps list\n            steps.append(code)\n        else:\n            # If it is not a Pipeline object, it is an operator\n            # Get the operator name and the parameters\n            operator_name = operator[0]\n            operator_params = operator[1]\n            # Get the operator class from the operators dictionary\n            operator_class = operators[operator_name]\n            # Format the operator code with the operator name and parameters\n            operator_code = f\"{operator_name}({', '.join(f'{k}={v}' for k, v in operator_params.items())})\"\n            # Append the operator code to the steps list\n            steps.append(operator_code)\n    # Determine the number of steps in the pipeline\n    num_steps = len(steps)\n    # If there is more than one step, format the steps into a make_pipeline call with indentation\n    if num_steps > 1:\n        steps_code = \"\\n\".join(f\"    {step},\" for step in steps)\n        code = f\"make_pipeline(\\n{steps_code}\\n)\"\n    # If there is only one step, format it directly without make_pipeline\n    else:\n        code = steps[0]\n    # Return the generated code as a string\n    return code\n", "CodeBleu_score": 29.196439572382705}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    # Initialize an empty list to store the steps of the pipeline\n    steps = []\n\n    # Iterate over the operators in the pipeline tree\n    for operator in pipeline_tree:\n        # If the operator is a tuple, it represents a pipeline step\n        if isinstance(operator, tuple):\n            # Extract the operator name and parameters from the tuple\n            operator_name, parameters = operator\n            # Create a string representation of the operator call\n            operator_call = f\"{operator_name}(\"\n            # Iterate over the parameters in the operator\n            for parameter_name, parameter_value in parameters.items():\n                # Add the parameter name and value to the operator call string\n                operator_call += f\"{parameter_name}={parameter_value}, \"\n            # Remove the trailing comma and space from the operator call string\n            operator_call = operator_call[:-2] + \")\"\n            # Add the operator call to the steps list\n            steps.append(operator_call)\n\n    # Determine the number of steps in the pipeline\n    num_steps = len(steps)\n\n    # If the pipeline has more than one step, format the steps into a make_pipeline call with indentation\n    if num_steps > 1:\n        # Create a string representation of the make_pipeline call\n        pipeline_call = \"make_pipeline(\\n\"\n        # Iterate over the steps in the pipeline\n        for i, step in enumerate(steps):\n            # If this is not the last step, add a comma and newline to the pipeline call string\n            if i < num_steps - 1:\n                pipeline_call += f\"    {step},\\n\"\n            # Otherwise, add just the step to the pipeline call string\n            else:\n                pipeline_call += f\"    {step}\\n\"\n        # Add the closing parenthesis and indentation to the pipeline call string\n        pipeline_call += \")\"\n    # If there is only one step, format it directly without make_pipeline\n    else:\n        # Create a string representation of the pipeline step\n        pipeline_call = steps[0]\n\n    # Return the generated Python code as a string\n    return pipeline_call\n\n\n\n\n", "CodeBleu_score": 26.972978064700637}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\n    # Identify the operator's name\n    operator_name = operator.name\n\n    # Check if the operator is a \"CombineDFs\" operation\n    if operator_name == \"CombineDFs\":\n        # Append the result of combining dataframes\n        return [operator.result]\n\n    # Retrieve the corresponding TPOT operator\n    tpot_operator = operators[operator_name]\n\n    # Process any input operators recursively\n    processed_inputs = []\n    for input_operator in operator.inputs:\n        if input_operator.name != \"CombineDFs\":\n            processed_inputs.append(_process_operator(input_operator, operators, depth + 1))\n\n    # If the operator is a root operator and the depth is greater than zero, wrap it in a StackingEstimator\n    if operator.is_root and depth > 0:\n        return [StackingEstimator(tpot_operator, processed_inputs)]\n\n    # Otherwise, return the processed operator\n    return [tpot_operator(*processed_inputs)]\n\n", "CodeBleu_score": 38.47676810026276}, {"code": "def _process_operator(operator, operators, depth=0)\n    name = operator.name\n    if name == 'CombineDFs':\n        steps.append(('union', CombineDFs()))\n    else:\n        op = tpot_operators[name]\n        steps.append([name, op(*operator.inputs)])\n        for input_ in operator.inputs:\n            if input_ != 'CombineDFs':\n                _process_operator(operators[input_], operators, depth=depth + 1)\n    if operator.is_root and depth > 0:\n        steps.append(('stacking', StackingEstimator(steps)))\n\n", "CodeBleu_score": 29.456930595804703}, {"code": "def _process_operator(operator, operators, depth=0)\n    name = operator.name\n    if name == \"CombineDFs\":\n        steps.append((\"CombineDFs\", combine_dataframes))\n    else:\n        op = tpot_operators[name]\n        steps.append((name, op))\n        for i, input_op in enumerate(operator.inputs):\n            if input_op != operators[0]:\n                steps.extend(_process_operator(input_op, operators, depth + 1))\n        if operator == operators[0] and depth > 0:\n            steps.append(\n                (\n                    \"StackingEstimator\",\n                    StackingEstimator(\n                        estimator=op,\n                        meta_regressor=DecisionTreeRegressor(\n                            max_depth=3, min_samples_leaf=4, min_samples_split=10\n                        ),\n                    ),\n                )\n            )\n    return steps\n\n", "CodeBleu_score": 37.89946154808261}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n    if self.threshold is None:\n        return X\n\n    # Select categorical features based on threshold\n    cat_features = []\n    for col in X.columns:\n        if X[col].nunique() <= self.threshold:\n            cat_features.append(col)\n\n    # Check if any categorical features are selected\n    if not cat_features:\n        raise ValueError(\"No categorical features found.\")\n\n    # Apply one-hot encoding to selected categorical features\n    X_cat = X[cat_features]\n    X_cat = pd.get_dummies(X_cat, columns=cat_features)\n\n    # Concatenate one-hot encoded features with original data\n    X_transformed = pd.concat([X, X_cat], axis=1)\n\n    return X_transformed", "CodeBleu_score": 31.3861268593711}, {"code": "def transform(self, X)\n    X_copy = X.copy()\n    self.categorical_features = []\n    for col in X_copy.columns:\n        if X_copy[col].dtype == \"object\":\n            self.categorical_features.append(col)\n    if len(self.categorical_features) == 0:\n        raise ValueError(\"No categorical features found in the input data.\")\n    else:\n        X_copy = pd.get_dummies(X_copy, columns=self.categorical_features)\n        return X_copy", "CodeBleu_score": 25.202493361115508}, {"code": "def transform(self, X)\n    X_transformed = X.copy()\n    cat_cols = X_transformed.select_dtypes(include=['object']).columns.tolist()\n    if not cat_cols:\n        raise ValueError(\"No categorical features found in the input data.\")\n    else:\n        for col in cat_cols:\n            if X_transformed[col].nunique() < self.threshold:\n                X_transformed = pd.concat([X_transformed, pd.get_dummies(X_transformed[col], prefix=col)], axis=1)\n                X_transformed.drop(col, axis=1, inplace=True)\n        return X_transformed", "CodeBleu_score": 32.15985822702006}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\n    # Get unique classes\n    classes = np.unique(y_true)\n\n    # Initialize variables\n    sensitivity_sum = 0\n    specificity_sum = 0\n\n    # Calculate sensitivity and specificity for each class\n    for c in classes:\n        # Get true positive and true negative counts\n        tp = np.sum((y_true == c) & (y_pred == c))\n        tn = np.sum((y_true != c) & (y_pred != c))\n\n        # Get actual positive and actual negative counts\n        ap = np.sum(y_true == c)\n        an = np.sum(y_true != c)\n\n        # Calculate sensitivity and specificity\n        sensitivity = tp / ap if ap > 0 else 0\n        specificity = tn / an if an > 0 else 1\n\n        # Add to sum\n        sensitivity_sum += sensitivity\n        specificity_sum += specificity\n\n    # Calculate balanced accuracy\n    balanced_accuracy = (sensitivity_sum + specificity_sum) / (2 * len(classes))\n\n    return balanced_accuracy\n\n", "CodeBleu_score": 21.632031584582105}, {"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(y_true)\n    class_accuracies = []\n    for class_ in classes:\n        true_positives = np.sum((y_true == class_) & (y_pred == class_))\n        true_negatives = np.sum((y_true != class_) & (y_pred != class_))\n        false_positives = np.sum((y_true != class_) & (y_pred == class_))\n        false_negatives = np.sum((y_true == class_) & (y_pred != class_))\n        sensitivity = true_positives / (true_positives + false_negatives)\n        specificity = true_negatives / (true_negatives + false_positives)\n        if len(classes) == 1:\n            specificity = 1\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n    return np.mean(class_accuracies)\n", "CodeBleu_score": 26.265191812193507}, {"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(y_true)\n    class_accuracies = []\n    for c in classes:\n        true_positives = np.sum((y_true == c) & (y_pred == c))\n        true_negatives = np.sum((y_true != c) & (y_pred != c))\n        false_positives = np.sum((y_true != c) & (y_pred == c))\n        false_negatives = np.sum((y_true == c) & (y_pred != c))\n        sensitivity = true_positives / (true_positives + false_negatives)\n        if y_true.shape[0] > 1:\n            specificity = true_negatives / (true_negatives + false_positives)\n        else:\n            specificity = 1\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 25.588554266934594}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    if not hasattr(X, \"tocoo\"):\n        X = check_array(X, accept_sparse='csc', dtype=np.float64)\n    n_samples, n_features = X.shape\n    if self.degree > 2:\n        raise ValueError(\"SplineTransformer only supports degrees up to 2.\")\n    if self.n_knots <= 0:\n        raise ValueError(\"Invalid number of knots, n_knots must be higher than 0.\")\n    knots = np.linspace(0, 1, self.n_knots)\n    knots = np.r_[[0] * self.degree, knots, [1] * self.degree]\n    spl = BSpline(knots, np.eye(self.n_knots + self.degree - 1), self.degree)\n    X_transformed = spl(X)\n    if self.include_bias:\n        X_transformed = np.c_[np.ones(n_samples), X_transformed]\n    if self.interaction_only:\n        X_transformed = X_transformed[:, self.n_knots - 1:]\n    return X_transformed\n\n", "CodeBleu_score": 29.91355175123943}, {"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    if not hasattr(X, \"tocoo\"):\n        raise TypeError(\"'%s' does not have a 'tocoo' method.\" % X.dtype)\n    X = X.tocoo()\n    n_samples, n_features = X.shape\n    nnz = X.nnz\n    if nnz == 0:\n        num_nonzeros, num_zeros = np.empty((n_samples, 1)), np.empty((n_samples, 1))\n        num_nonzeros.fill(0)\n        num_zeros.fill(n_features)\n    else:\n        _, nonzero_rows, nonzero_cols = X.row, X.col, X.data\n        _, zero_rows, zero_cols = np.setdiff1d(np.arange(n_samples), nonzero_rows), np.setdiff1d(np.arange(n_features), nonzero_cols), np.zeros(n_features - nonzero_cols.shape[0])\n        num_nonzeros, num_zeros = np.empty((n_samples, 1)), np.empty((n_samples, 1))\n        num_nonzeros.fill(0)\n        num_zeros.fill(n_features)\n        num_nonzeros[nonzero_rows] = nonzero_cols.shape[0]\n        num_zeros[zero_rows] = zero_cols.shape[0]\n    X_new = np.concatenate((num_nonzeros, num_zeros, X.toarray()), axis=1)\n    return X_new", "CodeBleu_score": 26.76794824478663}, {"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    n_samples, n_features = X.shape\n    if self.norm == 'l1':\n        X_norm = np.abs(X)\n    elif self.norm == 'l2':\n        X_norm = row_norms(X, squared=True)[:, np.newaxis]\n    else:\n        raise ValueError(\"'norm' should be either 'l1' or 'l2'\")\n    mask = self._get_mask(X_norm, self.threshold)\n    X_new = X.copy()\n    X_new.data[mask] = 0\n    if sp.issparse(X_new):\n        X_new.eliminate_zeros()\n    return X_new\n", "CodeBleu_score": 24.82032050648741}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n    unique_targets = len(set(target))\n    if len(features) < unique_targets:\n        self.pretest_X = features\n        self.pretest_y = target\n    else:\n        self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=unique_targets, random_state=self.random_state)\n    if len(set(self.pretest_y)) < unique_targets:\n        for i in range(unique_targets):\n            if i not in set(self.pretest_y):\n                self.pretest_X = np.vstack((self.pretest_X, features[target == i][0]))\n                self.pretest_y = np.append(self.pretest_y, i)\n", "CodeBleu_score": 30.07409146846884}, {"code": "def _init_pretest(self, features, target)\n    self.pretest_X = features\n    self.pretest_y = target\n    self.pretest_size = len(self.pretest_X)\n    self.pretest_y_unique = np.unique(self.pretest_y)\n    self.n_pretest_classes = len(self.pretest_y_unique)\n    self.pretest_y_counts = np.unique(self.pretest_y, return_counts=True)[1]\n    self.pretest_min_class_size = np.min(self.pretest_y_counts)\n\n    if self.pretest_size < self.n_pretest_classes:\n        raise ValueError(\"Training size must be at least the number of unique target classes.\")\n\n    if self.pretest_size < self.min_train_size:\n        raise ValueError(\"Training size must be at least the minimum training size.\")\n\n    self.pretest_X, self.pretest_y = train_test_split(self.pretest_X, self.pretest_y, test_size=self.pretest_min_class_size, random_state=self.random_state)\n\n    # Adjust pretest_y to include at least one example from each class\n    for i in range(self.n_pretest_classes):\n        if self.pretest_y_unique[i] not in self.pretest_y:\n            self.pretest_X = np.concatenate((self.pretest_X, self.pretest_X[np.where(self.pretest_y == self.pretest_y_unique[i])[0][0]]), axis=0)\n            self.pretest_y = np.concatenate((self.pretest_y, [self.pretest_y_unique[i]]), axis=0)\n\n    self.pretest_size = len(self.pretest_X)\n    self.pretest_y_counts = np.unique(self.pretest_y, return_counts=True)[1]\n\n    return self.pretest_X, self.pretest_y, self.pretest_", "CodeBleu_score": 29.418191539517057}, {"code": "def _init_pretest(self, features, target)\n    self.pretest_X = features\n    self.pretest_y = target\n    self.pretest_size = len(self.pretest_y)\n    self.pretest_classes = len(np.unique(self.pretest_y))\n    if self.pretest_size < self.pretest_classes:\n        self.pretest_size = self.pretest_classes\n    self.pretest_X, self.pretest_y = train_test_split(\n        self.pretest_X, self.pretest_y, train_size=self.pretest_size, random_state=self.random_state\n    )\n    if len(np.unique(self.pretest_y)) < self.pretest_classes:\n        for i in range(self.pretest_classes):\n            self.pretest_X = np.vstack((self.pretest_X, self.pretest_X[i]))\n            self.pretest_y = np.append(self.pretest_y, self.pretest_y[i])\n    return self\n\n", "CodeBleu_score": 27.85533914526448}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n", "CodeBleu_score": 49.19748004987954}, {"code": "def _to_numpy_type(dtype)\n    if dtype in (tf.float32, tf.float64):\n        return np.float64\n    elif dtype in (tf.int32, tf.int64):\n        return np.int64\n    elif dtype == tf.bool:\n        return np.bool\n    elif dtype == tf.string:\n        return np.str\n    else:\n        raise ValueError(\"Unsupported data type: {}\".format(dtype))\n", "CodeBleu_score": 42.38240494962085}, {"code": "def _to_numpy_type(dtype)\n    if dtype is None:\n        return None\n    if isinstance(dtype, str):\n        return dtype\n    if hasattr(dtype, \"as_numpy_dtype\"):\n        return dtype.as_numpy_dtype\n    raise ValueError(f\"Unsupported data type: {dtype}\")\n\n", "CodeBleu_score": 45.80554104682683}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        return ds.as_numpy_iterator()\n    except AttributeError:\n        return ds.make_one_shot_iterator().get_next()\n\n", "CodeBleu_score": 12.787245018108084}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        ds = ds.batch(batch_size)\n    except AttributeError:\n        ds = ds.batch(batch_size, drop_remainder=False)\n    return np.concatenate([batch for batch in ds], axis=0)\n\n", "CodeBleu_score": 14.814998092649745}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    if batch_size is None:\n        batch_size = 1\n    try:\n        ds = ds.batch(batch_size)\n    except AttributeError:\n        ds = ds.batch_size(batch_size)\n    for batch in ds:\n        yield batch.numpy()\n\n", "CodeBleu_score": 14.574985163772732}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n    import jax\n\n    if not isinstance(seed, int):\n        raise TypeError(\"Seed must be an integer.\")\n    if seed < 0:\n        raise ValueError(\"Seed must be a non-negative integer.\")\n\n    # Split the seed into two 32-bit unsigned integers\n    key1 = seed >> 32\n    key2 = seed & 0xFFFFFFFF\n\n    # Concatenate the two integers to form the PRNG key\n    key = jax.random.PRNGKey(key1)\n    key = jax.random.PRNGKey(key2)\n\n    return key\n\n", "CodeBleu_score": 25.003301416193064}, {"code": "def get_prng(seed)\n    assert isinstance(seed, int), \"Seed must be an integer\"\n    return jax.random.PRNGKey(seed)\n\n", "CodeBleu_score": 3.065159463279706}, {"code": "def get_prng(seed)\n    return jax.random.PRNGKey(seed)\n\n", "CodeBleu_score": 1.631651412290422}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif s.dtype.kind in \"mM\":\n        return pd.to_datetime(s, unit=\"ns\", utc=True).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)", "CodeBleu_score": 67.453891704379}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif s.dtype.kind in \"mM\":\n        return pd.to_datetime(s, unit=\"ms\", utc=True).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 61.044075885827866}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(s.dtype, datetime64):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(s.dtype, np.number):\n        return s.apply(lambda x: pd.to_datetime(x, unit=\"s\", utc=True).tz_localize(dtype.tz))\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 63.844203044720125}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if arg.dtype.tz is not None:\n        arg = arg.dt.tz_convert(\"UTC\")\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n    return indexer.to_dask_array()\n\n", "CodeBleu_score": 21.961883283110865}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if isinstance(arg.index, pd.DatetimeIndex):\n        if arg.index.tz is None:\n            arg = arg.tz_localize(\"UTC\")\n        else:\n            arg = arg.tz_convert(\"UTC\")\n    else:\n        raise ValueError(\"The index of the DataFrame must be a DatetimeIndex.\")\n\n    lower_bound = pd.Timestamp(lower_bound, tz=\"UTC\")\n    upper_bound = pd.Timestamp(upper_bound, tz=\"UTC\")\n\n    indexer = (arg.index >= lower_bound) & (arg.index <= upper_bound)\n\n    result = pd.Series(indexer, index=arg.index)\n    result = dd.from_pandas(result, npartitions=1)\n\n    return result", "CodeBleu_score": 35.53703005509502}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not isinstance(arg, dd.DataFrame):\n        return NotImplemented\n    if not isinstance(arg.index, pd.DatetimeIndex):\n        return NotImplemented\n    if arg.index.tzinfo is not None:\n        arg = arg.tz_convert(\"UTC\")\n    lower_bound = pd.to_datetime(lower_bound, utc=True)\n    upper_bound = pd.to_datetime(upper_bound, utc=True)\n    indexer = (arg.index >= lower_bound) & (arg.index <= upper_bound)\n    return arg[indexer].to_dask_array(lengths=True)", "CodeBleu_score": 31.597970464133773}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return dd.from_array(value.compute(), npartitions=1)\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, tuple):\n            return dd.from_array(np.broadcast_to(value, like), npartitions=1)\n        elif isinstance(like, list):\n            return dd.from_array(np.broadcast_to(value, len(like)), npartitions=1)\n        elif isinstance(like, dict):\n            return dd.from_array(np.broadcast_to(value, len(like)), npartitions=1)\n    return dd.from_array(value, npartitions=1)\n\n", "CodeBleu_score": 35.49295434057984}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value.to_array(), npartitions=1)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, tuple):\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, list):\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, dict):\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)\n\n", "CodeBleu_score": 38.79050514033979}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return dd.from_array(value.to_array(), npartitions=1)\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, tuple):\n            return dd.from_pandas(pd.Series(value, index=like), npartitions=1)\n        if isinstance(like, list):\n            return dd.from_pandas(pd.Series(value, index=range(len(like))), npartitions=1)\n        if isinstance(like, dict):\n            return dd.from_pandas(pd.Series(value, index=like.keys()), npartitions=1)\n    return dd.from_pandas(pd.Series(value), npartitions=1)\n\n", "CodeBleu_score": 38.416023311642874}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Value for key {key} must be a Dask or Pandas DataFrame, not {type(value)}\"\n            )\n    super().connect(dictionary)", "CodeBleu_score": 67.30984659485874}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Value for key '{key}' is not a valid DataFrame. Expected a Dask or Pandas DataFrame.\"\n            )\n    super().do_connect(dictionary)", "CodeBleu_score": 68.13948985868717}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(f\"{key} must be a Dask or Pandas DataFrame.\")\n    super().do_connect(dictionary)\n", "CodeBleu_score": 61.40131906511398}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n    from sklearn.base import BaseEstimator\n    from sklearn.utils import all_estimators\n    from loguru import logger\n    from mlmodels.util import get_module_members\n    from mlmodels.util import get_package_and_module_name\n\n    library_name, module_name = get_package_and_module_name(library_name)\n\n    try:\n        classes = get_module_members(library_name, module_name)\n    except Exception as e:\n        logger.error(f\"Error retrieving classes from module: {e}\")\n        return None\n\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator '{estimator_name}' could not be found in module '{module_name}'\")\n\n    matching_estimators = [cls for cls in classes if cls.__name__ == estimator_name]\n\n    if len(matching_estimators) > 1:\n        logger.debug(f\"Multiple classes found for estimator '{estimator_name}': {matching_estimators}\")\n        logger.warning(f\"Multiple classes found for estimator '{estimator_name}'. Using the first one: {matching_estimators[0]}\")\n\n    estimator_class = matching_estimators[0]\n\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"Class '{estimator_class.__name__}' is not a subclass of sklearn's BaseEstimator.\")\n\n    return estimator_class\n", "CodeBleu_score": 39.17284842629467}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    import logging\n    from sklearn.base import BaseEstimator\n    from utils.common import get_module_members\n\n    logger = logging.getLogger(__name__)\n\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} could not be found in {library_name}.\")\n\n    estimators = [\n        estimator for estimator in classes if estimator.lower() == estimator_name.lower()\n    ]\n    if len(estimators) > 1:\n        logger.debug(f\"Multiple estimators found: {estimators}\")\n        logger.warning(\n            f\"Multiple {estimator_name} were found in {library_name}. Will pick the first which is {estimators[0]}.\"\n        )\n\n    estimator_class = classes[estimators[0]]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(\n            f\"{estimator_class} is not an sklearn estimator. It will not have the get_params and set_params methods.\"\n        )\n\n    return estimator_class\n", "CodeBleu_score": 41.81583974243447}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    try:\n        from sklearn.base import BaseEstimator\n    except ImportError:\n        raise ImportError(\n            \"Unable to import sklearn. \"\n            \"The library is required to retrieve the estimator.\"\n        )\n\n    try:\n        module = importlib.import_module(library_name)\n    except ImportError:\n        raise ImportError(\n            f\"Unable to import {library_name}. \"\n            \"The library is required to retrieve the estimator.\"\n        )\n\n    classes = get_module_members(module, inspect.isclass)\n    if estimator_name not in classes:\n        raise ValueError(f\"The estimator {estimator_name} does not exist.\")\n\n    estimator_class = classes[estimator_name]\n    if len(classes) > 1:\n        logger.debug(f\"Multiple classes found: {classes}\")\n        logger.warning(\n            \"Multiple classes found. \"\n            \"Using the first one in the dictionary.\"\n        )\n\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(\n            f\"The class {estimator_class} is not a subclass of sklearn's \"\n            \"BaseEstimator. It may not work properly with the library.\"\n        )\n\n    return estimator_class\n\n", "CodeBleu_score": 41.118486951942714}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n\n    # Load the mapping from a JSON file\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\"), \"r\") as f:\n        mapping = json.load(f)\n\n    # Check if the estimator name has a postfix in the mapping\n    for postfix, task in mapping.items():\n        if estimator_name.endswith(postfix):\n            return task\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 43.243820554240735}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n\n    with open(\"estimator_to_task_mapping.json\", \"r\") as f:\n        mapping = json.load(f)\n\n    for task, postfixes in mapping.items():\n        if estimator_name.endswith(tuple(postfixes)):\n            return task\n\n    return \"unknown\"\n\n", "CodeBleu_score": 36.45410965114926}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n\n    # Load the mapping from a JSON file\n    mapping_file = os.path.join(\n        os.path.dirname(__file__), \"estimator_to_task_mapping.json\"\n    )\n    with open(mapping_file, \"r\") as f:\n        mapping = json.load(f)\n\n    # Check if the estimator name has a postfix that matches any of the keys in the mapping\n    for key in mapping:\n        if estimator_name.endswith(key):\n            return mapping[key]\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 42.37390379709227}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data = {}\n    for filename in os.listdir(data_cache):\n        if filename.endswith(\".json\"):\n            continue\n        if filename.startswith(data_name):\n            component = filename.split(data_name)[1].split(\".\")[0]\n            data[component] = load_data(os.path.join(data_cache, filename))\n    return data\n\n", "CodeBleu_score": 36.22876662422568}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data = {}\n    for file in os.listdir(data_cache):\n        if file.startswith(data_name):\n            if file.endswith(\".json\"):\n                continue\n            component, file_extension = file.split(\"_\")\n            data[component] = load_data(os.path.join(data_cache, file), file_extension)\n    return data\n\n", "CodeBleu_score": 36.65479342157326}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data_dict = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name) and not filename.endswith(\".json\"):\n            component, _ = os.path.splitext(filename)\n            component = component.split(\"_\")[1]\n            data_dict[component] = load_data_from_file(os.path.join(data_cache, filename))\n    return data_dict\n\n", "CodeBleu_score": 35.13984111016214}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n\n    if os.path.isfile(filename):\n        print(f\"{filename} already exists\")\n        return\n\n    if url.startswith(\"http\"):\n        try:\n            response = requests.get(url, stream=True)\n            if response.status_code != 200:\n                raise AssertionError(f\"Download failed with status code {response.status_code}\")\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n            block_size = 1024\n            written_size = 0\n\n            with open(filename, \"wb\") as f:\n                for data in response.iter_content(block_size):\n                    written_size += len(data)\n                    f.write(data)\n\n            if total_size > 0 and written_size != total_size:\n                raise AssertionError(\"Error, something went wrong\")\n        except Exception as e:\n            print(e)\n            if os.path.exists(filename):\n                os.remove(filename)\n\n", "CodeBleu_score": 59.07376443773571}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n\n    if os.path.isfile(filename):\n        print(f\"{filename} already exists, skipping ...\")\n        return\n\n    if url.startswith(\"http\"):\n        try:\n            response = requests.get(url, stream=True)\n            if response.status_code != 200:\n                raise AssertionError(f\"Failed to download {url}: {response.status_code}\")\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n            block_size = 1024\n            written = 0\n            with open(filename, \"wb\") as f:\n                for data in response.iter_content(block_size):\n                    written += len(data)\n                    f.write(data)\n            if total_size > 0 and written != total_size:\n                raise AssertionError(f\"Failed to download {url}: incomplete data\")\n        except Exception as e:\n            print(f\"Failed to download {url}: {e}\")\n            if os.path.exists(filename):\n                os.remove(filename)\n    else:\n        raise ValueError(f\"Invalid URL: {url}\")\n", "CodeBleu_score": 56.56257804291593}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n\n    if os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n\n        if response.status_code != 200:\n            raise AssertionError(f\"Failed to download {url}: status code {response.status_code}\")\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n\n        with open(filename, \"wb\") as f:\n            for data in response.iter_content(block_size):\n                written_size += len(data)\n                f.write(data)\n\n        if total_size > 0 and written_size != total_size:\n            raise AssertionError(f\"Failed to download {url}: incomplete data\")\n", "CodeBleu_score": 68.30195004104998}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    import numpy as np\n    from sklearn.datasets import fetch_covtype\n\n    data = fetch_covtype(data_home=raw_data_cache)\n    x = data[\"data\"]\n    y = data[\"target\"]\n    y = y - 1\n    if dataset_params[\"binary\"]:\n        y[y != 0] = 1\n    n_classes = len(np.unique(y))\n    default_split = {\n        \"train_size\": 0.8,\n        \"stratify\": y,\n    }\n    description = {\n        \"name\": data_name,\n        \"n_classes\": n_classes,\n        \"default_split\": default_split,\n    }\n    return {\n        \"x\": x,\n        \"y\": y,\n    }, description\n\n", "CodeBleu_score": 37.43878474358836}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    dataset = load_data(data_name, data_cache, raw_data_cache, dataset_params)\n    X, y = dataset[\"data\"], dataset[\"target\"]\n    y = y - 1\n    if dataset_params[\"binary\"]:\n        y = (y > 0).astype(int)\n    dataset[\"data\"], dataset[\"target\"] = X, y\n    n_classes = len(np.unique(y))\n    description = {\n        \"n_classes\": n_classes,\n        \"default_split\": {\n            \"train_size\": 0.8,\n            \"stratify\": True,\n        },\n    }\n    return dataset, description\n\n", "CodeBleu_score": 43.579817118106675}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    (x_train, y_train), (x_test, y_test) = load_data(\n        data_name, data_cache, raw_data_cache\n    )\n\n    y_train = y_train.flatten() - 1\n    y_test = y_test.flatten() - 1\n\n    if dataset_params[\"binary\"]:\n        y_train = (y_train == 2).astype(int)\n        y_test = (y_test == 2).astype(int)\n\n    dataset = {\n        \"train\": {\"x\": x_train, \"y\": y_train},\n        \"test\": {\"x\": x_test, \"y\": y_test},\n    }\n\n    description = {\n        \"name\": data_name,\n        \"n_classes\": len(np.unique(y_train)),\n        \"length\": len(x_train),\n        \"x_shape\": np.shape(x_train),\n        \"default_split\": {\n            \"train_size\": 0.7,\n            \"stratify\": y_train,\n        },\n    }\n\n    return dataset, description\n\n", "CodeBleu_score": 39.909256357612946}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    flat_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            flat_list.extend(flatten_list(element))\n        else:\n            flat_list.append(element)\n    if ensure_type_homogeneity:\n        flat_list = ensure_list_types_homogeneity(flat_list)\n    return flat_list\n\n", "CodeBleu_score": 36.96600537346331}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.271901029998034}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.271901029998034}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\n    if complex_mode == \"split\":\n        x = x.real\n        x = np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n        x = np.where(np.isnan(x), 0, x)\n        return x\n    elif complex_mode == \"magnitude\":\n        x = np.abs(x)\n        x = np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n        x = np.where(np.isnan(x), 0, x)\n        return x\n    elif complex_mode == \"jax\":\n        x = jnp.where(x >= 0, 1 / (1 + jnp.exp(-x)), jnp.exp(x) / (1 + jnp.exp(x)))\n        x = jnp.where(jnp.isnan(x), 0, x)\n        return x\n    else:\n        raise ValueError(\"complex_mode must be one of 'split', 'magnitude', 'jax'\")\n\n", "CodeBleu_score": 40.16445278772546}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return np.logit(x, eps=eps, out=out)", "CodeBleu_score": 17.1742890076788}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return np.logit(x, eps=eps, out=out)", "CodeBleu_score": 17.1742890076788}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    return np.where(np.logical_and(x > -lambd, x < lambd), 0, x)", "CodeBleu_score": 24.168028841508633}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.copy(x)\n    np.less(x, -lambd, out=out, where=np.less(x, -lambd))\n    np.greater(x, lambd, out=out, where=np.greater(x, lambd))\n    return out\n\n", "CodeBleu_score": 41.03030047497693}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.copy(x)\n    np.copyto(out, x)\n    out *= np.where((lambd < x) & (x < -lambd), 1 / x, 0)\n    return out\n\n", "CodeBleu_score": 41.05741589297016}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    window = []\n    for n in range(window_length):\n        window.append(round(math.sin((math.pi / 2) * (math.sin(math.pi * n / (window_length - 1)) ** 2)), 8))\n    if not isinstance(window, np.ndarray):\n        return np.array(window, dtype=dtype)\n    return window", "CodeBleu_score": 40.025569491463855}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    window = []\n    for n in range(window_length):\n        temp = math.sin(\n            (math.pi / 2) * (math.sin(math.pi * (n) / (window_length)) ** 2)\n        )\n        window.append(round(temp, 8))\n    return np.array(window, dtype=dtype)", "CodeBleu_score": 38.930385304240964}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    window = []\n    for n in range(window_length):\n        window.append(round(\n            sin((math.pi / 2) * (sin(math.pi * (n * 0.5) / (window_length - 1)) ** 2)) **\n            2, 8))\n    return np.array(window, dtype=dtype, out=out)\n\n", "CodeBleu_score": 36.130002375484075}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Set the precision mode to 'precise'\n    with np.errstate(precision='precise'):\n        # Promote the data types of coefficients and x to a common type\n        coeffs, x = np.array(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype)), np.array(x, dtype=np.promote_types(coeffs.dtype, x.dtype))\n        # Compute the polynomial values using numpy's polyval function\n        result = np.polyval(coeffs, x)\n    # Convert the result to an array with the promoted data type\n    result = np.array(result, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    # Return the computed array\n    return result\n\n", "CodeBleu_score": 51.198063884257024}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    x = np.array(x)\n    coeffs = np.array(coeffs)\n    if coeffs.dtype == np.float16:\n        return np.array(np.polyval(coeffs, x), dtype=np.float64)\n    return np.array(np.polyval(coeffs, x))\n\n", "CodeBleu_score": 28.263362320019958}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Compute the polynomial values using numpy's polyval\n    vals = np.polyval(coeffs, x)\n\n    # Convert the result to an array with the promoted data type\n    return np.array(vals, dtype=np.promote_types(coeffs.dtype, x.dtype))\n\n", "CodeBleu_score": 21.466413770870457}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        num = np.amax(x)\n    else:\n        num = np.amax(x, axis=axis, keepdims=keepdims)\n\n    if np.isscalar(num) and not keepdims:\n        return np.array(num)\n    return num\n\n", "CodeBleu_score": 46.896259084600445}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        num = len(x.shape)\n        axis = tuple(range(num))\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)", "CodeBleu_score": 49.318466036485155}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        num = np.amax(x)\n    else:\n        num = np.amax(x, axis=axis, keepdims=keepdims)\n    if np.isscalar(num) and out is not None:\n        return np.asarray(num, out)\n    return num", "CodeBleu_score": 47.543588293957605}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(x1, np.ndarray):\n        x1 = np.asarray(x1)\n    if not isinstance(x2, np.ndarray):\n        x2 = np.asarray(x2)\n    promoted_type = promote_types_of_inputs(x1, x2)\n    if promoted_type is None:\n        if not isinstance(x1, np.ndarray):\n            x1 = np.asarray(x1, dtype=np.float64)\n        if not isinstance(x2, np.ndarray):\n            x2 = np.asarray(x2, dtype=np.float64)\n        promoted_type = np.float64\n    x1, x2 = np.asarray(x1), np.asarray(x2)\n    x1 = x1.astype(promoted_type)\n    x2 = x2.astype(promoted_type)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 48.9665199806523}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(x1, np.ndarray):\n        x1 = np.asarray(x1)\n    if not isinstance(x2, np.ndarray):\n        x2 = np.asarray(x2)\n    promoted_type = promote_types_of_inputs(x1, x2)\n    if promoted_type.kind == \"f\":\n        promoted_type = np.float_\n    x1, x2 = np.asarray(x1, dtype=promoted_type), np.asarray(x2, dtype=promoted_type)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 52.17556238704191}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(x1, np.ndarray):\n        x1 = np.asarray(x1)\n    if not isinstance(x2, np.ndarray):\n        x2 = np.asarray(x2)\n    promoted_type = promote_types_of_inputs(x1, x2)\n    if promoted_type is not None:\n        x1, x2 = cast_to_safe_dtype(x1, promoted_type), cast_to_safe_dtype(\n            x2, promoted_type\n        )\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 51.43617785734938}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    return np.array(np.count_nonzero(a, axis=axis, keepdims=keepdims), dtype=dtype)\n\n", "CodeBleu_score": 47.69215538583996}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)", "CodeBleu_score": 46.196732391005405}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)", "CodeBleu_score": 33.64802727171017}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    pass", "CodeBleu_score": 29.600163260004287}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if spacing is None:\n        spacing = [1] * len(x.shape)\n    elif isinstance(spacing, int):\n        spacing = [spacing] * len(x.shape)\n    else:\n        spacing = list(spacing)\n    if axis is None:\n        axis = list(range(len(spacing)))\n    elif isinstance(axis, int):\n        axis = [axis]\n    else:\n        axis = list(axis)\n    if len(axis) == 1:\n        return np.gradient(x, spacing[axis[0]], edge_order)\n    else:\n        return [\n            np.gradient(x, spacing[axis[i]], edge_order, axis=axis[i])\n            for i in range(len(axis))\n        ]", "CodeBleu_score": 45.22176361215525}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if axis is None:\n        axis = tuple(range(x.ndim))\n    else:\n        axis = y\n    if not hasattr(spacing, \"__iter__\"):\n        ret = np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    else:\n        ret = np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n    if type(ret) is tuple:\n        return list(ret)\n    else:\n        return ret\n\n", "CodeBleu_score": 61.32192241936306}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if isinstance(x, np.ndarray):\n        pass\n    else:\n        x = np.asarray(x)\n    if isinstance(y, np.ndarray):\n        pass\n    else:\n        y = np.asarray(y)\n    if out is None:\n        pass\n    else:\n        out = np.asarray(out)\n    x, y = promote_types_of_inputs(x, y)\n    if np.all(x == 0):\n        return np.array(0.0, dtype=x.dtype)\n    else:\n        return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 54.90983851775858}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if isinstance(x, np.ndarray):\n        pass\n    else:\n        x = np.asarray(x)\n    if isinstance(y, np.ndarray):\n        pass\n    else:\n        y = np.asarray(y)\n    return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 43.78607488131566}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    return np.multiply(x, np.log(y), out=out)", "CodeBleu_score": 30.74010060045772}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    kernel, strides, depth_pooling = _determine_depth_pooling_helper(x, kernel, strides, dims, data_format)\n    if depth_pooling:\n        if data_format == \"channel_first\":\n            x = tf.transpose(x, (0, 2, 1))\n        else:\n            x = tf.transpose(x, (0, 3, 1, 2))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 45.09270804059249}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    kernel_size, strides, depth_pooling = _determine_depth_pooling(\n        kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        x = _transpose_nchw_to_nhwc(x)\n    return x, kernel_size, strides, depth_pooling\n\n", "CodeBleu_score": 35.84068001552899}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    (kernel, strides, depth_pooling) = _determine_depth_max_pooling_helper(dims, kernel, strides, data_format)\n    if depth_pooling:\n        x = tf.transpose(x, (0, 1, 3, 2))\n    return (x, kernel, strides, depth_pooling)\n", "CodeBleu_score": 38.46748804750915}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        _, x_shape = x.shape\n        mask = np.random.binomial(1, 1 - prob, size=(x_shape,))\n        scaled_x = x * (1.0 / (1 - prob))\n        x = np.where(mask, scaled_x, 0)\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n    return x", "CodeBleu_score": 50.3827360540539}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if training:\n        if x.ndim == 3:\n            N, C, W = x.shape\n            is_batched = True\n        else:\n            N, W = x.shape\n            is_batched = False\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        mask = np.random.binomial(1, 1 - prob, (N, W))\n        scaled_x = x * mask / (1 - prob)\n        if data_format == \"NCW\":\n            scaled_x = np.transpose(scaled_x, (0, 2, 1))\n        if out is not None:\n            out = scaled_x\n        return scaled_x\n    else:\n        return x\n\n", "CodeBleu_score": 50.78798136656894}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if training:\n        if x.ndim == 2:\n            x = x[np.newaxis, ...]\n            batch = False\n        else:\n            batch = True\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        mask = np.random.binomial(1, 1 - prob, size=x.shape)\n        x = x * mask / (1 - prob)\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        if not batch:\n            x = x[0]\n    return x", "CodeBleu_score": 51.67389249769969}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    # Check the axis value and get the number of dimensions of the input array\n    axis = axis if axis >= 0 else x.ndim + axis\n    ndim = x.ndim\n\n    # Check if the axis value is valid\n    if not -ndim <= axis < ndim:\n        raise ValueError(\"axis must be in the range [-ndim, ndim-1]\")\n\n    # Check if k is valid\n    if k < 1:\n        raise ValueError(\"k must be greater than 0\")\n\n    # Check if the input array has more than one element\n    if x.size == 1:\n        if x.item() != 0:\n            raise ValueError(\"k must be less than or equal to the number of elements in x\")\n        else:\n            # If the input array has only one element and it is zero, return the element and its index\n            return (x.item(), np.array([0]))\n\n    # Check if the input array has only one dimension\n    if ndim == 1:\n        # If the input array has only one dimension, sort it and return the top k elements and their indices\n        if largest:\n            indices = np.argsort(x)[-k:]\n        else:\n            indices = np.argsort(x)[:k]\n        values = x[indices]\n        if sorted:\n            values = np.sort(values)\n        return (values, indices)\n\n    # Get the shape of the input array\n    shape = x.shape\n\n    # Check if the axis value is valid\n    if not -ndim <= axis < ndim:\n        raise ValueError(\"axis must be in the range [-ndim, ndim-1]\")\n\n    # Check if k is valid\n    if k < 1:\n        raise ValueError(\"k must be greater than 0\")\n\n    # Check if the input array has more than one element\n    if x.size == 1:\n        if x.item() != 0:\n            raise ValueError(\"k must be less than or equal to the number of elements in x\")\n        else:\n            # If the input array has only one element and it is zero, return the element and its index\n            return (x.item(), np.array", "CodeBleu_score": 32.9797456752416}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    return np.top_k(x, k, axis, largest, sorted, out)\n\n", "CodeBleu_score": 15.910160905600055}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    if out is not None:\n        if hasattr(out, \"dtype\"):\n            out = np.array(out)\n        if hasattr(out, \"shape\"):\n            out = np.array(out)\n        if out.shape != (2, k):\n            raise ValueError(\n                \"output array must have shape (2, k) but got shape {}\".format(\n                    out.shape\n                )\n            )\n        if out.dtype != x.dtype:\n            raise ValueError(\n                \"output array must have dtype {} but got dtype {}\".format(\n                    x.dtype, out.dtype\n                )\n            )\n\n    if axis is None:\n        axis_size = x.size\n    else:\n        axis_size = x.shape[axis]\n    if k < 0:\n        raise ValueError(\"k must be non-negative, got {}\".format(k))\n    if k > axis_size:\n        raise ValueError(\n            \"k must be less than or equal to the axis size, got {}\".format(k)\n        )\n\n    if largest:\n        index_array = np.argpartition(x, -k, axis=axis)\n        index_array = np.take(index_array, np.arange(k), axis=axis)\n    else:\n        index_array = np.argpartition(x, k - 1, axis=axis)\n        index_array = np.take(index_array, np.arange(k), axis=axis)\n\n    topk_vals = np.take_along_axis(x, index_array, axis=axis)\n\n    if sorted:\n        sorted_indices = np.argsort(topk_vals, axis=axis)\n        if largest:\n            sorted_indices = np.flip(sorted_indices, axis=axis)\n        sorted_indices_k = np.take_along_axis(\n            sorted_indices, np.arange(k)[None, :], axis=axis\n        )\n        sorted_topk_vals = np.take_along_axis(topk_vals, sorted_indices_k, axis=", "CodeBleu_score": 42.55833526670129}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.divide(\n        x,\n        np.sum(\n            np.abs(x),\n            axis=axis,\n            keepdims=True,\n            out=out,\n        ),\n        out=out,\n    )\n\n", "CodeBleu_score": 27.95852327735523}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    nm = np.linalg.norm(x, 1, axis, keepdims=True)\n    if axis is None:\n        # avoid the warning \"divide by zero encountered in true_divide\"\n        nm = nm.reshape(\n            (\n                x.size,\n            )\n        )\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        return np.asarray(x / nm, dtype=x.dtype)", "CodeBleu_score": 35.005579907241355}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    nm = np.linalg.norm(x, 1, axis, keepdims=True)\n    if axis is None:\n        # avoid the warning \"RuntimeWarning: divide by zero encountered in true_divide\"\n        # when x is an empty array\n        if nm == 0:\n            nm = 1\n        return np.divide(x, nm, out=out)\n    elif isinstance(axis, int):\n        if x.shape[axis] == 0:\n            nm = 1\n        return np.divide(x, nm, out=out)\n    else:\n        raise ValueError(\"'axis' must be None, an integer, or a tuple of integers\")", "CodeBleu_score": 33.68143489687817}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    pass\n\n", "CodeBleu_score": 34.24167840743079}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    pass", "CodeBleu_score": 34.24167840743079}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    pass\n\n", "CodeBleu_score": 34.24167840743079}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    indices = np.argsort(x)\n    inverse = np.zeros_like(indices)\n    inverse[indices] = np.arange(len(x))\n    inverse = np.sort(inverse)\n    return inverse", "CodeBleu_score": 47.83983194265022}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    x = np.array(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse\n\n", "CodeBleu_score": 56.60266130052361}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse\n\n", "CodeBleu_score": 53.67721297299843}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import pandas as pd\n    import numpy as np\n    import os\n\n    # Set the style for the plot\n    plt.style.use('dark_background')\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n\n    # Read the data from the CSV file\n    data = pd.read_csv('data/medical_data.csv', parse_dates=['Date'])\n\n    # Calculate the total vomit per month\n    total_vomit_per_month = data.groupby(pd.Grouper(key='Date', freq='M'))['Vomit'].sum()\n\n    # Calculate the days between vomit\n    data['Days Between Vomit'] = (data['Date'] - data['Date'].shift(1)).dt.days\n\n    # Calculate the days in daycare\n    data['Days in Daycare'] = (data['Date'] - data['Date'].shift(1)).dt.days\n    data.loc[data['Daycare'] == 0, 'Days in Daycare'] = 0\n\n    # Calculate the doctor visits\n    data['Doctor Visits'] = data['Doctor'].diff().fillna(data['Doctor']).astype(int)\n\n    # Plot the total vomit per month on the first subplot\n    axs[0, 0].plot(total_vomit_per_month.index, total_vomit_per_month.values, color='red')\n    axs[0, 0].set_title('Total Vomit per Month')\n    axs[0, 0].set_xlabel('Date')\n    axs[0, 0].set_ylabel('Vomit')\n    axs[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n    axs[0, 0].xaxis.set_major_", "CodeBleu_score": 28.01945009465765}, {"code": "def plot_medical_charts() -> None\n    # Set the plotting style\n    plt.style.use(\"seaborn-darkgrid\")\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n\n    # Plot total vomit per month\n    axs[0, 0].bar(\n        df_vomit_per_month.index,\n        df_vomit_per_month[\"total_vomit\"],\n        color=\"#0066cc\",\n        alpha=0.7,\n    )\n    axs[0, 0].set_title(\"Total Vomit per Month\", fontsize=14, fontweight=\"bold\")\n    axs[0, 0].set_xlabel(\"Month\", fontsize=12)\n    axs[0, 0].set_ylabel(\"Total Vomit\", fontsize=12)\n    axs[0, 0].set_xticklabels(df_vomit_per_month.index, rotation=45, ha=\"right\")\n    axs[0, 0].grid(True, linestyle=\"--\", alpha=0.5)\n\n    # Plot days between vomit\n    axs[0, 1].plot(\n        df_days_between_vomit.index,\n        df_days_between_vomit[\"days_between_vomit\"],\n        marker=\"o\",\n        linestyle=\"-\",\n        color=\"#009933\",\n        alpha=0.7,\n    )\n    axs[0, 1].set_title(\"Days Between Vomit\", fontsize=14, fontweight=\"bold\")\n    axs[0, 1].set_xlabel(\"Date\", fontsize=12)\n    axs[0, 1].set_ylabel(\"Days\", fontsize=12)\n    axs[0, 1].grid(True, linestyle=\"--\", alpha=0.5)\n\n    # Plot days in daycare\n    axs[1,", "CodeBleu_score": 23.759701517855113}, {"code": "def plot_medical_charts() -> None\n    # Set the plot style\n    plt.style.use(\"dark_background\")\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(\n        nrows=2, ncols=2, figsize=(16, 10), gridspec_kw={\"hspace\": 0.4}\n    )\n\n    # Plot total vomit per month\n    axs[0, 0].plot(\n        df_vomit_per_month[\"date\"],\n        df_vomit_per_month[\"total_vomit\"],\n        color=\"red\",\n        linewidth=2,\n        marker=\"o\",\n    )\n    axs[0, 0].set_title(\"Total Vomit per Month\")\n    axs[0, 0].set_xlabel(\"Date\")\n    axs[0, 0].set_ylabel(\"Total Vomit\")\n    axs[0, 0].tick_params(axis=\"x\", rotation=45)\n\n    # Plot days between vomit\n    axs[0, 1].plot(\n        df_days_between_vomit[\"date\"],\n        df_days_between_vomit[\"days_between_vomit\"],\n        color=\"blue\",\n        linewidth=2,\n        marker=\"o\",\n    )\n    axs[0, 1].set_title(\"Days Between Vomit\")\n    axs[0, 1].set_xlabel(\"Date\")\n    axs[0, 1].set_ylabel(\"Days Between Vomit\")\n    axs[0, 1].tick_params(axis=\"x\", rotation=45)\n\n    # Plot days in daycare\n    axs[1, 0].plot(\n        df_days_in_daycare[\"date\"],\n        df_days_in_daycare[\"days_in_daycare\"],\n        color=\"green\",\n        linewidth=2,\n        marker=\"o\",\n    )\n    axs[1, 0].set_title(\"Days in Daycare\")", "CodeBleu_score": 21.33475787327387}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime, timedelta\n    from parse_raw_data import parse_raw_data\n    from export_figure import export_figure\n\n    # Import and extract feeding data\n    df = parse_raw_data()\n\n    # Set up plot with dark grid style\n    plt.style.use('dark_background')\n\n    # Compute offset from birthday based on the last solid feeding date\n    last_solid_feeding_date = df['Solid Feeding Start'].dropna().iloc[-1]\n    offset = (last_solid_feeding_date - datetime(2022, 12, 1)).days\n\n    # Create scatter plot of feeding times\n    plt.scatter(df['Bottle Time'], np.ones(len(df)), color='red', label='Bottle Feeding')\n    plt.scatter(df['Solid Feeding Start'], np.ones(len(df)) + 0.1, color='blue', label='Solid Feeding')\n\n    # Add legend to distinguish between bottle and solid feedings\n    plt.legend()\n\n    # Calculate end date for plot\n    end_date = last_solid_feeding_date + timedelta(days=7)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=1))\n    plt.xlim(datetime(2022, 12, 1) + timedelta(days=offset), end_date)\n    plt.ylim(0, 2)\n    plt.yticks([])\n\n    # Export figure\n    export_figure(plt, '24h_feeding_viz')\n\n", "CodeBleu_score": 25.644722901674584}, {"code": "def plot_feeding_24h_viz() -> None\n\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import datetime\n    import numpy as np\n    import pandas as pd\n    from datetime import datetime, timedelta\n    from parse_raw_data import parse_raw_data\n    from export_figure import export_figure\n\n    # Import and extract feeding data\n    df = parse_raw_data()\n\n    # Set up plot with dark grid style\n    plt.style.use('dark_background')\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Compute offset from birthday based on last solid feeding date\n    last_solid_feeding_date = df.loc[df['Solid/Bottle'] == 'Solid', 'Date'].max()\n    offset = (datetime.now() - last_solid_feeding_date).days\n\n    # Create scatter plot of feeding times\n    ax.scatter(df.loc[df['Solid/Bottle'] == 'Bottle', 'Date'],\n               df.loc[df['Solid/Bottle'] == 'Bottle', 'Time'],\n               c='red', label='Bottle')\n    ax.scatter(df.loc[df['Solid/Bottle'] == 'Solid', 'Date'],\n               df.loc[df['Solid/Bottle'] == 'Solid', 'Time'],\n               c='blue', label='Solid')\n\n    # Add legend\n    ax.legend()\n\n    # Calculate end date for plot\n    end_date = datetime.now() + timedelta(days=offset)\n\n    # Format plot to display feeding data over a week in 24-hour format\n    ax.set_xlim(end_date - timedelta(days=7), end_date)\n    ax.set_ylim(datetime.strptime('00:00', '%H:%M'),\n                datetime.strptime('23:59', '%H:%M'))\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1", "CodeBleu_score": 27.009545861898648}, {"code": "def plot_feeding_24h_viz() -> None\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime, timedelta\n    from parse_raw_data import parse_raw_data\n    from export_figure import export_figure\n\n    # Import and extract feeding data\n    feeding_df = parse_raw_data()\n\n    # Set up plot with dark grid style\n    plt.style.use('dark_background')\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    # Compute offset from birthday based on last solid feeding date\n    last_solid_feeding_date = feeding_df.loc[feeding_df['Solid/Bottle'] == 'Solid', 'Date'].max()\n    last_solid_feeding_date = datetime.strptime(last_solid_feeding_date, '%Y-%m-%d')\n    offset = (datetime.now() - last_solid_feeding_date).days\n\n    # Create scatter plot of feeding times\n    ax.scatter(feeding_df['Date'], feeding_df['Time'], c=feeding_df['Solid/Bottle'].map({'Solid': 'blue', 'Bottle': 'red'}), label=feeding_df['Solid/Bottle'])\n\n    # Add legend to distinguish between solid and bottle feedings\n    ax.legend()\n\n    # Calculate end date for plot\n    end_date = datetime.now() + timedelta(days=offset)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(datetime.now() - timedelta(days=offset), end_date)\n    ax.set_ylim(0, 24)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n    ax.set_xticks(pd.date_range(datetime.now() - timedelta(days=offset), end_date, freq='D'))\n    ax.set_yticks", "CodeBleu_score": 25.010978154188784}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    sns.barplot(x=self.target_names, y=self.target_counts[task], ax=ax, **kwargs)\n    ax.set_title(task.name)\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    ax.set_ylim(0, max(self.target_counts[task]) * 1.1)\n    ax.grid(True)\n    return ax", "CodeBleu_score": 33.52381386163948}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    counts = self.get_target_counts(task)\n    sns.barplot(x=counts.index, y=counts.values, ax=ax, **kwargs)\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"Target Counts for {task.name}\")\n    ax.tick_params(axis=\"x\", rotation=90)\n    return ax", "CodeBleu_score": 38.08801010554615}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    target_counts = self.get_target_counts(task)\n    sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax, **kwargs)\n    ax.set_title(f\"Target counts for {task.name}\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    return ax", "CodeBleu_score": 38.14509682812872}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n    import numpy as np\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            data, (width, height) = canvas.print_to_buffer()\n            buf = np.frombuffer(data, np.uint8)\n            buf.shape = (height, width, 4)\n            image = buf[:, :, :3]\n            images.append(image)\n            fig.clf()\n        return np.stack(images)\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        data, (width, height) = canvas.print_to_buffer()\n        buf = np.frombuffer(data, np.uint8)\n        buf.shape = (height, width, 4)\n        image = buf[:, :, :3]\n        figure.clf()\n        return image", "CodeBleu_score": 32.72285962372578}, {"code": "def render_to_rgb(figure)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            buf = canvas.buffer_rgba()\n            image = np.asarray(buf)\n            image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            images.append(image)\n            plt.close(fig)\n        return np.stack(images, axis=0)\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        buf = canvas.buffer_rgba()\n        image = np.asarray(buf)\n        image = image.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n        plt.close(figure)\n        return image\n\n", "CodeBleu_score": 38.39427117809247}, {"code": "def render_to_rgb(figure)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            data, (width, height) = canvas.print_to_buffer()\n            image = np.frombuffer(data, np.uint8).reshape((height, width, 4))\n            image = image[:, :, :3]\n            image = image.transpose(2, 0, 1)\n            images.append(image)\n        return np.stack(images, axis=0)\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        data, (width, height) = canvas.print_to_buffer()\n        image = np.frombuffer(data, np.uint8).reshape((height, width, 4))\n        image = image[:, :, :3]\n        image = image.transpose(2, 0, 1)\n        plt.close(figure)\n        return image\n\n", "CodeBleu_score": 36.80853892482448}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n    import torch\n    import random\n    import time\n    from beholder import Beholder\n\n    # Create a Beholder instance with a specified log directory\n    beholder = Beholder(\"logs/\")\n\n    # Define the number of iterations\n    num_iterations = 1000\n\n    # Loop through the iterations\n    for i in range(num_iterations):\n        # Create two lists of random tensors with associated names\n        tensor_list_1 = [(torch.rand(3, 4, 5), f\"tensor_{i}_{j}\") for j in range(10)]\n        tensor_list_2 = [(torch.rand(2, 3, 4), f\"tensor_{i}_{j}\") for j in range(10)]\n\n        # Update the Beholder visualization with the generated tensors and a random frame\n        beholder.update(tensor_list_1 + tensor_list_2, frame=random.randint(0, 100))\n\n        # Sleep briefly\n        time.sleep(0.1)\n\n        # Print the current iteration index\n        print(f\"Iteration {i} complete\")\n\n", "CodeBleu_score": 26.95178764797947}, {"code": "def beholder_pytorch()\n    import torch\n    import random\n    import time\n    from beholder import Beholder\n\n    log_dir = \"./logs\"\n    beholder = Beholder(log_dir=log_dir)\n\n    for i in range(1000):\n        tensors = [\n            (torch.rand(3, 256, 256), \"image\"),\n            (torch.rand(1, 3, 256, 256), \"image\"),\n            (torch.rand(100, 1, 28, 28), \"image\"),\n            (torch.rand(1000, 3, 32, 32), \"image\"),\n            (torch.rand(10000, 3, 16, 16), \"image\"),\n        ]\n\n        tensors_2 = [\n            (torch.rand(3, 256, 256), \"image\"),\n            (torch.rand(1, 3, 256, 256), \"image\"),\n            (torch.rand(100, 1, 28, 28), \"image\"),\n            (torch.rand(1000, 3, 32, 32), \"image\"),\n            (torch.rand(10000, 3, 16, 16), \"image\"),\n        ]\n\n        frame = random.randint(0, 10000)\n        beholder.update(tensors, frame)\n        beholder.update(tensors_2, frame)\n        time.sleep(0.01)\n        print(f\"Iteration {i}\")\n\n", "CodeBleu_score": 22.129111607424605}, {"code": "def beholder_pytorch()\n    import torch\n    import numpy as np\n    from beholder import Beholder\n    import time\n    import random\n\n    # Define a list of names for the tensors\n    names = ['a', 'b', 'c', 'd', 'e']\n\n    # Define the log directory for Beholder\n    log_dir = 'beholder_logs'\n\n    # Create a Beholder object with the specified log directory\n    beholder = Beholder(log_dir)\n\n    # Define the number of iterations\n    num_iterations = 1000\n\n    # Loop through the specified number of iterations\n    for i in range(num_iterations):\n        # Generate a random number of tensors\n        num_tensors = random.randint(1, len(names))\n\n        # Create a list of random tensors with the specified number of tensors\n        tensors = [torch.randn(3, 224, 224) for _ in range(num_tensors)]\n\n        # Create a list of random names for the tensors\n        tensor_names = random.sample(names, num_tensors)\n\n        # Update the Beholder visualization with the generated tensors and a random frame\n        beholder.update(tensors, tensor_names, frame=np.random.randint(0, 1000))\n\n        # Sleep briefly\n        time.sleep(0.1)\n\n        # Print the current iteration index\n        print(f\"Iteration {i}\")\n", "CodeBleu_score": 26.710543545201503}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = tf.make_ndarray(summary.tensor)\n    if len(tensor_data.shape) == 2:\n        tensor_data = tensor_data.reshape(\n            (1, tensor_data.shape[0], tensor_data.shape[1]))\n    elif len(tensor_data.shape) == 3:\n        tensor_data = tensor_data.reshape(\n            (tensor_data.shape[0], tensor_data.shape[1], tensor_data.shape[2]))\n    else:\n        raise ValueError(\n            \"PR Curve summary must have 2 or 3 dimensions\")\n    thresholds = [float(i) / float(num_thresholds - 1)\n                  for i in range(num_thresholds)]\n    pr_data = {}\n    for i in range(tensor_data.shape[0]):\n        tp = tensor_data[i, :, 0].flatten().tolist()\n        fp = tensor_data[i, :, 1].flatten().tolist()\n        tn = tensor_data[i, :, 2].flatten().tolist()\n        fn = tensor_data[i, :, 3].flatten().tolist()\n        precision = tensor_data[i, :, 4].flatten().tolist()\n        recall = tensor_data[i, :, 5].flatten().tolist()\n        pr_data[tag] = {\n            'true_positives': tp,\n            'false_positives': fp,\n            'true_negatives': tn,\n            'false_negatives': fn,\n            'precision': precision,\n            'recall': recall,\n            'thresholds': thresholds,\n        }\n    self.log_asset_data(pr_data, tag, step=step)", "CodeBleu_score": 39.31998276002374}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = tf.make_ndarray(summary.tensor)\n    if tensor_data.ndim == 2:\n        tensor_data = np.reshape(tensor_data, (1, -1, tensor_data.shape[1]))\n    elif tensor_data.ndim == 3:\n        tensor_data = np.reshape(tensor_data, (1, tensor_data.shape[0], tensor_data.shape[1] * tensor_data.shape[2]))\n    thresholds = [i * 1.0 / num_thresholds for i in range(num_thresholds + 1)]\n    pr_data = {}\n    for i in range(tensor_data.shape[1]):\n        tp, fp, tn, fn, precision, recall = [], [], [], [], [], []\n        for j in range(tensor_data.shape[0]):\n            tpjd, fpjd, tnjd, fnjd, pd, rd = self._process_confusion_matrix(tensor_data[j, i, :], thresholds)\n            tp.append(tpjd)\n            fp.append(fpjd)\n            tn.append(tnjd)\n            fn.append(fnjd)\n            precision.append(pd)\n            recall.append(rd)\n        pr_data[self._VAR_NAME + str(i)] = {\"true_positives\": np.array(tp).flatten(), \"false_positives\": np.array(fp).flatten(), \"true_negatives\": np.array(tn).flatten(), \"false_negatives\": np.array(fn).flatten(), \"precision\": np.array(precision).flatten(), \"recall\": np.array(recall).flatten(), \"thresholds\": np.array(thresholds).flatten()}\n    self.log_asset_data(pr_data, tag, step=step)", "CodeBleu_score": 28.375470774768708}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = tf.make_ndarray(summary.tensor)\n    tensor_data = tensor_data.reshape((-1, tensor_data.shape[-1]))\n    thresholds = [i * 1.0 / num_thresholds for i in range(num_thresholds + 1)]\n    TP = []\n    FP = []\n    TN = []\n    FN = []\n    precision = []\n    recall = []\n    for threshold in thresholds:\n        TP.append(tensor_data[0, 0])\n        FP.append(tensor_data[0, 1])\n        TN.append(tensor_data[1, 0])\n        FN.append(tensor_data[1, 1])\n        precision.append(tensor_data[0] / (tensor_data[0] + tensor_data[1]))\n        recall.append(tensor_data[0] / (tensor_data[0] + tensor_data[2]))\n    pr_data = {\"thresholds\": thresholds, \"tp\": TP, \"fp\": FP, \"tn\": TN, \"fn\": FN, \"precision\": precision, \"recall\": recall}\n    self.log_asset_data(pr_data, tag, step)", "CodeBleu_score": 26.08863501934109}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n    import gzip\n    import numpy as np\n\n    data = np.empty((num, 784), dtype=np.uint8)\n    target = np.empty((num,), dtype=np.uint8)\n\n    with gzip.open(images, 'rb') as f_images:\n        f_images.read(16)\n        for i in range(num):\n            for j in range(784):\n                data[i, j] = ord(f_images.read(1))\n\n    with gzip.open(labels, 'rb') as f_labels:\n        f_labels.read(8)\n        for i in range(num):\n            target[i] = ord(f_labels.read(1))\n\n    return data, target\n\n", "CodeBleu_score": 55.91423875209842}, {"code": "def load_mnist(images, labels, num)\n    import gzip\n    import numpy as np\n    with gzip.open(images, 'rb') as f:\n        data = np.frombuffer(f.read(), np.uint8, offset=16)\n        data = data.reshape(-1, 784)\n    with gzip.open(labels, 'rb') as f:\n        target = np.frombuffer(f.read(), np.uint8, offset=8)\n    return data[:num], target[:num]\n\n", "CodeBleu_score": 29.979011303448853}, {"code": "def load_mnist(images, labels, num)\n    import gzip\n    import struct\n    import numpy as np\n\n    # Open the image and label files\n    with gzip.open(images, 'rb') as f_images, gzip.open(labels, 'rb') as f_labels:\n        # Skip the header bytes\n        f_images.read(16)\n        f_labels.read(8)\n\n        # Initialize empty arrays for the data and target labels\n        data = []\n        target = []\n\n        # Read the specified number of images and labels\n        for i in range(num):\n            # Read the image data\n            image = np.frombuffer(f_images.read(784), dtype=np.uint8)\n            image = image.reshape(28, 28)\n            data.append(image)\n\n            # Read the label data\n            label = struct.unpack('B', f_labels.read(1))[0]\n            target.append(label)\n\n        # Convert the data and target lists to NumPy arrays\n        data = np.array(data)\n        target = np.array(target)\n\n        # Return the data and target arrays\n        return data, target\n", "CodeBleu_score": 44.690055948389364}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.dataframe_length = len(ldf)\n    for attr in ldf.columns:\n        self.unique_values[attr] = list(ldf.unique_values[attr])\n        self.cardinality[attr] = len(ldf.unique_values[attr])\n        if attr in ldf.data_type[\"quantitative\"]:\n            self.min_max[attr] = (ldf.data[attr].min(), ldf.data[attr].max())\n    if ldf.data_type[\"temporal\"] != []:\n        self.unique_values[ldf.data_type[\"temporal\"][0]] = [\n            str(x) for x in self.unique_values[ldf.data_type[\"temporal\"][0]]\n        ]\n    if not ldf.data.index.is_integer():\n        self.unique_values[\"index\"] = list(ldf.unique_values[\"index\"])\n        self.cardinality[\"index\"] = len(ldf.unique_values[\"index\"])\n        if ldf.data_type[\"temporal\"] != []:\n            self.unique_values[\"index\"] = [\n                str(x) for x in self.unique_values[\"index\"]\n            ]\n\n", "CodeBleu_score": 34.59184902037634}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.dataframe_length = len(ldf)\n    for attribute in ldf.columns:\n        self.unique_values[attribute] = list(ldf.unique_values[attribute])\n        self.cardinality[attribute] = len(ldf.unique_values[attribute])\n        if ldf.data_type[attribute] == \"quantitative\":\n            self.min_max[attribute] = (\n                min(ldf.unique_values[attribute]),\n                max(ldf.unique_values[attribute]),\n            )\n    if ldf.data_type[ldf.index_name] != \"integer\":\n        self.unique_values[ldf.index_name] = list(ldf.unique_index_values)\n        self.cardinality[ldf.index_name] = len(ldf.unique_index_values)\n\n", "CodeBleu_score": 32.42707518595351}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.cardinality = dict()\n    self.min_max = dict()\n    self.unique_values = dict()\n    self.dataset_size = len(ldf)\n    for attribute in ldf.columns:\n        self.cardinality[attribute] = len(ldf[attribute].unique())\n        self.unique_values[attribute] = list(ldf[attribute].unique())\n        if pd.api.types.is_numeric_dtype(ldf.data_type[attribute]):\n            self.min_max[attribute] = (ldf[attribute].min(), ldf[attribute].max())\n        if pd.api.types.is_datetime64_any_dtype(ldf.data_type[attribute]):\n            self.min_max[attribute] = (\n                ldf[attribute].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                ldf[attribute].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            )\n    if not pd.api.types.is_integer_dtype(ldf.data_type[ldf.index.name]):\n        self.cardinality[ldf.index.name] = len(ldf.index.unique())\n        self.unique_values[ldf.index.name] = list(ldf.index.unique())\n", "CodeBleu_score": 39.820436515655544}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    import pandas as pd\n    import pandas.io.sql as psql\n    from lux.utils.utils import get_SQL_connection\n\n    conn = get_SQL_connection()\n    preview_query = tbl.preview_query.format(tbl.table_name, preview_size)\n    preview_df = psql.read_sql(preview_query, conn)\n    return preview_df\n\n", "CodeBleu_score": 39.6280767573521}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    import pandas as pd\n    from lux.vis.Vis import Vis\n    from lux.vis.VisList import VisList\n    from lux.utils.utils import get_agg_title\n    import lux\n\n    if not lux.config.preview:\n        return pd.DataFrame()\n    if tbl.preview:\n        return tbl.preview\n\n    # TODO: change to top k\n    preview_size = min(tbl.nrows, preview_size)\n    preview_query = \"SELECT * FROM {} LIMIT {}\".format(tbl.table_name, preview_size)\n    df = tbl.conn.execute_and_fetchall(preview_query)\n    df.columns = df.keys()\n    tbl.preview = df\n    return df\n\n", "CodeBleu_score": 40.20717911885736}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    query = f\"\"\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\"\"\n    return tbl.sql_connection.execute_and_fetchall(query)\n\n", "CodeBleu_score": 17.933076859630766}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n    sample_flag = lux.config.sampling.SAMPLE_FLAG\n    sample_start = lux.config.sampling.SAMPLE_START\n    sample_cap = lux.config.sampling.SAMPLE_CAP\n    sample_frac = lux.config.sampling.SAMPLE_FRAC\n    if sample_flag:\n        table_len = len(tbl.unique_values)\n        limit = min(sample_cap, int(table_len * sample_frac))\n        query = f\"SELECT * FROM {tbl.table_name} LIMIT {limit}\"\n        tbl._sampled = tbl.query(query)\n\n", "CodeBleu_score": 23.336561125716553}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    SAMPLE_FLAG = lux.config.sampling.SAMPLE_FLAG\n    SAMPLE_START = lux.config.sampling.SAMPLE_START\n    SAMPLE_CAP = lux.config.sampling.SAMPLE_CAP\n    SAMPLE_FRAC = lux.config.sampling.SAMPLE_FRAC\n    if SAMPLE_FLAG:\n        if tbl._sampled is None:\n            tbl_len = len(tbl)\n            limit = min(\n                max(SAMPLE_START, round(tbl_len * SAMPLE_FRAC)),\n                SAMPLE_CAP,\n            )\n            tbl._sampled = tbl.query(f\"SELECT * FROM {tbl.tableName} LIMIT {limit}\")\n            tbl._sampled.show_progress = False", "CodeBleu_score": 26.16899574506344}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    import lux\n\n    tbl.sampled = False\n    if lux.config.sampling:\n        tbl._sampled = True\n        tbl._sample_start = lux.config.sampling_start\n        tbl._sample_cap = lux.config.sampling_cap\n        tbl._sample_frac = lux.config.sampling_frac\n        if tbl.cardinality:\n            length = tbl.cardinality\n        else:\n            length = len(tbl.unique_values[\"_merged\"])\n        if tbl._sample_cap and length > tbl._sample_cap:\n            length = tbl._sample_cap\n        limit = tbl._sample_frac * length\n        if tbl._sample_start:\n            start = tbl._sample_start\n        else:\n            start = 0\n        tbl._sample_start = start\n        tbl._sample_cap = limit\n        query = f\"SELECT * FROM {tbl.tableName} LIMIT {limit} OFFSET {start}\"\n        tbl._sampled = tbl.SQL(query)\n", "CodeBleu_score": 25.252021246138174}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    # Override default text settings with provided parameters\n    text_settings = {\n        \"text\": \"text\",\n        \"x\": 0.5,\n        \"y\": 0.5,\n        \"ha\": \"center\",\n        \"va\": \"center\",\n        \"fontsize\": 12,\n        \"color\": \"black\",\n        \"alpha\": 1,\n        \"zorder\": 100,\n        \"bbox\": {\n            \"boxstyle\": \"round\",\n            \"facecolor\": \"white\",\n            \"edgecolor\": \"black\",\n            \"linewidth\": 1,\n            \"alpha\": 0.5,\n        },\n    }\n    text_settings.update(params)\n\n    # Extract text, x, and y values\n    text = text_settings.pop(\"text\")\n    x = text_settings.pop(\"x\")\n    y = text_settings.pop(\"y\")\n\n    # Retrieve background bounds\n    bounds = background.bounds\n\n    # Interpolate x and y values to fit within background bounds\n    x = interpolate(x, bounds[0], bounds[2])\n    y = interpolate(y, bounds[1], bounds[3])\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **text_settings)\n\n", "CodeBleu_score": 23.465737287614832}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    params = {**text_params, **params}\n    text = params.pop(\"text\")\n    x = params.pop(\"x\")\n    y = params.pop(\"y\")\n    xmin, xmax, ymin, ymax = background.bounds\n    x = interpolate_to_fit_bounds(x, xmin, xmax)\n    y = interpolate_to_fit_bounds(y, ymin, ymax)\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 18.4511119184857}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    # Override default text settings\n    text_params = {\n        \"fontsize\": 12,\n        \"fontweight\": \"bold\",\n        \"fontfamily\": \"serif\",\n        \"color\": \"black\",\n        \"ha\": \"center\",\n        \"va\": \"center\",\n        \"bbox\": {\n            \"boxstyle\": \"round\",\n            \"facecolor\": \"white\",\n            \"edgecolor\": \"black\",\n            \"linewidth\": 1,\n            \"alpha\": 0.8,\n        },\n    }\n    text_params.update(params)\n\n    # Extract text, x, and y values\n    text = text_params.pop(\"text\")\n    x = text_params.pop(\"x\")\n    y = text_params.pop(\"y\")\n\n    # Retrieve background bounds\n    bounds = background.bounds\n\n    # Interpolate x and y values to fit within background bounds\n    x = interpolate_to_bounds(x, bounds)\n    y = interpolate_to_bounds(y, bounds)\n\n    # Draw text at calculated position\n    plt.text(x, y, text, **text_params)\n\n", "CodeBleu_score": 23.14845634710875}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n    import datetime as dt\n    import numpy as np\n\n    # Calculate the average number of days between consecutive dates\n    dates = np.array(dates)\n    diff = np.diff(dates)\n    avg_days = np.mean(diff) / dt.timedelta(days=1)\n\n    # If datetime_format is provided, return it\n    if datetime_format:\n        return datetime_format\n\n    # If the data is intraday (average days between points is less than 0.33)\n    if avg_days < 0.33:\n        # Check if the data spans more than one day\n        if dates.max() - dates.min() > dt.timedelta(days=1):\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        # Check if the data spans multiple years\n        if dates.max().year - dates.min().year > 0:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n", "CodeBleu_score": 36.38045330927958}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    if datetime_format is not None:\n        return datetime_format\n    import numpy as np\n    from datetime import timedelta\n    if len(dates) > 1:\n        average_days_between_points = np.mean(np.diff(dates))\n    else:\n        average_days_between_points = 0\n    if average_days_between_points < 0.33:\n        if len(dates) > 1:\n            if dates[0].date() == dates[-1].date():\n                return '%H:%M'\n            else:\n                return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if len(dates) > 1:\n            if dates[0].date() == dates[-1].date():\n                return '%H:%M'\n            else:\n                if dates[0].year == dates[-1].year:\n                    return '%b %d'\n                else:\n                    return '%Y-%b-%d'\n        else:\n            return '%H:%M'\n\n", "CodeBleu_score": 40.9935493857273}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    import numpy as np\n    import datetime as dt\n\n    # Calculate the average number of days between consecutive dates\n    dates = np.array(dates)\n    diffs = np.diff(dates)\n    avg_days = np.mean(diffs) / np.timedelta64(1, 'D')\n\n    if datetime_format:\n        return datetime_format\n\n    if avg_days < 0.33:  # intraday data\n        if dates[-1] - dates[0] > np.timedelta64(1, 'D'):\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:  # daily or less frequent data\n        if dates[-1].year - dates[0].year > 0:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n", "CodeBleu_score": 40.08129927892439}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n    if c.shape == (3,):\n        r, g, b = c\n        a = 1\n    elif c.shape == (4,):\n        r, g, b, a = c\n    else:\n        raise ValueError(f\"Invalid color input shape: {c.shape}\")\n    if r > 1 or g > 1 or b > 1:\n        r, g, b = (r / 255, g / 255, b / 255)\n    if alpha is not None:\n        a = alpha\n    return convert_color(f\"rgb({r}, {g}, {b})\", \"rgba\", alpha=a)", "CodeBleu_score": 17.782615510322074}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if c.shape == (3,):\n        if c.dtype == np.uint8:\n            return (c[0] / 255, c[1] / 255, c[2] / 255, 1)\n        else:\n            return (\n                c[0] / 255 if c[0] > 1 else c[0],\n                c[1] / 255 if c[1] > 1 else c[1],\n                c[2] / 255 if c[2] > 1 else c[2],\n                1,\n            )\n    elif c.shape == (4,):\n        if c.dtype == np.uint8:\n            return (c[0] / 255, c[1] / 255, c[2] / 255, c[3] / 255)\n        else:\n            return (\n                c[0] / 255 if c[0] > 1 else c[0],\n                c[1] / 255 if c[1] > 1 else c[1],\n                c[2] / 255 if c[2] > 1 else c[2],\n                c[3] / 255 if c[3] > 1 else c[3],\n            )\n    else:\n        raise ValueError(\"Invalid color input shape.\")\n\n", "CodeBleu_score": 12.610756935555212}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if c.shape[0] == 3:\n        c = np.concatenate((c, np.array([1.0])))\n    elif c.shape[0] != 4:\n        raise ValueError(\"Color must be in RGB or RGBA format\")\n    c = c.astype(np.float64)\n    if np.any(c[:3] > 1.0):\n        c[:3] /= 255.0\n    if alpha is not None:\n        c[3] = alpha\n    return tuple(int(255 * x) for x in c)\n", "CodeBleu_score": 25.355575020693262}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        style = kwargs['style']\n        del kwargs['style']\n    elif hasattr(self,'mpfstyle'):\n        style = self.mpfstyle\n    else:\n        style = None\n    style = self._check_for_and_apply_style(style)\n    self.mpfstyle = style\n    if len(args) > 0:\n        axes = self.add_subplot(*args,**kwargs)\n    else:\n        axes = self.add_subplot(111,**kwargs)\n    if isinstance(axes,matplotlib.axes.Axes):\n        axes.mpfstyle = style\n    elif isinstance(axes,np.ndarray):\n        for ax in axes:\n            ax.mpfstyle = style\n    else:\n        raise TypeError('Expecting an Axes or ndarray of Axes')\n    return axes", "CodeBleu_score": 47.50750904188098}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        self._check_for_and_apply_style(kwargs['style'])\n    elif hasattr(self, 'mpfstyle'):\n        self._check_for_and_apply_style(self.mpfstyle)\n    else:\n        self._check_for_and_apply_style('default')\n    result = self.fig.subplots(*args, **kwargs)\n    if isinstance(result, Axes):\n        result.mpfstyle = self.mpfstyle\n    elif isinstance(result, np.ndarray):\n        for ax in result:\n            ax.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError('Unexpected type of subplots: {}'.format(type(result)))\n    return result", "CodeBleu_score": 35.608900097433946}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        style = kwargs['style']\n        del kwargs['style']\n    elif hasattr(self, 'mpfstyle'):\n        style = self.mpfstyle\n    else:\n        style = None\n\n    style = self._check_for_and_apply_style(style)\n\n    result = self.figure.subplots(*args, **kwargs)\n\n    if isinstance(result, Axes):\n        result.mpfstyle = style\n    elif isinstance(result, np.ndarray):\n        for ax in result.flat:\n            ax.mpfstyle = style\n    else:\n        raise TypeError('Unexpected type for result: {}'.format(type(result)))\n\n    return result", "CodeBleu_score": 46.90167144899145}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if 'figsize' not in config:\n        fig_ratio = config['fig_ratio']\n        while fig_ratio > 1.5 or fig_ratio < 0.5:\n            fig_ratio = config['fig_ratio']\n        width = config['fig_width']\n        height = width/fig_ratio\n        fig.set_size_inches(width, height)\n    else:\n        fig.set_size_inches(config['figsize'])\n", "CodeBleu_score": 26.000501037772246}, {"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    if not config.get('figsize'):\n        ratio = config.get('figratio',1.618)\n        w = config['figwidth']\n        h = w/ratio\n        if h > 10:\n            w /= h\n            h = 10\n        fig.set_size_inches(w,h)\n    else:\n        fig.set_size_inches(*config['figsize'])", "CodeBleu_score": 23.488376210962794}, {"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    if 'figsize' not in config:\n        figratio = config['figratio']\n        figsize = config['figsize']\n        if figratio:\n            if figratio > 1:\n                figsize = (figsize,figsize/figratio)\n            else:\n                figsize = (figsize*figratio,figsize)\n        fig.set_size_inches(figsize)\n    else:\n        fig.set_size_inches(config['figsize'])", "CodeBleu_score": 30.535436172139786}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        apmav = config['mav']\n    if apwidth is None:\n        apwidth = config['width']\n    if isinstance(apmav,dict):\n        mav_shift = apmav['shift']\n        mav_period = apmav['period']\n    else:\n        mav_shift = [0]\n        mav_period = apmav\n    for i,p in enumerate(mav_period):\n        if i > 6:\n            break\n        mav = prices.rolling(window=p).mean()\n        if mav_shift[i] != 0:\n            mav = mav.shift(mav_shift[i])\n        ax.plot(xdates,mav,linewidth=apwidth,color=config['color_cycle'][i])\n    return mav\n", "CodeBleu_score": 29.52298592356117}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    from matplotlib.ticker import FuncFormatter\n    from matplotlib.dates import DateFormatter\n    import matplotlib.dates as mdates\n    import matplotlib.ticker as ticker\n    import matplotlib.gridspec as gridspec\n    import matplotlib.patches as patches\n    import matplotlib.lines as lines\n    import matplotlib.text as text\n    import matplotlib.colors as colors\n    import matplotlib.cm as cm\n    import matplotlib.colorbar as colorbar\n    import matplotlib.patheffects as patheffects\n    import matplotlib.collections as collections\n    import matplotlib.collections.event_collection as event_collection\n    import matplotlib.collections.collection as collection\n    import matplotlib.collections.path_collection as path_collection\n    import matplotlib.collections.quad_munch as quad_munch\n    import matplotlib.collections.line_collection as line_collection\n    import matplotlib.collections.poly_collection as poly_collection\n    import matplotlib.collections.registry as registry\n    import matplotlib.collections.collection as collection\n    import matplotlib.collections.path_collection as path_collection\n    import matplotlib.collections.quad_munch as quad_munch\n    import matplotlib.collections.line_collection as line_collection\n    import matplotlib.collections.poly_collection as poly_collection\n    import matplotlib.collections.registry as registry\n    import matplotlib.collections.collection as collection\n    import matplotlib.collections.path_collection as path_collection\n    import matplotlib.collections.quad_munch as quad_munch\n    import matplotlib.collections.line_collection as line_collection\n    import matplotlib.collections.poly_collection as poly_collection\n    import matplotlib.collections.registry as registry\n    import matplotlib.collections.collection as collection\n    import matplotlib.collections.", "CodeBleu_score": 9.519966577178726}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        mavs = config['mav']\n    else:\n        mavs = apmav\n    if isinstance(mavs,dict):\n        shift = mavs['shift']\n        periods = mavs['periods']\n    else:\n        shift = 0\n        periods = mavs\n    if len(periods) > 7:\n        periods = periods[:7]\n    for i,p in enumerate(periods):\n        mav = prices.rolling(p).mean()\n        if shift != 0:\n            mav = mav.shift(shift)\n        ax.plot(xdates,mav,linewidth=config['linewidth'],color=config['color_cycle'][i])\n    return mavs\n", "CodeBleu_score": 27.509321562922374}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    assert isinstance(opnode, ConvolutionForward)\n    assert len(inputs) == 1\n    assert len(outputs) == 1\n    input_tensor = inputs[0]\n    output_tensor = outputs[0]\n    assert isinstance(input_tensor, Tensor)\n    assert isinstance(output_tensor, Tensor)\n    assert input_tensor.shape[1] == output_tensor.shape[1]\n    assert input_tensor.shape[2] == output_tensor.shape[2]\n    assert input_tensor.shape[3] == output_tensor.shape[3]\n    kernel_height = opnode.kernel_height\n    kernel_width = opnode.kernel_width\n    input_channels = input_tensor.shape[0]\n    output_elements = output_tensor.shape[0] * output_tensor.shape[1] * output_tensor.shape[2] * output_tensor.shape[3]\n    flops = output_elements * input_channels * kernel_height * kernel_width\n    if opnode.bias is not None:\n        flops += output_elements\n    return flops\n\n", "CodeBleu_score": 16.223522922062084}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    assert len(inputs) == 1\n    assert len(outputs) == 1\n    input = inputs[0]\n    output = outputs[0]\n    assert input.shape[1] == output.shape[1]\n    kernel_ops = opnode.kernel_h * opnode.kernel_w\n    bias_ops = 1 if opnode.with_bias else 0\n    kernel_ops = kernel_ops * input.shape[1]\n    return (kernel_ops + bias_ops) * output.size\n\n", "CodeBleu_score": 30.612654702116025}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    # Get the kernel height and width from the input tensor's shape\n    kernel_height, kernel_width = opnode.kernel_size\n\n    # Determine the number of input channels\n    num_input_channels = inputs[0].shape[1]\n\n    # Calculate the total number of output elements\n    total_output_elements = outputs[0].numel()\n\n    # Compute the FLOPs\n    flops = total_output_elements * num_input_channels * kernel_height * kernel_width\n\n    # Check if the operation includes a bias term\n    if opnode.bias is not None:\n        # Add the bias term to the FLOPs\n        flops += total_output_elements\n\n    return flops\n\n", "CodeBleu_score": 15.26149926528263}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n    import numpy as np\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    from ..ir.type import _dtype_to_ir_type\n    return _dtype_to_ir_type[mge_dtype.type](mge_dtype)\n\n", "CodeBleu_score": 29.794681217650165}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    import numpy as np\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n", "CodeBleu_score": 22.773966519893015}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    import numpy as np\n    from ..ir.type import _dtype_to_ir_type\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n", "CodeBleu_score": 44.97593358659735}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = np.ascontiguousarray(x)\n    dtype = x.dtype\n    if dtype == np.dtype(np.bool_):\n        x = x.astype(np.int8)\n        dtype = x.dtype\n    if dtype == np.dtype(np.bfloat16):\n        x = x.astype(np.float32)\n        dtype = x.dtype\n    ir_type = _dtype_to_ir_type(dtype)\n    shape = x.shape\n    elements = ir.DenseElementsAttr.get(np.reshape(x, (-1,)), type=ir_type)\n    return [ir.ConstantOp(elements, ir.TypeAttr.get(ir_type), shape)]\n\n", "CodeBleu_score": 44.25658473972235}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = x.copy()\n        x.dtype = canonicalize_dtype(x.dtype)\n    ir_type = ir.RankedTensorType(x.shape, ir_type_from_dtype(x.dtype))\n    if x.dtype == np.bool_:\n        x = x.astype(np.int8)\n    if x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    elements = ir.DenseElementsAttr.get(x.shape, x.dtype, x)\n    return [ir.ConstantOp(elements, ir_type).result]", "CodeBleu_score": 44.289678986316204}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = x.copy()\n    if x.dtype.kind == \"b\":\n        x = x.astype(np.bool_)\n    elif x.dtype.kind == \"f\" and x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    if not x.flags[\"C_CONTIGUOUS\"]:\n        x = np.ascontiguousarray(x)\n    ir_type = ir.RankedTensorType(x.shape, to_ir_type(x.dtype))\n    return [ir.ConstantOp(ir_type, ir.DenseElementsAttr.get(x))]", "CodeBleu_score": 38.04978375356633}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if not val.size:\n        raise ValueError(\"Cannot convert empty NumPy array to TorchScript.\")\n    if val.strides:\n        # Collapse zero-stride axes.\n        zero_stride_axes = [i for i, stride in enumerate(val.strides) if stride == 0]\n        non_zero_stride_axes = [\n            i for i, stride in enumerate(val.strides) if stride != 0\n        ]\n        collapsed_array = np.ascontiguousarray(val)\n        if canonicalize_types:\n            collapsed_array = collapsed_array.astype(canonicalize_dtype(val.dtype))\n        collapsed_array = collapsed_array.reshape(\n            [val.shape[i] for i in non_zero_stride_axes]\n        )\n        # Broadcast the collapsed array to the original shape.\n        broadcasted_array = np.broadcast_to(\n            collapsed_array, [val.shape[i] for i in range(val.ndim)]\n        )\n        return _numpy_array_constant(broadcasted_array, canonicalize_types)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 37.265762116169526}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if (val.strides == (0,) * val.ndim) and (val.size != 0):\n        zero_stride_axes = [i for (i, stride) in enumerate(val.strides) if stride == 0]\n        non_zero_stride_axes = [i for (i, stride) in enumerate(val.strides) if stride != 0]\n        collapsed_array = val.transpose(non_zero_stride_axes)\n        if canonicalize_types:\n            collapsed_array = _canonicalize_dtype(collapsed_array)\n        broadcast_op = _numpy_array_constant(collapsed_array, canonicalize_types)\n        broadcast_op = ir.op.expand_dims(broadcast_op, axes=zero_stride_axes)\n        return [broadcast_op]\n    else:\n        return _numpy_array_constant(val, canonicalize_types)", "CodeBleu_score": 30.704451682957835}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if not val.shape:\n        return []\n    if not val.strides:\n        return [_numpy_array_constant(val, canonicalize_types)]\n    zero_stride_axes = []\n    non_zero_stride_axes = []\n    for i, stride in enumerate(val.strides):\n        if stride == 0:\n            zero_stride_axes.append(i)\n        else:\n            non_zero_stride_axes.append(i)\n    if not zero_stride_axes:\n        return [_numpy_array_constant(val, canonicalize_types)]\n    collapsed = np.squeeze(val, axis=tuple(zero_stride_axes))\n    if canonicalize_types:\n        collapsed = canonicalize_dtype(collapsed.dtype)\n    broadcasted = np.broadcast_to(collapsed, val.shape)\n    return [_numpy_array_constant(broadcasted, canonicalize_types)]\n\n", "CodeBleu_score": 30.71952389503887}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    bits = pack_bits(xs)\n    if len(xs) == 1:\n        bits = pack_bits([xs[0]] * 8)\n    return ir.DenseElementsAttr.get(\n        np.frombuffer(bits, dtype=np.uint8, count=len(bits)).astype(np.int8),\n        ir.IntegerType.signless(1),\n        shape=(len(xs),),\n    )\n\n", "CodeBleu_score": 36.942220687910954}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    bits = bytearray(math.ceil(len(xs) / 8))\n    for i, x in enumerate(xs):\n        bits[i // 8] |= x << (i % 8)\n    if len(bits) == 0:\n        bits = b\"\\x00\"\n    elif len(xs) % 8 == 0:\n        bits[-1] = bits[-1] & 0xFE\n    return ir.DenseElementsAttr.get_raw(\n        type=ir.IntegerType.get_signless(1),\n        shape=(len(xs),),\n        buffer=bits,\n    )\n\n", "CodeBleu_score": 32.21342053562159}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    if len(xs) == 1:\n        xs = [False] if xs[0] else [True]\n    bits = packbits(xs, endian=\"little\")\n    return ir.DenseElementsAttr.get(\n        ir.Type.get_signless_int_type(1),\n        shape=len(xs),\n        data=bits,\n    )\n\n", "CodeBleu_score": 43.705002240818715}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == 1 and len(ctx.vars_in) == 1 and len(ctx.vars_out) == 1\n    input_var, = ctx.vars_in\n    output_var, = ctx.vars_out\n    if input_var.bound_data is not None:\n        ctx.module.set_var_values({input_var.name: input_var.bound_data, output_var.name: output_var.bound_data})\n    assert input_var.shape == output_var.shape\n    if output_var.bound_data is not None:\n        data = output_var.bound_data.astype(output_var.dtype)\n    elif input_var.bound_data is not None:\n        data = input_var.bound_data.astype(input_var.dtype)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n    return HLOTensor(data, output_var.shape, output_var.dtype, name=output_var.name)\n\n", "CodeBleu_score": 45.715108769078775}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    var_in, var_out = ctx.vars_in[0], ctx.vars_out[0]\n    if var_in.bound_data is not None:\n        var_in.set_value(var_in.bound_data)\n        var_out.set_value(var_in.bound_data)\n    assert var_in.shape == var_out.shape\n    if var_in.bound_data is not None:\n        var_out.bound_data = var_out.bound_data.astype(var_out.dtype)\n    elif var_out.bound_data is not None:\n        var_in.bound_data = var_out.bound_data.astype(var_in.dtype)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n    return HLOTensor(var_out.bound_data)\n\n", "CodeBleu_score": 65.89265233007191}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    input_var, = ctx.vars_in\n    output_var, = ctx.vars_out\n    if input_var.bound_data is not None:\n        ctx.module.set_input(input_var.name, input_var.bound_data)\n        ctx.module.set_output(output_var.name, output_var.bound_data)\n    assert input_var.shape == output_var.shape\n    if input_var.bound_data is not None:\n        data = input_var.bound_data.asnumpy()\n    elif output_var.bound_data is not None:\n        data = output_var.bound_data.asnumpy()\n    else:\n        assert False, \"Only tensor creation from constants is supported.\"\n    return HLOTensor(data.astype(output_var.dtype), output_var.shape)\n\n", "CodeBleu_score": 49.34853286850579}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n    import numpy as np\n    import tensorflow as tf\n\n    def _get_dtype(dtype):\n        if dtype is None:\n            return np.float32\n        else:\n            return dtype\n\n    def _device_put(x, device):\n        with tf.device(device):\n            return tf.convert_to_tensor(x)\n\n    if isinstance(x, (int, float)):\n        x = np.array(x).reshape((1,))\n    if not isinstance(x, np.ndarray):\n        raise TypeError(f\"Expected a numpy array, got {type(x)}\")\n    if x.ndim != 0:\n        raise ValueError(f\"Expected a scalar, got {x.shape}\")\n    return _device_put(x, device)\n\n", "CodeBleu_score": 37.126226118221574}, {"code": "def _device_put_scalar(x, device)\n    return _device_put_array(_scalar_to_numpy(x), device)\n\n", "CodeBleu_score": 4.836453150586685}, {"code": "def _device_put_scalar(x, device)\n    return _device_put_array(_numpy_scalar_to_array(x), device)\n\n", "CodeBleu_score": 5.1391897539806894}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) > 0 and len(rhs_shape) > 0:\n        assert isinstance(lhs_shape[0], int)\n        assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 51.38877167769426}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list)) and isinstance(\n        rhs_shape, (tuple, list)\n    ), \"Shape must be a tuple or list\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert isinstance(lhs_shape[0], int) and isinstance(\n        rhs_shape[0], int\n    ), \"Shape must be a tuple or list of integers\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 52.312431903035026}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list)) and isinstance(rhs_shape, (tuple, list)), \"Shape must be a tuple or list.\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert isinstance(lhs_shape[0], int) and isinstance(rhs_shape[0], int), \"Shape must be a tuple or list of integers.\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 49.65955556441587}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    import pandas as pd\n\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    test = pd.read_csv(test_set_filename, dtype=dtypes)\n    train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_meta = pd.read_csv(\n        test_set_metadata_filename,\n        dtype={**meta_dtypes, \"target\": None},\n    )\n    return train, test, train_meta, test_meta\n\n", "CodeBleu_score": 34.91590011779161}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    import pandas as pd\n\n    train_df = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_df = pd.read_csv(test_set_filename, dtype=dtypes)\n    train_meta_df = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes_without_target = meta_dtypes.copy()\n    meta_dtypes_without_target.pop(\"target\")\n    test_meta_df = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes_without_target)\n    return train_df, test_df, train_meta_df, test_meta_df\n\n", "CodeBleu_score": 42.379079187358236}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    import pandas as pd\n\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n    training_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes_copy = meta_dtypes.copy()\n    meta_dtypes_copy.pop(\"target\")\n    test_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes_copy)\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 42.02398795164063}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n    from matplotlib import pyplot as plt\n    from matplotlib import rcmod\n    axes_style = rcmod.axes_style\n    with plt.style.context(axes_style(\"white\")):\n        fig, ax = plt.subplots(figsize=(6, 2), facecolor=\"w\")\n        ax.set_axis_off()\n        ax.imshow(np.linspace(0, 1, 256)[None], aspect=\"auto\", cmap=cmap)\n        plt.show()\n", "CodeBleu_score": 36.49356190846534}, {"code": "def _show_cmap(cmap)\n    from matplotlib import pyplot as plt\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import figure as figuremod\n    from matplotlib import rcParams as rcmod\n    from matplotlib", "CodeBleu_score": 10.667170934492438}, {"code": "def _show_cmap(cmap)\n    from matplotlib import pyplot as plt\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import colorbar as colorbarmod\n    from matplotlib import gridspec as gridspecmod\n    from matplotlib import ticker as tickermod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import colorbar as colorbarmod\n    from matplotlib import gridspec as gridspecmod\n    from matplotlib import ticker as tickermod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import colorbar as colorbarmod\n    from matplotlib import gridspec as gridspecmod\n    from matplotlib import ticker as tickermod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import colorbar as colorbarmod\n    from matplotlib import gridspec as gridspecmod\n    from matplotlib import ticker as tickermod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod\n    from matplotlib import colorbar as colorbarmod\n    from matplotlib import gridspec as gridspecmod\n    from matplotlib import ticker as tickermod\n    from matplotlib import rcParams as rcmod\n    from matplotlib import axes as axesmod\n    from matplotlib import cm as cmmod\n    from matplotlib import colors as colorsmod", "CodeBleu_score": 13.6809349109}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is None:\n        return xdata, ydata\n    elif obj.get_transform() != obj.axes.transData:\n        points = obj.get_transform().transform(list(zip(xdata, ydata)))\n        return points[:, 0], points[:, 1]\n    else:\n        return xdata, ydata\n\n", "CodeBleu_score": 46.15628803877465}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    x, y = xdata, ydata\n    if obj.axes is not None and obj.get_transform() != obj.axes.transData:\n        transform = obj.get_transform() + obj.axes.transData.inverted()\n        x, y = transform.transform((x, y))\n    return x, y\n\n", "CodeBleu_score": 54.79814507055665}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is None:\n        return xdata, ydata\n    elif obj.get_transform() != obj.axes.transData:\n        transform = obj.get_transform() + obj.axes.transData.inverted()\n        points = transform.transform(list(zip(xdata, ydata)))\n        return points[:, 0], points[:, 1]\n    else:\n        return xdata, ydata\n\n", "CodeBleu_score": 52.96039953059052}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, \"data\")\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 56.13783295127991}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, \"data\")\n    if data2 is not None and not isinstance(data2, (list, np.ndarray)):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n", "CodeBleu_score": 55.03745983926559}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, \"data\")\n    if data2 is not None and not isinstance(data2, (list, tuple)):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 54.923223626164244}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n    # Correct the word \"programa\" to \"programar\"\n    df.loc[df['token'] == 'programa', 'token'] = 'programar'\n\n    # Filter the dataframe to include only alphabetic tokens that are longer than one character and are not stop words\n    df = df[(df['token'].str.isalpha()) & (df['token'].str.len() > 1) & (~df['token'].isin(stopwords))]\n\n    # Count the occurrences of each word\n    word_counts = df['token'].value_counts()\n\n    # Select the top 20 most frequent words\n    top_words = word_counts.head(20)\n\n    # Create a bar plot showing the frequency of the top 20 words\n    plt.figure(figsize=(10, 6))\n    plt.bar(top_words.index, top_words.values)\n    plt.xticks(rotation=90)\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Most Frequent Words in the DataFrame')\n    plt.savefig('most_frequent_words.png')\n    plt.show()\n", "CodeBleu_score": 26.715596352462555}, {"code": "def plot_most_used_words(df)\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import nltk\n    from nltk.corpus import stopwords\n\n    # Correct the word \"programa\" to \"programar\"\n    df['text'] = df['text'].str.replace('programa', 'programar')\n\n    # Filter the dataframe to include only alphabetic tokens that are longer than one character and are not stop words\n    filtered_df = df[df['text'].apply(lambda x: x.isalpha() and len(x) > 1 and x not in stopwords.words('spanish'))]\n\n    # Count the occurrences of the remaining words\n    word_counts = filtered_df['text'].value_counts()\n\n    # Select the top 20 most frequent words\n    top_words = word_counts.head(20)\n\n    # Create a bar plot showing the frequency of the top 20 most frequent words\n    plt.figure(figsize=(10, 6))\n    plt.bar(top_words.index, top_words.values)\n    plt.xticks(rotation=90)\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Most Frequent Words')\n    plt.tight_layout()\n    plt.savefig('most_used_words.png')\n    plt.show()\n", "CodeBleu_score": 32.48042636821254}, {"code": "def plot_most_used_words(df)\n    df['token'] = df['token'].replace('programa', 'programar')\n    df = df[(df['token'].str.isalpha()) & (df['token'].str.len() > 1) & (~df['token'].isin(stopwords))]\n    df_count = df.groupby('token')['token'].count().sort_values(ascending=False)\n    df_count = df_count.head(20)\n    plt.figure(figsize=(10, 6))\n    plt.bar(df_count.index, df_count.values)\n    plt.xticks(rotation=90)\n    plt.xlabel('Palavras')\n    plt.ylabel('Frequ\u00eancia')\n    plt.title('Palavras mais frequentes')\n    plt.savefig('most_used_words.png')\n", "CodeBleu_score": 35.590457268367665}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    from bottleneck import move_nanmean, move_sum, move_std, move_min, move_max, move_median, move_argmin, move_argmax, move_rank\n\n    a = da.from_array(a, chunks=a.shape)\n    depth = {0: window[0], 1: window[1]}\n    boundary = {0: 'nearest', 1: 'nearest'}\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n\n    if moving_func == move_nanmean:\n        fill_value = da.nan\n    elif moving_func == move_sum:\n        fill_value = 0\n    elif moving_func == move_std:\n        fill_value = da.nan\n    elif moving_func == move_min:\n        fill_value = da.inf\n    elif moving_func == move_max:\n        fill_value = -da.inf\n    elif moving_func == move_median:\n        fill_value = da.nan\n    elif moving_func == move_argmin:\n        fill_value = da.inf\n    elif moving_func == move_argmax:\n        fill_value = -da.inf\n    elif moving_func == move_rank:\n        fill_value = da.nan\n\n    result = da.map_blocks(moving_func, ag, window=window, min_count=min_count, axis=axis, dtype=a.dtype, fill_value=fill_value)\n    result = da.overlap.trim_internal(result, depth)\n    return result\n", "CodeBleu_score": 37.843755116366566}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n\n    if axis < 0:\n        axis = a.ndim + axis\n\n    if a.dtype.kind == \"f\":\n        a = a.astype(\"float64\")\n        fill_value = np.nan\n    elif a.dtype.kind == \"i\":\n        a = a.astype(\"int64\")\n        fill_value = np.iinfo(np.int64).min\n    else:\n        raise ValueError(\"Unsupported dtype\")\n\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = window // 2\n    boundary = {d: fill_value for d in range(a.ndim)}\n\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    out = da.map_blocks(\n        moving_func, ag, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    out = da.overlap.trim_internal(out, depth)\n    return out\n\n", "CodeBleu_score": 68.67157300551581}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n\n    a = da.from_array(a, chunks=a.shape)\n    a = a.astype(a.dtype)\n    window = int(window)\n\n    if min_count is None:\n        min_count = int(window)\n    else:\n        min_count = int(min_count)\n\n    depth = {\n        1: 0,\n        2: 1,\n    }[a.ndim]\n\n    boundary = {\n        1: \"nearest\",\n        2: \"edge\",\n    }[a.ndim]\n\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    res = da.map_blocks(\n        moving_func, ag, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    res = da.overlap.trim_internal(res, depth)\n    return res\n\n", "CodeBleu_score": 52.147333795430015}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import dask.array as da\n    import numpy as np\n    from sklearn.linear_model import LinearRegression\n\n    # Convert input arrays to Dask arrays\n    lhs = da.from_array(lhs, chunks=(lhs.shape[0], -1))\n    rhs = da.from_array(rhs, chunks=(rhs.shape[0], -1))\n\n    # Handle missing values if skipna is True\n    if skipna:\n        # Reshape right-hand side array if necessary\n        if rhs.ndim == 1:\n            rhs = rhs.reshape(-1, 1)\n\n        # Apply polynomial fit along the specified axis\n        coeffs = da.apply_along_axis(lambda x: np.polyfit(lhs, x, deg=1, rcond=rcond)[0], axis=0, arr=rhs)\n        resids = rhs - da.apply_along_axis(lambda x: np.polyval(coeffs, lhs), axis=0, arr=lhs)\n\n    # Directly compute least squares solution if skipna is False\n    else:\n        # Compute coefficients and residuals using Dask's linear algebra functions\n        coeffs, _, _, _ = da.linalg.lstsq(lhs, rhs)\n        resids = rhs - lhs @ coeffs\n\n    return coeffs, resids\n\n", "CodeBleu_score": 41.02212577048512}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import dask.array as da\n    import numpy as np\n    import dask\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=(lhs.shape[0], -1))\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=(rhs.shape[0], -1))\n\n    if skipna:\n        if rhs.ndim == 1:\n            rhs = rhs.reshape(-1, 1)\n        elif rhs.ndim > 2:\n            raise ValueError(\"rhs must be 1D or 2D\")\n        if rhs.chunks[1] != (1,):\n            rhs = rhs.rechunk({1: -1})\n        coeffs, residuals, rank, s = da.linalg.lstsq(lhs, rhs)\n        return coeffs, residuals\n    else:\n        return da.linalg.lstsq(lhs, rhs, rcond=rcond)\n", "CodeBleu_score": 38.31335977262923}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import dask.array as da\n    import numpy as np\n\n    # Convert input arrays to Dask arrays\n    lhs = da.from_array(lhs, chunks=(lhs.shape[0], 1))\n    rhs = da.from_array(rhs, chunks=(rhs.shape[0], 1))\n\n    if skipna:\n        # Reshape right-hand side array if necessary\n        if rhs.ndim == 1:\n            rhs = rhs.reshape(-1, 1)\n\n        # Apply polynomial fit along the specified axis\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n        # Extract coefficients and residuals from the result\n        coeffs = coeffs.compute()\n        residuals = residuals.compute()\n    else:\n        # Directly compute the least squares solution using Dask's linear algebra functions\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n    return coeffs, residuals\n\n", "CodeBleu_score": 36.42031511799367}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)", "CodeBleu_score": 70.27898286466454}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, da.Array):\n        func, args = self.data.__dask_postpersist__()\n        return (\n            finalize,\n            (\n                self,\n                func,\n                args,\n            ),\n        )\n    raise AttributeError(\n        \"Data is not a dask array, so it cannot be post-persisted.\",\n    )\n\n", "CodeBleu_score": 48.6581176795047}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, dask.array.core.Array):\n        return self.data.__dask_postpersist__(self.finalize, self.data, self.data.dask)\n    else:\n        raise AttributeError(\n            f\"{type(self.data)} has no attribute __dask_postpersist__\",\n        )", "CodeBleu_score": 41.734454609103075}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, da.Array):\n        func, args = self.data.__dask_postpersist__()\n\n        def finalize(graph, key, value, *args, **kwargs):\n            return func(graph, key, value, *args, **kwargs)\n\n        return finalize, args\n    else:\n        raise AttributeError(\"Data is not a dask array\")\n\n", "CodeBleu_score": 47.22431280005023}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    f.__name__ = name\n    return f\n\n", "CodeBleu_score": 57.601020881798284}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 47.70928545705178}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f", "CodeBleu_score": 57.62769705687647}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask and isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        return dask.tokenize(arr1) == dask.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 51.744152029859066}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask and isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        return tokenize(arr1) == tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 55.44827558831069}, {"code": "def lazy_array_equiv(arr1, arr2)\n    import numpy as np\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if hasattr(arr1, \"dask\"):\n        if hasattr(arr2, \"dask\"):\n            import dask.array as da\n            if da.tokenize(arr1) == da.tokenize(arr2):\n                return True\n        return None\n    return None", "CodeBleu_score": 59.2482529682117}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n\n    if chunks is not None:\n        x = da.random.RandomState(seed).normal(0, 1, size=shape, chunks=chunks)\n    else:\n        x = np.random.RandomState(seed).normal(0, 1, size=shape)\n\n    if frac_nan is not None:\n        mask = da.random.RandomState(seed+1).random_sample(shape) < frac_nan\n        x = x.where(~mask)\n\n    return x", "CodeBleu_score": 51.018680763866044}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import dask.array as da\n    import numpy as np\n    import random\n    if chunks is not None:\n        x = da.random.random(shape, chunks=chunks)\n    else:\n        x = np.random.random(shape)\n    if frac_nan is not None:\n        x[np.random.random(shape) < frac_nan] = np.nan\n    return x\n\n", "CodeBleu_score": 42.927156019636755}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate the random numbers\n    if chunks:\n        # Use Dask to create the array with chunking\n        x = da.random.normal(size=shape, chunks=chunks)\n    else:\n        # Use NumPy to create the array\n        x = np.random.normal(size=shape)\n\n    # Set the specified fraction of elements to NaN\n    if frac_nan:\n        x[np.random.choice(np.arange(x.size),\n                           replace=False,\n                           size=int(x.size * frac_nan))] = np.nan\n\n    return x\n\n", "CodeBleu_score": 41.833663229090234}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target", "CodeBleu_score": 81.06081450739325}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.financial_statements:\n        raise ValueError(f\"Invalid financial statement name: {name}\")\n    if timescale not in self.periods:\n        raise ValueError(f\"Invalid timescale: {timescale}\")\n    try:\n        self._create_financials_table()\n    except Exception as e:\n        self.logger.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()\n    return self.financials_table", "CodeBleu_score": 29.81233757453539}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.allowed_financial_statements:\n        raise ValueError(f\"{name} not in {self.allowed_financial_statements}\")\n    if timescale not in self.allowed_periods:\n        raise ValueError(f\"{timescale} not in {self.allowed_periods}\")\n    try:\n        financials_table = self._create_financials_table(name, timescale, proxy)\n        return financials_table\n    except Exception as e:\n        self.logger.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()", "CodeBleu_score": 37.72598695497519}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.financial_statements:\n        raise ValueError(f\"Invalid financial statement name: {name}\")\n\n    if timescale not in self.periods:\n        raise ValueError(f\"Invalid timescale: {timescale}\")\n\n    try:\n        financials = self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        self.logger.error(f\"Error fetching financials for {name} with timescale {timescale}: {e}\")\n        return pd.DataFrame()\n\n    return financials\n", "CodeBleu_score": 39.12727088982976}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n    ownership = data['fundOwnership']['ownershipList']\n    for i in ownership:\n        i['reportDate'] = pd.to_datetime(i['reportDate'])\n        i.pop('maxAge')\n    df = pd.DataFrame(ownership)\n    if not df.empty:\n        df['reportDate'] = pd.to_datetime(df['reportDate'])\n        df = df.rename(columns={'reportDate': 'Date', 'organization': 'Organization', 'value': 'Value', 'pctHeld': 'Percentage'})\n    self._mutualfund = df\n", "CodeBleu_score": 41.48170612073282}, {"code": "def _parse_fund_ownership(self, data)\n    ownership = data[\"ownership\"]\n    ownership_list = []\n    for owner in ownership:\n        ownership_list.append(owner)\n    ownership_df = pd.DataFrame(ownership_list)\n    ownership_df = ownership_df.drop(\"maxAge\", axis=1)\n    ownership_df[\"reportDate\"] = pd.to_datetime(ownership_df[\"reportDate\"])\n    ownership_df = ownership_df.rename(\n        columns={\n            \"entityProperName\": \"name\",\n            \"latestPosting\": \"latest_posting\",\n            \"latestReported\": \"latest_reported\",\n            \"latestReportedDate\": \"latest_reported_date\",\n            \"latestReportedType\": \"latest_reported_type\",\n            \"reportedHolding\": \"reported_holding\",\n            \"reportedHoldingDate\": \"reported_holding_date\",\n            \"reportedHoldingType\": \"reported_holding_type\",\n            \"sharesHeld\": \"shares_held\",\n        }\n    )\n    self._mutualfund = ownership_df\n", "CodeBleu_score": 23.451887855867486}, {"code": "def _parse_fund_ownership(self, data)\n    data = data['result']['fundOwnership']\n    ownership_list = []\n    for ownership in data:\n        ownership.pop('maxAge')\n        ownership_list.append(ownership)\n    self._mutualfund = pd.DataFrame(ownership_list)\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n        self._mutualfund.rename(columns={'value': 'fundValue', 'name': 'fundName', 'share': 'fundShare', 'fundValue': 'fundValue', 'fundShare': 'fundShare'}, inplace=True)\n\n", "CodeBleu_score": 25.781311793060617}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n    if self.currency is not None:\n        return self.currency\n    else:\n        metadata = self.history_metadata(proxy=self.proxy)\n        self.currency = metadata[\"currency\"]\n        return self.currency\n", "CodeBleu_score": 58.25333479997333}, {"code": "def currency(self)\n    if self.currency:\n        return self.currency\n    else:\n        metadata = self._get_history_metadata(proxy=self.proxy)\n        self.currency = metadata['currency']\n        return self.currency\n", "CodeBleu_score": 50.13104388310716}, {"code": "def currency(self)\n    if self.currency is None:\n        metadata = self.history_metadata(use_proxy=self.use_proxy)\n        self.currency = metadata[\"currency\"]\n    return self.currency\n\n", "CodeBleu_score": 47.28600548632876}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = pd.DataFrame(\n                self._get_financial_data(\"recommendationTrend\")\n            )\n        except KeyError:\n            self._recommendations = pd.DataFrame()\n    if self._recommendations.empty:\n        return self._recommendations\n    try:\n        self._recommendations = self._recommendations[\"trend\"].apply(\n            pd.Series\n        )\n        self._recommendations.columns = [\"date\", \"recommendation\"]\n        self._recommendations[\"date\"] = pd.to_datetime(\n            self._recommendations[\"date\"], unit=\"s\"\n        )\n    except (KeyError, TypeError, ValueError):\n        raise YFDataException(\n            f\"Could not parse recommendation trend data for {self.symbol}\"\n        )\n    return self._recommendations\n\n", "CodeBleu_score": 40.83727514157328}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = self._proxy.recommendationTrend(\n                self.ticker)\n        except Exception:\n            self._recommendations = pd.DataFrame()\n    if self._recommendations is not None:\n        try:\n            self._recommendations = self._recommendations.recommendation.to_frame()\n            self._recommendations.index.name = 'Date'\n            self._recommendations.columns = ['Recommendation']\n        except Exception:\n            raise YFDataException(\n                \"Error parsing recommendation data for \" + self.ticker)\n    return self._recommendations", "CodeBleu_score": 41.523421989978445}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = self._fetch_data(\n                \"recommendationTrend\", proxy=self._proxy\n            )\n        except Exception as e:\n            self._recommendations = pd.DataFrame()\n\n    if self._recommendations.empty:\n        return self._recommendations\n\n    try:\n        self._recommendations = self._recommendations.set_index(\"symbol\")\n        self._recommendations = self._recommendations.apply(\n            lambda x: pd.Series(\n                [\n                    x.date,\n                    x.toBuy,\n                    x.hold,\n                    x.toSell,\n                    x.strongBuy,\n                    x.strongSell,\n                ]\n            ),\n            axis=1,\n        )\n        self._recommendations.columns = [\n            \"date\",\n            \"to_buy\",\n            \"hold\",\n            \"to_sell\",\n            \"strong_buy\",\n            \"strong_sell\",\n        ]\n        self._recommendations = self._recommendations.sort_index()\n    except Exception as e:\n        raise YFDataException(\n            \"Could not parse recommendation trend data from the proxy server.\"\n        )\n\n    return self._recommendations", "CodeBleu_score": 37.199423964808915}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    import time\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn import metrics\n    from sklearn.cluster import KMeans\n\n    # Check if the classifier has the 'n_clusters' attribute\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError(\n            \"The classifier does not have the 'n_clusters' attribute\")\n\n    # Set the cluster range if not provided\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n\n    # Create an empty list to store the SSE for each cluster number\n    sse = []\n\n    # Loop over the cluster range and calculate the SSE for each number of clusters\n    for cluster in cluster_ranges:\n        clf.n_clusters = cluster\n        clf.fit(X)\n        sse.append(clf.inertia_)\n\n    # Calculate the elbow point\n    n = np.argmin(np.diff(sse))\n    elbow_point = cluster_ranges[n]\n\n    # Plot the elbow curve\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, sse, 'bx-')\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors (SSE)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Plot the elbow point\n    ax.plot(elbow_point, sse[n], 'rx')\n    ax.annotate(f'Elbow Point: {elbow_point} clusters', xy=(\n        elbow_point, sse[n]), xytext=(elbow_point, sse[n]), fontsize=text_fontsize)\n\n    # Plot the clustering duration on a secondary y-axis\n    if show_cluster_time:\n        start_time", "CodeBleu_score": 45.67073641494159}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import time\n    from sklearn.cluster import KMeans\n\n    # Check if the classifier has the 'n_clusters' attribute\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError(\n            \"The classifier does not have the 'n_clusters' attribute.\")\n\n    # Set the cluster range if not provided\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n\n    # Create an empty list to store the sum of squared errors\n    sse = []\n\n    # Loop over the cluster range\n    for k in cluster_ranges:\n        # Set the number of clusters\n        clf.n_clusters = k\n\n        # Fit the classifier to the data\n        clf.fit(X)\n\n        # Calculate the sum of squared errors\n        sse.append(clf.inertia_)\n\n    # Convert the sum of squared errors to a NumPy array\n    sse = np.array(sse)\n\n    # Create a new figure if no axes are provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Plot the sum of squared errors against the number of clusters\n    ax.plot(cluster_ranges, sse, 'bx-')\n\n    # Set the title of the plot\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Set the x-axis label\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n\n    # Set the y-axis label\n    ax.set_ylabel('Sum of squared errors', fontsize=text_fontsize)\n\n    # Show the plot\n    plt.show()\n\n    return ax", "CodeBleu_score": 38.81187432078778}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    import time\n    import matplotlib.pyplot as plt\n    from sklearn import metrics\n    from sklearn.cluster import KMeans\n\n    try:\n        clf.n_clusters\n    except:\n        raise AttributeError(\n            \"{} should have attribute n_clusters\".format(clf.__class__.__name__))\n\n    if not cluster_ranges:\n        cluster_ranges = range(1, 15)\n\n    sse = []\n    for k in cluster_ranges:\n        clf.n_clusters = k\n        clf.fit(X)\n        sse.append(clf.inertia_)\n\n    # Create a figure if one hasn't been passed\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Plot the curve\n    ax.plot(cluster_ranges, sse, '-o')\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of Squared Errors (SSE)\", fontsize=text_fontsize)\n    ax.set_xticks(cluster_ranges)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Plot the clustering duration on a secondary axis\n    if show_cluster_time:\n        cluster_time = []\n        for k in cluster_ranges:\n            clf.n_clusters = k\n            start_time = time.time()\n            clf.fit(X)\n            end_time = time.time()\n            cluster_time.append(end_time - start_time)\n\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_time, '-o', color='red')\n        ax2.set_ylabel(\"Clustering Time (s)\", fontsize=text_fontsize)\n        ax2.set_ylim(0, max(cluster_time) * 1.1)\n\n    return ax\n\n", "CodeBleu_score": 40.82841632270884}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    from time import time\n    t0 = time()\n    clf = clf\n    clf.n_clusters = n_clusters\n    clf.fit(X)\n    t1 = time()\n    return clf.score(X), t1 - t0\n\n", "CodeBleu_score": 38.24824806288505}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    from time import time\n    start_time = time()\n    clf_clone = clf\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    end_time = time()\n    return clf_clone.score(X), end_time - start_time\n\n", "CodeBleu_score": 35.64021509628571}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    start_time = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    end_time = time.time()\n    return (end_time - start_time, silhouette_score(X, clf.labels_))\n\n", "CodeBleu_score": 36.85966931607771}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n    if not hasattr(clf, \"fit\"):\n        raise TypeError(\"The object must have the 'fit' method.\")\n    if not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\"The object must have the 'fit_predict' method.\")\n\n    def plot_silhouette(self, X, **kwargs):\n        \"\"\"\n        The function plot_silhouette takes a clustering object self and a dataset X as arguments.\\nIt first checks if the object has the required method 'fit_predict'.\\nIf the method is missing, it raises a TypeError indicating that the object is not a valid clusterer instance.\\nThe function then imports the necessary modules from sklearn and matplotlib.\\nIt calculates the silhouette scores for the dataset X and plots them using the SilhouetteVisualizer object.\\nThe function returns the SilhouetteVisualizer object.\"\"\"\n        if not hasattr(self, \"fit_predict\"):\n            raise TypeError(\"The object must have the 'fit_predict' method.\")\n        import matplotlib.pyplot as plt\n        from sklearn.metrics import silhouette_samples, silhouette_score\n        from yellowbrick.cluster import SilhouetteVisualizer\n\n        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n        visualizer = SilhouetteVisualizer(self, ax=ax, **kwargs)\n        visualizer.fit(X)\n        visualizer.finalize()\n        return visualizer\n\n    def plot_elbow_curve(self, X, **kwargs):\n        \"\"\"\n        The function plot_elbow_curve takes a clustering object self and a dataset X as arguments.\\nIt first checks if the object has the required method 'fit_predict'.\\nIf the method is missing, it raises a TypeError indicating that the object is not a valid clusterer instance.\\nThe function then imports the necessary modules from sklearn and matplotlib.\\nIt calculates the inertia values for different numbers of clusters and plots them using the KElbowVisualizer object.\\nThe function returns the KElbowVisualizer object.\"\"\"\n        if not hasattr(self, \"fit_predict\"):\n            raise TypeError(\"", "CodeBleu_score": 28.109726193524114}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, \"fit\") or not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\"The object passed is not a valid clusterer instance.\")\n\n    def plot_silhouette(self, X, **kwargs):\n        from yellowbrick.cluster import SilhouetteVisualizer\n\n        visualizer = SilhouetteVisualizer(self, **kwargs)\n        visualizer.fit(X)\n        visualizer.show()\n        return self\n\n    def plot_elbow_curve(self, X, **kwargs):\n        from yellowbrick.cluster import KElbowVisualizer\n\n        visualizer = KElbowVisualizer(self, **kwargs)\n        visualizer.fit(X)\n        visualizer.show()\n        return self\n\n    additional_methods = [plot_silhouette, plot_elbow_curve]\n    for method in additional_methods:\n        if hasattr(clf, method.__name__):\n            import warnings\n\n            warnings.warn(\n                f\"The object already has a method named '{method.__name__}'. The method will be overridden.\"\n            )\n        setattr(clf, method.__name__, method.__get__(clf))\n\n    return clf\n\n", "CodeBleu_score": 33.03160216405991}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, \"fit\"):\n        raise TypeError(\"fit method not found in the given object\")\n    if not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\"fit_predict method not found in the given object\")\n\n    def plot_silhouette(self, X, **kwargs):\n        from yellowbrick.cluster import SilhouetteVisualizer\n\n        visualizer = SilhouetteVisualizer(self, **kwargs)\n        visualizer.fit(X)\n        visualizer.show()\n\n    def plot_elbow_curve(self, X, **kwargs):\n        from yellowbrick.cluster import KElbowVisualizer\n\n        visualizer = KElbowVisualizer(self, **kwargs)\n        visualizer.fit(X)\n        visualizer.show()\n\n    additional_methods = {\n        \"plot_silhouette\": plot_silhouette,\n        \"plot_elbow_curve\": plot_elbow_curve,\n    }\n\n    for method_name, method_function in additional_methods.items():\n        if hasattr(clf, method_name):\n            warnings.warn(\n                f\"Method '{method_name}' already exists in the given object. Overriding it.\"\n            )\n        setattr(clf, method_name, method_function.__get__(clf))\n\n    return clf", "CodeBleu_score": 36.791548620577466}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if cv is None:\n        cv = ShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n\n    if scoring is None:\n        scoring = 'accuracy'\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.", "CodeBleu_score": 76.00222303555792}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\n    from sklearn.model_selection import learning_curve\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if train_sizes is not None:\n        ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    else:\n        ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n        scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1,\n                    color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=", "CodeBleu_score": 77.48838804402742}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if train_sizes is not None:\n        train_sizes, train_scores, test_scores = learning_curve(\n            clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n    else:\n        train_sizes, train_scores, test_scores = learning_curve(\n            clf, X, y, cv=cv, n_jobs=n_jobs, scoring=scoring)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test", "CodeBleu_score": 69.08298481930366}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    import matplotlib.pyplot as plt\n    from sklearn import cluster, metrics\n    import pandas as pd\n    import numpy as np\n\n    if cluster_ranges is None:\n        cluster_ranges = np.arange(1, 11, 2)\n\n    try:\n        clf.n_clusters\n    except AttributeError:\n        raise TypeError(\"The classifier passed in does not have a n_clusters attribute. Please check the classifier documentation.\")\n\n    clfs = [cluster.KMeans(n_clusters=i) for i in cluster_ranges]\n\n    score = [clf.fit(X).score(X) for clf in clfs]\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, np.abs(score))\n    ax.grid(True)\n    ax.set_xlabel('# of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    return ax\n", "CodeBleu_score": 45.0808218474615}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    from sklearn.metrics import silhouette_score\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier should have the n_clusters parameter\")\n\n    clfs = [clf(n_clusters=i) for i in cluster_ranges]\n    scores = [silhouette_score(X, clf.fit_predict(X))\n              for clf in clfs]\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'bo-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Silhouette Score', fontsize=text_fontsize)\n    ax.set_xticks(cluster_ranges)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax", "CodeBleu_score": 53.26737756327707}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier passed should have a n_clusters attribute\")\n\n    # Create a list of classifiers\n    classifiers = [\n        type(clf)(n_clusters=n) for n in cluster_ranges\n    ]\n\n    # Fit the classifiers\n    _ = [clf.fit(X) for clf in classifiers]\n\n    # Score the classifiers\n    scores = [clf.score(X) for clf in classifiers]\n\n    # Get the absolute value of the scores\n    scores = [abs(s) for s in scores]\n\n    # Get the axes\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Plot the scores\n    ax.plot(cluster_ranges, scores, 'b*-')\n\n    # Add grid lines\n    ax.grid(True)\n\n    # Set labels\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    return ax", "CodeBleu_score": 51.48531581642678}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n\n    X = PCA(n_components=2).fit_transform(X)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    for i, point in enumerate(X):\n        if y[i] == 0:\n            ax.scatter(*point, c='blue', label='Benign' if i == 0 else '')\n        else:\n            ax.scatter(*point, c='red', label='Malignant' if i == 0 else '')\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('First Principal Component', fontsize=text_fontsize)\n    ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)\n\n    ax.legend()\n    return ax\n\n", "CodeBleu_score": 44.13590904090095}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    y_unique = np.unique(y)\n    colors = plt.cm.get_cmap(cmap)(np.linspace(0, 1, len(y_unique)))\n\n    for label, color in zip(y_unique, colors):\n        mask = y == label\n        ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n                   color=color, label=label, alpha=0.5)\n\n    ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.2f}%)\",\n                  fontsize=text_fontsize)\n    ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.2f}%)\",\n                  fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend()\n\n    return ax\n\n", "CodeBleu_score": 54.6642743357287}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n\n    X = PCA(n_components=2).fit_transform(X)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    targets = list(set(y))\n    colors = ['r', 'b', 'g', 'y']\n    if len(colors) < len(targets):\n        colors = colors * (len(targets) // len(colors) + 1)\n    for target, color in zip(targets, colors):\n        ax.scatter(X[y == target, 0], X[y == target, 1],\n                   c=color, label=target)\n    ax.legend()\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax\n\n", "CodeBleu_score": 48.36995636678343}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    def clip_path(ax, x, y, id):\n        \"\"\"\n        Define a clipping path to clip the plotting area.\n        \"\"\"\n        from matplotlib.patches import PathPatch\n        from matplotlib.path import Path\n\n        # Define the vertices of the polygon\n        vertices = list(zip(x, y)) + [vertices[0]]\n        codes = [Path.MOVETO] + [Path.LINETO] * (len(vertices) - 2) + [Path.CLOSEPOLY]\n\n        # Create the clipping path\n        clip = Path(vertices, codes)\n        clip = PathPatch(clip, transform=ax.transData)\n\n        # Set the clip path for the plot\n        for item in ax.get_children():\n            if isinstance(item, PathPatch):\n                item.set_clip_path(clip)\n\n        return clip\n\n    # Generate data for the plot\n    x = np.arange(16)\n    y1 = np.random.randint(20, 70, size=16)\n    y2 = np.random.randint(20, 70, size=16)\n\n    # Define the vertices of the polygon\n    vertices = [(0, 0), (0, 1), (1, 1), (1, 0), (0, 0)]\n\n    # Create the first clipping path\n    clip1 = clip_path(ax, [0, 0, 1, 1], [0, 1, 1, 0], '1')\n\n    # Create the second clipping path\n    clip2 = clip_path(ax, [0, 0, 1, 1], [0, 1, 1, 0], '2')\n\n    # Plot the bars with different colors and clipping paths\n    ax.bar(x, y1, color='#AAAAAA', edgecolor='white', width=0.7, clip_path=clip1)\n    ax.bar(x, y2, color", "CodeBleu_score": 33.63623141502541}, {"code": "def barplot(ax)\n    from matplotlib.patches import PathPatch\n    from matplotlib.path import Path\n    import numpy as np\n\n    def clip_path(ax, x1, y1, x2, y2):\n        \"\"\"\n        This function takes in the axis object, and the coordinates of the two points that define the clipping path. It creates a Path object using the coordinates, and then creates a PathPatch object using the Path object. The function then adds the PathPatch object to the axis object as an artist, and returns the PathPatch object.\n        \"\"\"\n        path = Path([(x1, y1), (x2, y2)])\n        patch = PathPatch(path, transform=ax.transData)\n        ax.add_artist(patch)\n        return patch\n\n    # Create two clipping paths\n    clip1 = clip_path(ax, -1, 0, 16, 0)\n    clip2 = clip_path(ax, -1, 1, 16, 1)\n\n    # Create data for 16 bars\n    X = np.arange(16)\n    Y1 = np.random.uniform(0, 1, size=16)\n    Y2 = np.random.uniform(0, 1, size=16)\n\n    # Plot four sets of bars\n    ax.bar(X, Y1, color='#AAAAAA', clip_path=clip1)\n    ax.bar(X, Y1, color='#777777', clip_path=clip2)\n    ax.bar(X, Y2, color='#FF0000', clip_path=clip1)\n    ax.bar(X, Y2, color='#DD0000', clip_path=clip2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add text labels\n    ax.text(0, -0.1, 'Bar plot', fontsize=16, ha='left', va='top')\n    ax.text(0, -0.2, 'ax.bar()', fontsize=16", "CodeBleu_score": 37.72793383188283}, {"code": "def barplot(ax)\n    import numpy as np\n    from matplotlib.patches import PathPatch\n    from matplotlib.path import Path\n\n    def clip_path(ax, xy, width, height):\n        \"\"\"Create a clipping path for a rectangle with the lower left corner at xy = (x, y) and dimensions width x height.\"\"\"\n        patch = PathPatch(\n            Path([\n                xy,\n                [xy[0] + width, xy[1]],\n                [xy[0] + width, xy[1] + height],\n                [xy[0], xy[1] + height],\n                xy\n            ]))\n        patch.set_clip_on(False)\n        ax.add_patch(patch)\n        return patch.get_path()\n\n    # Create two clipping paths\n    clip_1 = clip_path(ax, xy=[0.25, 0.25], width=1.0, height=0.5)\n    clip_2 = clip_path(ax, xy=[0.25, 0.75], width=1.0, height=0.5)\n\n    # Generate data for 16 bars\n    Y1 = np.random.randint(low=10, high=50, size=16)\n    Y2 = np.random.randint(low=10, high=50, size=16)\n\n    # Plot four sets of bars\n    ax.bar(np.arange(16), Y1, color='#777777', edgecolor='white')\n    ax.bar(np.arange(16), Y2, color='#777777', edgecolor='white', clip_path=clip_1)\n    ax.bar(np.arange(16), Y1, color='#FF8C69', edgecolor='white')\n    ax.bar(np.arange(16), Y2, color='#FF8C69', edgecolor='white', clip_path=clip_2)\n\n    # Set x-axis limits\n    ax.set_x", "CodeBleu_score": 36.45288231069685}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n    import numpy as np\n\n    # Define the width of the fixed width\n    width = 1000\n\n    # Calculate the width of each segment\n    seg_width = width / n_segment\n\n    # Calculate the padding between segments\n    padding = seg_width / 2\n\n    # Initialize the starting and ending positions of the segments\n    X0 = np.zeros(n_segment)\n    X1 = np.zeros(n_segment)\n\n    # Calculate the starting and ending positions of each segment\n    for i in range(n_segment):\n        X0[i] = i * seg_width + padding\n        X1[i] = (i + 1) * seg_width - padding\n\n    return X0, X1\n\n", "CodeBleu_score": 32.72336326820073}, {"code": "def split(n_segment)\n    # Calculate the width of each segment\n    width_segment = width / n_segment\n\n    # Calculate the padding between segments\n    padding = width_segment / 10\n\n    # Determine the starting and ending positions of each segment\n    X0 = np.arange(0, width, width_segment)\n    X1 = np.arange(width_segment, width + width_segment, width_segment)\n\n    # Return the starting and ending positions of each segment\n    return X0, X1\n\n", "CodeBleu_score": 29.758030347996417}, {"code": "def split(n_segment)\n    # Calculate the width of each segment\n    width_segment = 1.0 / n_segment\n\n    # Calculate the padding between segments\n    padding = (1.0 - n_segment * width_segment) / 2\n\n    # Determine the starting and ending positions of each segment\n    X0 = np.linspace(0, n_segment, n_segment, endpoint=False) * width_segment + padding\n    X1 = X0 + width_segment\n\n    # Return the starting and ending positions of each segment\n    return X0, X1\n\n", "CodeBleu_score": 34.47181693544545}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')\n    fig.canvas.draw()\n    data = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    data = np.roll(data, 1, axis=2)\n    data = data.reshape((dpi, dpi, 4))\n    return data\n\n", "CodeBleu_score": 49.65350002831189}, {"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Create a figure and a canvas\n    fig, ax = plt.subplots(figsize=(1, 1))\n\n    # Place the character 'a' at the center of the figure\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n\n    # Remove the axis\n    ax.axis('off')\n\n    # Draw the canvas\n    fig.canvas.draw()\n\n    # Convert the canvas to an image array\n    img = np.array(fig.canvas.renderer.buffer_rgba())\n\n    # Reshape the array to the dimensions specified by dpi\n    img = img.reshape((dpi, dpi, 4))\n\n    # Adjust the color channels by rolling the array along the last axis\n    img = np.roll(img, 1, axis=-1)\n\n    # Return the resulting image array\n    return img\n\n", "CodeBleu_score": 44.56458155453413}, {"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n\n    # Create a figure and a canvas\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n\n    # Place the character 'a' at the center of the figure with a font size of 75\n    plt.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n\n    # Remove the axis\n    plt.axis('off')\n\n    # Draw the canvas\n    canvas.draw()\n\n    # Convert the canvas to an array\n    img_array = np.array(canvas.renderer.buffer_rgba())\n\n    # Reshape the array to the dimensions specified by dpi\n    img_array = img_array.reshape((dpi, dpi, 4))\n\n    # Adjust the color channels by rolling the array along the last axis\n    img_array = np.roll(img_array, 1, axis=-1)\n\n    return img_array\n\n", "CodeBleu_score": 47.21936029541714}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n    n = np.random.randint(1, 10)\n    centers = np.random.randint(0, 100, n)\n    widths = np.random.randint(1, 10, n)\n    scales = np.random.randint(1, 10, n)\n    widths = widths / np.sum(widths) * 10\n    scales = scales / np.sum(scales) * 100\n    curve = np.zeros(100)\n    for i in range(n):\n        curve += scales[i] * np.exp(-(np.arange(100) - centers[i]) ** 2 / (2 * widths[i] ** 2))\n    return curve\n\n", "CodeBleu_score": 34.43677221387178}, {"code": "def curve()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import random\n    import math\n\n    # Set the number of components\n    n_components = random.randint(3, 6)\n\n    # Set the range for the centers of the components\n    center_range = (0, 100)\n\n    # Set the range for the widths of the components\n    width_range = (1, 5)\n\n    # Set the range for the scales of the components\n    scale_range = (0.1, 1)\n\n    # Initialize an array of zeros\n    curve = np.zeros(100)\n\n    # Iterate over the number of components\n    for i in range(n_components):\n        # Generate random center, width, and scale values\n        center = random.uniform(*center_range)\n        width = random.uniform(*width_range)\n        scale = random.uniform(*scale_range)\n\n        # Generate the x values for the Gaussian component\n        x = np.linspace(0, 100, 100)\n\n        # Calculate the Gaussian component\n        gaussian = scale * np.exp(-((x - center) ** 2) / (2 * width ** 2))\n\n        # Add the Gaussian component to the curve\n        curve += gaussian\n\n    # Normalize the curve\n    curve = curve / np.max(curve)\n\n    # Return the curve\n    return curve\n\n", "CodeBleu_score": 28.57454151081651}, {"code": "def curve()\n    n = np.random.randint(1, 5)\n    centers = np.random.uniform(0, 1, n)\n    widths = np.random.uniform(0.05, 0.1, n)\n    scales = np.random.uniform(0.5, 1, n)\n    widths = widths / np.sum(widths) * 0.1\n    scales = scales / np.sum(scales) * 0.5\n    x = np.linspace(0, 1, 100)\n    y = np.zeros_like(x)\n    for i in range(n):\n        y += scales[i] * np.exp(-(x - centers[i]) ** 2 / (2 * widths[i] ** 2))\n    return y\n", "CodeBleu_score": 42.536474783562326}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n    derivative += 1\n    radius = accuracy // derivative\n    points = range(-radius, radius + 1)\n    matrix = np.vander(points, derivative, True)\n    return np.linalg.inv(matrix)[:, -1] / np.math.factorial(derivative - 1), points\n\n", "CodeBleu_score": 50.93372706535981}, {"code": "def difference(derivative, accuracy=1)\n    import numpy as np\n    from scipy.linalg import inv\n\n    if derivative == 0:\n        return np.array([1])\n\n    radius = (accuracy + 1) / 2\n    points = np.arange(-radius, radius + 1)\n    vandermonde = np.vander(points, len(points), True)\n    inverse = inv(vandermonde)\n    coefficients = inverse[derivative] * np.math.factorial(derivative - 1)\n\n    return coefficients, points\n\n", "CodeBleu_score": 39.68874922094795}, {"code": "def difference(derivative, accuracy=1)\n    derivative = derivative + 1\n    radius = 10 ** -accuracy\n    points = np.linspace(-radius, radius, derivative)\n    vandermonde = np.vander(points)\n    inverse = np.linalg.inv(vandermonde)\n    return inverse[derivative - 1] / np.math.factorial(derivative - 1), points\n\n", "CodeBleu_score": 42.399500910238544}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n    import numpy as np\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X) ** 2 + np.diff(Y) ** 2)\n\n    # Calculate the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    cumulative_distances = np.insert(cumulative_distances, 0, 0)\n\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cumulative_distances, X)\n    Y_interp = np.interp(T, cumulative_distances, Y)\n\n    # Calculate the total distance covered by the original coordinates\n    total_distance = np.sum(distances)\n\n    return X_interp, Y_interp, total_distance\n\n", "CodeBleu_score": 31.69366088516058}, {"code": "def interpolate(X, Y, T)\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Compute the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n\n    # Compute the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    cumulative_distances = np.insert(cumulative_distances, 0, 0)\n\n    # Compute the total distance\n    total_distance = cumulative_distances[-1]\n\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cumulative_distances, X)\n    Y_interp = np.interp(T, cumulative_distances, Y)\n\n    # Plot the original and interpolated coordinates\n    plt.plot(X, Y, 'o-', label='Original')\n    plt.plot(X_interp, Y_interp, 'o-', label='Interpolated')\n    plt.legend()\n    plt.show()\n\n    return X_interp, Y_interp, total_distance\n", "CodeBleu_score": 35.37861565019176}, {"code": "def interpolate(X, Y, T)\n    import numpy as np\n    import pandas as pd\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Calculate the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Calculate the total distance covered by the original coordinates\n    total_distance = np.sum(distances)\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cumulative_distances, X)\n    Y_interp = np.interp(T, cumulative_distances, Y)\n    # Return the interpolated X and Y coordinates along with the total distance covered by the original coordinates\n    return X_interp, Y_interp, total_distance\n", "CodeBleu_score": 36.8541563142964}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None]*1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 71.1619228944373}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None]*1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n", "CodeBleu_score": 71.1619228944373}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None]*1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 71.1619228944373}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n    window_len = int(2*sigma+1)\n    window_len = window_len + 1 if window_len % 2 == 0 else window_len\n    return smooth1d(smooth1d(A.T).T, window_len, sigma)\n", "CodeBleu_score": 14.93499965313951}, {"code": "def smooth2d(A, sigma=3)\n    A = np.array(A)\n    window = int(sigma) * 4 + 1\n    A_smooth = np.apply_along_axis(lambda m: smooth(m, window, 'flat'), axis=1, arr=A)\n    A_smooth = np.apply_along_axis(lambda m: smooth(m, window, 'flat'), axis=1, arr=A_smooth.T).T\n    return A_smooth\n\n", "CodeBleu_score": 27.241905618391794}, {"code": "def smooth2d(A, sigma=3)\n    from scipy.ndimage import gaussian_filter1d\n    window_len = int(sigma) * 4 + 1\n    if window_len % 2 == 0:\n        window_len += 1\n    A = gaussian_filter1d(A, sigma=sigma, axis=0)\n    A = gaussian_filter1d(A, sigma=sigma, axis=1)\n    return A\n", "CodeBleu_score": 21.897735560658894}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.ox, self.oy\n    px = 1 / dpi * 72\n    img = np.roll(padded_src, int(ox / px), axis=1)\n    img = np.roll(img, int(oy / px), axis=0)\n    return img\n\n", "CodeBleu_score": 41.5734838820455}, {"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.offsets\n    px = 1 / dpi * 72\n    return np.roll(np.roll(padded_src, int(ox / px), axis=1), int(oy / px), axis=0)\n\n", "CodeBleu_score": 30.251537504420828}, {"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.get_offsets()\n    px = 1 / dpi\n    px = int(px)\n    img = np.roll(padded_src, ox * px, axis=1)\n    img = np.roll(img, oy * px, axis=0)\n    return img\n\n", "CodeBleu_score": 36.35918538621384}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    shape = (shape,) * len(center)\n    center = (center,) * len(shape)\n    width = (width,) * len(shape)\n    return _gaussian(shape, width, center)\n\n", "CodeBleu_score": 10.571777646939617}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    shape = (shape, shape) if isinstance(shape, int) else shape\n    width = (width, width) if isinstance(width, float) else width\n    center = (center, center) if isinstance(center, float) else center\n    x, y = np.meshgrid(np.linspace(-1, 1, shape[0]), np.linspace(-1, 1, shape[1]))\n    d = np.sqrt(x * x + y * y)\n    return np.exp(-(d - center[0]) ** 2 / (2 * width[0] ** 2) - (d - center[1]) ** 2 / (2 * width[1] ** 2))\n\n", "CodeBleu_score": 21.72439666681453}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    shape = tuple(shape)\n    if len(shape) == 1:\n        width = (width,)\n        center = (center,)\n    elif len(shape) == 0:\n        raise ValueError(\"shape cannot be an empty tuple\")\n    width = tuple(width)\n    center = tuple(center)\n    if len(width) == 1:\n        width = width * len(shape)\n    elif len(width) != len(shape):\n        raise ValueError(\"width must be a scalar or have the same length as shape\")\n    if len(center) == 1:\n        center = center * len(shape)\n    elif len(center) != len(shape):\n        raise ValueError(\"center must be a scalar or have the same length as shape\")\n    indices = (\n        (slice(-x0, dim - x0) for x0, dim in zip(center, shape))\n        if np.isscalar(center[0])\n        else np.ogrid[tuple(slice(x0, dim - x0) for x0, dim in zip(center, shape))]\n    )\n    R2 = 0\n    for x_i, w_i in zip(indices, width):\n        R2 += (x_i / w_i) ** 2\n    return np.exp(-0.5 * R2)\n\n", "CodeBleu_score": 31.847829939849458}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n    import numpy as np\n    x, y = position\n    size = size / 2\n    R = np.sqrt((x - X)**2 + (y - Y)**2)\n    return intensity * np.exp(-R**2 / (2 * size**2))\n", "CodeBleu_score": 18.42759756604305}, {"code": "def stimulus(position, size, intensity)\n    import numpy as np\n    import math\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] + size * np.cos(position[1])\n    y = position[0] + size * np.sin(position[1])\n    # Normalize the stimulus\n    x = (x - np.min(x)) / (np.max(x) - np.min(x)) - 0.5\n    y = (y - np.min(y)) / (np.max(y) - np.min(y)) - 0.5\n    # Create a mesh grid representing the spatial domain\n    X, Y = np.meshgrid(x, y)\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = np.square(X) + np.square(Y)\n    # Compute the stimulus\n    S = intensity * np.exp(-R / (2 * size**2))\n    return S\n", "CodeBleu_score": 30.098356046451347}, {"code": "def stimulus(position, size, intensity)\n    import numpy as np\n    import math\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * math.cos(position[1])\n    y = position[0] * math.sin(position[1])\n\n    # Normalize the coordinates\n    x = x / size[0]\n    y = y / size[1]\n\n    # Create a mesh grid representing the spatial domain\n    x_range = np.linspace(-1, 1, size[0])\n    y_range = np.linspace(-1, 1, size[1])\n    X, Y = np.meshgrid(x_range, y_range)\n\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = (X - x)**2 + (Y - y)**2\n\n    # Compute the stimulus as an exponential decay function of the distances\n    stim = intensity * np.exp(-R)\n\n    return stim\n\n", "CodeBleu_score": 34.19241020111437}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xticklabels([\"0\", \"1\"], fontfamily=\"serif\", fontsize=12)\n    ax.set_yticklabels([\"0\", \"1\"], fontfamily=\"serif\", fontsize=12)\n    ax.set_xlabel(\"False Positive Rate\", fontfamily=\"serif\", fontsize=12)\n    ax.set_ylabel(\"True Positive Rate\", fontfamily=\"serif\", fontsize=12)\n    ax.text(\n        0.5,\n        0.5,\n        text,\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n        fontfamily=\"serif\",\n        fontsize=12,\n    )\n    ax.set_title(\"ROC Curve\", fontfamily=\"serif\", fontsize=14, fontweight=\"bold\")\n\n", "CodeBleu_score": 40.3089944712656}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontdict={'family': 'Arial', 'weight': 'bold', 'size': 24})\n    ax.set_title('Title', fontdict={'family': 'Arial', 'weight': 'bold', 'size': 24})\n\n", "CodeBleu_score": 39.935224500558746}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text,\n            horizontalalignment='center',\n            verticalalignment='center',\n            fontdict={'family': 'serif',\n                      'color': 'darkred',\n                      'weight': 'bold',\n                      'size': 16})\n    ax.set_title('Title', fontdict={'family': 'serif', 'weight': 'bold'})\n\n", "CodeBleu_score": 37.050120591460576}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n", "CodeBleu_score": 74.030392732948}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    Xt = X\n    for _, step in self._iter(with_final=False):\n        Xt = step.transform(Xt)\n    return self.steps[-1][1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 59.40622482549259}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][1].predict_cumulative_hazard_function(Xt, **kwargs)", "CodeBleu_score": 67.21059307747694}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return self.steps[-1][1].predict_survival_function(Xt, **kwargs)\n", "CodeBleu_score": 51.73340234170482}, {"code": "def predict_survival_function(self, X, **kwargs)\n    return self.predict_survival_function(X, **kwargs)\n\n", "CodeBleu_score": 8.495770640124103}, {"code": "def predict_survival_function(self, X, **kwargs)\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 59.20205917425212}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n    if results.solver.status == \"ok\":\n        if results.solver.termination_condition == \"optimal\":\n            pass\n        elif results.solver.termination_condition == \"infeasible\":\n            raise RuntimeError(\n                \"The problem is infeasible. Please check your model and data.\"\n            )\n        elif results.solver.termination_condition == \"unbounded\":\n            raise RuntimeError(\n                \"The cost function is unbounded. Please check your model and data.\"\n            )\n        elif results.solver.termination_condition == \"maxIterations\":\n            logger.warning(\n                \"The solver reached the maximum number of iterations. Please check your model and data.\"\n            )\n        elif results.solver.termination_condition == \"other\":\n            raise RuntimeError(\n                \"The solver returned an unknown exit flag. Please check your model and data.\"\n            )\n        else:\n            raise RuntimeError(\n                \"The solver returned an unknown exit flag. Please check your model and data.\"\n            )\n    else:\n        raise RuntimeError(\n            \"The solver returned an unknown exit flag. Please check your model and data.\"\n        )", "CodeBleu_score": 25.475183258243046}, {"code": "def _check_success(self, results)\n    if results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.optimal:\n        pass\n    elif results.solver.termination_condition in [TerminationCondition.maxIterations, TerminationCondition.minStepLength]:\n        self.logger.warning('Check the convergence of the optimization problem.')\n    elif results.solver.status == SolverStatus.error:\n        raise RuntimeError('Solver (%s) reported error.' % results.solver.name)\n    elif results.solver.termination_condition == TerminationCondition.infeasible:\n        raise RuntimeError('Locally (%s) infeasible problem.' % results.solver.name)\n    elif results.solver.termination_condition == TerminationCondition.unbounded:\n        raise RuntimeError('Locally (%s) unbounded problem.' % results.solver.name)\n    elif results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded:\n        raise RuntimeError('Locally (%s) infeasible or unbounded problem.' % results.solver.name)\n    elif results.solver.termination_condition == TerminationCondition.locallyOptimal:\n        raise RuntimeError('Locally (%s) optimal problem.' % results.solver.name)\n    elif results.solver.termination_condition == TerminationCondition.other:\n        raise RuntimeError('Solver (%s) for ModelCode %s reports exit flag of %s.' % (results.solver.name, self.model_code, results.solver.termination_condition))\n    else:\n        raise RuntimeError('Unexpected termination condition (%s) from %s solver.' % (results.solver.termination_condition, results.solver.name))", "CodeBleu_score": 32.57126472660583}, {"code": "def _check_success(self, results)\n    if results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.optimal:\n        logger.info('Optimal solution found.')\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.maxIterations:\n        logger.warning('Optimization terminated with maximum iterations.')\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.other:\n        logger.warning('Optimization terminated with unknown status.')\n    elif results.solver.status == SolverStatus.error:\n        raise RuntimeError('Solver returned an error. See the solver log for more information.')\n    elif results.solver.status == SolverStatus.warning:\n        raise RuntimeError('Solver returned a warning. See the solver log for more information.')\n    elif results.solver.status == SolverStatus.aborted:\n        raise RuntimeError('Solver was aborted.')\n    elif results.solver.status == SolverStatus.maxTimeLimit:\n        raise RuntimeError('Solver hit time limit.')\n    elif results.solver.status == SolverStatus.maxIterations:\n        raise RuntimeError('Solver hit maximum number of iterations.')\n    elif results.solver.status == SolverStatus.maxEvaluations:\n        raise RuntimeError('Solver hit maximum number of evaluations.')\n    elif results.solver.status == SolverStatus.globallyOptimal:\n        logger.info('Optimal solution found.')\n    elif results.solver.status == SolverStatus.locallyOptimal:\n        logger.info('Locally optimal solution found.')\n    elif results.solver.status == SolverStatus.infeasibleProblemDetected:\n        raise RuntimeError('Solver detected an infeasible problem.')\n    elif results.solver.status == SolverStatus.noSolutionAvailable:\n        raise RuntimeError('Solver could not find a solution.')\n    elif results.solver.status == SolverStatus.infeasible:\n        raise RuntimeError('Solver detected inf", "CodeBleu_score": 35.206074033418034}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n    eigvals, eigvecs = linalg.eigh(P)\n    eigvals = eigvals.real\n    eigvecs = eigvecs.real\n    max_eigval = eigvals.max()\n    cond = max_eigval / (P.shape[0] * machine_epsilon)\n    if cond > 1e15:\n        print(\n            \"WARNING: badly conditioned eigenvalue matrix,\",\n            \"errors up to %e\" % (cond * machine_epsilon),\n        )\n    above_cutoff = eigvals >= eigvals.max() * -self.cutoff\n    P_decomp = eigvecs[:, above_cutoff]\n    P_decomp /= P_decomp.max()\n    return P_decomp.T, max_eigval\n\n", "CodeBleu_score": 27.03466032353381}, {"code": "def _decompose(self, P)\n    eigvals, eigvecs = linalg.eigh(P)\n    eigvals = eigvals.real\n    eigvecs = eigvecs.real\n    idx = eigvals.argsort()[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n    eigvalmax = eigvals[0]\n    condition_number = eigvalmax / (np.product(P.shape) * np.finfo(float).eps)\n    if condition_number > 1000:\n        raise ValueError(\"The input matrix P is ill-conditioned.\")\n    eigvals = eigvals[eigvals > -np.finfo(float).eps]\n    eigvecs = eigvecs[:, eigvals > np.finfo(float).eps]\n    eigvals = eigvals[eigvals > np.finfo(float).eps]\n    eigvals = eigvals / eigvals[0]\n    return eigvecs.T, eigvals[0]\n", "CodeBleu_score": 25.444546054461203}, {"code": "def _decompose(self, P)\n    (eigenvalues, eigenvectors) = linalg.eigh(P)\n    max_eigenvalue = max(eigenvalues)\n    cutoff = max_eigenvalue * max(P.shape) * finfo(float).eps\n    above_cutoff = eigenvalues > cutoff\n    below_cutoff = eigenvalues < -cutoff\n    eigenvalues = eigenvalues[above_cutoff]\n    eigenvectors = eigenvectors[:, above_cutoff]\n    total = sum(eigenvalues)\n    if total <= 0.0:\n        raise ValueError('The input matrix is not positive definite.')\n    if max_eigenvalue / total < finfo(float).eps:\n        raise ValueError('The input matrix is rank-deficient.')\n    if len(eigenvalues) < len(eigenvalues):\n        raise ValueError('SVD did not converge')\n    eigenvalues = eigenvalues / max_eigenvalue\n    return (eigenvectors.T, max_eigenvalue)", "CodeBleu_score": 27.91079788535211}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n    self.coef_ = coef * D\n    return self.coef_\n\n", "CodeBleu_score": 19.173861618326697}, {"code": "def _update_coef(self, coef, D)\n    self.coef_ = coef\n    self.coef_ *= D\n    self.coef_ = np.clip(self.coef_, -self.max_coef, self.max_coef)\n", "CodeBleu_score": 15.837221723027529}, {"code": "def _update_coef(self, coef, D)\n    coef_ = coef.copy()\n    coef_[coef_ < self.threshold] = 0\n    coef_[coef_ != 0] = coef_[coef_ != 0] / D[coef_ != 0]\n    self.coef_ = coef_\n", "CodeBleu_score": 18.07568690821997}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    y_true = self._get_y_true(y_true)\n    event_type = y_true[0]\n    event_time = y_true[1]\n    risk_set_size = self._get_risk_set_size(event_type)\n    if sample_weight is None:\n        sample_weight = np.ones_like(event_type)\n    gradient = self._get_gradient(event_type, event_time, risk_set_size, raw_prediction)\n    return -sample_weight * gradient\n", "CodeBleu_score": 24.271763345578623}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    y_true = self._check_y_true(y_true)\n    event_time = y_true[:, 0]\n    event_type = y_true[:, 1]\n    if sample_weight is None:\n        sample_weight = np.ones_like(event_time)\n    hazard = self._compute_hazard(raw_prediction)\n    grad = np.empty_like(raw_prediction)\n    for i in range(len(raw_prediction)):\n        if event_type[i] == 1:\n            grad[i] = -1 * hazard[i] * sample_weight[i]\n        else:\n            grad[i] = hazard[i] * sample_weight[i]\n    return grad", "CodeBleu_score": 27.02407083667572}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    event_type, event_time = y_true[:, 0], y_true[:, 1]\n    hazard = self.hazard(raw_prediction)\n    cumulative_hazard = self.cumulative_hazard(raw_prediction)\n    gradient = (event_type - hazard) * cumulative_hazard\n    if sample_weight is not None:\n        gradient *= sample_weight\n    return gradient", "CodeBleu_score": 44.203365317375095}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if self.missing_values_in_features_ is None:\n        if self.dataset_properties_['missing_values_in_features']:\n            self.missing_values_in_features_ = np.any(np.isnan(X), axis=0)\n        else:\n            self.missing_values_in_features_ = np.any(~np.isfinite(X), axis=0)\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any(self.missing_values_in_features_):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\")\n            if np.any", "CodeBleu_score": 25.47005276747586}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.support_missing_values:\n        if not np.isfinite(X).all():\n            raise ValueError(\"Input X contains NaN or infinity.\")\n        return None\n    if estimator_name is None:\n        estimator_name = \"estimator\"\n    if self.missing_values == \"raise\":\n        raise ValueError(f\"{estimator_name} does not support missing values.\")\n    if self.missing_values == \"ignore\":\n        return None\n    if self.missing_values == \"np.nan\":\n        X_ = np.asarray(X)\n        if np.any(np.isnan(X_)):\n            return np.isnan(X_).any(axis=0)\n        return None\n    if self.missing_values == \"np.inf\":\n        X_ = np.asarray(X)\n        if np.any(np.isinf(X_)):\n            return np.isinf(X_).any(axis=0)\n        return None\n    if callable(self.missing_values):\n        return self.missing_values(X)\n    raise ValueError(f\"{self.missing_values} is not a valid strategy.\")\n\n", "CodeBleu_score": 29.642797750461234}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if self.missing_values_in_features_supported_:\n        estimator_name = estimator_name if estimator_name is not None else 'Pipeline'\n        if self.missing_values_in_features_ == 'NaN':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_Inf':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_Inf_and_negative_Infs':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_negative_Infs':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_positive_Infs':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_negative_Infs_and_positive_Infs':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_name)\n        elif self.missing_values_in_features_ == 'NaN_and_negative_Infs_and_positive_Infs_and_zero_Infs':\n            if not np.isfinite(X).all():\n                raise ValueError('Input contains NaN, infinity or a value too large for %r.' % estimator_", "CodeBleu_score": 19.247039989960165}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if self.memory is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n    if check_input:\n        X = check_array(X, accept_sparse=True)\n    if not sp.issparse(X):\n        X = sp.csr_matrix(X)\n    X_pred = self.tree_.predict(X)\n    X_pred = X_pred[:, 1]\n    if return_array:\n        return X_pred\n    else:\n        unique_times = np.unique(X_pred)\n        step_function = StepFunction(unique_times, np.arange(len(unique_times)))\n        return step_function(X_pred)", "CodeBleu_score": 42.80053869382062}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if self.memory is not None:\n        self.memory.check_memory()\n    self._check_is_fitted()\n    X = check_array(X, accept_sparse=True, force_all_finite=False)\n    if check_input:\n        X = self._validate_data(X, dtype=None, order=\"C\", reset=False)\n    X_new = self._get_baseline_prediction(X)\n    if self.event_type_ == \"right-censoring\":\n        X_new = np.array([np.unique(x) for x in X_new])\n    if return_array:\n        return X_new\n    else:\n        return StepFunction(\n            np.unique(X_new[0]),\n            np.array([np.sum(x >= t) / x.shape[0] for t in np.unique(X_new[0])]),\n        )\n\n", "CodeBleu_score": 35.481906699780886}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if check_input:\n        X = check_array(X, accept_sparse='csr')\n    if not hasattr(self, 'estimator_'):\n        raise NotFittedError('The model is not fitted yet.')\n    if self.event_type_ == 'right-censoring':\n        raise ValueError('The model is not able to predict survival functions for right-censored data.')\n    if self.event_type_ == 'interval-censoring':\n        raise ValueError('The model is not able to predict survival functions for interval-censored data.')\n    X_pred = self.estimator_.predict(X)\n    if return_array:\n        return X_pred[:, 1]\n    times = np.unique(X_pred[:, 0])\n    y_pred = np.vstack([1 - X_pred[X_pred[:, 0] >= t].shape[0] / X_pred.shape[0] for t in times])\n    return StepFunction(times, y_pred)\n\n", "CodeBleu_score": 40.35089373609848}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n    X, y = check_X_y(X, y, accept_sparse=True)\n    if len(y) < self.min_samples:\n        raise ValueError(\"The number of samples is too small.\")\n    if self.normalize:\n        X = X.astype(np.float64)\n        X = self._center_scale_X(X, self.center, self.scale)\n        X = np.asfortranarray(X)\n    else:\n        X = X.astype(np.float64)\n    event, time = check_y_survival(y)\n    f = np.empty(X.shape[1], dtype=bool)\n    f.fill(True)\n    X, y, f = self._pre_fit_process(X, y, event, time, f)\n    X, y, f = self._sort_by_time(X, y, f)\n    return X, y, f\n\n", "CodeBleu_score": 30.05109463076623}, {"code": "def _pre_fit(self, X, y)\n    (X, y) = check_X_y(X, y, accept_sparse=['csr', 'csc'], dtype=np.float64)\n    n_samples = X.shape[0]\n    if (n_samples < self.min_samples):\n        raise ValueError(('The minimum number of samples required to train an estimator is %d; got %d.') % (self.min_samples, n_samples))\n    (event, time) = y[:, 0], y[:, 1]\n    if (np.max(event) > 1):\n        raise ValueError('The maximum value of the event must be 1.')\n    if (np.min(time) < 0):\n        raise ValueError('The minimum value of the time must be 0.')\n    if (np.max(time) > self.max_time):\n        raise ValueError(('The maximum value of the time must be %d.' % self.max_time))\n    X = (X - np.mean(X, axis=0))\n    if self.normalize:\n        X = (X / np.std(X, axis=0))\n    (X, event, time) = _sort_arrays(X, event, time)\n    X = np.asfortranarray(X)\n    return (X, event, time, np.mean(X, axis=0), np.std(X, axis=0))", "CodeBleu_score": 26.060540037842593}, {"code": "def _pre_fit(self, X, y)\n    if self.normalize:\n        X = self._normalize(X)\n    X = check_array(X, accept_sparse=True, force_all_finite=True)\n    if self.min_samples is not None:\n        if X.shape[0] < self.min_samples:\n            raise ValueError(\"The number of samples is too small. \" \"Please increase the number of samples.\")\n    X = np.asfortranarray(X, dtype=np.float64)\n    y = check_array(y, ensure_2d=False, force_all_finite=True)\n    event, time = check_y_survival(y)\n    if self.center:\n        X = self._center(X)\n    return X, event, time\n\n", "CodeBleu_score": 29.87613219620976}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n    if self.alphas is None:\n        self.alphas = np.linspace(self.alpha_min, self.alpha_max, self.n_alphas)\n        new_path = True\n    else:\n        self.alphas = np.array(self.alphas, dtype=float)\n        if self.alphas.ndim != 1:\n            raise ValueError(\"Alpha must be a 1D array\")\n        if not np.all(np.isfinite(self.alphas)) or np.any(self.alphas < 0):\n            raise ValueError(\"Alpha must be positive\")\n        new_path = False\n    return self.alphas, new_path\n\n", "CodeBleu_score": 39.68627237225063}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        if not isinstance(self.n_alphas, int) or self.n_alphas < 1:\n            raise ValueError(\"n_alphas must be a positive integer\")\n        self.alphas = np.linspace(0, 1, self.n_alphas)\n        self.new_path = True\n    else:\n        self.alphas = np.array(self.alphas, dtype=float)\n        if self.alphas.ndim != 1:\n            raise ValueError(\"alphas must be a 1D array\")\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if not np.all(self.alphas >= 0):\n            raise ValueError(\"alphas must be non-negative\")\n        self.new_path = False\n    return self.alphas, self.new_path\n\n", "CodeBleu_score": 39.99211599586786}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        if self.n_alphas < 1:\n            raise ValueError(\"n_alphas=%d must be greater than zero\" % self.n_alphas)\n        alphas = np.logspace(-10, -2, self.n_alphas)\n        self.coefs_path_ = [None] * self.n_alphas\n        self.alphas = alphas\n        self.dual_gap_ = np.inf\n        self.n_iter_ = self.n_alphas\n        return alphas, True\n    else:\n        alphas = np.array(self.alphas)\n        if alphas.ndim != 1:\n            raise ValueError(\"alphas must be 1D, got ndim=%d\" % alphas.ndim)\n        if not np.all(np.isfinite(alphas)) or np.any(alphas <= 0):\n            raise ValueError(\"alphas must be positive, got min_alpha=%.1e\" % alphas.min())\n        return alphas, False\n\n", "CodeBleu_score": 31.915275431422085}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n    # Check if event is a list or a numpy array\n    if not isinstance(event, (list, np.ndarray)):\n        raise TypeError(\"event must be a list or a numpy array\")\n\n    # Check if time is a list or a numpy array\n    if not isinstance(time, (list, np.ndarray)):\n        raise TypeError(\"time must be a list or a numpy array\")\n\n    # Check if event and time have the same length\n    if len(event) != len(time):\n        raise ValueError(\"event and time must have the same length\")\n\n    # Check if all elements in event are 0 or 1\n    if not all(e in [0, 1] for e in event):\n        raise ValueError(\"all elements in event must be 0 or 1\")\n\n    # Check if all elements in time are positive\n    if not all(t > 0 for t in time):\n        raise ValueError(\"all elements in time must be positive\")\n\n    # Convert event and time to numpy arrays\n    event = np.array(event)\n    time = np.array(time)\n\n    # Sort time in ascending order\n    time = np.sort(time)\n\n    # Get unique times\n    unique_times = np.unique(time)\n\n    # Initialize variables\n    n_events = np.zeros(len(unique_times))\n    n_at_risk = np.zeros(len(unique_times))\n\n    # Loop through unique times\n    for i, t in enumerate(unique_times):\n        # Get indices of observations at time t\n        indices = np.where(time == t)[0]\n\n        # Get number of events at time t\n        n_events[i] = np.sum(event[indices])\n\n        # Get number of subjects at risk at time t\n        n_at_risk[i] = len(indices)\n\n    # Compute cumulative hazard\n    cumulative_hazard = np.cumsum(n_events / n_at_risk)\n\n    return unique_times, cumulative_hazard\n\n", "CodeBleu_score": 30.891134933406754}, {"code": "def nelson_aalen_estimator(event, time)\n    if not isinstance(event, np.ndarray):\n        raise TypeError(\"event must be a numpy array.\")\n    if not isinstance(time, np.ndarray):\n        raise TypeError(\"time must be a numpy array.\")\n    if event.ndim != 1:\n        raise ValueError(\"event must be a 1-dimensional array.\")\n    if time.ndim != 1:\n        raise ValueError(\"time must be a 1-dimensional array.\")\n    if event.shape != time.shape:\n        raise ValueError(\"event and time must have the same shape.\")\n    unique_times = np.unique(time)\n    n_events = np.sum(event)\n    n_at_risk = len(time) - np.cumsum(np.bincount(time)[:-1])\n    cum_hazard = np.cumsum(event / n_at_risk)\n    return unique_times, cum_hazard\n\n", "CodeBleu_score": 29.10349276990289}, {"code": "def nelson_aalen_estimator(event, time)\n    import numpy as np\n    import pandas as pd\n    # Check if event and time are of the same length\n    if len(event) != len(time):\n        raise ValueError(\"event and time must have the same length\")\n    # Check if event is binary\n    if not np.array_equal(np.unique(event), np.array([0, 1])):\n        raise ValueError(\"event must be binary\")\n    # Check if time is sorted\n    if not np.all(np.diff(time) >= 0):\n        raise ValueError(\"time must be sorted in ascending order\")\n    # Check if there are any ties in time\n    if np.any(np.diff(time) == 0):\n        raise ValueError(\"time must not have any ties\")\n    # Compute unique times\n    unique_times = np.unique(time)\n    # Compute number of events at each time\n    num_events = np.zeros(len(unique_times))\n    for i in range(len(unique_times)):\n        num_events[i] = np.sum(event[time == unique_times[i]])\n    # Compute number of subjects at risk at each time\n    num_subjects_at_risk = np.zeros(len(unique_times))\n    for i in range(len(unique_times)):\n        num_subjects_at_risk[i] = np.sum(time >= unique_times[i])\n    # Compute cumulative hazard\n    cumulative_hazard = np.cumsum(num_events / num_subjects_at_risk)\n    # Return unique times and cumulative hazard\n    return unique_times, cumulative_hazard\n", "CodeBleu_score": 26.64163147279181}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n    if all(event):\n        return np.ones(len(event))\n    km = KaplanMeierFitter().fit(time, event)\n    weights = np.ones(len(event))\n    idx = np.searchsorted(km.timeline, time) - 1\n    weights[event == 1] = 1. / km.survival_function_.values[idx][event == 1]\n    weights[weights == np.inf] = 0\n    return weights\n", "CodeBleu_score": 37.342001984450135}, {"code": "def ipc_weights(event, time)\n    if np.all(event == 1):\n        return np.ones(len(event))\n    km_estimator = KaplanMeierFitter().fit(time, event)\n    ipcw = np.array([km_estimator.survival_function_.loc[km_estimator.survival_function_.index >= t].iloc[0, 0] for t in time])\n    ipcw_idx = np.searchsorted(km_estimator.survival_function_.index, time) - 1\n    ipcw = ipcw[ipcw_idx]\n    ipcw[ipcw == 0] = np.min(ipcw[ipcw > 0])\n    weights = 1 / ipcw\n    return weights\n\n", "CodeBleu_score": 34.68499430021346}, {"code": "def ipc_weights(event, time)\n    import numpy as np\n    # Check if all events have occurred\n    if np.all(event == 1):\n        return np.ones(len(event))\n    # Compute Kaplan-Meier estimator for event and time data in reverse\n    km_estimator = np.flip(np.cumprod(np.flip(1 - event)))\n    # Find index positions of event times within unique time values\n    event_indices = np.searchsorted(np.unique(time), time[event == 1])\n    # Retrieve corresponding survival probabilities\n    survival_probs = km_estimator[event_indices]\n    # Ensure all retrieved probabilities are positive\n    survival_probs[survival_probs <= 0] = 1e-10\n    # Create weights array\n    weights = np.zeros(len(event))\n    # Assign inverse probabilities to events\n    weights[event == 1] = 1 / survival_probs\n    return weights\n", "CodeBleu_score": 35.641204533338154}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\"Invalid data type\")\n\n    connector_list = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connector_list.append(PandasConnector(df))\n        elif isinstance(df, modin.pandas.DataFrame):\n            connector_list.append(ModinConnector(df))\n        elif isinstance(df, pl.DataFrame):\n            connector_list.append(PolarsConnector(df))\n        elif isinstance(df, BaseConnector):\n            connector_list.append(df)\n        else:\n            raise ValueError(\"Invalid data type\")\n    return connector_list\n\n", "CodeBleu_score": 38.6050485046699}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\n            \"Invalid input type. Expected pd.DataFrame, BaseConnector, or List[Union[pd.DataFrame, BaseConnector]]\"\n        )\n\n    connector_list = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connector = PandasConnector(df)\n        elif isinstance(df, BaseConnector):\n            connector = df\n        elif isinstance(df, modin.pandas.DataFrame):\n            connector = ModinConnector(df)\n        elif isinstance(df, pl.DataFrame):\n            connector = PolarsConnector(df)\n        else:\n            raise ValueError(\n                \"Invalid input type. Expected pd.DataFrame, BaseConnector, or List[Union[pd.DataFrame, BaseConnector]]\"\n            )\n        connector_list.append(connector)\n\n    return connector_list\n\n", "CodeBleu_score": 43.970472126738905}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        for i, df in enumerate(dfs):\n            if isinstance(df, pd.DataFrame):\n                dfs[i] = PandasConnector(df)\n            elif isinstance(df, pa.DataFrame):\n                dfs[i] = PolarsConnector(df)\n            elif isinstance(df, md.DataFrame):\n                dfs[i] = ModinConnector(df)\n            elif not isinstance(df, BaseConnector):\n                raise ValueError(\n                    \"Invalid data type. Expected pandas, modin, or polars dataframe or connector.\"\n                )\n    else:\n        raise ValueError(\n            \"Invalid data type. Expected pandas, modin, or polars dataframe or connector.\"\n        )\n    return dfs\n\n", "CodeBleu_score": 36.40056807551015}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n    if self.cache_file:\n        if self.filters:\n            cache_file = self.cache_file + \"_\" + \"_\".join(self.filters)\n        else:\n            cache_file = self.cache_file\n        if os.path.exists(cache_file):\n            df = pd.read_csv(cache_file)\n            return df\n    if isinstance(self.data, pd.DataFrame):\n        return self.data\n    self.data = self.fetch()\n    if self.cache_file:\n        if self.filters:\n            cache_file = self.cache_file + \"_\" + \"_\".join(self.filters)\n        else:\n            cache_file = self.cache_file\n        self.data.to_csv(cache_file, index=False)\n    return self.data\n\n", "CodeBleu_score": 43.41436261880152}, {"code": "def execute(self) -> pd.DataFrame\n    if self._cached_data is not None:\n        return self._cached_data\n    if self._cached_data_with_filters is not None:\n        return self._cached_data_with_filters\n    if isinstance(self._data, pd.DataFrame):\n        return self._data\n    self._data = self.fetch()\n    return self._data", "CodeBleu_score": 54.08206362713814}, {"code": "def execute(self) -> pd.DataFrame\n    if self.cache_path is not None:\n        if self.cache_path.exists():\n            return pd.read_parquet(self.cache_path)\n    if isinstance(self.data, pd.DataFrame):\n        return self.data\n    data = self.fetch()\n    self.data = data\n    return data\n\n", "CodeBleu_score": 43.346064447718405}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.df()\n    col_str = \"|\".join(self.columns)\n    formula = f\"{col_str}|{self.formula}\"\n    return hashlib.sha256(formula.encode()).hexdigest()\n\n", "CodeBleu_score": 39.763427206858}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_dataframe()\n    return hashlib.sha256(\n        \"|\".join(self.columns).encode() + self.formula.encode()\n    ).hexdigest()\n\n", "CodeBleu_score": 32.83010714274523}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_dataframe()\n    col_names = \"|\".join(self.columns)\n    formula = \"formula\"\n    string = col_names + formula\n    return hashlib.sha256(string.encode()).hexdigest()\n\n", "CodeBleu_score": 39.829606810560406}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"Input must be a pandas DataFrame, pandas Series, list, or dictionary.\"\n        )\n    return df", "CodeBleu_score": 46.95435305624518}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    else:\n        raise ValueError(\"Input must be a pandas DataFrame, Series, list, or dictionary.\")\n    return df\n\n", "CodeBleu_score": 43.702937349286955}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    else:\n        raise ValueError(\"Unsupported data type. Please provide a pandas DataFrame, Series, list, or dictionary.\")\n    return df\n", "CodeBleu_score": 44.28607067507329}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > self.shape[0]:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sample = pd.DataFrame()\n        for col in self.columns:\n            if self[col].dtype == \"object\":\n                sample[col] = self[col].sample(n, replace=True)\n            else:\n                sample[col] = self[col].sample(n, replace=True)\n        sample = sample.reset_index(drop=True)\n        sample.iloc[:n, :] = self.iloc[:n, :].sample(frac=1).reset_index(drop=True)\n        return sample", "CodeBleu_score": 32.98848862476592}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > self.shape[0]:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sample_df = pd.DataFrame()\n        for col in self.columns:\n            if self[col].dtype == \"object\":\n                sample_df[col] = self[col].sample(n, replace=True)\n            else:\n                sample_df[col] = self[col].sample(n, replace=True)\n        sample_df.iloc[:n, :] = sample_df.iloc[:n, :].apply(\n            lambda x: x.apply(lambda y: self.anonymize(y))\n        )\n        return sample_df", "CodeBleu_score": 34.52020405782138}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if len(self) < n:\n        return self.sample(frac=1).reset_index(drop=True)\n\n    sample_df = pd.DataFrame()\n    for col in self.columns:\n        if self[col].dtype == \"object\":\n            sample_df[col] = self[col].sample(n, replace=True)\n        else:\n            sample_df[col] = self[col].sample(n, replace=True)\n\n    # Anonymize the first few rows of the sampled DataFrame\n    sample_df.iloc[:3, :] = \"****\"\n\n    return sample_df\n\n", "CodeBleu_score": 36.779872318822505}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    if df.empty:\n        return \"\"\n    df_name = df.name\n    df_description = df.description\n    df_details = f\"<details>{df.shape[0]} rows, {df.shape[1]} columns</details>\"\n    df_csv = df.to_csv(index=False)\n    df_csv = df_csv.replace(\"\\n\", \"<br>\")\n    df_csv = df_csv.replace('\"', \"\")\n    df_csv = df_csv.replace(\",\", \"</td><td>\")\n    df_csv = df_csv.replace(\"<br>\", \"</td></tr><tr><td>\")\n    df_csv = f\"<table><tr><td>{df_csv}</td></tr></table>\"\n    df_csv = df_csv.replace(\"<td>\", \"<td style='text-align: center;'>\")\n    df_csv = df_csv.replace(\"<tr>\", \"<tr style='text-align: center;'>\")\n    df_csv = df_csv.replace(\"<table>\", \"<table style='text-align: center;'>\")\n    df_csv = df_csv.replace(\"</td>\", \"</td>\\n\")\n    df_csv = df_csv.replace(\"</tr>\", \"</tr>\\n\")\n    df_csv = df_csv.replace(\"</table>\", \"</table>\\n\")\n    df_csv = df_csv.replace(\"</td>\\n</tr>\\n</table>\", \"</td></tr></table>\")\n    df_csv = df_csv.replace(\"</td>\\n</tr>\\n</table>\", \"</td></tr></table>\")\n    df_csv = df_csv.replace(\"</td>\\n</tr>\\n</table>\", \"</td></tr></table>\")\n    df_csv = df_csv.replace(\"</td>\\n</tr>\\n</", "CodeBleu_score": 26.65704169240513}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    # Create a new XML element for the DataFrame\n    df_element = ET.Element('dataframe')\n\n    # Add the DataFrame's name and description as attributes\n    df_element.set('name', df.name)\n    df_element.set('description', extras['description'])\n\n    # Add the DataFrame's dimensions as attributes\n    df_element.set('rows', str(len(df)))\n    df_element.set('columns', str(len(df.columns)))\n\n    # Add the DataFrame's content as a CSV string\n    df_csv = df.to_csv(index=False)\n    df_element.text = df_csv\n\n    # Add additional details about the DataFrame\n    df_element.set('dtype', str(df.dtypes))\n    df_element.set('memory_usage', str(df.memory_usage()))\n\n    # Convert the XML element to a string\n    df_xml = ET.tostring(df_element, encoding='unicode')\n\n    return df_xml\n", "CodeBleu_score": 35.560703462216686}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    # Construct the opening tag with name and description attributes\n    tag = f\"<{df.name}\"\n    if \"description\" in extras:\n        tag += f' description=\"{extras[\"description\"]}\"'\n    tag += \">\"\n    # Add details about the DataFrame's dimensions and content in CSV format\n    tag += f\"\\n<dimensions>{df.shape[0]},{df.shape[1]}</dimensions>\"\n    tag += f\"\\n<content>{df.to_csv(index=False)}</content>\"\n    # Close the tag\n    tag += f\"\\n</{df.name}>\"\n    return tag\n\n", "CodeBleu_score": 34.827089417167734}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n    if \"description\" in df.metadata:\n        description = df.metadata[\"description\"]\n        html = f\"<table name='{df.name}' description='{description}'>\\n\"\n    else:\n        html = f\"<table name='{df.name}'>\\n\"\n    html += df.head().to_csv(index=False)\n    html += \"</table>\"\n    return html", "CodeBleu_score": 38.64713988585628}, {"code": "def convert_df_sql_connector_to_str\n    pass", "CodeBleu_score": 2.4514239784450392e-14}, {"code": "def convert_df_sql_connector_to_str\n    return df.to_html(name=name, index=False, justify=\"left\", border=0, escape=False)", "CodeBleu_score": 6.847846792686123}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    df_json = self.convert_df_to_json(df)\n    df_yaml = yaml.dump(df_json)\n    if extras.get(\"is_direct_sql\", False):\n        df_yaml = f\"<table>{df_yaml}</table>\"\n    return df_yaml\n\n", "CodeBleu_score": 40.545257256899106}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = self.convert_df_to_json(df, extras)\n    yaml_str = yaml.dump(json.loads(json_str))\n    if extras.get(\"is_direct_sql\", False):\n        yaml_str = f\"<table>{yaml_str}</table>\"\n    return yaml_str\n\n", "CodeBleu_score": 54.456803790759665}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = self.convert_df_to_json(df, extras)\n    yml_str = yaml.dump(json.loads(json_str))\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yml_str}</table>\"\n    return yml_str\n\n", "CodeBleu_score": 60.20682425019539}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, int) or isinstance(self.value, float)\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, pd.DataFrame) or isinstance(self.value, pd.Series)\n    elif expected_type == \"plot\":\n        return isinstance(self.value, str) and (self.value.endswith(\".png\") or self.value.endswith(\".jpg\") or self.value.endswith(\".jpeg\") or self.value.endswith(\".svg\")) or isinstance(self.value, dict)\n    else:\n        return False", "CodeBleu_score": 52.87911798530705}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if not expected_type:\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, str) and self.value.endswith(\n            (\".png\", \".jpg\", \".jpeg\", \".svg\", \".pdf\")\n        ) or isinstance(self.value, dict)\n    else:\n        raise ValueError(\n            f\"Invalid expected_type: {expected_type}. Expected one of 'number', 'string', 'dataframe', or 'plot'.\"\n        )\n\n", "CodeBleu_score": 60.028265056733254}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"number\":\n        if isinstance(self.value, int) or isinstance(self.value, float):\n            return True\n        else:\n            return False\n    elif expected_type == \"string\":\n        if isinstance(self.value, str):\n            return True\n        else:\n            return False\n    elif expected_type == \"dataframe\":\n        if isinstance(self.value, pd.DataFrame) or isinstance(self.value, pd.Series):\n            return True\n        else:\n            return False\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str) and re.match(r\"^(?:[a-zA-Z]:)?(?:\\\\\\\\|/)(?:.+\\\\\\\\|)(.+)$\", self.value):\n            return True\n        elif isinstance(self.value, dict):\n            return True\n        else:\n            return False\n    elif expected_type == \"\":\n        return True\n    else:\n        return False\n\n", "CodeBleu_score": 48.954692796296264}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)    if not self.distance_params:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            if \"X\" in inspect.signature(self.distance).parameters:\n                return self.distance(X=X, **self.distance_params)\n            else:\n                return self.distance(X, **self.distance_params)\n        else:\n            return self.distance(X, X2, **self.distance_params)", "CodeBleu_score": 27.32323008106817}, {"code": " def _distance(self, X, X2=None)    if self.distance_params is None:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        X = check_array(X)\n        if X2 is None:\n            X2 = X\n        else:\n            X2 = check_array(X2)\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            sig = signature(self.distance)\n            if len(sig.parameters) == 1:\n                return self.distance(X)\n            else:\n                return self.distance(X, X)\n        else:\n            return self.distance(X, X2, **self.distance_params)\n\n", "CodeBleu_score": 37.05142972997532}, {"code": " def _distance(self, X, X2=None)    if self.distance_params is None:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        X = check_array(X)\n        if X2 is None:\n            X2 = X\n        else:\n            X2 = check_array(X2)\n        if self.distance in PAIRWISE_DISTANCE_FUNCTIONS:\n            X = X.reshape((X.shape[0], -1))\n            X2 = X2.reshape((X2.shape[0], -1))\n            distance_params = self.distance_params\n            if self.distance == \"euclidean\":\n                distance_params = {}\n            X_distance = pairwise_distance(X, X2, metric=self.distance, **distance_params)\n        else:\n            raise ValueError(\"Unknown metric string.\")\n    else:\n        X_distance = self.distance(X, X2, **self.distance_params)\n    return X_distance\n\n", "CodeBleu_score": 42.83056270762222}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        X = X.copy()\n        X.reset_index(drop=True, inplace=True)\n        X = X.pivot(index=None, columns=\"dim_0\", values=\"value\")\n        X.fillna(0, inplace=True)\n        X = X.to_numpy()\n        lengths = [X.shape[0]] * X.shape[1]\n        X = np.column_stack((lengths, X))\n        return X\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 2:\n            lengths = [X.shape[0]] * X.shape[1]\n            X = np.column_stack((lengths, X))\n            return X\n        else:\n            return X.flatten()\n    else:\n        raise ValueError(\"Input data must be a pandas DataFrame or numpy array.\")", "CodeBleu_score": 34.61421154492099}, {"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        if X.shape[0] != len(X.index.unique()):\n            X = X.reset_index()\n            X = X.pivot(index=\"index\", columns=\"dim_0\", values=0)\n            X = X.fillna(0)\n            X = X.to_numpy()\n            lengths = np.array([len(X[i]) for i in range(X.shape[0])])\n            X = np.concatenate((lengths.reshape(-1, 1), X), axis=1)\n        else:\n            X = X.to_numpy().flatten()\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 2:\n            lengths = np.array([len(X[i]) for i in range(X.shape[0])])\n            X = np.concatenate((lengths.reshape(-1, 1), X), axis=1)\n        else:\n            X = X.flatten()\n    elif isinstance(X, list):\n        X = np.array(X)\n        if X.ndim == 2:\n            lengths = np.array([len(X[i]) for i in range(X.shape[0])])\n            X = np.concatenate((lengths.reshape(-1, 1), X), axis=1)\n        else:\n            X = X.flatten()\n    return X\n\n", "CodeBleu_score": 35.996255512589244}, {"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        if X.shape[0] != X.shape[1]:\n            X = X.reset_index(drop=True)\n            X = X.pivot(\n                index=\"index\", columns=\"dim_0\", values=\"value\"\n            ).fillna(0).to_numpy()\n            self.n_instances_ = X.shape[0]\n            self.n_dims_ = [X.shape[1]]\n        else:\n            self.n_instances_ = 1\n            self.n_dims_ = [X.shape[1]]\n            X = X.to_numpy().flatten()\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape((1, X.shape[0]))\n        self.n_instances_, self.n_dims_ = X.shape[0], [X.shape[1]]\n    elif isinstance(X, list):\n        if not isinstance(X[0], np.ndarray):\n            X = np.asarray(X)\n        if X.ndim == 1:\n            X = X.reshape((1, X.shape[0]))\n        self.n_instances_, self.n_dims_ = X.shape[0], [X.shape[1]]\n    else:\n        raise ValueError(\n            \"Input data must be either a pandas DataFrame, numpy array or a list of numpy arrays.\"\n        )\n    return X\n\n", "CodeBleu_score": 33.79219733762853}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n    from sktime.base import BaseObject\n    from sklearn.base import BaseEstimator\n\n    if not isinstance(obj, type):\n        return False\n    return issubclass(obj, BaseEstimator) and not issubclass(obj, BaseObject)", "CodeBleu_score": 24.652188298630247}, {"code": "def is_sklearn_estimator(obj)\n    if not isinstance(obj, type):\n        return False\n    return issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)", "CodeBleu_score": 22.216868586077418}, {"code": "def is_sklearn_estimator(obj)\n    if not isinstance(obj, type):\n        return False\n    if issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject):\n        return True\n    return False\n\n", "CodeBleu_score": 23.87742667136114}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n    from .sklearn_regressor_pipeline import SklearnRegressorPipeline\n    from .transformer_pipeline import TransformerPipeline\n    if isinstance(other, BaseTransformer):\n        transformer_pipeline = TransformerPipeline(other, self)\n        return SklearnRegressorPipeline(self.regressor, transformer_pipeline)\n    return NotImplemented", "CodeBleu_score": 32.251299247168724}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return TransformerPipeline(other, self)\n    return NotImplemented", "CodeBleu_score": 13.952327625158528}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return TransformerPipeline(other, self)\n    return NotImplemented", "CodeBleu_score": 13.952327625158528}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = list(df.columns)\n    for col in df.columns:\n        if type(col) != str:\n            try:\n                df.rename({col: str(col)}, axis=1, inplace=True)\n            except Exception as e:\n                print(e)\n                return None\n    return df\n", "CodeBleu_score": 44.265922980470634}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = list(map(str, df.columns))\n    if all(isinstance(col, str) for col in df.columns):\n        return df\n    else:\n        df.columns = list(map(str, df.columns))\n        return df", "CodeBleu_score": 49.58575891443899}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = list(map(str, df.columns))\n    cols = df.columns\n    cols = [col for col in cols if type(col) != str]\n    if len(cols) > 0:\n        for col in cols:\n            df[col] = df[col].astype(str)\n    return df", "CodeBleu_score": 52.47509249184916}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n    rgen = np.random.RandomState(self.random_state)\n    return np.array([self.classes_[i] for i in rgen.choice([np.argmax(self.predict_proba(xi)) for xi in X], 1)])\n\n", "CodeBleu_score": 27.75974392543743}, {"code": "def predict(self, X)\n    rgen = np.random.RandomState(self.random_state)\n    probabilities = self.predict_proba(X)\n    predicted_indices = np.argmax(probabilities, axis=1)\n    return np.array([self.classes_[index] for index in predicted_indices])", "CodeBleu_score": 26.61694829011877}, {"code": "def predict(self, X)\n    rgen = np.random.RandomState(self.random_state)\n    predictions = np.argmax(self.softmax(X), axis=1)\n    return self.classes_[predictions]\n", "CodeBleu_score": 18.01559869418679}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_transformed = []\n    for group in groups:\n        X_group = X[:, group]\n        X_group_transformed = pcas[group].transform(X_group)\n        X_transformed.append(X_group_transformed)\n    X_transformed = np.concatenate(X_transformed, axis=1)\n    X_transformed[np.isnan(X_transformed)] = 0\n    proba = clf.predict_proba(X_transformed)\n    if proba.shape[1] != self.n_classes_:\n        proba = self._adjust_proba(proba)\n    return proba\n\n", "CodeBleu_score": 33.49679377888002}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_pca = np.zeros((X.shape[0], 0))\n    for i, group in enumerate(groups):\n        X_pca = np.concatenate(\n            (X_pca, pcas[i].transform(X[:, group])), axis=1)\n    X_pca = np.nan_to_num(X_pca)\n    y_pred_proba = clf.predict_proba(X_pca)\n    if y_pred_proba.shape[1] != self.n_classes_:\n        y_pred_proba = np.concatenate(\n            (y_pred_proba, np.zeros((X.shape[0], self.n_classes_ - y_pred_proba.shape[1]))), axis=1)\n    return y_pred_proba\n\n", "CodeBleu_score": 44.19779624899773}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_ = np.zeros((X.shape[0], 0))\n    for i in range(len(pcas)):\n        X_ = np.concatenate((X_, pcas[i].transform(X[:, groups[i]])), axis=1)\n    X_[np.isnan(X_)] = 0\n    if clf.predict_proba(X_).shape[1] != self.n_classes_:\n        return self._fix_proba(clf.predict_proba(X_), self.n_classes_)\n    return clf.predict_proba(X_)\n", "CodeBleu_score": 28.45743321701138}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n    if not self.is_fitted_:\n        raise ValueError(\"The model is not fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n    if self.n_classes_ == 1:\n        return np.ones((len(X), 1))\n    if X.ndim == 3 and X.shape[2] == 1:\n        X = X.reshape((X.shape[0], X.shape[1]))\n    elif X.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array or a 3D numpy array with a single feature.\")\n    X = self.validate_X(X, dtype=DTYPE, y_numeric=False, allow_nan=True)\n    proba = np.zeros((len(X), self.n_classes_), dtype=DTYPE)\n    for i, x in enumerate(X):\n        proba[i] = self.root_.predict_proba(x)\n    return proba", "CodeBleu_score": 38.11920007499299}, {"code": "def predict_proba(self, X)\n    if not self.fitted:\n        raise Exception('The model is not fitted yet.')\n    if len(self.classes_) == 1:\n        return np.ones((X.shape[0], 1))\n    if X.ndim == 3 and X.shape[2] == 1:\n        X = X.reshape(X.shape[0], X.shape[1])\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        raise Exception('Input must be a 2D numpy array.')\n    if not np.isfinite(X).all():\n        raise Exception('Input must not contain NaN values.')\n    proba = []\n    for x in X:\n        proba.append(self.root.predict_proba(x))\n    return np.array(proba)", "CodeBleu_score": 29.425505817527352}, {"code": "def predict_proba(self, X)\n    check_is_fitted(self)\n    if self.n_classes_ == 1:\n        return np.ones((len(X), 1))\n    X = check_array(X, accept_sparse=\"csr\")\n    if issparse(X):\n        X.sort_indices()\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\"Number of features of the model must match the input. \"\n                         \"Model n_features_in_ is %s and input n_features is %s \"\n                         % (self.n_features_in_, X.shape[1]))\n    if self.n_outputs_ == 1:\n        X = np.expand_dims(X, axis=1)\n    proba = np.zeros((X.shape[0], self.n_classes_, self.n_outputs_),\n                      dtype=np.float64)\n    for k in range(self.n_outputs_):\n        for i in range(X.shape[0]):\n            proba[i, :, k] = self.root_.predict_proba(X[i, :, k], self.n_classes_)\n    if self.n_outputs_ == 1:\n        proba = proba[:, :, 0]\n        proba = proba.reshape((proba.shape[0], -1))\n    return proba", "CodeBleu_score": 42.23063745211687}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        return X\n    elif self.transformers_.scitype == \"Primitives\":\n        return convert(X, from_type=self.transformers_.scitype, to_type=\"Table\")\n    elif self.transformers_.scitype == \"Series\":\n        return convert(X, from_type=self.transformers_.scitype, to_type=\"Panel\")\n    else:\n        raise TypeError(\n            \"unexpected output type from transformers: \" + self.transformers_.scitype\n        )\n\n", "CodeBleu_score": 30.361152101428235}, {"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        return X\n    if isinstance(self.transformers_, list):\n        if len(self.transformers_) > 0:\n            scitype = self.transformers_[0].get_scitype()\n        else:\n            scitype = \"Series\"\n    else:\n        scitype = self.transformers_.get_scitype()\n    if scitype == \"Primitives\":\n        X = convert(X, from_type=\"nested_univ\", to_type=\"numpy2D\")\n    elif scitype == \"Series\":\n        X = convert(X, from_type=\"nested_univ\", to_type=\"numpyflat\")\n    else:\n        raise TypeError(\n            \"unexpected output type from transformers, should be \"\n            \"Primitives or Series, but found %s\" % scitype\n        )\n    return X\n\n", "CodeBleu_score": 47.812619207128634}, {"code": "def _convert_X_to_sklearn(self, X)\n    Xt = self.transform(X)\n    scitype = scitype_to_instance(self.transformers_[-1])\n    if scitype == \"Primitives\":\n        return Xt.to_numpy().reshape(Xt.shape[0], -1)\n    elif scitype == \"Series\":\n        return Xt.to_numpy().reshape(Xt.shape[0], -1)\n    else:\n        raise TypeError(\n            f\"Expected output of scitype Series or Primitives, but found scitype {scitype}.\"\n        )", "CodeBleu_score": 22.92914298152456}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n    if random_state is None:\n        return estimator\n    if isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    params = {}\n    for key in estimator.get_params(deep=False):\n        if key.endswith('random_state'):\n            params[key] = random_state.randint(1000000000)\n    if len(params) > 0:\n        estimator.set_params(**params)\n    return estimator\n\n", "CodeBleu_score": 42.14096250637847}, {"code": "def set_random_state(estimator, random_state=0)\n    if isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    elif not isinstance(random_state, np.random.RandomState):\n        raise ValueError(\n            \"random_state must be either an int or a RandomState instance.\"\n        )\n    params_to_update = {}\n    for param_name, param_value in estimator.get_params().items():\n        if param_name == \"random_state\":\n            params_to_update[param_name] = random_state.randint(0, 2**32 - 1)\n        elif param_name.endswith(\"random_state\"):\n            params_to_update[param_name] = random_state.randint(0, 2**32 - 1)\n    if len(params_to_update) > 0:\n        estimator.set_params(**params_to_update)\n\n", "CodeBleu_score": 32.372334152493764}, {"code": "def set_random_state(estimator, random_state=0)\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state must be an integer.\")\n    params = {}\n    for key in estimator.get_params(deep=True):\n        if \"random_state\" in key:\n            params[key] = random_state\n    if params:\n        estimator.set_params(**params)\n\n", "CodeBleu_score": 38.378898475175305}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n    n_instances, _ = X.shape\n    n_intervals = len(intervals)\n    transformed = np.zeros((n_instances, 3 * n_intervals))\n\n    for interval_index in range(n_intervals):\n        start, end = intervals[interval_index]\n        temp = X[:, start:end]\n        means = temp.mean(axis=1)\n        stds = temp.std(axis=1)\n        slope = np.apply_along_axis(lambda x: _slope(x, axis=0), axis=1, arr=temp)\n        transformed[:, 3 * interval_index] = means * 1000\n        transformed[:, 3 * interval_index + 1] = stds * 1000\n        transformed[:, 3 * interval_index + 2] = slope\n\n    return transformed.transpose()\n\n", "CodeBleu_score": 37.575822355110695}, {"code": "def _transform(X, intervals)\n    n_instances, n_intervals = intervals.shape\n    transformed = np.empty((n_instances, 3 * n_intervals))\n    for i in range(n_instances):\n        for j in range(n_intervals):\n            start, end = intervals[i, j]\n            x = X[i, start:end]\n            mean = np.mean(x)\n            std = np.std(x)\n            slope = (x[-1] - x[0]) / (end - start)\n            transformed[i, 3 * j] = mean\n            transformed[i, 3 * j + 1] = std\n            transformed[i, 3 * j + 2] = slope\n    return transformed.T\n\n", "CodeBleu_score": 35.89201878141006}, {"code": "def _transform(X, intervals)\n    n_instances, n_intervals = intervals.shape\n    Xt = np.zeros((n_instances, 3 * n_intervals))\n    for i in range(n_instances):\n        for j in range(n_intervals):\n            start, end = intervals[i, j]\n            x = X[start:end]\n            Xt[i, 3 * j] = np.mean(x)\n            Xt[i, 3 * j + 1] = np.std(x)\n            Xt[i, 3 * j + 2] = np.mean(np.diff(x))\n    return Xt.T\n\n", "CodeBleu_score": 29.288536238894135}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    # Initialize annotation and operation parameters if they are not provided\n    if annotation_params is None:\n        annotation_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    # Create the plot with given parameters and apply pre-plot axis operations\n    ax = self.create_plot(plot, plot_params, *args, **kwargs)\n    ax = self.apply_operations(ax, ax_op_before)\n\n    # Configure and annotate the plot based on provided configuration and annotation functions\n    ax = self.configure_plot(ax, configuration)\n    ax = self.annotate_plot(ax, annotation_func, annotation_params,\n                            annotate_params)\n\n    # Apply post-plot axis operations\n    ax = self.apply_operations(ax, ax_op_after)\n\n    # Return the output of the plotting process\n    return ax\n\n", "CodeBleu_score": 51.89430050189059}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    if annotate_params is None:\n        annotate_params = {}\n    if annotation_params is None:\n        annotation_params = {}\n\n    # Initialize annotation and operation parameters\n    annotation_params = self.init_params(\n        annotation_params,\n        self.annotation_params_template,\n        self.annotation_params_default)\n    op_params = self.init_params(\n        annotation_params,\n        self.op_params_template,\n        self.op_params_default)\n\n    # Create the plot\n    plot_params = self.init_params(\n        plot_params,\n        self.plot_params_template,\n        self.plot_params_default)\n    plot_params = self.init_params(\n        plot_params,\n        self.plot_params_template,\n        self.plot_params_default)\n    plot_func = getattr(sns, plot)\n    plot_func(**plot_params)\n\n    # Apply pre-plot axis operations\n    for op in ax_op_before:\n        if isinstance(op, str):\n            self.apply_ax_op(op, op_params)\n        elif isinstance(op, list):\n            self.apply_ax_op(op[0], op_params, *op[1:])\n        elif isinstance(op, dict):\n            self.apply_ax_op(list(op.keys())[0], op_params, **op[list(op.keys())[0]])\n\n    # Configure and annotate the plot\n    for conf in configuration:\n        if isinstance(conf, str):\n            self.configure(conf, annotation_params)\n        elif isinstance(conf, list):\n            self.configure(conf[0], annotation_params, *conf[1:])\n        elif isinstance(conf, dict):\n            self.configure(list(conf.keys())[0], annotation_params, **conf[list(conf.keys())[0]])", "CodeBleu_score": 42.382862908967276}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    if annotation_params is None:\n        annotation_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if annotate_params is None:\n        annotate_params = {}\n\n    # Initialize annotation and operation parameters if they are not provided\n    annotation_params = self._init_annotation_params(\n        plot, annotation_func, annotation_params, configuration, plot_params,\n        *args, **kwargs)\n    ax_op_before = self._init_ax_op(ax_op_before, plot_params, configuration)\n    ax_op_after = self._init_ax_op(ax_op_after, plot_params, configuration)\n\n    # Create the plot with given parameters and apply pre-plot axis operations\n    output = self._create_plot(plot, plot_params, configuration)\n    output = self._apply_ax_op(output, ax_op_before, plot_params, configuration)\n\n    # Configure and annotate the plot based on provided configuration and annotation functions\n    output = self._configure_plot(output, plot, plot_params, configuration)\n    output = self._annotate_plot(output, plot, plot_params, configuration,\n                                 annotation_func, annotation_params)\n\n    # Apply post-plot axis operations and return the output of the plotting process\n    return self._apply_ax_op(output, ax_op_after, plot_params, configuration)\n\n", "CodeBleu_score": 43.79626791127635}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n        raise ValueError('Model not fitted')\n    X = check_array(X)\n    if X.shape[1] != self.expected_feature_count:\n        raise ValueError('Expected {} features, got {}'.format(self.expected_feature_count, X.shape[1]))\n    scores = self._program(X)\n    probs = np.zeros((X.shape[0], 2))\n    probs[:, 0] = 1 / (1 + np.exp(scores))\n    probs[:, 1] = 1 - probs[:, 0]\n    return probs\n", "CodeBleu_score": 42.31076522642419}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, \"_program\"):\n        raise ValueError(\"The model has not been fitted yet.\")\n    X = self._validate_data(X)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\n            \"The number of features in the input data does not match the expected number of features.\"\n        )\n    scores = self._program.execute(X)\n    probs = self._transform_scores_to_probs(scores)\n    return np.column_stack((1 - probs, probs))", "CodeBleu_score": 37.73444399592224}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, \"_program\"):\n        raise ValueError(\"Model not fitted\")\n    X = self._validate_data(X, dtype=np.float64)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\n            f\"X has {X.shape[1]} features per sample; expecting {self.n_features_in_}.\"\n        )\n    scores = self._program.predict_scores(X)\n    proba = self._program.transform_scores_to_proba(scores)\n    return np.hstack((1 - proba, proba))\n\n", "CodeBleu_score": 38.15004096060803}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n    mutated_program = self.copy()\n    modified_nodes = []\n    for node_index, node in enumerate(mutated_program.nodes):\n        if random_state.uniform() < self.point_mutation_probability:\n            if node.arity == 0:\n                if random_state.uniform() < 0.5:\n                    constant = random_state.uniform(self.constant_range[0], self.constant_range[1])\n                    mutated_program.nodes[node_index] = ConstantNode(constant)\n                else:\n                    variable = random_state.randint(0, self.num_features)\n                    mutated_program.nodes[node_index] = VariableNode(variable)\n            else:\n                valid_functions = [f for f in self.functions if f.arity == node.arity]\n                new_function = random_state.choice(valid_functions)\n                mutated_program.nodes[node_index] = FunctionNode(new_function)\n            modified_nodes.append(node_index)\n    return mutated_program, modified_nodes\n", "CodeBleu_score": 25.558638561433522}, {"code": "def point_mutation(self, random_state)\n    # Create a copy of the program\n    mutated_program = copy.deepcopy(self)\n\n    # Identify nodes to modify based on a uniform random distribution and a probability threshold\n    nodes_to_modify = [node for node in mutated_program.nodes if random_state.uniform() < self.point_mutation_probability]\n\n    # For each node to be modified\n    for node in nodes_to_modify:\n        # If the node is a function\n        if node.arity > 0:\n            # Find a valid replacement function with the same arity\n            replacement_function = random_state.choice([f for f in self.functions if f.arity == node.arity])\n            # Update the node with the new function\n            node.function = replacement_function\n        # If the node is a terminal\n        else:\n            # Replace the terminal with either a constant or a variable\n            if random_state.uniform() < 0.5:\n                # Replace with a constant\n                node.function = random_state.uniform(self.constant_range[0], self.constant_range[1])\n            else:\n                # Replace with a variable\n                node.function = random_state.randint(0, self.num_features)\n\n    # Return the mutated program and the list of modified node indices\n    return mutated_program, nodes_to_modify\n\n", "CodeBleu_score": 18.394818905511908}, {"code": "def point_mutation(self, random_state)\n    # Create a copy of the program\n    mutated_program = self.copy()\n\n    # Identify nodes to modify based on a uniform random distribution\n    nodes_to_modify = [\n        i for i, node in enumerate(mutated_program.nodes) if random_state.uniform() < self.mutation_probability\n    ]\n\n    # For each node to be mutated\n    for node_index in nodes_to_modify:\n        node = mutated_program.nodes[node_index]\n\n        # If the node is a function\n        if node.arity > 0:\n            # Find a valid replacement function with the same arity\n            replacement_function = random_state.choice(\n                [f for f in self.functions if f.arity == node.arity and f != node.function]\n            )\n\n            # Update the node\n            mutated_program.nodes[node_index].function = replacement_function\n\n        # If the node is a terminal\n        else:\n            # Replace it with either a constant or a variable\n            if random_state.uniform() < 0.5:\n                # Replace with a constant within the defined constant range\n                mutated_program.nodes[node_index].function = random_state.uniform(\n                    self.constant_range[0], self.constant_range[1]\n                )\n            else:\n                # Replace with a variable from the input features\n                mutated_program.nodes[node_index].function = random_state.randint(0, self.num_features)\n\n    # Return the mutated program and the list of modified node indices\n    return mutated_program, nodes_to_modify\n\n", "CodeBleu_score": 24.301936917863603}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"sma\"\n    offset = int(offset) if offset and offset != 0 else 0\n\n    # Calculate Result\n    if mamode == \"sma\":\n        ma = close.rolling(length).mean()\n    elif mamode == \"ema\":\n        ma = close.ewm(span=length, adjust=False).mean()\n    elif mamode == \"wma\":\n        weights = np.arange(1, length + 1)\n        ma = close.rolling(length).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n    elif mamode == \"hma\":\n        weights = np.arange(1, length + 1)\n        ma = close.rolling(length).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n        ma = ma.rolling(length // 2).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n        ma = ma.rolling(int(np.sqrt(length))).mean()\n    elif mamode == \"zlsma\":\n        ma = close.rolling(length).apply(lambda x: (x[length - 1] + x.sum()) / length, raw=True)\n    else:\n        raise ValueError(f\"Invalid mamode: {mamode}\")\n\n    bias = (close / ma - 1) * 100\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill", "CodeBleu_score": 50.21965593008729}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"sma\"\n    offset = int(offset) if offset and offset != 0 else 0\n\n    # Calculate Result\n    if mamode == \"sma\":\n        ma = close.rolling(length).mean()\n    elif mamode == \"ema\":\n        ma = close.ewm(span=length, adjust=False).mean()\n    else:\n        ma = close.rolling(length).mean()\n\n    bias = (close / ma) - 1\n\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 59.179391456695974}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode.lower() if mamode else \"sma\"\n    offset = get_offset(offset)\n\n    # Calculate Result\n    if close is not None:\n        if mamode == \"sma\":\n            ma = close.rolling(length).mean()\n        elif mamode == \"ema\":\n            ma = close.ewm(span=length, adjust=False).mean()\n        else:\n            ma = close.rolling(length).mean()\n\n        bias = (close / ma) - 1\n        bias = bias * 100\n        bias = bias.round(2)\n\n        # Offset\n        if offset != 0:\n            bias = bias.shift(offset)\n\n        # Handle fills\n        if \"fillna\" in kwargs:\n            bias.fillna(kwargs[\"fillna\"], inplace=True)\n        if \"fill_method\" in kwargs:\n            bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n        # Name and Categorize it\n        bias.name = f\"BIAS_{length}\"\n        bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 58.39963359697635}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import requests\n    import pandas as pd\n    import datetime\n    import traceback\n\n    # Define the URL based on the broker, cryptocurrency code, kline type, and size\n    url = f'https://api.binance.com/api/v3/klines?symbol={code.upper()}USDT&interval={ktype}&limit={size}'\n    if broker == 'hb':\n        url = f'https://api.huobi.pro/market/history/kline?period={ktype}&size={size}&symbol={code.lower()}usdt'\n    elif broker == 'okx':\n        url = f'https://www.okx.com/api/v5/market/candles?instId={code.upper()}-USDT&bar={ktype}&limit={size}'\n    elif broker == 'kucoin':\n        url = f'https://api.kucoin.com/api/v1/market/candles?type={ktype}&symbol={code.upper()}-USDT&startAt=null&endAt=null&limit={size}'\n\n    # Fetch the data\n    try:\n        data = requests.get(url).json()\n    except Exception as e:\n        print(f'Error fetching data: {e}')\n        return None\n\n    # Handle any broker-specific data structures\n    if broker == 'hb':\n        data = data['data']\n    elif broker == 'okx':\n        data = data['data']\n    elif broker == 'kucoin':\n        data = data['data']\n\n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data, columns=['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'count', 'volume', 'vwap', 'last_close'])\n\n    # Adjust the date formatting based on the broker and kline type\n    if broker == 'hb':\n        df['date'] = pd.to_datetime(df['date'], unit='s')\n    elif broker == 'okx':\n        df['date'] =", "CodeBleu_score": 30.032039846784564}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import requests\n    import pandas as pd\n    from datetime import datetime\n    import traceback\n\n    # Define the base URL for the broker\n    if broker == 'hb':\n        url = 'https://api.huobi.pro/market/history/kline'\n    elif broker == 'binance':\n        url = 'https://api.binance.com/api/v3/klines'\n    elif broker == 'okx':\n        url = 'https://www.okx.com/api/v5/market/candles'\n    else:\n        print('Invalid broker')\n        return None\n\n    # Define the parameters for the request\n    params = {\n        'symbol': code.upper(),\n        'period': ktype,\n        'size': size\n    }\n\n    # Make the request to the broker's API\n    response = requests.get(url, params=params)\n\n    # Handle any errors that may occur\n    if response.status_code != 200:\n        print(f'Error: {response.status_code}')\n        return None\n\n    # Parse the response data based on the broker\n    if broker == 'hb':\n        data = response.json()['data']\n    elif broker == 'binance':\n        data = response.json()\n    elif broker == 'okx':\n        data = response.json()['data']\n\n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n\n    # Format the date column based on the broker and kline type\n    if broker == 'hb':\n        if ktype == 'D':\n            df['timestamp'] = df['timestamp'].apply(lambda x: datetime.fromtimestamp(x / 1000).strftime('%Y-%m-%d'))\n        else:\n            df['timestamp'] = df['timestamp'].apply(lambda x: datetime.fromtimestamp(x / 1000).strftime('%Y-%m-%d %H:%M:%S'))\n    elif broker == 'binance':", "CodeBleu_score": 37.705607889881705}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        url = f'https://www.binance.com/fapi/v1/klines?symbol={code.upper()}&interval={ktype}&limit={size}'\n        if broker == 'hb':\n            url = f'https://api.hbdm.com/market/history/kline?period={ktype}&size={size}&symbol={code}'\n        if broker == 'b':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code.upper()}&interval={ktype}&limit={size}'\n        data = requests.get(url)\n        if broker == 'hb':\n            data = data.json()['data']\n        if broker == 'b':\n            data = data.json()\n        df = pd.DataFrame(data)\n        if broker == 'hb':\n            df = df.iloc[:, 0:6]\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = pd.to_datetime(df['date'], unit='s')\n            df['date'] = df['date'] + pd.Timedelta(hours=8)\n        if broker == 'b':\n            df = df.iloc[:, 0:6]\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n            df['date'] = df['date'] + pd.Timedelta(hours=8)\n        return df\n    except Exception as e:\n        print(traceback.format_exc())\n\n", "CodeBleu_score": 36.033588531736434}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    import pandas as pd\n    import traceback\n    from datetime import datetime\n    from .helpers import get_snapshot\n\n    try:\n        snapshot = get_snapshot(broker, code, size)\n\n        if broker == 'hb':\n            timestamp = datetime.fromtimestamp(snapshot['timestamp'] / 1000)\n        else:\n            timestamp = datetime.fromtimestamp(snapshot['timestamp'])\n\n        ask_df = pd.DataFrame(snapshot['asks']).rename(columns={0: 'price', 1: 'size'})\n        ask_df['timestamp'] = timestamp\n\n        bid_df = pd.DataFrame(snapshot['bids']).rename(columns={0: 'price', 1: 'size'})\n        bid_df['timestamp'] = timestamp\n\n        data = {'ask': ask_df, 'bid': bid_df}\n\n        return pd.Panel(data)\n\n    except:\n        traceback.print_exc()\n\n", "CodeBleu_score": 38.58809458925619}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    import pandas as pd\n    import numpy as np\n    import traceback\n    from datetime import datetime\n\n    # Retrieve data from helper function\n    try:\n        data = get_coins_snapshot(broker, code, size)\n    except Exception as e:\n        print(f\"Error retrieving data from {broker}: {e}\")\n        traceback.print_exc()\n        return None\n\n    # Process data\n    try:\n        if broker == 'hb':\n            # Process timestamp for Huobi\n            timestamp = datetime.fromtimestamp(data['ts'] / 1000)\n            ask_df = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid_df = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n        else:\n            # Process timestamp for other brokers\n            timestamp = datetime.fromtimestamp(data['timestamp'])\n            ask_df = pd.DataFrame(data['asks'], columns=['price', 'amount', 'count'])\n            bid_df = pd.DataFrame(data['bids'], columns=['price', 'amount', 'count'])\n\n        # Add timestamp to each dataframe\n        ask_df['timestamp'] = timestamp\n        bid_df['timestamp'] = timestamp\n\n        # Create panel\n        panel = pd.Panel({'ask': ask_df, 'bid': bid_df})\n\n        return panel\n    except Exception as e:\n        print(f\"Error processing data from {broker}: {e}\")\n        traceback.print_exc()\n        return None\n\n", "CodeBleu_score": 39.219618878925075}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    import pandas as pd\n    import traceback\n\n    def fetch_data(url):\n        import requests\n        import json\n        response = requests.get(url)\n        data = json.loads(response.text)\n        return data\n\n    if broker == 'hb':\n        url = f'https://api.huobi.pro/market/depth?symbol={code}usdt&type=step0'\n        data = fetch_data(url)\n        timestamp = pd.to_datetime(data['ts'], unit='ms')\n        ask = pd.DataFrame(data['tick']['asks'], columns=['price', 'quantity'])\n        bid = pd.DataFrame(data['tick']['bids'], columns=['price', 'quantity'])\n    elif broker == 'bh':\n        url = f'https://api.binance.com/api/v3/depth?symbol={code}USDT&limit={size}'\n        data = fetch_data(url)\n        timestamp = pd.to_datetime(data['E'], unit='ms')\n        ask = pd.DataFrame(data['asks'], columns=['price', 'quantity'])\n        bid = pd.DataFrame(data['bids'], columns=['price', 'quantity'])\n    else:\n        raise ValueError(\"Invalid broker. Supported brokers are 'hb' and 'bh'.\")\n\n    ask['timestamp'] = timestamp\n    bid['timestamp'] = timestamp\n\n    data = {'ask': ask, 'bid': bid}\n\n    panel = pd.Panel(data)\n\n    return panel\n\n", "CodeBleu_score": 40.486105694592304}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    import requests\n    import json\n    import datetime\n\n    # Set the URL based on the broker and cryptocurrency code\n    url = f'https://api.hbdm.com/market/history/trade?symbol={code.upper()}usdt&size=1000'\n    if broker == 'ok':\n        url = f'https://www.okx.com/api/v5/market/trades?instId={code.upper()}-USDT'\n    if broker == 'chbtc':\n        url = f'https://api.chbtc.com/data/v1/trades?symbol={code.upper()}_USDT'\n\n    # Retrieve the data from the URL\n    response = requests.get(url)\n    data = response.json()\n\n    # If the data is not None, process it\n    if data is not None:\n        # For 'hb', select and rename specific columns and convert timestamps\n        if broker == 'hb':\n            df = pd.DataFrame(data['data'])\n            df = df[['trade-id', 'ts', 'trade-price', 'trade-volume', 'direction']]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='ms')\n            df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n\n        # For 'ok' and 'chbtc', select, rename columns, and convert timestamps\n        if broker == 'ok':\n            df = pd.DataFrame(data['data'])\n            df = df[['trade-id', 'ts', 'px', 'sz', 'side']]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='ms')\n            df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d", "CodeBleu_score": 44.20897235535939}, {"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    import requests\n    import json\n    import datetime\n    import time\n    import numpy as np\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n\n    # Define the API URL based on the broker and cryptocurrency code\n    if broker == 'hb':\n        url = f\"https://api.huobi.pro/market/history/trade?symbol={code.upper()}usdt&size=2000\"\n    elif broker == 'ok':\n        url = f\"https://www.okx.com/api/v5/market/history-trades?instId={code.upper()}-USDT\"\n    elif broker == 'chbtc':\n        url = f\"https://api.chbtc.com/data/v1/trades?symbol={code.upper()}_USDT\"\n\n    # Send a GET request to the API and retrieve the data\n    data = requests.get(url).json()\n\n    # If the data is not None, process it based on the broker\n    if data:\n        if broker == 'hb':\n            # Select and rename specific columns\n            df = pd.DataFrame(data['data'])\n            df = df[['id', 'ts', 'data']]\n            df.columns = ['tid', 'datetime', 'price_amount']\n            # Convert timestamps to datetime format\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n            df['datetime'] = df['datetime'].dt.tz_localize('utc').dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')\n            # Split price and amount into separate columns\n            df[['price', 'amount']] = df['price_amount'].apply(pd.Series)\n            df = df[['tid', 'datetime', 'price', 'amount']]\n        elif broker in ['ok', 'chbtc']:\n            # Select and rename specific columns\n            df = pd.DataFrame(data['data'])\n            df = df[['trade_id", "CodeBleu_score": 36.662166383991455}, {"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    import numpy as np\n    import requests\n    import time\n    import datetime\n    import warnings\n    warnings.filterwarnings('ignore')\n\n    # Define the URLs for the data sources\n    urls = {'hb': f'https://api.hbdm.com/market/history/trade?symbol={code.upper()}',\n            'ok': f'https://www.okx.com/api/v5/market/trades?instId={code.upper()}-USDT',\n            'chbtc': f'https://api.chbtc.com/data/v1/trades?currency={code.lower()}'}\n\n    # Retrieve the data from the specified broker\n    data = requests.get(urls[broker]).json()\n\n    # If the data is not None, process it differently depending on the broker\n    if data is not None:\n        if broker == 'hb':\n            df = pd.DataFrame(data['data'])\n            df = df[['id', 'ts', 'price', 'amount', 'direction']]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(lambda x: datetime.datetime.fromtimestamp(x/1000))\n            df['type'] = df['type'].apply(lambda x: 'buy' if x == 'buy' else 'sell')\n        elif broker in ['ok', 'chbtc']:\n            df = pd.DataFrame(data['data'])\n            df = df[['trade-id', 'ts', 'px', 'sz', 'side']]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(lambda x: datetime.datetime.fromtimestamp(x/1000))\n            df['type'] = df['type'].apply(lambda x: 'buy' if x == 'buy' else 'sell')\n\n    # Standardize the column names\n    df.columns = ['tid', 'datetime', 'price', '", "CodeBleu_score": 44.01496121082179}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n    import requests\n    import json\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime\n\n    try:\n        response = requests.get(url)\n        response.encoding = response.apparent_encoding\n        text = response.text\n        text = text.replace('jsonpgz(', '').replace(');', '')\n        data = json.loads(text)\n\n        if fund_type == 'open':\n            columns = ['fund_id', 'fund_name', 'fund_name_abbr', 'fund_company', 'fund_type', 'fund_purchase_status', 'fund_risk_level', 'fund_purchase_start_date', 'fund_purchase_end_date', 'fund_redeem_end_date', 'fund_purchase_min_amount', 'fund_purchase_max_amount', 'fund_manager', 'fund_trustee', 'fund_dividend_type', 'fund_trustee_fee', 'fund_management_fee', 'fund_purchase_fee', 'fund_purchase_fee_rate', 'fund_redeem_fee', 'fund_redeem_fee_rate', 'fund_min_purchase_amount', 'fund_max_purchase_amount', 'fund_purchase_start_date_str', 'fund_purchase_end_date_str', 'fund_redeem_end_date_str', 'fund_purchase_min_amount_str', 'fund_purchase_max_amount_str', 'fund_trustee_fee_str', 'fund_management_fee_str', 'fund_purchase_fee_str', 'fund_redeem_fee_str', 'fund_min_purchase_amount_str', 'fund_max_purchase_amount_str']\n        elif fund_type == 'close':\n            columns = ['fund_id', 'fund_name', 'fund_name_abbr', 'fund_company', 'fund_type', 'fund_purchase_status', 'fund_risk_level', 'fund_purchase_start_date', 'fund_purchase_end_date', '", "CodeBleu_score": 25.726496287917634}, {"code": "def _parse_fund_data(url, fund_type='open')\n    import requests\n    import pandas as pd\n    import json\n    import datetime\n    import numpy as np\n\n    # Print the URL being requested\n    print(f\"Requesting URL: {url}\")\n\n    # Attempt to request the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Read the response\n    text = response.text\n\n    # Handle text encoding\n    text = text.encode('utf8').decode('ascii', 'ignore')\n\n    # Extract relevant data\n    data = text.split(\"jsonpgz\")[1].split(\";var pg_infos=\")[0]\n\n    # Format the text into valid JSON\n    json_data = json.loads(data)\n\n    # Create a pandas DataFrame with specific columns based on the fund type\n    if fund_type == 'open':\n        columns = ['code', 'name', 'unit', 'net_worth', 'accumulated_net_worth', 'last_week_growth_rate', 'last_month_growth_rate', 'last_three_months_growth_rate', 'last_six_months_growth_rate', 'last_year_growth_rate', 'this_year_growth_rate', 'since_establishment_growth_rate', 'establishment_date']\n        df = pd.DataFrame(columns=columns)\n    else:\n        columns = ['code', 'name', 'unit', 'net_worth', 'accumulated_net_worth', 'last_week_growth_rate', 'last_month_growth_rate', 'last_three_months_growth_rate', 'last_six_months_growth_rate', 'last_year_growth_rate', 'this_year_growth_rate', 'since_establishment_growth_rate', 'establishment_date', 'last_week_growth_rate_before_ten_thousand', 'last_month_growth_rate_before_", "CodeBleu_score": 32.88596532873723}, {"code": "def _parse_fund_data(url, fund_type='open')\n    import requests\n    import json\n    import pandas as pd\n    import numpy as np\n\n    # Attempt to request the URL and handle any errors that occur\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Read the response, handle text encoding, and extract relevant data\n    response_text = response.content.decode('utf-8')\n    response_text = response_text.replace('\\t', '')\n    response_text = response_text.replace('\\n', '')\n    response_text = response_text.replace('\\r', '')\n    response_text = response_text.replace(' ', '')\n    response_text = response_text.replace('\\u3000', '')\n    response_text = response_text.replace('\\xa0', '')\n    response_text = response_text.replace('\\u200b', '')\n    response_text = response_text.replace('\\u200d', '')\n    response_text = response_text.replace('\\u200c', '')\n    response_text = response_text.replace('\\u200e', '')\n    response_text = response_text.replace('\\u200f', '')\n    response_text = response_text.replace('\\ufeff', '')\n    response_text = response_text.replace('\\u202a', '')\n    response_text = response_text.replace('\\u202c', '')\n    response_text = response_text.replace('\\u202d', '')\n    response_text = response_text.replace('\\u202e', '')\n    response_text = response_text.replace('\\u2060', '')\n    response_text = response_text.replace('\\u2061', '')\n    response_text = response_text.replace('\\u2062', '')\n    response_text = response_text.replace('", "CodeBleu_score": 24.404016066182194}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if market == 'CFFEX':\n        func = get_daily_cffex\n    elif market == 'CZCE':\n        func = get_daily_czce\n    elif market == 'DCE':\n        func = get_daily_dce\n    elif market == 'INE':\n        func = get_daily_ine\n    elif market == 'SHFE':\n        func = get_daily_shfe\n    else:\n        raise ValueError('Invalid market')\n\n    if start is None:\n        start = datetime.today().strftime('%Y-%m-%d')\n    else:\n        start = pd.to_datetime(start).strftime('%Y-%m-%d')\n\n    if end is None:\n        end = datetime.today().strftime('%Y-%m-%d')\n    else:\n        end = pd.to_datetime(end).strftime('%Y-%m-%d')\n\n    date_list = pd.date_range(start=start, end=end, freq='D')\n    df_list = []\n\n    for date in date_list:\n        date_str = date.strftime('%Y-%m-%d')\n        df = func(date_str)\n        if not df.empty:\n            df_list.append(df)\n\n    if df_list:\n        return pd.concat(df_list, ignore_index=True)\n    else:\n        return pd.DataFrame()\n", "CodeBleu_score": 46.12610054558738}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if market == 'CFFEX':\n        func = get_cffex_daily\n    elif market == 'CZCE':\n        func = get_czce_daily\n    elif market == 'DCE':\n        func = get_dce_daily\n    elif market == 'INE':\n        func = get_ine_daily\n    elif market == 'SHFE':\n        func = get_shfe_daily\n    else:\n        raise ValueError(\"Invalid market: {}\".format(market))\n\n    # Convert start and end dates to datetime objects\n    if start is None:\n        start = datetime.datetime.today()\n    else:\n        start = datetime.datetime.strptime(start, \"%Y-%m-%d\")\n\n    if end is None:\n        end = datetime.datetime.today()\n    else:\n        end = datetime.datetime.strptime(end, \"%Y-%m-%d\")\n\n    # Initialize an empty list to hold data frames\n    dfs = []\n\n    # Iterate over each date in the range\n    for date in pd.date_range(start, end):\n        # Retrieve daily data using the appropriate function\n        df = func(date)\n\n        # Append data to list if data is found\n        if not df.empty:\n            dfs.append(df)\n\n    # Concatenate data frames if any data was collected\n    if dfs:\n        return pd.concat(dfs)\n    else:\n        return pd.DataFrame()\n", "CodeBleu_score": 47.35348752918625}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if market == 'CFFEX':\n        get_daily_func = get_daily_cffex\n    elif market == 'SHFE':\n        get_daily_func = get_daily_shfe\n    elif market == 'CZCE':\n        get_daily_func = get_daily_czce\n    elif market == 'DCE':\n        get_daily_func = get_daily_dce\n    elif market == 'INE':\n        get_daily_func = get_daily_ine\n    else:\n        raise ValueError(f\"Unsupported market: {market}\")\n\n    if start is None:\n        start = datetime.datetime.now().strftime('%Y-%m-%d')\n    else:\n        start = pd.to_datetime(start).strftime('%Y-%m-%d')\n\n    if end is None:\n        end = datetime.datetime.now().strftime('%Y-%m-%d')\n    else:\n        end = pd.to_datetime(end).strftime('%Y-%m-%d')\n\n    date_range = pd.date_range(start=start, end=end, freq='D')\n\n    data = []\n    for date in date_range:\n        date_str = date.strftime('%Y%m%d')\n        daily_data = get_daily_func(date_str)\n        if daily_data is not None:\n            data.append(daily_data)\n\n    if len(data) > 0:\n        return pd.concat(data)\n    else:\n        return None\n", "CodeBleu_score": 44.675642263303224}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    for _ in range(retry_count):\n        try:\n            response = requests.get(url)\n            data = response.json()\n            if data:\n                df = pd.DataFrame(data['data2'])\n                df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n                df['timestamp'] = pd.Timestamp.now()\n                return df\n        except:\n            time.sleep(pause)\n    return None\n", "CodeBleu_score": 30.619939021824816}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import pandas as pd\n    import time\n    import json\n    import datetime\n    import numpy as np\n    import os\n    import sys\n    import logging\n    import traceback\n    from pandas import json_normalize\n\n    url = 'https://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json?key=430156241533f1d058c603178cc3ca0e&targetDt=20230119'\n\n    for i in range(retry_count):\n        try:\n            response = requests.get(url)\n            data = json.loads(response.text)\n            df = pd.DataFrame(data['boxOfficeResult']['dailyBoxOfficeList'])\n            df.drop(['movieImg', 'movieId'], axis=1, inplace=True)\n            df['timestamp'] = datetime.datetime.now()\n            return df\n        except Exception as e:\n            logging.error(f'Error fetching real-time box office data: {e}')\n            time.sleep(pause)\n\n    logging.error(f'Failed to fetch real-time box office data after {retry_count} attempts')\n    return None\n\n", "CodeBleu_score": 30.56598618354568}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import pandas as pd\n    import time\n    url = 'https://www.endata.com.cn/API/GetData.ashx'\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    payload = {'showDate': '', 'tdate': '2023-03-10', 'MethodName': 'BoxOffice_GetHourBoxOffice', 'xnm': '', 'xqm': '', 'xdate': '', 'location': ''}\n    for i in range(retry_count):\n        try:\n            response = requests.post(url, headers=headers, data=payload)\n            response.raise_for_status()\n            data = response.json()\n            df = pd.DataFrame(data['Data']['Table'])\n            df = df.drop(['MovieImg', 'mId'], axis=1)\n            df['timestamp'] = pd.Timestamp.now()\n            return df\n        except (requests.exceptions.RequestException, ValueError):\n            time.sleep(pause)\n    return None\n\n", "CodeBleu_score": 32.129655488121124}]}
