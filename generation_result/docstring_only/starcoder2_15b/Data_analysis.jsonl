{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 68.12968840831522}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, stratify=label, test_size=1 - support_ratio, random_state=42\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 71.05148810096175}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 68.12968840831522}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    # Load the data\n    X, y = make_regression(\n        n_samples=100, n_features=3, n_informative=2, random_state=42\n    )\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    # Return the data\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 37.66756062311237}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 43.66228026188036}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(\n        n_samples=1000, n_features=3, random_state=42, noise=10, bias=10\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 37.5494762072823}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_new, Y_new = load_svmlight_file(data_path)\n        X_new = X_new.toarray()\n        if X_new.shape[1]!= X.shape[1]:\n            raise ValueError(\n                \"Number of features in data files do not match.\"\n            )\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 37.84293384996638}, {"code": "def datafiles_fusion(data_paths):\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_new, Y_new = load_svmlight_file(data_path)\n        X_new = X_new.toarray()\n        assert X_new.shape[1] == X.shape[1]\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 36.87459643915229}, {"code": "def datafiles_fusion(data_paths):\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_new, Y_new = load_svmlight_file(data_path)\n        X_new = X_new.toarray()\n        if X.shape[1]!= X_new.shape[1]:\n            raise ValueError(\n                f\"The number of features in {data_path} does not match the first file.\"\n            )\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 36.971280513795385}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 40.38465881740944}, {"code": "def eval_learning(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 40.38465881740944}, {"code": "def eval_learning(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 40.38465881740944}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for batch_idx, (data, labels) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            if batch_idx % 100 == 99:\n                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}\")\n                running_loss = 0.0\n\n", "CodeBleu_score": 50.71974329276707}, {"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 100 == 0:\n                print(\n                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                        epoch,\n                        batch_idx * len(data),\n                        len(train_loader.dataset),\n                        100.0 * batch_idx / len(train_loader),\n                        loss.item(),\n                    )\n                )\n\n", "CodeBleu_score": 48.85926536652542}, {"code": "def train(model, train_loader, num_epochs=1)\n    model.train()\n    loss_fn = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 10 == 0:\n                print(\n                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                        epoch,\n                        batch_idx * len(data),\n                        len(train_loader.dataset),\n                        100.0 * batch_idx / len(train_loader),\n                        loss.item(),\n                    )\n                )\n\n", "CodeBleu_score": 44.62967962764096}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.type == \"LIBSVM\"\n    data_path = os.path.join(cfg.data_dir, cfg.name)\n    data, labels = load_svmlight_file(data_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.int64)\n    print(\"Unique labels:\", np.unique(labels, return_counts=True))\n    print(\"Data shape:\", data.shape)\n    if cfg.name == \"mushrooms\":\n        labels[labels!= 1] = 0\n    else:\n        raise RuntimeError(\"Only mushrooms dataset is supported.\")\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 26.684393858969855}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.type == \"LIBSVM\"\n    path = cfg.path\n    name = cfg.name\n    data, labels = load_svmlight_file(path, dtype=np.float32)\n    data = data.toarray().astype(np.float32)\n    print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape: {data.shape}\")\n    if name == \"MUSHROOMS\":\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(f\"Dataset {name} not supported\")\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 23.683166021171218}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.type == \"LIBSVM\"\n    dataset_path = Path(cfg.path)\n    dataset_name = cfg.name\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.int32)\n    if dataset_name == \"mushrooms\":\n        labels[labels == 1] = 0\n        labels[labels!= 0] = 1\n    else:\n        raise RuntimeError(f\"Unknown dataset name: {dataset_name}\")\n    print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 30.14026282877282}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res\n    else:\n        return res * weight\n\n", "CodeBleu_score": 76.33849214626092}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 91.48358956788488}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if selected == \"all\":\n        return transform(X)\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, pd.Series):\n        X = X.values.reshape(-1, 1)\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\n            \"X must be a 2D numpy array, a pandas DataFrame, or a pandas Series.\"\n        )\n    if copy:\n        X = X.copy()\n    if not isinstance(transform, FunctionTransformer):\n        transform = FunctionTransformer(transform)\n    if not isinstance(selected, list):\n        selected = [selected]\n    if isinstance(X, sparse.csc_matrix):\n        selected_features = [\n            i for i, column in enumerate(X.indices) if column in selected\n        ]\n        non_selected_features = [\n            i for i, column in enumerate(X.indices) if column not in selected\n        ]\n        X_selected = transform.fit_transform(X[:, selected_features].toarray())\n        X_non_selected = X[:, non_selected_features]\n        X = sparse.hstack((X_selected, X_non_selected))\n    else:\n        selected_features = [i for i in range(X.shape[1]) if i in selected]\n        non_selected_features = [i for i in range(X.shape[1]) if i not in selected]\n        X_selected = transform.fit_transform(X[:, selected_features])\n        X_non_selected = X[:, non_selected_features]\n        X = np.hstack((X_selected, X_non_selected))\n    return X\n\n", "CodeBleu_score": 38.93590956103331}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if not isinstance(X, (np.ndarray, sp.sparse.spmatrix)):\n        raise ValueError(\n            \"X must be a numpy array or a sparse matrix. Got %s instead.\"\n            % type(X)\n        )\n    if not isinstance(transform, TransformerMixin):\n        raise ValueError(\n            \"transform must be a TransformerMixin. Got %s instead.\" % type(transform)\n        )\n    if selected == \"all\":\n        return transform.fit_transform(X)\n    elif selected == \"none\":\n        return X\n    else:\n        if copy:\n            X = X.copy()\n        if not isinstance(selected, list):\n            raise ValueError(\n                \"selected must be a list. Got %s instead.\" % type(selected)\n            )\n        if not all(isinstance(s, int) for s in selected):\n            raise ValueError(\n                \"selected must be a list of integers. Got %s instead.\"\n                % type(selected[0])\n            )\n        if not all(s >= 0 for s in selected):\n            raise ValueError(\"selected must be a list of positive integers.\")\n        if not all(s < X.shape[1] for s in selected):\n            raise ValueError(\n                \"selected must be a list of integers in the range [0, %d).\"\n                % X.shape[1]\n            )\n        selected = np.array(selected)\n        if sp.issparse(X):\n            if not isinstance(X, sp.csc_matrix):\n                X = X.tocsc()\n            selected_features = X[:, selected].toarray()\n            non_selected_features = X[:, np.setdiff1d(np.arange(X.shape[1]), selected)]\n            selected_features = transform.fit_transform(selected_features)\n            return sp.hstack(\n                [selected_features, non_selected_features], format=\"csc\"\n            ).tocsr()\n        else:\n            selected_features = X[:, selected]\n            non_selected_features = X[:, np.setdiff1d(np.arange(X.shape[1]), selected)]\n            selected_features = transform.fit_transform(selected_features)\n            return np.hstack([selected_features, non_selected_features])\n\n", "CodeBleu_score": 33.010043801985155}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if transform is None:\n        return X\n    if selected == \"all\":\n        return transform.transform(X)\n    elif len(selected) == 0:\n        return X\n    else:\n        X_selected = transform.transform(X[:, selected])\n        X_not_selected = X[:, ~np.in1d(np.arange(X.shape[1]), selected)]\n        return sp.hstack((X_selected, X_not_selected), format=\"csc\") if copy else sp.hstack((X_selected, X_not_selected))\n\n", "CodeBleu_score": 31.05438279475954}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n    X = check_array(X, dtype=None)\n    n_samples, n_features = X.shape\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        valid_mask = np.in1d(X[:, i], self.categories_[i])\n        if not np.all(valid_mask):\n            if self.handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                X[:, i][~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n    if self.encoding == 'ordinal':\n        return X_int.astype(self.dtype, copy=False)\n\n    mask = X_mask.ravel()\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    indices = np.cumsum(n_values)\n\n    column_indices = (X_int + indices[:-1]).ravel()[mask]\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                            n_features)[mask]\n    data = np.ones(n_samples * n_features)[mask]\n\n    out = sparse.csr_matrix((data, (row_indices, column_indices)),\n                            shape=(n_samples, indices[-1]),\n                            dtype=self.dtype)\n    if self.encoding == 'onehot-dense':\n        return out.toarray()\n    else:\n        return out\n", "CodeBleu_score": 53.21380355555942}, {"code": "def _transform_new(self, X)\n    if not hasattr(self, 'categories_'):\n        raise ValueError(\"This label encoder has not been fitted yet\")\n    X = check_array(X, accept_sparse='csc', dtype=self.dtype_,\n                    force_all_finite=False)\n    n_samples, n_features = X.shape\n    n_categories = np.array([len(cats) for cats in self.categories_])\n    if n_features!= len(self.categories_):\n        raise ValueError(\"Unexpected input dimension %d, expected %d\"\n                         % (n_features, len(self.categories_)))\n\n    # use only data points that are non-missing to compute the categories\n    X_mask = _get_mask(X)\n    X_int, _ = _encode_data(X, n_samples, n_features, n_categories,\n                            self.categories_, X_mask)\n\n    # split X_int into a sparse matrix and a dense matrix\n    X_int_sparse = X_int[~X_mask]\n    X_int_dense = X_int[X_mask]\n\n    # create the output array\n    if self.sparse:\n        # create the sparse matrix\n        X_trans = sparse.csr_matrix((np.ones(n_samples), X_int_sparse,\n                                     np.arange(n_samples + 1)),\n                                    shape=(n_samples, np.sum(n_categories)))\n    else:\n        # create the dense matrix\n        X_trans = np.zeros((n_samples, np.sum(n_categories)), dtype=np.float64)\n        X_trans[X_mask, X_int_dense] = 1\n\n    return X_trans\n\n", "CodeBleu_score": 35.77232520363961}, {"code": "def _transform_new(self, X)\n    X = check_array(X, accept_sparse='csc', dtype=None,\n                    force_all_finite=False, copy=True)\n    n_samples, n_features = X.shape\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n        if not np.all(valid_mask):\n            if self.handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                X[:, i][~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n    if self.encoding == 'ordinal':\n        return X_int.astype(self.dtype, copy=False)\n\n    mask = X_mask.ravel()\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    indices = np.cumsum(n_values)\n\n    column_indices = (X_int + indices[:-1]).ravel()[mask]\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                            n_features)[mask]\n    data = np.ones(n_samples * n_features)[mask]\n\n    out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                            shape=(n_samples, indices[-1]),\n                            dtype=self.dtype).tocsr()\n    if self.encoding == 'onehot-dense':\n        return out.toarray()\n    else:\n        return out\n", "CodeBleu_score": 51.16872531407082}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n    if self.log:\n        return LogUniform(self.lower, self.upper)\n    else:\n        return Uniform(self.lower, self.upper)\n", "CodeBleu_score": 21.638237566872114}, {"code": "def convert_to_sklearn(self)\n    if self.log:\n        return scipy.stats.loguniform(self.lower_bound, self.upper_bound)\n    else:\n        return scipy.stats.uniform(self.lower_bound, self.upper_bound - self.lower_bound)\n\n", "CodeBleu_score": 27.281404581541636}, {"code": "def convert_to_sklearn(self)\n    if self.log:\n      return LogUniform(self.lower, self.upper)\n    else:\n      return Uniform(self.lower, self.upper)\n", "CodeBleu_score": 20.984479092162893}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = state.args.get(\"additional_args\", {})\n    figure_args = state.args.get(\"figure_args\", {})\n    features = state.args.get(\"features\", [])\n    if state.args.get(\"headers\", []):\n        st.header(\"Partial Dependence Plots\")\n    fig, axes = plt.subplots(\n        nrows=len(features), ncols=len(features), **figure_args\n    )\n    kwargs = {**additional_args, **{\"line_kw\": {\"color\": \"red\"}}}\n    if state.args.get(\"two_way_interaction\", False):\n        data = state.data.dropna()\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        ax=axes,\n        **kwargs,\n    )\n    fig.tight_layout()\n    st.pyplot(fig)\n\n", "CodeBleu_score": 43.09241136665579}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = state.additional_arguments\n    figure_args = state.figure_arguments\n    features = state.features\n\n    if state.headers:\n        self._render_header(state.headers)\n\n    fig, axes = plt.subplots(**figure_args)\n\n    kwargs = {**additional_args, **{\"kind\": \"pdp\", \"ax\": axes}}\n\n    if state.two_way_interaction_plots:\n        data = state.data[state.data[state.feature].notnull()]\n    else:\n        data = state.data\n\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        **kwargs,\n    )\n\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 42.143063624222215}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = state.additional_args\n    figure_args = state.figure_args\n    features = state.features\n    if state.headers:\n        state.headers.render()\n    fig, ax = plt.subplots(**figure_args)\n    kwargs = {**additional_args, **{\"ax\": ax}}\n    if state.two_way:\n        data = state.data[state.data[state.target].notnull()]\n        PartialDependenceDisplay.from_estimator(\n            state.model,\n            data[features],\n            features,\n            **kwargs,\n        )\n    else:\n        PartialDependenceDisplay.from_estimator(\n            state.model,\n            state.data[features],\n            features,\n            **kwargs,\n        )\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 40.45579726731718}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if not self.feature_generator:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    X_ = X.copy()\n    if self.feature_generator.features:\n        X_ = self.feature_generator.transform(X_)\n    X_ = X_.fillna(0).values.astype(np.float32)\n    return X_\n\n", "CodeBleu_score": 46.819435962058556}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if not self.feature_generator:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    X = X.copy()\n    X[self.feature_generator.features] = self.feature_generator.transform(X)\n    X.fillna(0, inplace=True)\n    return X.values.astype(np.float32)\n\n", "CodeBleu_score": 41.806972036338955}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if not self.feature_generator:\n      self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n      self.feature_generator.fit(X)\n    X = X.copy()\n    for feature in self.feature_generator.features:\n      X[feature] = self.feature_generator.transform(X[feature])\n    X = X.fillna(0).to_numpy().astype(np.float32)\n    return X\n", "CodeBleu_score": 50.22996062485349}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n    if self._final_estimator == \"auto\":\n        n_estimators = self._get_n_estimators(X, **kwargs)\n    else:\n        n_estimators = self._final_estimator.get_params()[\"n_estimators\"]\n    if n_estimators < 40 or self._search_space is not None:\n        n_estimators = 40\n    n_trees_per_estimator = self._get_n_trees_per_estimator(X, **kwargs)\n    bytes_per_estimator = X.shape[0] * X.shape[1] * 8 * n_trees_per_estimator / n_estimators\n    return bytes_per_estimator * n_estimators\n", "CodeBleu_score": 37.869459589235205}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    # This function estimates the minimum memory usage required for a model based on the input data X.\n    # It retrieves model parameters and determines the final number of estimators.\n    # A minimum of 40 estimators is set if the final number is less than 40 or if a search space is defined.\n    # It calculates the number of trees per estimator and estimates the bytes used per estimator based on the size of X, with an adjustment factor.\n    # The expected minimum memory usage is calculated by multiplying the bytes per estimator by the minimum number of estimators.\n    # The function returns the estimated minimum memory usage.\n    # Get model parameters\n    params = self._get_params()\n    # Set minimum number of estimators to 40 if final number is less than 40 or if a search space is defined\n    if params[\"n_estimators\"] < 40 or params[\"search_space\"] is not None:\n        params[\"n_estimators\"] = 40\n    # Calculate the number of trees per estimator\n    trees_per_estimator = params[\"n_estimators\"] // params[\"n_estimators_per_model\"]\n    # Estimate the bytes used per estimator based on the size of X, with an adjustment factor\n    bytes_per_estimator = (\n        X.nbytes * 0.15 / params[\"n_estimators_per_model\"]\n    )  # Adjustment factor: 0.15\n    # Calculate the expected minimum memory usage\n    expected_memory_usage = bytes_per_estimator * params[\"n_estimators\"]\n    # Return the estimated minimum memory usage\n    return expected_memory_usage\n\n", "CodeBleu_score": 22.3923933980268}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    model_params = self._get_model_params()\n    if self.n_estimators is None:\n        self.n_estimators = model_params[\"n_estimators\"]\n    if self.n_estimators < 40 or self.search_space is not None:\n        self.n_estimators = max(self.n_estimators, 40)\n    trees_per_estimator = model_params[\"n_estimators\"] // self.n_estimators\n    bytes_per_estimator = (\n        X.shape[0] * X.shape[1] * trees_per_estimator * 4 * 2 * 1.1\n    )  # 4 bytes per float, 2 for min/max, 1.1 for adjustment factor\n    return bytes_per_estimator * self.n_estimators\n\n", "CodeBleu_score": 36.263803429546485}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    # Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = counter.most_common(vocab_size)\n    # Sort the names of the top n n-grams\n    sorted_ngrams = sorted([ngram for ngram, count in top_ngrams])\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: index for index, ngram in enumerate(sorted_ngrams)}\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n", "CodeBleu_score": 36.325503428745954}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_n_ngrams = ngram_counter.most_common(vocab_size)\n    # Sort the names of these top n n-grams\n    sorted_top_n_ngrams = sorted(top_n_ngrams, key=lambda x: x[0])\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: idx for idx, (ngram, _) in enumerate(sorted_top_n_ngrams)}\n    # Update the vectorizer's vocabulary with this new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n\n", "CodeBleu_score": 32.05144382693626}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = [x[0] for x in ngram_counter.most_common(vocab_size)]\n    top_ngrams.sort()\n    new_vocab = {ngram: i for i, ngram in enumerate(top_ngrams)}\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n", "CodeBleu_score": 33.014074409499}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n    # Initialize a dictionary to store feature types\n    feature_types = {'continuous': [],'skewed': []}\n\n    # Get the skewness threshold from the parameters\n    skewness_threshold = self.parameters['skewness_threshold']\n\n    # Loop through each feature in the DataFrame\n    for feature in features:\n        # Calculate the skewness of the feature\n        skewness = df[feature].skew()\n\n        # Classify the feature based on its skewness\n        if abs(skewness) > skewness_threshold:\n            feature_types['skewed'].append(feature)\n        else:\n            feature_types['continuous'].append(feature)\n\n    return feature_types\n\n  def _select_discrete(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either discrete or continuous based on their number of unique values.\\nIt initializes a dictionary to store these feature types and retrieves the number of unique values threshold from the parameters.\\nFor each feature, it calculates the number of unique values and compares it to the threshold.\\nFeatures with a number of unique values exceeding the threshold are classified as \"discrete\", while others are classified as \"continuous\".\\nThe function returns a dictionary with lists of discrete and continuous features.\"\"\"\n    # Initialize a dictionary to store feature types\n    feature_types = {'discrete': [], 'continuous': []}\n\n    # Get the number of unique values threshold from the parameters\n    num_unique_threshold = self.parameters['num_unique_threshold']\n\n    # Loop through each feature in the DataFrame\n    for feature in features:\n        # Calculate the number of unique values of the feature\n        num_unique = df[feature].nunique()\n\n        # Classify the feature based on the number of unique values\n        if num_unique > num_unique_threshold:\n            feature_types['discrete'].append(feature)\n        else:\n            feature_types['continuous'].append(feature)\n\n    return feature_types\n\n  def _select_ordinal(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either ordinal or non-ordinal based on their data type.\\nIt initializes a dictionary to store these feature types and retrieves the ordinal data types from the parameters.\\nFor each feature, it checks if its data type is in the list of ordinal data types.\\nFeatures with ordinal data types are classified as \"ordinal\", while others are classified as \"non-ordinal\".\\n", "CodeBleu_score": 38.21683426997703}, {"code": " def _select_continuous(self, df, features)\n    feature_types = {'continuous': [],'skewed': []}\n    skewness_threshold = self.params['skewness_threshold']\n    for feature in features:\n        skewness = df[feature].skew()\n        if skewness > skewness_threshold:\n            feature_types['skewed'].append(feature)\n        else:\n            feature_types['continuous'].append(feature)\n    return feature_types\n\n  def _select_categorical(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either categorical or numerical based on their data type.\\nIt initializes a dictionary to store these feature types and iterates through the features.\\nFor each feature, it checks its data type and adds it to the appropriate list in the dictionary.\\nThe function returns a dictionary with lists of categorical and numerical features.\"\"\"\n    feature_types = {'categorical': [], 'numerical': []}\n    for feature in features:\n        if df[feature].dtype in ['int64', 'float64']:\n            feature_types['numerical'].append(feature)\n        else:\n            feature_types['categorical'].append(feature)\n    return feature_types\n\n  def _select_ordinal(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either ordinal or non-ordinal based on their cardinality.\\nIt initializes a dictionary to store these feature types and retrieves the cardinality threshold from the parameters.\\nFor each feature, it calculates the cardinality and compares it to the threshold.\\nFeatures with cardinality exceeding the threshold are classified as \"non-ordinal\", while others are classified as \"ordinal\".\\nThe function returns a dictionary with lists of ordinal and non-ordinal features.\"\"\"\n    feature_types = {'ordinal': [], 'non-ordinal': []}\n    cardinality_threshold = self.params['cardinality_threshold']\n    for feature in features:\n        cardinality = df[feature].nunique()\n        if cardinality > cardinality_threshold:\n            feature_types['non-ordinal'].append(feature)\n        else:\n            feature_types['ordinal'].append(feature)\n    return feature_types\n\n  def _select_binary(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either binary or non-binary based on their cardinality.\\nIt initializes a dictionary to store these feature types and retrieves the cardinality threshold from the parameters.\\nFor each feature, it calculates", "CodeBleu_score": 39.772689559774186}, {"code": " def _select_continuous(self, df, features)\n    # Initialize a dictionary to store feature types\n    feature_types = {'continuous': [],'skewed': []}\n    # Get the skewness threshold from the parameters\n    skewness_threshold = self.parameters.get('skewness_threshold', 1)\n    # Iterate over each feature in the DataFrame\n    for feature in features:\n        # Calculate the skewness of the feature\n        skewness = df[feature].skew()\n        # Classify the feature as continuous or skewed based on its skewness\n        if abs(skewness) > skewness_threshold:\n            feature_types['skewed'].append(feature)\n        else:\n            feature_types['continuous'].append(feature)\n    # Return the dictionary with feature types\n    return feature_types", "CodeBleu_score": 49.10957896954823}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n    if weights == None or weights == 'uniform':\n        return None\n    elif weights == 'distance':\n        # handle infinite distances by setting them to a very large value\n        dist[dist == np.inf] = dist.max()\n        # calculate the inverse of the distances\n        weights = 1 / dist\n    elif callable(weights):\n        weights = weights(dist)\n    else:\n        raise ValueError(\"weights should be 'uniform', 'distance', or a callable function\")\n    return weights\n", "CodeBleu_score": 42.36218019463123}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # weight points by the inverse of their distance\n        weights = 1.0 / dist\n        # set infinite weights to zero (rely on masked values)\n        weights[~np.isfinite(weights)] = 0.0\n        # normalize weights within each neighborhood\n        weights /= np.sum(weights, axis=1)[:, np.newaxis]\n        return weights\n    elif callable(weights):\n        # weight points by the return value of the function\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized; should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 55.1270064095024}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # handle infinite weights\n        if sp.issparse(dist):\n            dist = dist.tocoo()\n            row = dist.row\n            col = dist.col\n            data = dist.data\n            data = 1.0 / data\n            mask = np.isinf(data)\n            data[mask] = 0.0  # set infinite weights to zero\n            dist = sp.coo_matrix((data, (row, col)), shape=dist.shape)\n            dist.sum_duplicates()  # combine duplicate entries\n        else:\n            dist = np.asarray(dist)\n            dist[dist == 0.0] = np.inf\n            np.seterr(divide=\"ignore\", invalid=\"ignore\")  # ignore divide by zero\n            dist = 1.0 / dist\n        return dist\n    elif callable(weights):\n        # return the output of the callable function\n        try:\n            return weights(dist)\n        except TypeError:\n            raise ValueError(\"weights function is not valid\")\n    else:\n        raise ValueError(\"weights not recognized\")\n\n", "CodeBleu_score": 45.83355937713569}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n    # Leave-one-out prediction\n    X = self._fit_X\n    y = self._y\n    n_samples = X.shape[0]\n    # Get nearest neighbor indices and distances\n    neighbors = self.kneighbors(X, return_distance=True)\n    indices = neighbors[1]\n    distances = neighbors[0]\n    weights = self._get_weights(distances)\n    # Make predictions\n    y_pred = np.zeros(n_samples)\n    for i in range(n_samples):\n        # Get indices and weights of neighbors excluding the current sample\n        idx = indices[i, 1:]\n        w = weights[i, 1:]\n        # Predict based on weighted average of neighbors\n        if self.weights == \"uniform\":\n            y_pred[i] = np.mean(y[idx])\n        else:\n            y_pred[i] = np.sum(w * y[idx]) / np.sum(w)\n    # Reshape y_pred if original target variable was one-dimensional\n    if self._y_original_shape == (n_samples,):\n        y_pred = y_pred.reshape(-1, 1)\n    return y_pred\n\n", "CodeBleu_score": 29.085912323549096}, {"code": "def predict_loo(self)\n    X = self._fit_X\n    y = self._y\n    n_samples = X.shape[0]\n    if y.ndim == 1:\n        y = y.reshape((-1, 1))\n    y_pred = np.empty((n_samples, self.n_outputs_))\n    for i in range(n_samples):\n        X_train = np.delete(X, i, axis=0)\n        y_train = np.delete(y, i, axis=0)\n        self._fit_X = X_train\n        self._y = y_train\n        self.fit(X_train, y_train)\n        X_test = X[i, :].reshape((1, -1))\n        y_pred[i] = self.predict(X_test)[0]\n    self._fit_X = X\n    self._y = y\n    if y.shape[1] == 1:\n        y_pred = np.ravel(y_pred)\n    return y_pred\n\n", "CodeBleu_score": 32.49892686270671}, {"code": "def predict_loo(self)\n    X = self._fit_X\n    y = self._y\n    n_samples = X.shape[0]\n    predictions = np.zeros(n_samples, dtype=np.float64)\n    for i in range(n_samples):\n      distances, indices = self.kneighbors(X[i], n_neighbors=self.n_neighbors)\n      weights = self._get_weights(distances)\n      if self.weights == 'uniform':\n        weights = np.ones_like(weights)\n      if y.ndim == 1:\n        y = y.reshape(-1, 1)\n      predictions[i] = np.average(y[indices], weights=weights, axis=1)\n      if y.ndim == 1:\n        predictions = predictions.ravel()\n    return predictions\n", "CodeBleu_score": 35.90748804657958}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    transformed_data = []\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds=\"clip\")\n        ir.fit(quantile_list, input_data[i])\n        transformed_data.append(ir.transform(quantile_list))\n    return np.stack(transformed_data)\n\n", "CodeBleu_score": 41.049686051291694}, {"code": "def isotonic(input_data, quantile_list)\n    quantile_list = np.array(quantile_list).reshape(-1, 1)\n    batch_size = input_data.shape[0]\n    transformed_data = []\n    for i in range(batch_size):\n        ir = IsotonicRegression(increasing=True)\n        ir.fit(quantile_list, input_data[i])\n        transformed_data.append(ir.transform(quantile_list))\n    transformed_data = np.stack(transformed_data)\n    return transformed_data\n\n", "CodeBleu_score": 35.79690435974044}, {"code": "def isotonic(input_data, quantile_list)\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    transformed_data = []\n    for i in range(batch_size):\n        row = input_data[i, :]\n        transformer = IsotonicRegression(\n            y_min=0, y_max=1, increasing=\"auto\", out_of_bounds=\"clip\"\n        )\n        transformer.fit(quantile_list, row)\n        transformed_row = transformer.transform(quantile_list)\n        transformed_data.append(transformed_row)\n    transformed_data = np.stack(transformed_data)\n    return transformed_data\n\n", "CodeBleu_score": 36.19384383140994}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer.transform(text_data)\n\n", "CodeBleu_score": 34.67230403994112}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    matrix = vectorizer.transform(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, matrix\n\n", "CodeBleu_score": 41.78962233024335}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n    # Remove rows with missing entries\n    df = df.replace('--', np.nan)\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.drop(\n        [col for col in df.columns if df[col].nunique() <= 1 or \"id\" in col.lower()], axis=1)\n\n    # Drop shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.drop(\n        [col for col in df.columns if \"days_to_death\" in col.lower() or \"year_of_death\" in col.lower()], axis=1)\n\n    # Split the dataframe into training and testing sets\n    X = df.drop(\"vital_status\", axis=1)\n    y = df[\"vital_status\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 31.022850324213174}, {"code": "def preprocess(df, test_size, shuffle)\n    df = df.dropna(axis=0, how='all')\n    df = df.dropna(axis=1, how='all')\n    df = df.replace('--', np.nan)\n    df = df.dropna(axis=0, how='any')\n    df = df.dropna(axis=1, how='any')\n    df = df.drop(columns=[col for col in df.columns if df[col].nunique() <= 1])\n    df = df.drop(columns=[col for col in df.columns if 'id' in col])\n    df = df.drop(columns=['days_to_death', 'year_of_death'])\n    X = df.drop(columns=['vital_status'])\n    y = df['vital_status']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n    return X_train, X_test, y_train, y_test\n", "CodeBleu_score": 31.41001110319732}, {"code": "def preprocess(df, test_size, shuffle)\n    # Remove rows with missing entries\n    df = df.replace('--', np.nan)\n    df = df.dropna()\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.drop(\n        [col for col in df.columns if df[col].nunique() <= 1 or \"id\" in col.lower()],\n        axis=1,\n    )\n    # Drop shortcut columns\n    df = df.drop([\"days_to_death\", \"year_of_death\"], axis=1)\n    # Split into training and testing sets\n    X = df.drop(\"survival_status_after_5_years\", axis=1)\n    y = df[\"survival_status_after_5_years\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, shuffle=shuffle\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 30.103939561735434}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    X_train, X_test, y_train, y_test = load_dataset(dataset, sparse=sparse)\n    search_space = classifier.get_hyperparameter_search_space()\n    default_config = get_default_config(search_space)\n    clf = classifier(default_config, random_state=1)\n    for i in range(2, 17, 2):\n        clf.fit(X_train, y_train, n_iter=i, refit=True)\n        predictions = clf.predict(X_test)\n        yield predictions, y_test, clf\n\n", "CodeBleu_score": 22.514137377044488}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    # Load the training and testing data\n    X_train, X_test, y_train, y_test = load_dataset(dataset, sparse=sparse)\n\n    # Retrieve the classifier's hyperparameter search space and obtain the default configuration\n    search_space = classifier.get_hyperparameter_search_space()\n    default_config = get_default_configuration(search_space)\n\n    # Initialize the classifier with the default configuration and a random state, then fit it iteratively to the training data\n    classifier.initialize(default_config, random_state=42)\n    classifier.iterative_fit(X_train, y_train, refit=True, increment=2)\n\n    # Continue iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted\n    for increment in [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n        classifier.iterative_fit(X_train, y_train, refit=True, increment=increment)\n\n    # Make predictions on the test data and return the predictions, true test labels, and the fitted classifier\n    predictions = classifier.predict(X_test)\n    return predictions, y_test, classifier\n\n", "CodeBleu_score": 23.767317282258716}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    # Load the training and testing data\n    X_train, X_test, y_train, y_test = load_dataset(dataset, sparse=sparse)\n\n    # Retrieve the classifier's hyperparameter search space\n    search_space = classifier.get_hyperparameter_search_space()\n\n    # Obtain the default configuration\n    default_config = search_space.get_default_configuration()\n\n    # Initialize the classifier with the default configuration and a random state\n    classifier = classifier.get_initialized_model(default_config, random_state=42)\n\n    # Fit the classifier iteratively to the training data for an initial two iterations with refitting enabled\n    classifier.iterative_fit(X_train, y_train, refit=True, batch_size=32, n_iter=2)\n\n    # Continue iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted\n    for i in range(2, 10):\n        classifier.iterative_fit(X_train, y_train, refit=True, batch_size=32, n_iter=2 ** i)\n\n    # Make predictions on the test data\n    predictions = classifier.predict(X_test)\n\n    # Return the predictions, true test labels, and the fitted classifier\n    return predictions, y_test, classifier\n\n", "CodeBleu_score": 32.96615122420634}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n    # This function loads models based on the specified ensemble class and resampling strategy.\n    # If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    # If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n    # If the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\n    # For specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\n    # If no ensemble is loaded, it initializes empty lists for models and cross-validated models.\n    if self._ensemble_class is not None:\n        self._models = self._backend.load_ensemble(\n            self._ensemble_class,\n            self._seed,\n            self._ensemble_nbest,\n            self._ensemble_eval_metric,\n            self._ensemble_eval_metric_sign,\n            self._ensemble_eval_metric_higher_is_better,\n            self._ensemble_eval_metric_abs,\n            self._ensemble_eval_metric_tol,\n            self._ensemble_eval_metric_norm,\n            self._ensemble_eval_metric_sign_norm,\n            self._ensemble_eval_metric_higher_is_better_norm,\n            self._ensemble_eval_metric_abs_norm,\n            self._ensemble_eval_metric_tol_norm,\n            self._ensemble_eval_metric_norm_abs,\n            self._ensemble_eval_metric_tol_norm_abs,\n            self._ensemble_eval_metric_norm_sign,\n            self._ensemble_eval_metric_tol_norm_sign,\n            self._ensemble_eval_metric_norm_abs_sign,\n            self._ensemble_eval_metric_tol_norm_abs_sign,\n        )\n        if len(self._models) > 0:\n            self._models_cv = self._backend.load_cv_models(\n                self._models, self._ensemble_class, self._seed\n            )\n        else:\n            self._models_cv = None\n    elif (\n        self._output_models\n        and self._output_cv_models\n        and self._eval_metric is not None\n        and self._eval_metric_sign is not None\n        and self._eval_metric_higher_is_better is not None\n        and self._", "CodeBleu_score": 36.974719310027126}, {"code": "def _load_models(self)\n    if self.ensemble is not None:\n        self.ensemble = self.backend.load_ensemble(self.ensemble, self.seed)\n        if self.ensemble is not None:\n            self.models = self.ensemble.models\n            self.cv_models = self.ensemble.cv_models\n            return\n    if self.ensemble is None and self.evaluator.output_conditions and self.resampling_strategy in [\n        \"cv\",\n        \"cv_with_test\",\n        \"cv_with_validation\",\n    ]:\n        self.models = self.backend.load_model(self.evaluator.best_individual_model)\n        self.cv_models = None\n        return\n    if self.ensemble is None:\n        self.models = []\n        self.cv_models = None\n    else:\n        self.models = self.backend.load_models(self.ensemble.identifiers)\n        self.cv_models = self.backend.load_cv_models(self.ensemble.identifiers)\n\n", "CodeBleu_score": 39.53109269940226}, {"code": "def _load_models(self)\n    if self.ensemble:\n        self.models = self.backend.load_ensemble(\n            self.ensemble, self.seed, self.ensemble_class\n        )\n        self.models_cv = self.backend.load_ensemble_cv(\n            self.ensemble, self.seed, self.ensemble_class\n        )\n        self.model_ids = self.models.keys()\n    elif (\n        self.evaluator.output_conditions\n        and self.evaluator.output_conditions.get(\"models\")\n        and self.resampling_strategy\n        and self.resampling_strategy.get(\"strategy\")\n        and self.resampling_strategy[\"strategy\"]!= \"cv\"\n    ):\n        self.models = self.backend.load_best_individual_models(\n            self.evaluator.output_conditions[\"models\"]\n        )\n        self.models_cv = None\n        self.model_ids = self.models.keys()\n    else:\n        self.models = {}\n        self.models_cv = None\n        self.model_ids = []\n\n", "CodeBleu_score": 39.493092868838545}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y()\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X[X_train]\n    X_test = X[X_test]\n    y_train = y[y_train]\n    y_test = y[y_test]\n    dataset = task.get_dataset()\n    mapping = dataset.retrieve_class_mapping()\n    reverse_mapping = {v: k for k, v in mapping.items()}\n    y_train = [reverse_mapping[y_i] for y_i in y_train]\n    y_test = [reverse_mapping[y_i] for y_i in y_test]\n    return X_train, X_test, y_train, y_test, categorical_indicator\n\n", "CodeBleu_score": 27.8644855844509}, {"code": "def load_task(task_id)\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y()\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X[X_train]\n    X_test = X[X_test]\n    y_train = y[y_train]\n    y_test = y[y_test]\n    dataset = task.get_dataset()\n    class_labels = dataset.retrieve_class_labels()\n    y_train = np.searchsorted(class_labels, y_train)\n    y_test = np.searchsorted(class_labels, y_test)\n    return X_train, X_test, y_train, y_test, categorical_indicator\n\n", "CodeBleu_score": 17.551692363917397}, {"code": "def load_task(task_id)\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y()\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices()\n    X_train = X[X_train]\n    X_test = X[X_test]\n    y_train = y[y_train]\n    y_test = y[y_test]\n    dataset = task.get_dataset()\n    label_names = dataset.target_names\n    y_train_indices = [label_names.index(label) for label in y_train]\n    y_test_indices = [label_names.index(label) for label in y_test]\n    return X_train, X_test, y_train_indices, y_test_indices, categorical_indicator\n", "CodeBleu_score": 23.31914101994131}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if sparse.issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 52.26426506048918}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if isinstance(y, sp.sparse.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.9206743264562}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if isinstance(y, sp.sparse.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.9206743264562}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n    self.weights_ = np.zeros(len(self.models_))\n    for i in range(len(self.models_)):\n        self.weights_[i] = self.models_[i].count / len(self.models_)\n    if self.weights_.sum() < 1:\n        self.weights_ /= self.weights_.sum()\n    self.weights_ = self.weights_.reshape(-1, 1)\n\n", "CodeBleu_score": 23.84338912244162}, {"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.ensemble_indices)\n    # Initialize the weights array with zeros\n    weights = np.zeros(len(self.models))\n    # Calculate the weight for each ensemble member\n    for i, count in enumerate(counts):\n        weights[i] = count / len(self.ensemble_indices)\n    # Normalize the weights if the sum is less than 1\n    if weights.sum() < 1:\n        weights /= weights.sum()\n    # Store the weights in the instance variable\n    self.weights_ = weights\n", "CodeBleu_score": 26.619957115764542}, {"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.ensemble_indices_)\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    self.weights_ = np.zeros(len(counts))\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size\n    for i, count in enumerate(counts):\n        self.weights_[i] = count / len(self.ensemble_indices_)\n    # If the sum of weights is less than 1, normalize the weights to ensure their sum equals 1\n    if self.weights_.sum() < 1:\n        self.weights_ /= self.weights_.sum()\n\n", "CodeBleu_score": 23.816201904421746}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    check_is_fitted(self, 'classes_')\n    y_pred = self.predict(X, batch_size=batch_size, n_jobs=n_jobs)\n    y_pred_proba = self.base_estimator.predict_proba(X)\n    if self.target_type_ =='multiclass':\n        assert np.allclose(y_pred_proba.sum(axis=1), 1)\n    assert np.all((y_pred_proba >= 0) & (y_pred_proba <= 1))\n    return y_pred_proba\n\n", "CodeBleu_score": 36.22967267327882}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    check_is_fitted(self, 'classes_')\n    if self.target_type_ == \"multilabel-indicator\":\n        return super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    else:\n        probs = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n        assert np.allclose(np.sum(probs, axis=1), 1), \"Probabilities for each instance do not sum to 1\"\n        assert np.all((probs >= 0) & (probs <= 1)), \"Probabilities should be between 0 and 1\"\n        return probs\n", "CodeBleu_score": 48.202556005044904}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    # This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\n    # It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    # If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    # It also asserts that all probability values lie between 0 and 1.\n    # The function returns the predicted probabilities.\n    # YOUR CODE HERE\n    raise NotImplementedError()\n    return proba\n\n", "CodeBleu_score": 6.665337195860236}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, np.arange(Ybin.shape[1]))\n", "CodeBleu_score": 36.03567744702259}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    return Ybin.dot(np.arange(Ybin.shape[1]))\n", "CodeBleu_score": 16.13457153350703}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin.astype(int)\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1])).astype(int)\n", "CodeBleu_score": 34.41257260078718}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n    fit_params = fit_params or {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config)\n        self.config.classifier_weights = balancing.get_classifier_weights()\n        self.config.preprocessor_weights = balancing.get_preprocessor_weights()\n    self.init_params.update(self.config.to_dict())\n    self.set_hyperparams(self.config, self.init_params)\n    fit_params = {**fit_params, **self.fit_params}\n    return super().fit_transformer(X, y, fit_params)\n", "CodeBleu_score": 33.43203115970323}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    fit_params = fit_params or {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config, self.init_params)\n        weights = balancing.get_weights(self.config.classifier, self.config.feature_preprocessor)\n        self.init_params.update(weights)\n    self.config.update_hyperparams(self.init_params)\n    fit_params = {**fit_params, **self.config.fit_params}\n    return super().fit_transformer(X, y, fit_params)\n", "CodeBleu_score": 35.586693123787406}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    fit_params = fit_params if fit_params is not None else {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config.classifier, self.config.feature_preprocessor)\n        fit_params.update(balancing.get_weights(X, y))\n    self.init_params.update(self.config.to_dict())\n    self.set_hyperparams(self.config, self.init_params)\n    fit_params = {**fit_params, **self.fit_params}\n    return super().fit_transformer(X, y, fit_params=fit_params)", "CodeBleu_score": 38.031454327345024}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n    # Create a Spark session\n    spark = SparkSession.builder.appName(\"GLM_Identity_Link_Func_Test\").getOrCreate()\n\n    # Load the dataset\n    dataset = spark.read.csv(\"test_data/test_dataset.csv\", header=True, inferSchema=True)\n\n    # Define the features and label columns\n    features_col = \"features\"\n    label_col = \"label\"\n\n    # Assemble the features\n    assembler = VectorAssembler(inputCols=dataset.columns[:-1], outputCol=features_col)\n    dataset = assembler.transform(dataset)\n\n    # Split the dataset into training and testing sets\n    train_data, test_data = dataset.randomSplit([0.7, 0.3])\n\n    # Define the GLM model\n    glm_model = GLM(featuresCol=features_col, labelCol=label_col, family=\"tweedie\", link=\"power(1)\")\n\n    # Fit the GLM model\n    glm_model = glm_model.fit(train_data)\n\n    # Assemble the model into an expression\n    expression = assemble_glm_model(glm_model)\n\n    # Define the expected expression\n    expected_expression = \"features_0 * 0.2 + features_1 * 0.3 + features_2 * 0.4 + features_3 * 0.5 + 0.1\"\n\n    # Assert that the assembled expression matches the expected expression\n    assert expression == expected_expression\n\n    # Stop the Spark session\n    spark.stop()\n\n", "CodeBleu_score": 31.902479510635406}, {"code": "def test_glm_identity_link_func()\n    # Create a Spark session\n    spark = SparkSession.builder.appName(\"GLMIdentityLinkFuncTest\").getOrCreate()\n\n    # Create a DataFrame with a single column of integers\n    data = spark.createDataFrame([(1,), (2,), (3,)], [\"x\"])\n\n    # Create a GLM model with an identity link function\n    glm = GLM(family=\"tweedie\", link=\"power(1)\")\n\n    # Fit the model to the data\n    model = glm.fit(data)\n\n    # Convert the fitted model into an expression\n    expr = model.toExpr()\n\n    # Create an expected expression\n    expected_expr = \"x\"\n\n    # Compare the assembled expression to the expected expression\n    assert expr == expected_expr\n\n    # Stop the Spark session\n    spark.stop()\n\n", "CodeBleu_score": 20.857898102924167}, {"code": "def test_glm_identity_link_func()\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled", "CodeBleu_score": 0.33783783783783783}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        name, agg = agg\n    else:\n        name = agg.__name__ if callable(agg) else agg\n    return self.groupby(self.index).agg({col: agg}).rename(columns={col: f\"{col}_{name}\"})\n\n", "CodeBleu_score": 64.10038562010676}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        name, agg = agg\n    elif isinstance(agg, str):\n        name = agg\n    elif callable(agg):\n        name = agg.__name__\n    else:\n        raise ValueError(f\"Invalid aggregation type: {type(agg)}\")\n    return self.groupby(col).agg(name=agg)\n\n", "CodeBleu_score": 52.38492072919371}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name = agg[0]\n        agg_func = agg[1]\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n    return self.groupby(self.index).agg({col: agg_func}).rename(columns={col: f\"{col}_{agg_name}\"})\n\n", "CodeBleu_score": 56.858761815998534}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    if 'hue' in kws:\n        hue_level = kws['hue_level']\n        if r > 0:\n            ax.text(0.97, 0.9, f\"{r:.2f}\", transform=ax.transAxes,\n                    ha='right', va='top', color=f'C{hue_level}',\n                    bbox=dict(boxstyle='round', fc='white', ec='white', alpha=0.5))\n        else:\n            ax.text(0.03, 0.9, f\"{r:.2f}\", transform=ax.transAxes,\n                    ha='left', va='top', color=f'C{hue_level}',\n                    bbox=dict(boxstyle='round', fc='white', ec='white', alpha=0.5))\n    else:\n        if r > 0:\n            ax.text(0.97, 0.9, f\"{r:.2f}\", transform=ax.transAxes,\n                    ha='right', va='top', color='black',\n                    bbox=dict(boxstyle='round', fc='white', ec='white', alpha=0.5))\n        else:\n            ax.text(0.03, 0.9, f\"{r:.2f}\", transform=ax.transAxes,\n                    ha='left', va='top', color='black',\n                    bbox=dict(boxstyle='round', fc='white', ec='white', alpha=0.5))\n", "CodeBleu_score": 23.87562439214761}, {"code": "def _corrfunc(self, x, y, **kws)\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    if self.hue is None:\n        ax.annotate(\"{:.2f}\".format(r), xy=(0.5, 0.5), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=16)\n    else:\n        ax.annotate(\n            \"{:.2f}\".format(r),\n            xy=(0.5, 0.5),\n            xycoords=\"axes fraction\",\n            ha=\"center\",\n            va=\"center\",\n            fontsize=16 * (1 + abs(r)) / 2,\n        )\n\n", "CodeBleu_score": 14.953388377169603}, {"code": "def _corrfunc(self, x, y, **kws)\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1,.9), xycoords=ax.transAxes)\n", "CodeBleu_score": 10.900904044378585}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        return round(src, sig=rounddigit)\n    elif method == 'format':\n        return float(f'{{:.{rounddigit}g}}'.format(src))\n    else:\n        raise ValueError('Unknown rounding method')\n\n", "CodeBleu_score": 58.7269550881287}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        from decimal import Decimal\n        return Decimal(str(src)).quantize(Decimal(f'1e-{rounddigit}'))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError('Invalid rounding method')\n\n", "CodeBleu_score": 53.526245674569175}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        return round(src, rounddigit)\n    elif method == 'format':\n        return round(src, rounddigit)\n    else:\n        raise ValueError(f'Invalid method: {method}')\n", "CodeBleu_score": 56.30190158423907}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')    if rounddigit is None:\n        return srcdict\n    else:\n        return {k: _round_digits(v, rounddigit, method) for k, v in srcdict.items()}\n\n", "CodeBleu_score": 26.791330978290507}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')    if not rounddigit:\n        return srcdict\n    if method == 'decimal':\n        return {k: cls._round_digits(v, rounddigit) for k, v in srcdict.items()}\n    elif method =='sig':\n        return {k: cls._round_sig(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'format':\n        return {k: cls._round_format(v, rounddigit) for k, v in srcdict.items()}\n    else:\n        raise ValueError('Method must be one of \"decimal\", \"sig\", or \"format\".')\n\n", "CodeBleu_score": 28.507154363850347}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')    if rounddigit is None:\n        return srcdict\n    else:\n        return {k: cls._round_digits(v, rounddigit, method) for k, v in srcdict.items()}\n\n", "CodeBleu_score": 27.42214428350527}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    # Create a DataFrame from the x and y values\n    df = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue_data is provided, add a hue field to the DataFrame\n    if hue_data is not None:\n        df[\"hue\"] = hue_data\n\n    # Create the scatter plot\n    if hue_data is not None:\n        sns.scatterplot(\n            data=df,\n            x=x_name,\n            y=y_name,\n            hue=\"hue\",\n            ax=ax,\n            **scatter_kws,\n        )\n    else:\n        sns.scatterplot(\n            data=df,\n            x=x_name,\n            y=y_name,\n            ax=ax,\n            **scatter_kws,\n        )\n\n    # If no title is specified for the legend, set the title to the hue field name\n    if \"title\" not in legend_kws:\n        legend_kws[\"title\"] = hue_name\n\n    # Set the legend title and other legend properties\n    ax.legend(title=legend_kws[\"title\"], **legend_kws)\n\n", "CodeBleu_score": 40.06136163835707}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[\"hue\"] = hue_data\n        hue_name = hue_name or \"hue\"\n\n    sns.scatterplot(\n        data=df,\n        x=x_name,\n        y=y_name,\n        hue=\"hue\",\n        ax=ax,\n        **scatter_kws,\n    )\n\n    if hue_data is not None:\n        legend_title = legend_kws.get(\"title\", hue_name)\n        legend_kws[\"title\"] = legend_title\n        legend = ax.legend(**legend_kws)\n        legend.set_title(legend_title)\n    else:\n        ax.legend().remove()\n\n    return ax\n\n", "CodeBleu_score": 45.96651248327834}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df['hue'] = hue_data\n        hue_name = hue_name or 'hue'\n        legend_kws = legend_kws or {}\n        legend_kws.setdefault('title', hue_name)\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue='hue', ax=ax, **scatter_kws, **legend_kws)\n", "CodeBleu_score": 31.08771908106685}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n    # Get the main time series data\n    ts_data = self.get_ts_data()\n    # Check for any exogenous variables\n    exog = None\n    if self.exog is not None:\n        exog = self.get_exog_data()\n    # Fit the model\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model_class(\n            ts_data, exog=exog, **self.model_args, **fit_args\n        )\n        self.model.fit()\n    return self\n\n", "CodeBleu_score": 48.566690607215094}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    # Retrieve the main time series data\n    time_series_data = self.get_data()\n    # Check for any exogenous variables specified in the node\n    exogenous_data = self.get_exogenous_data()\n    # Suppress warnings related to user actions and convergence\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        # Fit the model using the endogenous and exogenous data, along with any additional fitting arguments provided\n        self.model = self.model_class(\n            endog=time_series_data, exog=exogenous_data, **fit_args\n        )\n        self.model_fit = self.model.fit()\n    return self\n\n", "CodeBleu_score": 49.38295535926355}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    data = self.get_data()\n    exogenous = None\n    if self.exogenous_variables is not None:\n        exogenous = data[self.exogenous_variables]\n    model = self.model_class(data[self.endogenous_variable], exogenous, **fit_args)\n    model.fit()\n    return model\n\n", "CodeBleu_score": 37.64323144152897}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)    if self.model_type == \"holt_winters\":\n        data = self.get_transformed_data()\n        model = ExponentialSmoothing(\n            data,\n            trend=kwargs.get(\"trend\", \"add\"),\n            seasonal=kwargs.get(\"seasonal\", \"add\"),\n            seasonal_periods=kwargs.get(\"seasonal_periods\", 12),\n        ).fit()\n        return model\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logging.error(\n                \"AutoARIMA not installed. Please install it using 'pip install pmdarima'.\"\n            )\n            sys.exit(1)\n        model = AutoARIMA(\n            start_p=kwargs.get(\"start_p\", 1),\n            start_q=kwargs.get(\"start_q\", 1),\n            max_p=kwargs.get(\"max_p\", 5),\n            max_q=kwargs.get(\"max_q\", 5),\n            start_P=kwargs.get(\"start_P\", 0),\n            start_Q=kwargs.get(\"start_Q\", 0),\n            max_P=kwargs.get(\"max_P\", 2),\n            max_Q=kwargs.get(\"max_Q\", 2),\n            seasonal=kwargs.get(\"seasonal\", True),\n            stepwise=kwargs.get(\"stepwise\", True),\n            suppress_warnings=kwargs.get(\"suppress_warnings\", True),\n            error_action=\"ignore\",\n        )\n        return model\n    elif self.model_type == \"sarimax\":\n        data = self.get_transformed_data()\n        exog = self.get_exog_data()\n        if exog is not None:\n            model = SARIMAX(\n                data,\n                exog=exog,\n                order=kwargs.get(\"order\", (1, 1, 1)),\n                seasonal_order=kwargs.get(\"seasonal_order\", (1, 1, 1, 12)),\n            ).fit()\n        else:\n            model = SARIMAX(\n                data,\n                order=kwargs.get(\"order\", (1, 1, 1)),\n                seasonal_order=kwargs.get(\"seasonal_order\", (1, 1, 1,", "CodeBleu_score": 38.90337052159016}, {"code": "def create_model(self, **kwargs)    model_type = kwargs.get('model_type', 'auto_arima')\n    if model_type == 'holt_winters':\n        transformed_data = self.get_transformed_data()\n        model = ExponentialSmoothing(transformed_data,\n                                     trend=kwargs.get('trend', 'add'),\n                                     seasonal=kwargs.get('seasonal', 'add'),\n                                     seasonal_periods=kwargs.get('seasonal_periods', 12)\n                                     ).fit()\n    elif model_type == 'auto_arima':\n        try:\n            from pmdarima.arima import AutoARIMA\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it using 'pip install pmdarima'.\")\n            exit(1)\n        model = AutoARIMA(**kwargs)\n    elif model_type =='sarimax':\n        transformed_data = self.get_transformed_data()\n        exog = kwargs.get('exog', None)\n        if exog is not None:\n            exog = self.get_transformed_data(exog)\n        model = SARIMAX(transformed_data, exog=exog, **kwargs).fit()\n    else:\n        raise Exception(f\"Model type {model_type} is not recognized.\")\n    return model\n\n", "CodeBleu_score": 43.81254183560014}, {"code": "def create_model(self, **kwargs)    if self.model_type == \"holt_winters\":\n      self.model = ExponentialSmoothing(self.transformed_data, trend=self.trend, seasonal=self.seasonal, seasonal_periods=self.seasonal_periods).fit()\n    elif self.model_type == \"auto_arima\":\n      try:\n        from pmdarima import AutoARIMA\n        self.model = AutoARIMA(self.transformed_data, **kwargs)\n      except:\n        print(\"pmdarima not installed. Please install it.\")\n        exit(0)\n    elif self.model_type == \"sarimax\":\n      if self.exogenous_data is None:\n        self.model = SARIMAX(self.transformed_data, order=self.order, seasonal_order=self.seasonal_order, enforce_stationarity=self.enforce_stationarity, enforce_invertibility=self.enforce_invertibility).fit()\n      else:\n        self.model = SARIMAX(self.transformed_data, exog=self.exogenous_data, order=self.order, seasonal_order=self.seasonal_order, enforce_stationarity=self.enforce_stationarity, enforce_invertibility=self.enforce_invertibility).fit()\n    else:\n      raise Exception(\"Model type not recognized. Please specify either 'holt_winters', 'auto_arima', or'sarimax'.\")\n    return self.model\n", "CodeBleu_score": 35.704851535542204}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 74.82032138356422}, {"code": "def _to_chx(array)\n    if isinstance(array, np.ndarray):\n        return _from_numpy(array)\n    if isinstance(array, chx.ndarray):\n        return array\n    if isinstance(array, cupy.ndarray) and cupy is not None:\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 67.98436164921557}, {"code": "def _to_chx(array)\n    if isinstance(array, np.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, chx.ndarray):\n        return array\n    elif isinstance(array, cp.ndarray) and cp is not None:\n        return _from_cupy(array)\n    else:\n        return array", "CodeBleu_score": 62.91424916395728}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if not all(len(y.shape) == len(x.shape) for y in ys):\n        # calculate expected shapes for the split segments\n        split_sizes = np.array_split(np.empty(x.shape), indices_or_sections, axis=axis).shape[1:]\n        # reshape segments to the expected shapes\n        ys = [np.reshape(y, (split_sizes[i],) + y.shape[1:]) for i, y in enumerate(ys)]\n    return ys\n", "CodeBleu_score": 36.09013188841194}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    # Check if all arrays in the input list ys have the same number of dimensions as the array x\n    if not all(y.ndim == x.ndim for y in ys):\n        # Calculate the expected shapes for the split segments using a temporary split on an empty array with the same shape as x along the specified axis\n        split_sizes = np.array_split(np.empty_like(x), indices_or_sections, axis=axis).shape[1:]\n        # For each segment, if the dimensions do not match, reshape the segment to the expected shape based on the calculated split sizes\n        for i, y in enumerate(ys):\n            if y.ndim!= x.ndim:\n                ys[i] = y.reshape(split_sizes[i])\n    return ys\n\n", "CodeBleu_score": 38.66754470617544}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    split_sizes = np.array(np.split(np.empty_like(x), indices_or_sections, axis=axis)).shape[1:]\n    for i, y in enumerate(ys):\n        if y.ndim!= x.ndim:\n            ys[i] = y.reshape(split_sizes[i])\n    return ys\n\n", "CodeBleu_score": 21.31184100206688}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    else:\n        xp = cuda.get_array_module(batch[0])\n        with cuda.get_device_from_id(device):\n            concat = xp.concatenate(batch, axis=0)\n            sections = np.cumsum([len(x) for x in batch[:-1]], dtype=np.int32)\n            return xp.split(concat, sections)\n\n", "CodeBleu_score": 60.598089990639345}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    else:\n        xp = backend.get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = np.cumsum([len(x) for x in batch[:-1]], dtype='i')\n        concat_dev = chainer.dataset.to_device(device, concat)\n        return xp.split(concat_dev, sections)\n", "CodeBleu_score": 64.38070454716474}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    else:\n        xp = cuda.get_array_module(batch[0])\n        with cuda.get_device_from_id(device):\n            concat = xp.concatenate(batch, axis=0)\n            sections = xp.cumsum([len(x) for x in batch[:-1]], dtype=xp.int32)\n            concat_dev = cuda.to_gpu(concat, device=device)\n            return cuda.cupy.split(concat_dev, sections)\n", "CodeBleu_score": 65.2988047291151}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n    # Generate images from the current state of the generator\n    # Set a random seed for reproducibility\n    torch.manual_seed(0)\n\n    # Determine the number of images to generate\n    n_images = 16\n\n    # Create a latent variable z\n    z = torch.randn(n_images, trainer.latent_dim, 1, 1, device=trainer.device)\n\n    # Generate images using the generator model with the training mode turned off\n    with torch.no_grad():\n        images = trainer.generator(z).cpu()\n\n    # Scale the images to a [0, 255] range\n    images = (images * 0.5 + 0.5) * 255\n\n    # Convert the images to uint8 type\n    images = images.numpy().astype(np.uint8)\n\n    # Reshape and transpose the images to form a grid\n    images = images.reshape(4, 4, 32, 32, 3)\n    images = images.transpose(0, 2, 1, 3, 4)\n    images = images.reshape(4 * 32, 4 * 32, 3)\n\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = \"preview\"\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n\n    # Save the generated image as a PNG file named according to the current training iteration\n    preview_path = os.path.join(preview_dir, f\"preview_{trainer.iteration}.png\")\n    imageio.imwrite(preview_path, images)\n\n", "CodeBleu_score": 38.15866093345548}, {"code": "def make_image(trainer)\n    # Set a random seed\n    random.seed(0)\n    # Determine the number of images to generate\n    n_images = 100\n    # Create a latent variable z\n    z = torch.randn(n_images, trainer.latent_dim, device=trainer.device)\n    # Generate images using a generator model with the training mode turned off\n    with torch.no_grad():\n        generated_images = trainer.generator(z).cpu()\n    # Scale to a [0, 255] range\n    generated_images = (generated_images * 127.5 + 127.5).clamp(0, 255).to(torch.uint8)\n    # Reshape and transpose to form a grid of images\n    generated_images = generated_images.reshape(n_images, 1, 28, 28)\n    generated_images = generated_images.transpose(1, 3)\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = \"preview\"\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    # Save the generated image as a PNG file named according to the current training iteration\n    preview_path = os.path.join(preview_dir, f\"preview_{trainer.iteration}.png\")\n    torchvision.utils.save_image(generated_images, preview_path, nrow=10)\n\n", "CodeBleu_score": 35.63725453391569}, {"code": "def make_image(trainer)\n    # set the seed for reproducibility\n    seed = torch.manual_seed(0)\n    # determine the number of images to generate\n    num_images = 100\n    # create a latent variable z\n    z = torch.randn(num_images, trainer.latent_dim, 1, 1, device=trainer.device)\n    # generate images using the generator model with the training mode turned off\n    with torch.no_grad():\n        generated_images = trainer.generator(z)\n    # move the generated images to the CPU, scale them to a [0, 255] range, and convert them to uint8 type\n    generated_images = generated_images.cpu().numpy()\n    generated_images = (generated_images * 127.5 + 127.5).astype(np.uint8)\n    # reshape and transpose the generated images to form a grid of images\n    generated_images = generated_images.reshape(num_images, 3, 32, 32)\n    generated_images = np.transpose(generated_images, (0, 2, 3, 1))\n    # create a directory for saving the preview image if it doesn't exist\n    preview_dir = \"preview\"\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    # save the generated image as a PNG file named according to the current training iteration\n    preview_path = os.path.join(preview_dir, f\"preview_{trainer.iteration}.png\")\n    imsave(preview_path, generated_images, format=\"png\")\n\n", "CodeBleu_score": 36.8998938627574}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"bBhHiIlLqQpPefdgFDGSUV\"\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 46.994889398127704}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"biufcmMOSUV\"\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 48.129113917279085}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"biufc\"\n    return _typechars.index(from_dtype.char) >= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 48.32904462601418}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    if g is None:\n        raise ValueError(\"Must pass in a plot object\")\n    if labels is None or colors is None:\n        raise ValueError(\"Must pass in labels and colors\")\n    if len(labels)!= len(colors):\n        raise ValueError(\"Number of labels must match number of colors\")\n    if type(g) == list:\n        g = g[0]\n    if type(g)!= mpl.axes.Axes:\n        raise ValueError(\"Must pass in a matplotlib Axes object\")\n    legend_handles = []\n    for label, color in zip(labels, colors):\n        legend_handles.append(patches.Patch(color=color, label=label))\n    return g.legend(handles=legend_handles, **legend_kwargs)\n\n", "CodeBleu_score": 41.17531094217168}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    if isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, None):\n        ax = plt.gca()\n    else:\n        ax = g\n    if len(labels)!= len(colors):\n        raise ValueError(\"Number of labels and colors must match\")\n    handles = [patches.Patch(color=c, label=l) for l, c in zip(labels, colors)]\n    return ax.legend(handles=handles, **legend_kwargs)\n\n", "CodeBleu_score": 51.594545252597854}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, Visualizer):\n        ax = g.ax\n    else:\n        ax = g\n    if len(labels)!= len(colors):\n        raise ValueError(\"labels and colors must be the same length\")\n    legend_handles = [patches.Patch(color=c, label=l) for l, c in zip(labels, colors)]\n    return ax.legend(handles=legend_handles, **legend_kwargs)\n", "CodeBleu_score": 51.703807811564076}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n    if fast:\n        speed = 1000\n    else:\n        speed = 10\n    fig, ax = plt.subplots()\n    visualizer = ParallelCoordinates(ax=ax, speed=speed)\n    start_time = time.time()\n    visualizer.fit_transform(X, y)\n    elapsed_time = time.time() - start_time\n    plt.close(fig)\n    return elapsed_time", "CodeBleu_score": 51.75679371537198}, {"code": "def pcoords_time(X, y, fast=True)\n    fig, ax = plt.subplots()\n    visualizer = ParallelCoordinates(classes=classes, fast=fast, ax=ax)\n    start = time.time()\n    visualizer.fit_transform(X, y)\n    end = time.time()\n    plt.close(fig)\n    return end - start\n\n", "CodeBleu_score": 29.085751546337846}, {"code": "def pcoords_time(X, y, fast=True)\n    # Create a plot and initialize the visualizer\n    _, ax = plt.subplots()\n    visualizer = ParallelCoordinates(classes=classes, ax=ax, fast=fast)\n    start = time.time()\n    visualizer.fit_transform(X, y)\n    end = time.time()\n    plt.close()\n    return end - start\n\n", "CodeBleu_score": 28.335752405599823}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n    # Retrieve the alpha transparency value from the object's attributes or default to 0.25\n    alpha = kwargs.get(\"alpha\", self.alpha)\n    # For each instance, extract the feature vector and corresponding label, determine the color based on the label, and plot the feature vector against a predefined increment sequence on the axis\n    for i, (x, y) in enumerate(zip(X, y)):\n        color = self.colors[y]\n        plt.plot(x[0], x[1], marker=\"o\", color=color, alpha=alpha)\n    # Return the axis object with the plotted instances\n    return plt.gca()\n", "CodeBleu_score": 29.197694449730637}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    for x, y in zip(X, y):\n        color = self.colors[y]\n        plt.scatter(self.x_increment, x, color=color, alpha=alpha)\n    return plt.gca()\n", "CodeBleu_score": 27.79864133211527}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = kwargs.pop('alpha', self.alpha)\n    for label, marker, color in zip(self.labels, self.markers, self.colors):\n        # plot each class on its own to get different colors\n        # and set alpha (transparency) so we can see instances of\n        # both classes\n        ax.scatter(x=X[:, 0][y == label],\n                   y=X[:, 1][y == label],\n                   color=color,\n                   marker=marker,\n                   label=label,\n                   alpha=alpha)\n    return ax\n", "CodeBleu_score": 26.502999935827944}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\\n\\n    alpha = kwargs.get('alpha', self.alpha)\\n    ax = kwargs.get('ax', None)\\n\\n    # Add a column of ones to separate between different classes.\\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\\n    increments = np.hstack((0, np.cumsum(np.unique(y, return_counts=True)[1])))\\n    increments = np.hstack((increments, [None]))\\n\\n    # Plot each class.\\n    classes = np.unique(y)\\n    for c in classes:\\n        i = np.where(y == c)[0]\\n        color = self.colors[int(c)]\\n        ax.plot(X[i, 0], X[i, 1], color=color, alpha=alpha, label=f'Class {c}')\\n\\n    return ax\\n\\n\\ndef draw_decision_regions(self, X, y, **kwargs)::\\n    \"\"\"# This function draws decision regions for the provided data X and labels y on a given axis.\\nIt sets the alpha transparency value from the object's attributes or defaults to 0.5.\\nIt adds a column of ones to X to separate the instances visually and appends None to the increments list to create breaks between instances.\\nThe function identifies unique class labels in y and iterates over them.\\nFor each class, it determines the color, selects the instances of X belonging to that class, and plots them as a single line plot with the specified color and alpha transparency.\\nThe function returns the axis object with the plotted classes.\"\"\"\\n\\n    alpha = kwargs.get('alpha', self.alpha)\\n    ax = kwargs.get('ax', None)\\n\\n    # Add a column of ones to separate between different classes.\\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\\n    increments = np.hstack((0, np.cumsum(np.unique(y, return_counts=True)[1])))\\n    increments = np.hstack((increments, [None]))\\n\\n    # Plot each class.\\n    classes = np.unique(y)\\", "CodeBleu_score": 10.181645337346797}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n    increments = kwargs.pop(\"increments\", None)\n    if increments is None:\n        increments = np.ones((X.shape[0], 1))\n    X = np.hstack((X, increments))\n    classes = np.unique(y)\n    for c in classes:\n        color = self.colors[c]\n        mask = y == c\n        plt.plot(X[mask, 0], X[mask, 1], \"o\", color=color, alpha=alpha)\n    return self", "CodeBleu_score": 32.70384995444711}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n    increments = kwargs.pop(\"increments\", [0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1.0])\n    X = np.hstack([X, np.ones((X.shape[0], 1))])\n    unique_classes = np.unique(y)\n    for i, c in enumerate(unique_classes):\n        color = self.colors[i % len(self.colors)]\n        X_c = X[y == c]\n        self.ax.plot(X_c[:, 0], X_c[:, 1], color=color, alpha=alpha)\n        increments.append(None)\n    return self.ax", "CodeBleu_score": 41.82149637132947}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\n    # Create a side-by-side axes grid with a specified figure size\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 8))\n\n    # Perform RadViz analysis using occupancy data\n    X = occupancy_data.drop(\"Occupancy\", axis=1)\n    y = occupancy_data[\"Occupancy\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    X_train_norm = (X_train - X_train.mean()) / X_train.std()\n    X_test_norm = (X_test - X_train.mean()) / X_train.std()\n    viz1 = RadViz(classes=occupancy_data[\"Occupancy\"].unique().tolist())\n    viz1.fit(X_train_norm, y_train)\n    viz1.transform(X_test_norm)\n    viz1.finalize()\n\n    # Perform Rank2D analysis using concrete data\n    X = concrete_data.drop(\"Concrete compressive strength(MPa, megapascals) \", axis=1)\n    y = concrete_data[\"Concrete compressive strength(MPa, megapascals) \"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    viz2 = Rank2D(features=X.columns.tolist(), algorithm=\"pearson\")\n    viz2.fit(X_train, y_train)\n    viz2.transform(X_test)\n    viz2.finalize()\n\n    # Save the resulting figure to a specified file path\n    plt.savefig(fname)\n    plt.close()\n\n", "CodeBleu_score": 30.936335061205554}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    # Set up a side-by-side axes grid with a specified figure size\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n    # Perform RadViz analysis using occupancy data\n    X = occupancy_data.drop(\"Occupancy\", axis=1)\n    y = occupancy_data[\"Occupancy\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    radviz_visualizer = RadViz(classes=[\"Occupied\", \"Not Occupied\"], ax=ax1)\n    radviz_visualizer.fit(X_train, y_train)\n    radviz_visualizer.transform(X_test)\n    radviz_visualizer.finalize()\n\n    # Perform Rank2D analysis using concrete data\n    X = concrete_data.drop(\"Compressive Strength (28-day)(MPa)\", axis=1)\n    y = concrete_data[\"Compressive Strength (28-day)(MPa)\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    rank2d_visualizer = Rank2D(algorithm=\"pearson\", ax=ax2)\n    rank2d_visualizer.fit(X_train, y_train)\n    rank2d_visualizer.transform(X_test)\n    rank2d_visualizer.finalize()\n\n    # Save the resulting figure to a specified file path\n    plt.savefig(fname)\n\n", "CodeBleu_score": 21.548342523240844}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    # set up a side-by-side axes grid with a specified figure size\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    # perform RadViz analysis using occupancy data\n    radviz_visualizer = RadViz(classes=classes, features=features, ax=ax1)\n    radviz_visualizer.fit(X_occupancy, y_occupancy)\n    radviz_visualizer.transform(X_occupancy)\n    radviz_visualizer.finalize()\n    # perform Rank2D analysis using concrete data\n    rank2d_visualizer = Rank2D(features=features, ax=ax2)\n    rank2d_visualizer.fit(X_concrete, y_concrete)\n    rank2d_visualizer.transform(X_concrete)\n    rank2d_visualizer.finalize()\n    # save the resulting figure to a specified file path\n    plt.savefig(fname)\n\n", "CodeBleu_score": 18.831941144051793}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\n    # Define the range of alpha values for regularization\n    alphas = np.logspace(-4, -0.5, 30)\n\n    # Load the concrete dataset\n    data = pd.read_excel(\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\"\n    )\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(\"Concrete compressive strength(MPa, megapascals) \", axis=1),\n        data[\"Concrete compressive strength(MPa, megapascals) \"],\n        test_size=0.2,\n        random_state=42,\n    )\n\n    # Create a figure with two subplots of size 18x6 inches\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=alphas).fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, lasso.predict(X_test))\n    ax1.plot(lasso.alphas_, lasso.mse_path_.mean(axis=1), label=\"LassoCV\")\n    ax1.axhline(mae, linestyle=\"--\", color=\"r\", label=\"Mean Absolute Error\")\n    ax1.set_title(\"Prediction Error\")\n    ax1.set_xlabel(\"alpha\")\n    ax1.set_ylabel(\"Mean Squared Error\")\n    ax1.legend()\n\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=alphas).fit(X_train, y_train)\n    ax2.scatter(ridge.predict(X_test), ridge.predict(X_test) - y_test, label=\"RidgeCV\")\n    ax2.set_title(\"Residuals\")\n    ax2.set_xlabel(\"Predicted Values\")\n    ax2.set_ylabel(\"Residuals\")\n    ax2.legend()\n\n    # Adjust the layout\n    plt.tight_layout()\n\n    #", "CodeBleu_score": 35.45112159623656}, {"code": "def regression(fname=\"regression.png\")\n    # Define the range of alpha values for regularization\n    alphas = np.logspace(-4, 0, 50)\n\n    # Load the concrete dataset\n    X, y = load_concrete()\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Create a figure with two subplots of size 18x6 inches\n    fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=alphas, random_state=42).fit(X_train, y_train)\n    mae = mean_absolute_error(y_test, lasso.predict(X_test))\n    axs[0].plot(lasso.alphas_, lasso.mse_path_.mean(axis=-1))\n    axs[0].plot(lasso.alpha_, mae, \"ro\", label=\"LassoCV\")\n    axs[0].set_xscale(\"log\")\n    axs[0].set_xlabel(\"alpha\")\n    axs[0].set_ylabel(\"Mean squared error\")\n    axs[0].legend()\n    axs[0].set_title(\"Prediction error\")\n\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=alphas).fit(X_train, y_train)\n    axs[1].scatter(ridge.predict(X_test), ridge.predict(X_test) - y_test)\n    axs[1].hlines(y=0, xmin=0, xmax=80, color=\"r\", lw=2)\n    axs[1].set_xlabel(\"Predicted values\")\n    axs[1].set_ylabel(\"Residuals\")\n    axs[1].set_title(\"Residual plot\")\n\n    # Adjust the layout\n    plt.tight_layout()\n\n    # Save the figure to a specified path\n    plt.savefig(fname)\n\n    # Return the path\n    return fname\n\n", "CodeBleu_score": 37.35557332321741}, {"code": "def regression(fname=\"regression.png\")\n    # Generate and save regression model evaluation figures\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n    # Load the concrete dataset\n    data = pd.read_csv(\n        \"https://raw.githubusercontent.com/PacktPublishing/hands-on-time-series-analysis-with-python/master/Chapter%201/concrete.csv\"\n    )\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.iloc[:, :-1], data.iloc[:, -1], test_size=0.3, random_state=42\n    )\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=alphas, random_state=42).fit(X_train, y_train)\n    mse = mean_squared_error(y_test, lasso.predict(X_test))\n    print(\"LassoCV best alpha: {:.2f}\".format(lasso.alpha_))\n    print(\"LassoCV test set MSE: {:.2f}\".format(mse))\n    plot_prediction_error(lasso, ax=ax1)\n    # Plot the residuals using RidgeCV with the same alphas\n    ridge = RidgeCV(alphas=alphas).fit(X_train, y_train)\n    mse = mean_squared_error(y_test, ridge.predict(X_test))\n    print(\"RidgeCV best alpha: {:.2f}\".format(ridge.alpha_))\n    print(\"RidgeCV test set MSE: {:.2f}\".format(mse))\n    plot_prediction_error(ridge, ax=ax2)\n    # Adjust the layout and save the figure to a specified path\n    plt.tight_layout()\n    plt.savefig(fname)\n    return fname\n\n", "CodeBleu_score": 38.41887444144021}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n    # Create a figure and a set of subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    # Load the spam dataset and split it into training and testing sets\n    spam_data = pd.read_csv(\"data/spam.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        spam_data[\"text\"], spam_data[\"target\"], random_state=0\n    )\n    # Fit a MultinomialNB classifier to the training data and generate a ClassificationReport\n    vect = CountVectorizer().fit(X_train)\n    X_train_vectorized = vect.transform(X_train)\n    clf = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)\n    y_pred = clf.predict(vect.transform(X_test))\n    plot_classification_report(y_test, y_pred, ax=ax1)\n    # Load the spam dataset and fit a LogisticRegression model\n    X, y = spam_data[\"text\"], spam_data[\"target\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    vect = CountVectorizer().fit(X_train)\n    X_train_vectorized = vect.transform(X_train)\n    clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n    # Generate a DiscriminationThreshold plot\n    y_pred_prob = clf.predict_proba(vect.transform(X_test))[:, 1]\n    plot_discrimination_threshold(y_test, y_pred_prob, ax=ax2)\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 35.551394918398564}, {"code": "def classification(fname=\"classification.png\")\n    # Load the spam dataset\n    df = pd.read_csv(\"https://raw.githubusercontent.com/PacktPublishing/hands-on-natural-language-processing-with-python/master/datasets/spam.csv\")\n\n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(df[\"Message\"], df[\"Category\"], test_size=0.2, random_state=42)\n\n    # Create a pipeline with a TF-IDF vectorizer and a MultinomialNB classifier\n    text_clf = Pipeline([\n        (\"tfidf\", TfidfVectorizer()),\n        (\"clf\", MultinomialNB())\n    ])\n\n    # Fit the classifier to the training data\n    text_clf.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = text_clf.predict(X_test)\n\n    # Create a ClassificationReport visualization\n    viz1 = ClassificationReport(text_clf, classes=[\"ham\", \"spam\"], support=True)\n    viz1.fit(X_train, y_train)\n    viz1.score(X_test, y_test)\n    viz1.finalize()\n\n    # Load the entire dataset\n    X, y = df[\"Message\"], df[\"Category\"]\n\n    # Create a LogisticRegression model\n    model = LogisticRegression()\n\n    # Create a DiscriminationThreshold visualization\n    viz2 = DiscriminationThreshold(model)\n    viz2.fit(X, y)\n    viz2.finalize()\n\n    # Create a grid of two axes with a specified figure size\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\n    # Display the ClassificationReport on the left axis\n    axes[0].set_title(\"Classification Report\")\n    viz1.ax = axes[0]\n    viz1.poof()\n\n    # Display the DiscriminationThreshold on the right axis\n    axes[1].set_title(\"Discrimination Threshold\")\n    viz2.ax = axes[1]\n    viz2.poof()\n\n    # Adjust the layout and save the figure to a specified file path\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.close()\n", "CodeBleu_score": 38.049698691261945}, {"code": "def classification(fname=\"classification.png\")\n    # Load the dataset and split it into training and testing sets\n    spam = pd.read_csv(\"spam.csv\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        spam.iloc[:, :-1], spam.iloc[:, -1], test_size=0.33, random_state=42\n    )\n    # Set the figure size\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    # Plot the classification report on the left axis\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True, ax=ax1)\n    ax1.set_title(\"Classification Report\")\n    # Plot the discrimination threshold on the right axis\n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    optimal_idx = np.argmax(tpr - fpr)\n    optimal_threshold = thresholds[optimal_idx]\n    sns.lineplot(x=fpr, y=tpr, ax=ax2)\n    ax2.plot(\n        [0, 1], [0, 1], linestyle=\"--\", color=\"gray\", linewidth=1, label=\"Random guess\"\n    )\n    ax2.scatter(\n        fpr[optimal_idx],\n        tpr[optimal_idx],\n        marker=\"o\",\n        color=\"red\",\n        label=\"Optimal threshold\",\n    )\n    ax2.set_xlabel(\"False Positive Rate\")\n    ax2.set_ylabel(\"True Positive Rate\")\n    ax2.set_title(\"Discrimination Threshold\")\n    ax2.legend()\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 33.6519424249522}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n    try:\n        from matplotlib import __version__ as mplver\n        if mplver >= '1.5.0':\n            from matplotlib import rcParams\n            prop_cycle = rcParams['axes.prop_cycle']\n            return [c['color'] for c in prop_cycle]\n        else:\n            from matplotlib import rcParams\n            color_cycle = rcParams['axes.color_cycle']\n            return color_cycle\n    except KeyError:\n        from matplotlib import rcParams\n        color_cycle = rcParams['axes.color_cycle']\n        return color_cycle\n\n", "CodeBleu_score": 34.22521490188884}, {"code": "def get_color_cycle()\n    try:\n        # Attempt to get the color cycle from the \"axes.prop_cycle\" parameter\n        return list(mpl.rcParams[\"axes.prop_cycle\"])\n    except KeyError:\n        # Fall back to the \"axes.color_cycle\" parameter\n        return list(mpl.rcParams[\"axes.color_cycle\"])\n\n", "CodeBleu_score": 21.915703659041046}, {"code": "def get_color_cycle()\n    try:\n        return plt.rcParams['axes.prop_cycle'].by_key()['color']\n    except KeyError:\n        return plt.rcParams['axes.color_cycle']\n\n", "CodeBleu_score": 21.48439262237854}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x_norm, self.y_norm)\n    plt.plot(self.x_dist, self.y_dist, \"r\")\n    plt.xticks(np.arange(min(self.x_norm), max(self.x_norm), 0.1))\n    plt.yticks(np.arange(min(self.y_norm), max(self.y_norm), 0.1))\n    plt.axvline(self.knee, color=\"k\")\n    plt.show()\n\n", "CodeBleu_score": 45.37373161246124}, {"code": "def plot_knee_normalized(\n        self,\n    )\n    # This function plots a normalized curve along with a distance curve and the knee point if it exists.\n    # It imports the matplotlib.pyplot module for plotting.\n    import matplotlib.pyplot as plt\n\n    # The plot is created with a figure size of 8x8 inches.\n    plt.figure(figsize=(8, 8))\n\n    # It plots the normalized x and y values, followed by the distance curve in red.\n    plt.plot(self.x_norm, self.y_norm)\n    plt.plot(self.x_norm, self.dist, color=\"red\")\n\n    # The x and y ticks are set at intervals of 0.1 based on the minimum and maximum values of the respective data.\n    plt.xticks(np.arange(min(self.x_norm), max(self.x_norm) + 0.1, 0.1))\n    plt.yticks(np.arange(min(self.y_norm), max(self.y_norm) + 0.1, 0.1))\n\n    # A vertical line is drawn at the knee point to highlight it on the plot.\n    plt.axvline(self.knee, color=\"black\")\n\n    # The plot is shown.\n    plt.show()\n\n", "CodeBleu_score": 43.918320733446166}, {"code": "def plot_knee_normalized(\n        self,\n    )\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.plot(self.x, self.distance, \"r\")\n    plt.xticks(np.arange(min(self.x), max(self.x) + 0.1, 0.1))\n    plt.yticks(np.arange(min(self.y), max(self.y) + 0.1, 0.1))\n    plt.vlines(self.knee, 0, 1, \"b\")\n    plt.show()\n\n", "CodeBleu_score": 45.012678517552615}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n    # Plot curve\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    # Draw vertical line at knee point\n    plt.axvline(self.knee, color=\"red\")\n    plt.show()\n\n", "CodeBleu_score": 41.24110539351812}, {"code": "def plot_knee(\n        self,\n    )\n    fig = plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.plot(self.x[self.knee_index], self.y[self.knee_index], \"ro\")\n    plt.vlines(self.x[self.knee_index], ymin=0, ymax=self.y[self.knee_index])\n    plt.show()\n\n", "CodeBleu_score": 48.841360214287846}, {"code": "def plot_knee(\n        self,\n    )\n    # This function plots a curve and its knee point, if it exists.\n    # It uses matplotlib to create a plot with a specified figure size of 8x8 inches.\n    # The function plots the data points defined by self.x and self.y.\n    # It then draws a vertical line at the knee point along the y-axis limits.\n    # The plot is displayed with the curve and the knee point highlighted.\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.axvline(x=self.knee, ymin=0, ymax=1, color=\"red\")\n    plt.show()\n\n", "CodeBleu_score": 48.11540351849678}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    data_pca = pca.transform(data)\n\n    def show_decomp(index):\n        plt.figure(figsize=(10, 5))\n        plt.plot(data.iloc[index], label=\"Original Data\")\n        for i in range(n_components):\n            plt.plot(data_pca[index, i] * pca.components_[i], label=f\"PC {i+1}\")\n        plt.legend()\n        plt.show()\n\n    interact(show_decomp, index=(0, len(data) - 1))\n\n", "CodeBleu_score": 56.73009103455093}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    data_pca = pca.transform(data)\n\n    def show_decomp(index):\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(data[index])\n        plt.title(\"Original Data\")\n        plt.subplot(1, 2, 2)\n        plt.plot(data_pca[index])\n        plt.title(\"PCA Components\")\n        plt.show()\n\n    interact(show_decomp, index=(0, len(data) - 1))", "CodeBleu_score": 56.186724154286225}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n    \n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    pca_data = pca.transform(data)\n    \n    def show_decomp(index=0):\n        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n        ax[0].plot(data[index])\n        ax[0].set_title('Original Data')\n        ax[1].plot(pca_data[index])\n        ax[1].set_title('PCA Components')\n        plt.show()\n    \n    interact(show_decomp, index=(0, len(data)-1))\n", "CodeBleu_score": 58.55075540015753}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    # Train the kNN classifier on the Iris dataset\n    knn.fit(X, y)\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the decision boundaries as a color plot\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    plt.xlabel(\"Sepal length (cm)\")\n    plt.ylabel(\"Sepal width (cm)\")\n    plt.title(\"k-Nearest Neighbors (kNN) on the Iris dataset\")\n    plt.show()", "CodeBleu_score": 57.970785628973644}, {"code": "def plot_iris_knn()\n    # Load the Iris dataset\n    iris = load_iris()\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n    # Train a k-Nearest Neighbors (kNN) classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the decision boundaries\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('k-Nearest Neighbors (kNN) on the Iris dataset')\n    plt.show()\n", "CodeBleu_score": 57.52028016209211}, {"code": "def plot_iris_knn()\n    # Load the Iris dataset\n    iris = load_iris()\n    X = iris.data[:, :2]  # first two features\n    y = iris.target\n\n    # Train a k-Nearest Neighbors classifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundaries\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    plt.xlabel(\"Sepal length\")\n    plt.ylabel(\"Sepal width\")\n    plt.title(\"k-Nearest Neighbors on the Iris dataset\")\n    plt.show()\n\n", "CodeBleu_score": 56.84096237511484}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n    # We create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # Fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01,\n                        max_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.plot()\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.RdYlBu,\n                edgecolor='black', s=150)\n    plt.show()\n\n", "CodeBleu_score": 39.41628561781498}, {"code": "def plot_sgd_separator()\n    # generate 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01,\n                        max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # create a mesh to plot in\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n\n    # plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n                linestyles=['--', '-', '--'])\n\n    # plot support vectors\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n                linewidth=1, facecolors='none', edgecolors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n\n    plt.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 38.31077165084927}, {"code": "def plot_sgd_separator()\n    # Generate 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n    # Fit the SGD classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # Plot the decision boundary\n    xx, yy = np.meshgrid(np.linspace(-1, 5, 50), np.linspace(-1, 5, 50))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.title(\"Decision boundary of a linear SGD classifier\")\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n    plt.contour(xx, yy, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"])\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n    plt.axis(\"tight\")\n    plt.show()\n\n", "CodeBleu_score": 31.117975789856057}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n    X, y = self.data[:, :-1], self.data[:, -1]\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 32.17044400405285}, {"code": "def dump_svmlight_file(self, file)\n    X = self.data[:, :2]\n    y = self.data[:, 2]\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 40.86883949371065}, {"code": "def dump_svmlight_file(self, file)\n    X = self.data[:, :-1]\n    y = self.data[:, -1]\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 38.35901271101158}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n    print(\"Fitting model...\")\n    X, y = self.data.get_data()\n    self.model = SVC(C=self.complexity,\n                     gamma=self.gamma,\n                     coef0=self.coef0,\n                     degree=self.degree,\n                     kernel=self.kernel)\n    if len(np.unique(y)) == 1:\n        self.model = OneClassSVM(C=self.complexity,\n                                 gamma=self.gamma,\n                                 coef0=self.coef0,\n                                 degree=self.degree,\n                                 kernel=self.kernel)\n    self.model.fit(X, y)\n    if hasattr(self.model, \"score\"):\n        print(\"Accuracy: \", self.model.score(X, y))\n    self.surface = self.model.decision_function(self.data.get_surface())\n    self.surface_type = \"model\"\n    self.fitted = True\n    self.surface_changed.emit()\n\n", "CodeBleu_score": 35.047823822166805}, {"code": "def fit(self)\n    print(\"Fitting SVM model...\")\n    X = self.data.X\n    y = self.data.y\n    self.complexity = float(self.complexity)\n    self.gamma = float(self.gamma)\n    self.coef0 = float(self.coef0)\n    self.degree = int(self.degree)\n    self.kernel = str(self.kernel)\n    if np.all(y == 1):\n      self.model = OneClassSVM(nu=self.complexity, gamma=self.gamma, coef0=self.coef0, degree=self.degree, kernel=self.kernel)\n    else:\n      self.model = SVC(C=self.complexity, gamma=self.gamma, coef0=self.coef0, degree=self.degree, kernel=self.kernel)\n    self.model.fit(X, y)\n    if hasattr(self.model, \"score\"):\n      print(\"Accuracy: \", self.model.score(X, y))\n    self.surface = self.model.decision_function(X)\n    self.surface_type = \"SVM\"\n    self.fitted = True\n    self.surface_changed.emit()\n", "CodeBleu_score": 33.40275143873557}, {"code": "def fit(self)\n    print(\"Fitting the model...\")\n    X, y = self.data.X, self.data.y\n    clf = SVC(kernel=self.kernel, C=self.complexity, gamma=self.gamma,\n              coef0=self.coef0, degree=self.degree)\n    if np.unique(y).size == 1:\n        clf = OneClassSVM(kernel=self.kernel, nu=self.complexity,\n                          gamma=self.gamma, coef0=self.coef0,\n                          degree=self.degree)\n    clf.fit(X)\n    if hasattr(clf, \"score\"):\n        print(\"Accuracy: %f\" % clf.score(X, y))\n    self.surface = clf.decision_function(X)\n    self.surface_type = \"decision_function\"\n    self.fitted = True\n    self.trigger_event(\"surface_changed\")", "CodeBleu_score": 29.461426514593846}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n    if verbose > 2:\n        import sys\n        sys.stderr.write(\"source_decode: sourcecode = %s\\n\" % sourcecode)\n    if sourcecode.startswith(\"tpot.\"):\n        module, obj = sourcecode.split(\".\", 1)\n    else:\n        module, obj = sourcecode, None\n    if verbose > 2:\n        sys.stderr.write(\"source_decode: module = %s\\n\" % module)\n        sys.stderr.write(\"source_decode: obj = %s\\n\" % obj)\n    try:\n        imported = __import__(module, fromlist=[obj])\n        if obj is not None:\n            imported = getattr(imported, obj)\n    except ImportError as e:\n        if verbose > 2:\n            raise\n        else:\n            print(\"Warning: %s\" % e)\n            imported = None\n    if verbose > 2:\n        sys.stderr.write(\"source_decode: imported = %s\\n\" % imported)\n    return module, obj, imported\n\n", "CodeBleu_score": 40.22574278706867}, {"code": "def source_decode(sourcecode, verbose=0)\n    if sourcecode.startswith(\"tpot.\"):\n        import_str, obj_str = sourcecode.split(\".\", 1)\n        try:\n            obj = getattr(import_module(import_str), obj_str)\n            return import_str, obj_str, obj\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(f\"Warning: {e}\")\n    else:\n        import_str, obj_str = sourcecode.rsplit(\".\", 1)\n        try:\n            obj = getattr(import_module(import_str), obj_str)\n            return import_str, obj_str, obj\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(f\"Warning: {e}\")\n    return None, None, None\n\n", "CodeBleu_score": 38.58899186452773}, {"code": "def source_decode(sourcecode, verbose=0)\n    if sourcecode.startswith(\"tpot.\"):\n        module_name = sourcecode.split(\".\")[0]\n        object_name = sourcecode.split(\".\")[1]\n    else:\n        module_name = sourcecode\n        object_name = None\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Could not import module {module_name}: {e}\")\n        return module_name, object_name, None\n    if object_name is not None:\n        try:\n            obj = getattr(module, object_name)\n        except AttributeError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(f\"Warning: Could not import object {object_name} from module {module_name}: {e}\")\n            return module_name, object_name, None\n    else:\n        obj = module\n    return module_name, object_name, obj\n\n", "CodeBleu_score": 41.24884778698085}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step[1], \"fit\"):\n                if \"sample_weight\" in inspect.signature(step[1].fit).parameters:\n                    sample_weight_dict[f\"{step[0]}_sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 47.38760284835779}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step_name, step_instance in pipeline_steps:\n            if hasattr(step_instance, \"fit\"):\n                if \"sample_weight\" in inspect.signature(step_instance.fit).parameters:\n                    sample_weight_dict[f\"{step_name}__sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    return None\n\n", "CodeBleu_score": 46.11371667933529}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weights = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            fit_method = getattr(step[1], \"fit\")\n            if \"sample_weight\" in inspect.getfullargspec(fit_method).args:\n                sample_weights[f\"{step[0]}_sample_weight\"] = sample_weight\n    if sample_weights:\n        return sample_weights\n    else:\n        return None\n\n", "CodeBleu_score": 38.46059483867108}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n    if not self.fitted_:\n        raise RuntimeError('No optimized pipeline found. Please run fit() first.')\n\n    pipeline_str = self.pareto_front_fitted_pipelines_[0].export()\n    if self.verbosity > 0:\n        print('Best pipeline: %s' % pipeline_str)\n        if self.verbosity > 1:\n            print()\n\n    # Compile and fit the best pipeline\n    pipeline_model = self.pareto_front_fitted_pipelines_[0]\n    pipeline_model.fit(features, target)\n\n    # Compile and fit the entire Pareto front of pipelines\n    for pipeline in self.pareto_front_fitted_pipelines_:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            pipeline.fit(features, target)\n\n    return pipeline_model\n", "CodeBleu_score": 24.948105548032856}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\n    if self.fitted_pipeline_ is None:\n        raise RuntimeError(\"No optimized pipeline found. Please run fit() before calling this method.\")\n    pipeline_str = self.fitted_pipeline_.export()\n    if self.verbosity > 0:\n        print(f\"Best pipeline: {pipeline_str}\")\n        if self.verbosity > 1:\n            print()\n    pipeline = self._compile_pipeline(pipeline_str)\n    pipeline.fit(features, target)\n    if self.verbosity > 0:\n        print(f\"Pipeline has been fitted.\")\n        if self.verbosity > 1:\n            print()\n    if self.pareto_front_ is not None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for model in self.pareto_front_:\n                model.fit(features, target)\n\n", "CodeBleu_score": 28.1802815181173}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if not self.fitted_pipeline_:\n        raise RuntimeError(\"No optimized pipeline found. Please run fit() first.\")\n\n    pipeline_str = self.fitted_pipeline_.export()\n    pipeline = self._compile_pipeline(pipeline_str, features, target)\n\n    if self.verbosity >= 1:\n        print(\"Best pipeline: %s\" % pipeline_str)\n        if self.verbosity >= 2:\n            print()\n\n    if self.verbosity >= 1:\n        print(\"Pipeline fitting:\")\n\n    pipeline.fit(features, target)\n\n    if self.verbosity >= 1:\n        print(\"Done\")\n\n    if self.verbosity >= 2:\n        print()\n\n    if self.verbosity >= 1:\n        print(\"Computing Pareto front\")\n\n    self._pareto_front = self._compute_pareto_front(features, target)\n\n    if self.verbosity >= 1:\n        print(\"Done\")\n\n    if self.verbosity >= 2:\n        print()\n\n    return pipeline\n", "CodeBleu_score": 27.836361757899773}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n      raise ValueError(\"The pipeline has not been optimized.\")\n\n    self._validate_features_and_target(testing_features, testing_target)\n\n    if isinstance(self.scoring, str):\n      scorer = SCORERS[self.scoring]\n    else:\n      scorer = self.scoring\n\n    return scorer(self.pipeline, testing_features, testing_target)\n", "CodeBleu_score": 27.887018514873507}, {"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n      raise ValueError(\"Pipeline must be optimized before scoring.\")\n    self._validate_testing_data(testing_features, testing_target)\n    if isinstance(self.scoring, str):\n      scorer = SCORERS[self.scoring]\n    else:\n      scorer = self.scoring\n    return scorer(self.fitted_pipeline, testing_features, testing_target)\n", "CodeBleu_score": 27.83778614000304}, {"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n      raise ValueError(\"The pipeline has not been optimized yet. Please run the optimize() method first.\")\n    self.validate_features_target(testing_features, testing_target)\n    if isinstance(self.scorer, str):\n      scorer = SCORERS[self.scorer]\n    else:\n      scorer = self.scorer\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score\n", "CodeBleu_score": 32.62960629648388}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n      print(\"Imputing missing values...\")\n    if not self._imputer:\n      self._imputer = SimpleImputer(strategy=\"median\")\n      self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n  def _get_feature_names(self):\n    \"\"\"# This function returns the feature names of the dataset. If the feature names have not been set yet, it returns a list of feature names in the format \"feature_i\" where i is the index of the feature.\"\"\"\n    if self.feature_names is None:\n      return [f\"feature_{i}\" for i in range(self.features.shape[1])]\n    return self.feature_names\n\n  def _get_feature_importances(self):\n    \"\"\"# This function returns the feature importances of the model. If the feature importances have not been set yet, it returns a list of zeros with the length equal to the number of features.\"\"\"\n    if self.feature_importances_ is None:\n      return [0] * self.features.shape[1]\n    return self.feature_importances_\n\n  def _get_feature_importance_ranks(self):\n    \"\"\"# This function returns the feature importance ranks of the model. If the feature importance ranks have not been set yet, it returns a list of zeros with the length equal to the number of features.\"\"\"\n    if self.feature_importance_ranks_ is None:\n      return [0] * self.features.shape[1]\n    return self.feature_importance_ranks_\n\n  def _get_feature_importance_rank_names(self):\n    \"\"\"# This function returns the feature importance rank names of the model. If the feature importance rank names have not been set yet, it returns a list of zeros with the length equal to the number of features.\"\"\"\n    if self.feature_importance_rank_names_ is None:\n      return [0] * self.features.shape[1]\n    return self.feature_importance_rank_names_\n\n  def _get_feature_importance_rank_names_with_importances(self):\n    \"\"\"# This function returns the feature importance rank names with importances of the model. If the feature importance rank names with importances have not been set yet, it returns a list of zeros with the length equal to the number of features.\"\"\"\n   ", "CodeBleu_score": 58.73921443796832}, {"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n      print(\"Imputing missing values...\")\n    if not self._imputer:\n      self._imputer = SimpleImputer(strategy=\"median\")\n      self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n  def _scale_values(self, features):\n    \"\"\"# This function scales the feature set using the StandardScaler.\\nIf verbosity is greater than 1, it prints a message indicating the scaling process.\\nIf the scaler has not been fitted yet, it creates a StandardScaler and fits it to the features.\\nThe function then transforms the features using the fitted scaler and returns the scaled feature set.\"\"\"\n    if self.verbosity > 1:\n      print(\"Scaling features...\")\n    if not self._scaler:\n      self._scaler = StandardScaler()\n      self._scaler.fit(features)\n    return self._scaler.transform(features)\n\n  def _encode_labels(self, labels):\n    \"\"\"# This function encodes the labels using the LabelEncoder.\\nIf verbosity is greater than 1, it prints a message indicating the encoding process.\\nIf the encoder has not been fitted yet, it creates a LabelEncoder and fits it to the labels.\\nThe function then transforms the labels using the fitted encoder and returns the encoded labels.\"\"\"\n    if self.verbosity > 1:\n      print(\"Encoding labels...\")\n    if not self._encoder:\n      self._encoder = LabelEncoder()\n      self._encoder.fit(labels)\n    return self._encoder.transform(labels)\n\n  def _split_data(self, features, labels):\n    \"\"\"# This function splits the features and labels into training and testing sets.\\nIf verbosity is greater than 1, it prints a message indicating the splitting process.\\nThe function uses the train_test_split function from scikit-learn to split the data into training and testing sets with a test size of 0.25 and a random state of 42.\\nThe function then returns the training and testing sets for both features and labels.\"\"\"\n    if self.verbosity > 1:\n      print(\"Splitting data...\")\n    return train_test_split(features, labels, test_size=0.25, random_state=42)\n\n  def _train_model(self, model, X_train, y_train):\n    \"\"\"# This function trains a model on a given set", "CodeBleu_score": 51.56972826681766}, {"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n      print(\"Imputing missing values...\")\n    if not self.imputer:\n      self.imputer = SimpleImputer(strategy=\"median\")\n      self.imputer.fit(features)\n    return self.imputer.transform(features)\n\n  def _scale_values(self, features):\n    \"\"\"# This function scales the values of a feature set.\\nIf verbosity is greater than 1, it prints a message indicating the scaling process.\\nIf the scaler has not been fitted yet, it creates a StandardScaler and fits it to the features.\\nThe function then transforms the features using the fitted scaler and returns the scaled feature set.\"\"\"\n    if self.verbosity > 1:\n      print(\"Scaling values...\")\n    if not self.scaler:\n      self.scaler = StandardScaler()\n      self.scaler.fit(features)\n    return self.scaler.transform(features)\n\n  def _encode_values(self, features):\n    \"\"\"# This function encodes categorical values in a feature set.\\nIf verbosity is greater than 1, it prints a message indicating the encoding process.\\nIf the encoder has not been fitted yet, it creates a OneHotEncoder and fits it to the features.\\nThe function then transforms the features using the fitted encoder and returns the encoded feature set.\"\"\"\n    if self.verbosity > 1:\n      print(\"Encoding categorical values...\")\n    if not self.encoder:\n      self.encoder = OneHotEncoder()\n      self.encoder.fit(features)\n    return self.encoder.transform(features)\n\n  def _get_feature_names(self):\n    \"\"\"# This function returns the names of the features in the feature set.\\nIf the feature names have not been determined yet, it creates a list of feature names based on the number of columns in the feature set and returns it.\\nIf the feature names have already been determined, it returns the stored feature names.\"\"\"\n    if not self.feature_names:\n      self.feature_names = [f\"feature_{i}\" for i in range(self.features.shape[1])]\n    return self.feature_names\n\n  def _get_target_names(self):\n    \"\"\"# This function returns the names of the targets in the target set.\\nIf the target names have not been determined yet, it creates a list of target names based on the number of columns in the target set and returns it.\\nIf", "CodeBleu_score": 51.07287715517204}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n    # Define a set of operators that can be used to construct a scikit-learn pipeline\n    operators = {\n        \"StandardScaler\": StandardScaler,\n        \"MinMaxScaler\": MinMaxScaler,\n        \"RobustScaler\": RobustScaler,\n        \"MaxAbsScaler\": MaxAbsScaler,\n        \"Normalizer\": Normalizer,\n        \"OneHotEncoder\": OneHotEncoder,\n        \"OrdinalEncoder\": OrdinalEncoder,\n        \"SimpleImputer\": SimpleImputer,\n        \"KNNImputer\": KNNImputer,\n        \"IterativeImputer\": IterativeImputer,\n        \"PCA\": PCA,\n        \"KernelPCA\": KernelPCA,\n        \"TruncatedSVD\": TruncatedSVD,\n        \"LatentDirichletAllocation\": LatentDirichletAllocation,\n        \"LinearDiscriminantAnalysis\": LinearDiscriminletAnalysis,\n        \"FastICA\": FastICA,\n        \"NMF\": NMF,\n        \"GaussianRandomProjection\": GaussianRandomProjection,\n        \"SparseRandomProjection\": SparseRandomProjection,\n        \"PolynomialFeatures\": PolynomialFeatures,\n        \"SelectKBest\": SelectKBest,\n        \"SelectPercentile\": SelectPercentile,\n        \"SelectFpr\": SelectFpr,\n        \"SelectFdr\": SelectFdr,\n        \"SelectFwe\": SelectFwe,\n        \"VarianceThreshold\": VarianceThreshold,\n        \"RFE\": RFE,\n        \"RFECV\": RFECV,\n        \"SelectFromModel\": SelectFromModel,\n        \"LinearRegression\": LinearRegression,\n        \"LogisticRegression\": LogisticRegression,\n        \"Ridge\": Ridge,\n        \"Lasso\": Lasso,\n        \"ElasticNet\": ElasticNet,\n        \"SGDRegressor\": SGDRegressor,\n        \"SGDClassifier\": SGDClassifier,\n        \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor,\n        \"PassiveAggressiveClassifier\": PassiveAggressiveClassifier,\n        \"KNeighborsRegressor\": KNeighborsRegressor,\n        \"KNeighborsClassifier\": KNeighborsClassifier,\n        \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor,\n        \"RadiusNeighborsClassifier\": RadiusNeighborsClassifier,\n        \"NearestCentroid\": NearestCentroid,\n        \"DecisionTreeRegressor\": DecisionTreeRegressor,\n        \"DecisionTreeClassifier\": DecisionTreeClassifier,\n        \"ExtraTreeRegressor\": ExtraTreeRegressor,\n        \"ExtraTreeClassifier\": ExtraTreeClassifier,\n        \"RandomForestRegressor\": RandomForestRegressor,\n        \"RandomForestClassifier\": RandomForestClassifier,\n        \"ExtraTreesRegressor\": ExtraTrees", "CodeBleu_score": 22.399260740066357}, {"code": "def _compile_to_sklearn(self, expr)\n    # Convert the expression to a pipeline code string\n    pipeline_code = self._expr_to_code(expr)\n\n    # Evaluate the pipeline code string within a specific context\n    pipeline = eval(pipeline_code, self.context)\n\n    # Set the pipeline's memory attribute\n    pipeline.memory = self.memory\n\n    # Set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        self._set_random_state(pipeline, self.random_state)\n\n    # Return the configured scikit-learn pipeline\n    return pipeline", "CodeBleu_score": 29.130085104109956}, {"code": "def _compile_to_sklearn(self, expr)\n    code_str = self._compile_to_code(expr)\n    pipeline = eval(code_str, self._context)\n    pipeline.memory = self._memory\n    if self._random_state is not None:\n        self._set_random_state(pipeline, self._random_state)\n    return pipeline\n", "CodeBleu_score": 25.438164776374716}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n    individuals_eligible_for_crossover = []\n    for individual1 in population:\n        for individual2 in population:\n            if individual1!= individual2:\n                primitive_nodes_individual1 = set(individual1.primitives)\n                primitive_nodes_individual2 = set(individual2.primitives)\n                if len(primitive_nodes_individual1 & primitive_nodes_individual2) > 0:\n                    individuals_eligible_for_crossover.append((individual1, individual2))\n    if len(individuals_eligible_for_crossover) == 0:\n        return None, None\n    random_pair = random.choice(individuals_eligible_for_crossover)\n    return random_pair[0], random_pair[1]\n", "CodeBleu_score": 20.30591981876066}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    individuals = []\n    for individual in population:\n        primitive_nodes = set(individual.primitive_nodes)\n        individual_string = str(individual)\n        for i in range(len(population)):\n            if i!= individual.index:\n                other_primitive_nodes = set(population[i].primitive_nodes)\n                other_individual_string = str(population[i])\n                if primitive_nodes & other_primitive_nodes and individual_string!= other_individual_string:\n                    individuals.append((individual, population[i]))\n                    individuals.append((population[i], individual))\n    if individuals:\n        return random.choice(individuals)\n    return None, None\n\n", "CodeBleu_score": 23.0944774232588}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    def convert_individual_to_string(individual):\n        \"\"\"The function convert_individual_to_string(individual) converts an individual to a string representation. It first extracts the primitive nodes from the individual's tree and converts them to strings. It then concatenates the strings into a single string representation of the individual.\"\"\"\n        primitive_nodes = set(individual.tree.primitives)\n        primitive_strings = [str(node) for node in primitive_nodes]\n        return \" \".join(primitive_strings)\n\n    def find_eligible_pairs(population):\n        \"\"\"The function find_eligible_pairs(population) finds pairs of individuals from a given population that share at least one primitive and are not identical. It first creates sets of primitive nodes for each individual and converts each individual to a string representation. It then identifies pairs of individuals that share at least one primitive and are not identical. Eligible pairs are considered in both possible orders. If no eligible pairs are found, the function returns an empty list. Otherwise, it returns a list of eligible pairs.\"\"\"\n        primitive_sets = [set(individual.tree.primitives) for individual in population]\n        string_representations = [convert_individual_to_string(individual) for individual in population]\n        eligible_pairs = []\n        for i in range(len(population) - 1):\n            for j in range(i + 1, len(population)):\n                if primitive_sets[i] & primitive_sets[j] and string_representations[i]!= string_representations[j]:\n                    eligible_pairs.append((i, j))\n                    eligible_pairs.append((j, i))\n        return eligible_pairs\n\n    eligible_pairs = find_eligible_pairs(population)\n    if not eligible_pairs:\n        return None, None\n    pair_index = random.randint(0, len(eligible_pairs) - 1)\n    return population[eligible_pairs[pair_index][0]], population[eligible_pairs[pair_index][1]]\n", "CodeBleu_score": 35.868164873815495}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    pipeline_code = \"\"\n    num_steps = len(pipeline_tree)\n    if num_steps > 1:\n        pipeline_code += \"make_pipeline(\"\n    for i, step in enumerate(pipeline_tree):\n        operator = operators[step[\"operator_id\"]]\n        operator_code = generate_export_operator_code(operator, step[\"parameters\"])\n        pipeline_code += operator_code\n        if i < num_steps - 1:\n            pipeline_code += \",\\n\"\n    if num_steps > 1:\n        pipeline_code += \")\"\n    return pipeline_code\n\n", "CodeBleu_score": 27.757223588580406}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    # Process the operators to create the steps of the pipeline\n    steps = []\n    for operator in operators:\n        step = f\"{operator.name}()\"\n        steps.append(step)\n\n    # Determine the number of steps\n    num_steps = len(steps)\n\n    # Format the steps into a make_pipeline call with indentation\n    if num_steps > 1:\n        pipeline_code = f\"pipeline = make_pipeline(\\n\"\n        for step in steps:\n            pipeline_code += f\"    {step},\\n\"\n        pipeline_code += f\")\"\n    # Format the step directly without make_pipeline\n    else:\n        pipeline_code = f\"pipeline = {steps[0]}\"\n\n    return pipeline_code\n", "CodeBleu_score": 27.530787813728697}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    steps = []\n    num_steps = len(pipeline_tree.children)\n    for i, child in enumerate(pipeline_tree.children):\n        if child.operator is not None:\n            operator_code = operators[child.operator.name]\n            operator_code = operator_code.replace(\"    \", \"  \")\n            steps.append(operator_code)\n    if num_steps > 1:\n        pipeline_code = \"make_pipeline(\\n\"\n        for step in steps:\n            pipeline_code += \"  \" + step + \",\\n\"\n        pipeline_code = pipeline_code[:-2] + \"\\n)\"\n    else:\n        pipeline_code = steps[0]\n    return pipeline_code\n\n", "CodeBleu_score": 29.91025216543041}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n    name = operator.name\n    if name == \"CombineDFs\":\n        return _process_combine_dfs(operator, operators)\n    else:\n        tpot_op = _get_tpot_operator(name)\n        steps = []\n        for input_op in operator.inputs:\n            if input_op.name!= \"input_matrix\":\n                steps.extend(_process_operator(input_op, operators, depth=depth + 1))\n        if name == \"input_matrix\":\n            steps.append((\"input_matrix\", tpot_op))\n        else:\n            if depth > 0:\n                tpot_op = StackingEstimator(tpot_op)\n            steps.append((name, tpot_op))\n        return steps\n\n", "CodeBleu_score": 37.79725544161381}, {"code": "def _process_operator(operator, operators, depth=0)\n    if operator.name == \"CombineDFs\":\n        steps = []\n        for op in operator.operators:\n            steps += _process_operator(op, operators, depth)\n        return steps\n    else:\n        op = operators[operator.name]\n        if op.name == \"input\":\n            return []\n        elif op.name == \"root\":\n            if depth > 0:\n                return [(\"stacking_estimator\", StackingEstimator(op.fitted_pipeline))]\n            else:\n                return []\n        else:\n            steps = []\n            for op_input in operator.inputs:\n                steps += _process_operator(op_input, operators, depth + 1)\n            steps += [(op.name, op.fitted_pipeline)]\n            return steps\n\n", "CodeBleu_score": 35.622226041842644}, {"code": "def _process_operator(operator, operators, depth=0)\n    if operator.name == \"CombineDFs\":\n        return operator.steps\n    else:\n        operator_name = operator.name\n        if operator_name in operators:\n            operator_class = operators[operator_name]\n        else:\n            raise ValueError(f\"Operator {operator_name} not found.\")\n        if operator.inputs:\n            steps = []\n            for input_operator in operator.inputs:\n                if input_operator.name == \"input_matrix\":\n                    steps.append(operator_class)\n                else:\n                    steps.extend(_process_operator(input_operator, operators, depth=depth + 1))\n            if depth > 0:\n                steps = [(\"stacking_estimator\", StackingEstimator(estimator=Pipeline(steps)))]\n            return steps\n        else:\n            return [(operator_name, operator_class)]\n\n", "CodeBleu_score": 33.33877840610922}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n    if not self.categorical_features:\n      raise ValueError(\"No categorical features found.\")\n    else:\n      return pd.get_dummies(X, columns=self.categorical_features)\n", "CodeBleu_score": 15.025832759883762}, {"code": "def transform(self, X)\n    if self.categorical_features is None:\n      raise ValueError(\"No categorical features found.\")\n    else:\n      return pd.get_dummies(X, columns=self.categorical_features, prefix_sep=\"__\")\n", "CodeBleu_score": 16.698783865358635}, {"code": "def transform(self, X)\n    # Check if any categorical features are present\n    if not self.categorical_features:\n      raise ValueError(\"No categorical features found.\")\n\n    # Apply one-hot encoding to categorical features\n    X_transformed = pd.get_dummies(X, columns=self.categorical_features)\n\n    return X_transformed\n", "CodeBleu_score": 23.616667004378396}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(y_true)\n    accuracies = []\n    for c in classes:\n        y_true_c = y_true == c\n        y_pred_c = y_pred == c\n        tp = np.sum(np.logical_and(y_true_c, y_pred_c))\n        fp = np.sum(np.logical_and(np.logical_not(y_true_c), y_pred_c))\n        tn = np.sum(np.logical_and(np.logical_not(y_true_c), np.logical_not(y_pred_c)))\n        fn = np.sum(np.logical_and(y_true_c, np.logical_not(y_pred_c)))\n        if tp + fn == 0:\n            sensitivity = 1\n        else:\n            sensitivity = tp / (tp + fn)\n        if tn + fp == 0:\n            specificity = 1\n        else:\n            specificity = tn / (tn + fp)\n        accuracies.append((sensitivity + specificity) / 2)\n    return np.mean(accuracies)\n\n", "CodeBleu_score": 24.25711504055586}, {"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(y_true)\n    accuracies = []\n    for c in classes:\n        # Find the true and false positives and negatives for each class\n        tp = np.sum((y_true == c) & (y_pred == c))\n        fp = np.sum((y_true!= c) & (y_pred == c))\n        tn = np.sum((y_true!= c) & (y_pred!= c))\n        fn = np.sum((y_true == c) & (y_pred!= c))\n        # Calculate the sensitivity and specificity for each class\n        sensitivity = tp / (tp + fn)\n        specificity = tn / (tn + fp)\n        # If the true labels consist of only one class, set specificity to 1\n        if specificity == 0:\n            specificity = 1\n        # Calculate the class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n        accuracies.append(class_accuracy)\n    return np.mean(accuracies)\n\n", "CodeBleu_score": 22.948204204221774}, {"code": "def balanced_accuracy(y_true, y_pred)\n    unique_classes = np.unique(y_true)\n    class_accuracies = []\n    for class_label in unique_classes:\n        true_positives = np.sum(np.logical_and(y_true == class_label, y_pred == class_label))\n        false_negatives = np.sum(np.logical_and(y_true == class_label, y_pred!= class_label))\n        true_negatives = np.sum(np.logical_and(y_true!= class_label, y_pred!= class_label))\n        if np.sum(y_true == class_label) == 0:\n            specificity = 1\n        else:\n            specificity = true_negatives / np.sum(y_true!= class_label)\n        sensitivity = true_positives / np.sum(y_true == class_label)\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 24.024072704293904}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n    check_array(X)\n    non_zero_count = np.count_nonzero(X, axis=1)\n    zero_count = X.shape[1] - non_zero_count\n    return np.hstack((np.array(non_zero_count).reshape(-1, 1), np.array(zero_count).reshape(-1, 1), X))\n", "CodeBleu_score": 17.73800275691388}, {"code": "def transform(self, X, y=None)\n    X = check_array(X)\n    non_zero = np.count_nonzero(X, axis=1)\n    zero = np.count_nonzero(X == 0, axis=1)\n    return np.hstack([non_zero.reshape(-1, 1), zero.reshape(-1, 1), X])\n", "CodeBleu_score": 14.285825318298153}, {"code": "def transform(self, X, y=None)\n    X = check_array(X)\n    X_transformed = np.zeros((X.shape[0], X.shape[1] + 2))\n    X_transformed[:, 0] = X[:, 0]\n    X_transformed[:, 1] = X[:, 1]\n    X_transformed[:, 2] = np.count_nonzero(X, axis=1)\n    X_transformed[:, 3] = np.count_nonzero(X == 0, axis=1)\n    return X_transformed\n\n", "CodeBleu_score": 21.30815917728113}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n    self.classes = len(np.unique(target))\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.classes, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.classes:\n        self.pretest_y = target[np.isin(target, np.unique(self.pretest_y))]\n        self.pretest_X = features[np.isin(target, np.unique(self.pretest_y))]\n    self.pretest_X = self.pretest_X.values\n    self.pretest_y = self.pretest_y.values\n    self.pretest_y = np.squeeze(self.pretest_y)\n", "CodeBleu_score": 27.25346863122185}, {"code": "def _init_pretest(self, features, target)\n    self.classes = np.unique(target)\n    self.n_classes = len(self.classes)\n    if self.n_classes < self.train_size:\n        self.train_size = self.n_classes\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if len(self.classes)!= len(np.unique(self.pretest_y)):\n        self.pretest_y = target[np.isin(target, self.classes)]\n\n", "CodeBleu_score": 23.53920302231761}, {"code": "def _init_pretest(self, features, target)\n    self.n_classes = len(np.unique(target))\n    self.training_size = len(target)\n    if self.training_size < self.n_classes:\n        raise ValueError(\n            \"The training size must be at least the number of unique target classes.\"\n        )\n    self.pretest_X, self.pretest_y = train_test_split(\n        features, target, train_size=self.training_size, random_state=self.random_state\n    )\n    self.pretest_y = self.pretest_y.reset_index(drop=True)\n    if self.n_classes > 1:\n        unique_target = np.unique(self.pretest_y)\n        if len(unique_target) < self.n_classes:\n            for i in range(self.n_classes - len(unique_target)):\n                self.pretest_y.loc[len(self.pretest_y)] = unique_target[i]\n            self.pretest_y = self.pretest_y.reset_index(drop=True)\n\n", "CodeBleu_score": 33.86683782246034}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n    if dtype is None:\n        return dtype\n    if isinstance(dtype, tf.DType):\n        return np.dtype(dtype.as_numpy_dtype)\n    return dtype\n\n", "CodeBleu_score": 60.880351947455146}, {"code": "def _to_numpy_type(dtype)\n    if dtype is None:\n        return None\n    elif isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"dtype must be a TensorFlow or NumPy data type.\")\n", "CodeBleu_score": 55.26328447884256}, {"code": "def _to_numpy_type(dtype)\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    else:\n        return dtype\n\n", "CodeBleu_score": 43.53879182400563}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n    if batch_size is not None:\n        try:\n            ds = ds.batch(batch_size)\n        except AttributeError:\n            ds = ds.batch(batch_size, drop_remainder=True)\n    return np.concatenate([batch for batch in ds], axis=0)\n\n", "CodeBleu_score": 16.345090423081473}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        if batch_size is None:\n            return next(iter(ds))\n        else:\n            return next(iter(ds.batch(batch_size)))\n    except AttributeError:\n        return next(iter(ds))\n\n", "CodeBleu_score": 10.532922530948358}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        if batch_size is None:\n            batch_size = ds.batch_size\n        return np.concatenate([batch.numpy() for batch in ds.batch(batch_size)], axis=0)\n    except AttributeError:\n        return np.concatenate([batch for batch in ds], axis=0)\n\n", "CodeBleu_score": 15.09855921848656}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n    seed = int(seed)\n    assert seed >= 0, \"Seed must be a non-negative integer.\"\n    return jax.random.PRNGKey(seed)\n", "CodeBleu_score": 11.503921586169147}, {"code": "def get_prng(seed)\n    if not isinstance(seed, (int, np.integer)):\n        raise TypeError(f\"Seed must be an integer, got {type(seed)}.\")\n    return jax.random.PRNGKey(np.uint32(seed))\n\n", "CodeBleu_score": 13.894426758473616}, {"code": "def get_prng(seed)\n    seed = jnp.asarray(seed, dtype=jnp.uint32)\n    assert seed.ndim == 0, \"Seed must be a scalar.\"\n    return jax.random.PRNGKey(seed)\n\n", "CodeBleu_score": 13.382330008039538}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif is_datetime64_dtype(s):\n        return s.dt.tz_localize(dtype.tz)\n    elif is_numeric_dtype(s):\n        return pd.to_datetime(s, unit=\"ns\").dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 69.75272085794154}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.tz)\n    elif dtype.kind == \"M\":\n        return s.dt.tz_localize(dtype.tz)\n    elif dtype.kind == \"m\":\n        return pd.to_datetime(s, unit=dtype.unit).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, unit=\"ns\").dt.tz_localize(\"UTC\").dt.tz_convert(\n            dtype.tz\n        )\n\n", "CodeBleu_score": 60.06385306028794}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if s.dtype.tz is not None:\n        return s\n    if s.dtype.tz is None and isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif s.dtype.tz is None and s.dtype.kind == \"M\":\n        return s.dt.tz_localize(dtype.tz)\n    elif s.dtype.tz is None and s.dtype.kind == \"m\":\n        return s.astype(\"datetime64[ns]\").dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 58.20379831265886}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not arg.known_divisions:\n        raise ValueError(\"BetweenTime operation requires known divisions\")\n    if not isinstance(arg, dd.DataFrame):\n        raise ValueError(\"BetweenTime operation requires a Dask DataFrame\")\n    if not isinstance(lower_bound, pd.Timestamp):\n        raise ValueError(\"BetweenTime operation requires a pandas Timestamp for lower bound\")\n    if not isinstance(upper_bound, pd.Timestamp):\n        raise ValueError(\"BetweenTime operation requires a pandas Timestamp for upper bound\")\n    if lower_bound.tz is None and upper_bound.tz is None:\n        tz = None\n    elif lower_bound.tz is not None and upper_bound.tz is not None:\n        if lower_bound.tz!= upper_bound.tz:\n            raise ValueError(\"BetweenTime operation requires timestamps with the same timezone\")\n        tz = lower_bound.tz\n    elif lower_bound.tz is not None:\n        tz = lower_bound.tz\n    else:\n        tz = upper_bound.tz\n    if tz is not None and tz!= \"UTC\":\n        lower_bound = lower_bound.tz_convert(\"UTC\")\n        upper_bound = upper_bound.tz_convert(\"UTC\")\n    lower_bound = lower_bound.tz_localize(None)\n    upper_bound = upper_bound.tz_localize(None)\n    if lower_bound.tz is None and upper_bound.tz is None:\n        tz = None\n    elif lower_bound.tz is not None and upper_bound.tz is not None:\n        if lower_bound.tz!= upper_bound.tz:\n            raise ValueError(\"BetweenTime operation requires timestamps with the same timezone\")\n        tz = lower_bound.tz\n    elif lower_bound.tz is not None:\n        tz = lower_bound.tz\n    else:\n        tz = upper_bound.tz\n    if tz is not None and tz!= \"UTC\":\n        lower_bound = lower_bound.tz_convert(\"UTC\")\n        upper_bound = upper_bound.tz_convert(\"UTC\")\n    lower_bound = lower_bound.tz_localize(None)\n    upper_bound = upper_bound.tz_localize(None)\n    if lower_bound.tz is None and upper_bound.tz is None:\n        tz = None\n    elif lower_bound.tz is not None and upper_bound.tz is not None:\n        if lower_bound.tz!=", "CodeBleu_score": 20.34714740242557}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not arg.known_divisions:\n        raise ValueError(\"Cannot perform operation on a Dask DataFrame with unknown divisions.\")\n    if not arg.known_divisions[0]:\n        raise ValueError(\"Cannot perform operation on a Dask DataFrame with unknown divisions.\")\n\n    if arg.known_divisions[0][0] is not None:\n        lower_bound = pd.to_datetime(lower_bound)\n        upper_bound = pd.to_datetime(upper_bound)\n\n        if arg.known_divisions[0][0].tz is not None:\n            lower_bound = lower_bound.tz_convert(arg.known_divisions[0][0].tz)\n            upper_bound = upper_bound.tz_convert(arg.known_divisions[0][0].tz)\n\n        lower_bound = lower_bound.tz_localize(None)\n        upper_bound = upper_bound.tz_localize(None)\n\n    return arg.map_partitions(\n        lambda df: df[\n            (df[op.column_name] >= lower_bound) & (df[op.column_name] <= upper_bound)\n        ]\n    )\n\n", "CodeBleu_score": 29.892724107297365}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not arg.known_divisions:\n        raise ValueError(\"Cannot process time-based operations on Dask DataFrames without known divisions.\")\n    if arg.known_divisions[0] is None:\n        raise ValueError(\"Cannot process time-based operations on Dask DataFrames without known divisions.\")\n    if isinstance(arg.known_divisions[0], tuple):\n        raise ValueError(\"Cannot process time-based operations on Dask DataFrames with tuple divisions.\")\n    if arg.known_divisions[0] is not None and isinstance(arg.known_divisions[0], datetime.datetime):\n        raise ValueError(\"Cannot process time-based operations on Dask DataFrames with datetime divisions.\")\n    if isinstance(arg.known_divisions[0], np.datetime64):\n        raise ValueError(\"Cannot process time-based operations on Dask DataFrames with numpy datetime divisions.\")\n    if isinstance(lower_bound, datetime.datetime):\n        lower_bound = pd.Timestamp(lower_bound)\n    if isinstance(upper_bound, datetime.datetime):\n        upper_bound = pd.Timestamp(upper_bound)\n    if not isinstance(lower_bound, pd.Timestamp):\n        raise ValueError(\"Lower bound must be a Pandas Timestamp.\")\n    if not isinstance(upper_bound, pd.Timestamp):\n        raise ValueError(\"Upper bound must be a Pandas Timestamp.\")\n    if arg.known_divisions[0] is not None and not isinstance(arg.known_divisions[0], pd.Timestamp):\n        raise ValueError(\"Dask DataFrame divisions must be Pandas Timestamps.\")\n    if arg.known_divisions[0] is not None and not isinstance(arg.known_divisions[0], pd.Timestamp):\n        raise ValueError(\"Dask DataFrame divisions must be Pandas Timestamps.\")\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.tz_convert(\"UTC\")\n    if upper_bound.tz is not None:\n        upper_bound = upper_bound.tz_convert(\"UTC\")\n    if arg.known_divisions[0] is not None and arg.known_divisions[0].tz is not None:\n        arg = arg.tz_convert(\"UTC\")\n    if op.time_unit == \"s\":\n        lower_bound = lower_bound.round(\"s\")\n        upper_bound = upper_bound.round(\"s\")\n    elif", "CodeBleu_score": 23.596643021450745}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return dd.from_array(value.compute(), npartitions=1).squeeze()\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, tuple):\n            return cls(tuple(asseries(v, like=None) for v in like), like=like)\n        if isinstance(like, list):\n            return cls(list(asseries(v, like=None) for v in like), like=like)\n        if isinstance(like, dict):\n            return cls(dict((k, asseries(v, like=None)) for k, v in like.items()), like=like)\n        if isinstance(like, pd.DataFrame):\n            return cls(dd.from_pandas(like, npartitions=1).squeeze(), like=like)\n        if isinstance(like, dd.DataFrame):\n            return cls(dd.from_pandas(like.compute(), npartitions=1).squeeze(), like=like)\n        if isinstance(like, dd.Series):\n            return cls(like, like=like)\n        if isinstance(like, dd.Scalar):\n            return cls(dd.from_array(like.compute(), npartitions=1).squeeze(), like=like)\n        if isinstance(like, pd.Series):\n            return cls(dd.from_pandas(like, npartitions=1).squeeze(), like=like)\n    return cls(dd.from_array(value, npartitions=1).squeeze(), like=value)\n\n", "CodeBleu_score": 33.30806388725431}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(np.array([value.compute()]), npartitions=1)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (tuple, list, dict)):\n        return cls(value, like=like)\n    else:\n        return dd.from_array(np.array([value]), npartitions=1)\n\n", "CodeBleu_score": 33.75162013234657}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, da.Array):\n        value = da.from_array(value, chunks=(1, value.shape[1]))\n    if isinstance(value, da.Series):\n        return value\n    if isinstance(value, da.Scalar):\n        value = da.from_array(value, chunks=(1, 1))\n    if isinstance(value, pd.Series):\n        value = da.from_array(value, chunks=(1, value.shape[0]))\n    if like is not None:\n        if isinstance(like, tuple):\n            return cls(\n                [\n                    cls(\n                        [\n                            cls(\n                                [\n                                    cls(\n                                        [\n                                            cls(\n                                                [\n                                                    cls(\n                                                        [\n                                                            cls(\n                                                                [\n                                                                    cls(\n                                                                        [\n                                                                            cls(\n                                                                                [\n                                                                                    cls(\n                                                                                        [\n                                                                                            cls(\n                                                                                                [\n                                                                                                    cls(\n                                                                                                        [\n                                                                                                            cls(\n                                                                                                                [\n                                                                                                                    cls(\n                                                                                                                        [\n                                                                                                                            cls(\n                                                                                                                                [\n                                                                                                                                    cls(\n                                                                                                                                        [\n                                                                                                                                            cls(\n                                                                                                                                                [\n                                                                                                                                                    cls(\n                                                                                                                                                        [\n                                                                                                                                                            cls(\n                                                                                                                                                                [\n                                                                                                                                                                    cls(\n                                                                                                                                                                        [\n                                                                                                                                                                            cls(\n                                                                                                                                                                                [\n                                                                                                                                                                                    cls(\n                                                                                                                                                                                        [\n                                                                                                                                                                                            cls(\n                                                                                                                                                                                                [\n                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                        [\n                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                [\n                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                                                                                [\n                                                                                                                                                                                                                                                                                                                                                                    cls(\n                                                                                                                                                                                                                                                                                                                                                                        [\n                                                                                                                                                                                                                                                                                                                                                                            cls(\n                                                                                                                                                                                                                                                                                                ", "CodeBleu_score": 20.480798179554633}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for value in dictionary.values():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                \"The dictionary values must be either Dask or Pandas DataFrames.\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 55.25498216615643}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"The value of key '{key}' is not a valid DataFrame. \"\n                \"It must be a Dask or Pandas DataFrame.\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 67.95687397401046}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"The value of key '{key}' is not a valid DataFrame. It must be a Dask or Pandas DataFrame.\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 67.09234143893863}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in module {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for estimator {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(\n            f\"Multiple classes found for estimator {estimator_name}. Using the first one: {classes[estimator_name][0]}\"\n        )\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"Estimator {estimator_class} is not a subclass of sklearn's BaseEstimator\")\n    return estimator_class\n\n", "CodeBleu_score": 40.58522122806978}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logging.debug(\n            f\"Multiple classes found for {estimator_name} in {library_name}: {classes[estimator_name]}. Using the first one.\"\n        )\n        warnings.warn(\n            f\"Multiple classes found for {estimator_name} in {library_name}. Using the first one.\"\n        )\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logging.info(\n            f\"Estimator {estimator_class} is not a subclass of sklearn.BaseEstimator.\"\n        )\n    return estimator_class\n\n", "CodeBleu_score": 39.756530813115695}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Found multiple estimators with name {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(f\"Multiple estimators found with name {estimator_name}. Using the first one: {classes[estimator_name][0]}\")\n    selected_class = classes[estimator_name][0]\n    if not issubclass(selected_class, BaseEstimator):\n        logger.info(f\"Selected estimator {selected_class} is not a subclass of sklearn.BaseEstimator\")\n    return selected_class", "CodeBleu_score": 38.399896083988295}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task.json\"), \"r\") as f:\n        estimator_to_task = json.load(f)\n    if estimator_name in estimator_to_task:\n        return estimator_to_task[estimator_name]\n    else:\n        return \"unknown\"\n\n", "CodeBleu_score": 35.25233256659094}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    with open(os.path.join(os.path.dirname(__file__), \"estimators_to_tasks.json\"), \"r\") as f:\n        estimators_to_tasks = json.load(f)\n    for task, estimators in estimators_to_tasks.items():\n        if any(estimator_name.endswith(estimator) for estimator in estimators):\n            return task\n    return \"unknown\"\n\n", "CodeBleu_score": 46.554470708850445}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    if not estimator_name:\n        return \"unknown\"\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\"), \"r\") as f:\n        mapping = json.load(f)\n    for postfix, task in mapping.items():\n        if estimator_name.endswith(postfix):\n            return task\n    return \"unknown\"\n\n", "CodeBleu_score": 47.88264792119147}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict    data_dict = {}\n    for file in os.listdir(data_cache):\n        if not file.startswith(data_name):\n            continue\n        if file.endswith(\".json\"):\n            continue\n        component, ext = file.split(\".\")\n        data_dict[component] = load_data_from_file(data_cache, file)\n    return data_dict\n\n", "CodeBleu_score": 30.413224806048195}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict    data = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name):\n            component = filename.split(\".\")[0][len(data_name) + 1 :]\n            ext = filename.split(\".\")[1]\n            if ext == \"json\":\n                continue\n            data[component] = load_data_from_file(os.path.join(data_cache, filename))\n    return data\n\n", "CodeBleu_score": 39.29899190219432}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict    data_cache_dir = os.path.join(data_cache, data_name)\n    if not os.path.exists(data_cache_dir):\n        return {}\n    files = [f for f in os.listdir(data_cache_dir) if f.startswith(data_name)]\n    data = {}\n    for file in files:\n        if file.endswith(\".json\"):\n            continue\n        component, ext = os.path.splitext(file)\n        data[component] = load_data_from_file(os.path.join(data_cache_dir, file))\n    return data\n\n", "CodeBleu_score": 38.656423676197115}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                file.write(data)\n                written_size += len(data)\n                if total_size > 0:\n                    print(f\"Downloaded {written_size} of {total_size} bytes\")\n        if total_size!= written_size:\n            raise AssertionError(f\"Downloaded size {written_size} does not match expected content length {total_size}\")\n    else:\n        shutil.copy(url, filename)\n\n", "CodeBleu_score": 60.368080621465204}, {"code": "def retrieve(url: str, filename: str) -> None\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        total_size = int(response.headers.get(\"Content-Length\", 0))\n        block_size = 1024\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                file.write(data)\n                written_size += block_size\n                print(f\"Downloaded {written_size} bytes of {total_size}\", end=\"\\r\")\n        if written_size!= total_size:\n            raise AssertionError(f\"Downloaded size ({written_size}) does not match expected content length ({total_size})\")\n\n", "CodeBleu_score": 57.582655806449466}, {"code": "def retrieve(url: str, filename: str) -> None\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        content_length = int(response.headers.get(\"Content-Length\", 0))\n        block_size = 1024\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                file.write(data)\n                if content_length:\n                    print(f\"Downloaded {file.tell()}/{content_length} bytes\")\n        if content_length and file.tell()!= content_length:\n            raise AssertionError(\"Download size mismatch\")\n    else:\n        raise AssertionError(\"Unsupported URL scheme\")\n\n", "CodeBleu_score": 58.21034492947603}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    # Load the dataset\n    data = load_dataset(data_name, data_cache, raw_data_cache, dataset_params)\n\n    # Process the target labels\n    y = data[\"target\"]\n    y -= y.min()\n    if dataset_params.get(\"binary_target\", False):\n        y = (y > 0).astype(int)\n\n    # Create a description dictionary\n    description = {\n        \"num_classes\": len(np.unique(y)),\n        \"default_split_params\": {\n            \"train_size\": 0.8,\n            \"test_size\": 0.2,\n            \"random_state\": 0,\n        },\n    }\n\n    return data, description\n\n", "CodeBleu_score": 45.326914444459135}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    # Load dataset\n    data_path = os.path.join(data_cache, data_name)\n    raw_data_path = os.path.join(raw_data_cache, data_name)\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n    if not os.path.exists(raw_data_path):\n        os.makedirs(raw_data_path)\n    if not os.path.exists(os.path.join(data_path, \"X.npy\")):\n        print(\"Downloading Covertype dataset...\")\n        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n        urllib.request.urlretrieve(url, os.path.join(raw_data_path, \"covtype.data.gz\"))\n        print(\"Extracting Covertype dataset...\")\n        with gzip.open(os.path.join(raw_data_path, \"covtype.data.gz\"), \"rb\") as f_in:\n            with open(os.path.join(raw_data_path, \"covtype.data\"), \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        print(\"Processing Covertype dataset...\")\n        X = np.loadtxt(os.path.join(raw_data_path, \"covtype.data\"), delimiter=\",\")\n        y = X[:, -1].astype(int)\n        y -= 1  # make labels zero-indexed\n        if dataset_params.get(\"binary\", False):\n            y = (y > 2).astype(int)\n        np.save(os.path.join(data_path, \"X.npy\"), X[:, :-1])\n        np.save(os.path.join(data_path, \"y.npy\"), y)\n    else:\n        X = np.load(os.path.join(data_path, \"X.npy\"))\n        y = np.load(os.path.join(data_path, \"y.npy\"))\n    # Create description dictionary\n    description = {\n        \"num_classes\": len(np.unique(y)),\n        \"default_split\": {\n            \"train_size\": 0.8,\n            \"test_size\": 0.2,\n            \"shuffle", "CodeBleu_score": 39.736945886928495}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    # Load dataset\n    dataset = load_dataset(\n        data_name, data_cache, raw_data_cache, dataset_params, split=False\n    )\n    # Process target labels\n    y = dataset[\"train\"][\"label\"]\n    y = y - y.min()\n    if dataset_params[\"binary\"]:\n        y = y.long()\n        y[y > 0] = 1\n    dataset[\"train\"][\"label\"] = y\n    # Create description\n    description = {\n        \"num_classes\": len(torch.unique(y)),\n        \"default_split_params\": {\"train_val_test_split\": [0.7, 0.1, 0.2]},\n    }\n    return dataset, description\n\n", "CodeBleu_score": 38.22171202015549}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    flattened_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            flattened_list.extend(flatten_list(element))\n        else:\n            flattened_list.append(element)\n    if ensure_type_homogeneity:\n        flattened_list = ensure_list_types_homogeneity(flattened_list)\n    return flattened_list\n\n", "CodeBleu_score": 37.049809486296326}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.271901029998034}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    flattened_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            flattened_list.extend(flatten_list(element))\n        else:\n            flattened_list.append(element)\n    if ensure_type_homogeneity:\n        flattened_list = ensure_list_types_homogeneity(flattened_list)\n    return flattened_list\n\n", "CodeBleu_score": 37.049809486296326}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    if out is None:\n        out = np.empty(x.shape, dtype=x.dtype)\n    if complex_mode == \"split\":\n        real_out = np.empty(x.shape, dtype=x.dtype)\n        imag_out = np.empty(x.shape, dtype=x.dtype)\n        np.log(np.real(x), out=real_out)\n        np.log(np.imag(x), out=imag_out)\n        np.subtract(real_out, imag_out, out=out)\n    elif complex_mode == \"magnitude\":\n        np.log(np.abs(x), out=out)\n    elif complex_mode == \"jax\":\n        out = jax.numpy.log(x)\n    else:\n        raise ValueError(\"complex_mode must be'split','magnitude', or 'jax'\")\n    if eps is not None:\n        out = np.clip(out, np.log(eps), np.log(1 - eps))\n    return out\n\n", "CodeBleu_score": 40.76630585576624}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    if eps is None:\n        eps = 0.0\n    if complex_mode == \"split\":\n        real_x = np.real(x)\n        imag_x = np.imag(x)\n        real_out = np.log(real_x / (1.0 - real_x))\n        imag_out = np.arctan2(imag_x, real_x)\n        if out is None:\n            return real_out + 1j * imag_out\n        else:\n            out[...] = real_out + 1j * imag_out\n            return out\n    elif complex_mode == \"magnitude\":\n        x_abs = np.abs(x)\n        x_phase = np.angle(x)\n        if out is None:\n            return np.log(x_abs / (1.0 - x_abs)) + 1j * x_phase\n        else:\n            out[...] = np.log(x_abs / (1.0 - x_abs)) + 1j * x_phase\n            return out\n    elif complex_mode == \"jax\":\n        x = np.asarray(x)\n        if out is None:\n            return np.where(\n                (x >= eps) & (x <= 1.0 - eps), np.log(x / (1.0 - x)), np.nan\n            )\n        else:\n            out[...] = np.where(\n                (x >= eps) & (x <= 1.0 - eps), np.log(x / (1.0 - x)), np.nan\n            )\n            return out\n    else:\n        raise ValueError(\"Invalid value for complex_mode\")\n\n", "CodeBleu_score": 37.9881186981003}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    if eps is None:\n        eps = np.finfo(x.dtype).eps\n    if complex_mode == \"split\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"magnitude\":\n        return np.log(np.abs(x)) + 1j * np.angle(x)\n    elif complex_mode == \"jax\":\n        return jax.numpy.log(x)\n    else:\n        raise ValueError(f\"Invalid complex_mode: {complex_mode}\")\n    return out\n\n", "CodeBleu_score": 38.140153738827024}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty_like(x)\n    out[:] = x\n    out[np.abs(x) <= lambd] = 0\n    return out\n\n", "CodeBleu_score": 31.036842294970153}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty_like(x)\n    np.copyto(out, x)\n    out[np.abs(x) <= lambd] = 0\n    return out\n\n", "CodeBleu_score": 33.16296714652428}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty_like(x)\n    np.subtract(x, lambd, out=out, where=x > lambd)\n    np.subtract(x, -lambd, out=out, where=x < -lambd)\n    return out\n\n", "CodeBleu_score": 41.48635404689573}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        out.resize(window_length, refcheck=False)\n    n = np.arange(window_length)\n    n = np.where(n < window_length // 2, n, window_length - n - 1)\n    out[:] = np.sin(np.pi / 2 * np.sin(np.pi * n / (window_length - 1))**2)\n    return out\n\n", "CodeBleu_score": 40.61041839797429}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.shape == (window_length,)\n        assert out.dtype == dtype\n\n    for n in range(window_length):\n        out[n] = np.sin(np.pi * n / (window_length - 1)) * np.sin(np.pi * n / window_length)\n\n    return out\n\n", "CodeBleu_score": 40.53816011744757}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.shape == (window_length,)\n        assert out.dtype == dtype\n    out[:] = np.sin(0.5 * np.pi * np.arange(window_length) / (window_length - 1))\n    return out\n\n", "CodeBleu_score": 36.973774988243875}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    np.set_printoptions(precision=50)\n    return np.array(np.polyval(coeffs, x), dtype=np.promote_types(coeffs.dtype, x.dtype))\n\n", "CodeBleu_score": 21.513627715181105}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    np.set_printoptions(precision=15, suppress=True)\n    dtype = np.promote_types(coeffs.dtype, x.dtype)\n    return np.polyval(coeffs.astype(dtype), x.astype(dtype)).astype(dtype)\n\n", "CodeBleu_score": 27.639719157952108}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    np.set_printoptions(precision=10)\n    coeffs = np.array(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    x = np.array(x, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    return np.polyval(coeffs, x).astype(coeffs.dtype)", "CodeBleu_score": 31.313117097343156}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    result = np.amax(x, axis=axis, keepdims=keepdims, out=out)\n    if np.isscalar(result):\n        result = np.array(result)\n    return result\n\n", "CodeBleu_score": 59.38081075633861}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 26.902649231113717}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    result = np.amax(x, axis=axis, keepdims=keepdims, out=out)\n    if np.isscalar(result):\n        result = np.array(result)\n    return result\n\n", "CodeBleu_score": 59.38081075633861}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if not np.issubdtype(x1.dtype, np.floating):\n        x1 = x1.astype(np.float64)\n        x2 = x2.astype(np.float64)\n    if out is not None:\n        out = out.astype(np.float64)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 56.361712421290974}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    if x1.dtype.kind not in \"fc\":\n        x1 = x1.astype(np.float64)\n        x2 = x2.astype(np.float64)\n    if out is None:\n        out = np.copysign(x1, x2)\n    else:\n        out = np.copysign(x1, x2, out=out)\n    return out\n\n", "CodeBleu_score": 46.274114797101404}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 21.98690428214215}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray    if isinstance(axis, int):\n        axis = (axis,)\n    if out is not None:\n        np.count_nonzero(a, axis=axis, keepdims=keepdims, out=out)\n    else:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims)\n    if dtype is not None:\n        out = out.astype(dtype)\n    if out.size == 1:\n        out = np.array(out, dtype=dtype)\n    return out\n\n", "CodeBleu_score": 63.55816148665928}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray    if isinstance(axis, int):\n        axis = (axis,)\n    return np.array(np.count_nonzero(a, axis=axis, keepdims=keepdims), dtype=dtype, out=out)\n\n", "CodeBleu_score": 46.6505827481903}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray    if axis is None:\n        return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    else:\n        if isinstance(axis, int):\n            axis = (axis,)\n        return np.array(\n            [np.count_nonzero(a, axis=ax, keepdims=keepdims, dtype=dtype, out=out) for ax in axis],\n            dtype=dtype,\n        )\n\n", "CodeBleu_score": 50.78736730484604}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n    if isinstance(spacing, (list, tuple)):\n        if len(spacing)!= x.ndim:\n            raise ValueError(\"Spacing must be a single value or a list/tuple of length equal to the number of dimensions of the input array\")\n        if any(not isinstance(s, int) for s in spacing):\n            raise TypeError(\"Spacing must be an int or a list/tuple of ints\")\n        if any(s <= 0 for s in spacing):\n            raise ValueError(\"Spacing must be positive\")\n        return [gradient(x, spacing=s, axis=a, edge_order=edge_order) for a, s in zip(range(x.ndim), spacing)]\n    elif isinstance(spacing, int):\n        if spacing <= 0:\n            raise ValueError(\"Spacing must be positive\")\n        if axis is None:\n            return [gradient(x, spacing=spacing, axis=a, edge_order=edge_order) for a in range(x.ndim)]\n        if not isinstance(axis, int):\n            raise TypeError(\"Axis must be an int\")\n        if axis < 0:\n            axis += x.ndim\n        if axis >= x.ndim:\n            raise ValueError(\"Axis is out of bounds\")\n        if edge_order not in (1, 2):\n            raise ValueError(\"Edge order must be 1 or 2\")\n        if edge_order == 1:\n            return np.gradient(x, spacing, axis=axis)\n        if edge_order == 2:\n            return np.gradient(np.gradient(x, spacing, axis=axis), spacing, axis=axis)\n    else:\n        raise TypeError(\"Spacing must be an int or a list/tuple of ints\")\n\n", "CodeBleu_score": 49.21533686682165}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if isinstance(spacing, (list, tuple)):\n        return [\n            gradient(x, spacing=s, axis=axis, edge_order=edge_order)\n            for s in spacing\n        ]\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 58.12344028393718}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if isinstance(spacing, (list, tuple)):\n        spacing = tuple(spacing)\n        if len(spacing)!= x.ndim:\n            raise ValueError(\n                \"Spacing must be a single value or a sequence of values with the same length as the number of dimensions of the input array.\"\n            )\n    else:\n        spacing = (spacing,) * x.ndim\n    if axis is None:\n        axis = tuple(range(x.ndim))\n    elif isinstance(axis, int):\n        axis = (axis,)\n    else:\n        axis = tuple(axis)\n    if not all(isinstance(i, int) and 0 <= i < x.ndim for i in axis):\n        raise ValueError(\"Axis must be a valid axis or sequence of valid axes.\")\n    if not all(isinstance(i, int) and i > 0 for i in spacing):\n        raise ValueError(\"Spacing must be a positive integer or sequence of positive integers.\")\n    if not isinstance(edge_order, int) or edge_order < 1:\n        raise ValueError(\"Edge order must be a positive integer.\")\n    if edge_order > 8:\n        raise ValueError(\"Edge order must be less than or equal to 8.\")\n    if edge_order > x.ndim:\n        raise ValueError(\"Edge order must be less than or equal to the number of dimensions.\")\n    if edge_order > len(spacing):\n        raise ValueError(\"Edge order must be less than or equal to the length of the spacing.\")\n    grad = []\n    for i in axis:\n        if spacing[i] == 1:\n            grad.append(np.gradient(x, axis=i, edge_order=edge_order))\n        else:\n            grad.append(np.gradient(x, spacing[i], axis=i, edge_order=edge_order))\n    if len(grad) == 1:\n        return grad[0]\n    return grad\n\n", "CodeBleu_score": 43.29820268743807}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty(x.shape, dtype=np.promote_types(x.dtype, y.dtype))\n    if np.all(x == 0):\n        return out.fill(0.0)\n    np.log(y, out=out)\n    np.multiply(x, out, out=out)\n    return out\n\n", "CodeBleu_score": 43.03983177606436}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if np.all(x == 0):\n        return 0.0\n    return np.log(y) * x\n\n", "CodeBleu_score": 40.14953641256599}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty(x.shape, dtype=np.promote_types(x.dtype, y.dtype))\n    if np.all(x == 0):\n        out.fill(0.0)\n    else:\n        np.log(y, out=out)\n        np.multiply(x, out, out=out)\n    return out\n\n", "CodeBleu_score": 42.41899970120646}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    # Determine if depth pooling is required\n    kernel, strides, depth_pooling = _get_depth_pooling_params(kernel, strides, dims, data_format)\n\n    # If depth pooling is required, transpose the input tensor\n    if depth_pooling:\n        x = tf.transpose(x, perm=_get_transpose_for_depth_pooling(x, data_format))\n\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 44.02800282011014}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    kernel, strides, need_pooling = _get_kernel_strides_pooling_flag(kernel, strides, dims, data_format)\n    if need_pooling:\n        x = tf.transpose(x, [0, 3, 1, 2])\n    return x, kernel, strides, need_pooling\n", "CodeBleu_score": 29.704908683359665}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    if data_format == \"channel_last\":\n        dims = [1, 2]\n    else:\n        dims = [2, 3]\n    kernel_size, strides, depth_pooling_required = _get_updated_kernel_strides_depth_pooling(\n        x, kernel, strides, dims\n    )\n    if depth_pooling_required:\n        if data_format == \"channel_last\":\n            x = tf.transpose(x, perm=[0, 3, 1, 2])\n        else:\n            x = tf.transpose(x, perm=[0, 2, 3, 1])\n    return x, kernel_size, strides, depth_pooling_required\n\n", "CodeBleu_score": 36.939356158889545}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(prob, float):\n        raise TypeError(f\"prob must be a float, not {type(prob)}\")\n    if not 0 <= prob <= 1:\n        raise ValueError(f\"prob must be between 0 and 1, not {prob}\")\n    if not isinstance(training, bool):\n        raise TypeError(f\"training must be a bool, not {type(training)}\")\n    if not isinstance(data_format, str):\n        raise TypeError(f\"data_format must be a str, not {type(data_format)}\")\n    if not data_format in (\"NWC\", \"NCW\"):\n        raise ValueError(f\"data_format must be 'NWC' or 'NCW', not {data_format}\")\n    if not isinstance(out, (type(None), np.ndarray)):\n        raise TypeError(f\"out must be a np.ndarray or None, not {type(out)}\")\n    if not out is None:\n        if out.dtype!= x.dtype:\n            raise ValueError(f\"out must have the same dtype as x, not {out.dtype}\")\n        if out.shape!= x.shape:\n            raise ValueError(f\"out must have the same shape as x, not {out.shape}\")\n    if training:\n        if data_format == \"NWC\":\n            x = x.transpose((0, 2, 1))\n        shape = x.shape\n        if shape[0] == 1:\n            mask = np.random.binomial(1, 1 - prob, shape[1:])\n        else:\n            mask = np.random.binomial(1, 1 - prob, shape)\n        if out is None:\n            out = np.where(mask, x * (1 / (1 - prob)), np.zeros_like(x))\n        else:\n            np.where(mask, x * (1 / (1 - prob)), np.zeros_like(x), out=out)\n        if data_format == \"NWC\":\n            out = out.transpose((0, 2, 1))\n    else:\n        if out is None:\n            out = x\n        else:\n            np.copyto(out, x)\n    return out\n\n", "CodeBleu_score": 44.51626439514507}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a NumPy array\")\n    if not isinstance(prob, (int, float)):\n        raise TypeError(\"prob must be a number\")\n    if not isinstance(training, bool):\n        raise TypeError(\"training must be a boolean\")\n    if not isinstance(data_format, str):\n        raise TypeError(\"data_format must be a string\")\n    if not isinstance(out, (np.ndarray, type(None))):\n        raise TypeError(\"out must be a NumPy array or None\")\n    if not (0 <= prob <= 1):\n        raise ValueError(\"prob must be between 0 and 1\")\n    if not data_format in [\"NWC\", \"NCW\"]:\n        raise ValueError(\"data_format must be 'NWC' or 'NCW'\")\n    if out is not None and not isinstance(out, np.ndarray):\n        raise TypeError(\"out must be a NumPy array\")\n    if out is not None and not out.shape == x.shape:\n        raise ValueError(\"out must have the same shape as x\")\n    if out is not None and not out.dtype == x.dtype:\n        raise ValueError(\"out must have the same dtype as x\")\n    if out is None:\n        out = np.empty_like(x)\n    if data_format == \"NCW\":\n        x = x.transpose((0, 2, 1))\n    if training:\n        if x.ndim == 3:\n            n, c, w = x.shape\n            mask = np.random.binomial(1, 1 - prob, (n, c, w))\n            out[:] = x * mask / (1 - prob)\n        elif x.ndim == 2:\n            n, w = x.shape\n            mask = np.random.binomial(1, 1 - prob, (n, w))\n            out[:] = x * mask / (1 - prob)\n        else:\n            raise ValueError(\"x must have 2 or 3 dimensions\")\n    else:\n        out[:] = x\n    if data_format == \"NCW\":\n        out = out.transpose((0, 2, 1))\n    return out\n\n", "CodeBleu_score": 41.9913317494335}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a NumPy array.\")\n    if not isinstance(prob, float):\n        raise TypeError(\"prob must be a float.\")\n    if not 0.0 <= prob <= 1.0:\n        raise ValueError(\"prob must be between 0.0 and 1.0.\")\n    if not isinstance(training, bool):\n        raise TypeError(\"training must be a bool.\")\n    if not isinstance(data_format, str):\n        raise TypeError(\"data_format must be a str.\")\n    if data_format not in (\"NWC\", \"NCW\"):\n        raise ValueError(\"data_format must be 'NWC' or 'NCW'.\")\n    if out is not None and not isinstance(out, np.ndarray):\n        raise TypeError(\"out must be a NumPy array.\")\n    if out is not None and out.shape!= x.shape:\n        raise ValueError(\"out must have the same shape as x.\")\n    if out is None:\n        out = np.empty_like(x)\n    if training:\n        if data_format == \"NWC\":\n            x = x.transpose(1, 0, 2)\n        if x.ndim == 2:\n            mask = np.random.binomial(1, 1 - prob, size=x.shape[1])\n            out = x * mask / (1 - prob)\n        elif x.ndim == 3:\n            mask = np.random.binomial(1, 1 - prob, size=x.shape[2])\n            out = x * mask / (1 - prob)\n        else:\n            raise ValueError(\"x must have 2 or 3 dimensions.\")\n        if data_format == \"NWC\":\n            out = out.transpose(1, 0, 2)\n    else:\n        out = x\n    return out\n\n", "CodeBleu_score": 41.76733585951921}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    if x.ndim == 0:\n        raise ValueError(\"cannot select elements from a 0-dimensional array\")\n    if axis >= x.ndim or axis < -x.ndim:\n        raise ValueError(f\"invalid axis {axis} for array of dimension {x.ndim}\")\n    if k < 1:\n        raise ValueError(f\"invalid k {k}\")\n    if k > x.shape[axis]:\n        raise ValueError(f\"k {k} exceeds size of axis {axis} {x.shape[axis]}\")\n    if out is not None and (\n        len(out)!= 2 or out[0].shape!= out[1].shape!= x.shape\n    ):\n        raise ValueError(\"invalid out\")\n    if out is None:\n        out = (np.empty_like(x), np.empty_like(x, dtype=np.intp))\n    if largest:\n        idx = np.argpartition(x, -k, axis=axis)[..., -k:]\n    else:\n        idx = np.argpartition(x, k - 1, axis=axis)[..., :k]\n    if sorted:\n        idx = idx.take(np.argsort(x.take(idx, axis=axis), axis=axis), axis=axis)\n    out[0].flat = x.take(idx, axis=axis).flat\n    out[1].flat = idx.flat\n    return out\n\n", "CodeBleu_score": 39.4219050487605}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a NumPy array.\")\n    if not isinstance(k, int):\n        raise TypeError(\"k must be an integer.\")\n    if not isinstance(axis, int):\n        raise TypeError(\"axis must be an integer.\")\n    if not isinstance(largest, bool):\n        raise TypeError(\"largest must be a boolean.\")\n    if not isinstance(sorted, bool):\n        raise TypeError(\"sorted must be a boolean.\")\n    if out is not None and not isinstance(out, tuple):\n        raise TypeError(\"out must be a tuple.\")\n    if out is not None and len(out)!= 2:\n        raise ValueError(\"out must be a tuple of length 2.\")\n    if out is not None and not all(isinstance(o, np.ndarray) for o in out):\n        raise TypeError(\"out must be a tuple of NumPy arrays.\")\n    if out is not None and not all(o.ndim == 1 for o in out):\n        raise ValueError(\"out must be a tuple of 1-dimensional NumPy arrays.\")\n    if out is not None and not all(o.dtype == np.int64 for o in out):\n        raise TypeError(\"out must be a tuple of NumPy arrays of dtype int64.\")\n    if out is not None and not all(o.shape == (k,) for o in out):\n        raise ValueError(f\"out must be a tuple of NumPy arrays of shape ({k},).\")\n    if axis < -x.ndim or axis >= x.ndim:\n        raise ValueError(f\"axis must be in the range [-{x.ndim}, {x.ndim}).\")\n    if k < 0:\n        raise ValueError(\"k must be non-negative.\")\n    if k > x.shape[axis]:\n        raise ValueError(\"k must be less than or equal to the size of the specified axis.\")\n    if out is not None and not all(o.shape == x.shape for o in out):\n        raise ValueError(\"out must be a tuple of NumPy arrays with the same shape as x.\")\n    if out is not None and not all(o.dtype == x.dtype for o in out):\n        raise TypeError(\"out must be a tuple of NumPy arrays with the same dtype as x.\")\n    if out is not None:\n        out[0][:] = x\n        out[1][:] = np.arange(x", "CodeBleu_score": 24.547596421404133}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    # Find the indices of the top k elements along the specified axis\n    if axis < 0:\n        axis += x.ndim\n    if axis < 0 or axis >= x.ndim:\n        raise ValueError(\"Invalid axis\")\n    if k <= 0:\n        raise ValueError(\"Invalid k\")\n    if k > x.shape[axis]:\n        raise ValueError(\"k is larger than the size of the specified axis\")\n\n    if largest:\n        # Find the indices of the top k largest elements along the specified axis\n        idx = np.argpartition(x, -k, axis=axis)[..., -k:]\n        if sorted:\n            idx = np.take_along_axis(idx, np.argsort(x[idx], axis=axis), axis=axis)\n    else:\n        # Find the indices of the top k smallest elements along the specified axis\n        idx = np.argpartition(x, k - 1, axis=axis)[..., :k]\n        if sorted:\n            idx = np.take_along_axis(idx, np.argsort(x[idx], axis=axis), axis=axis)\n\n    if out is None:\n        # Extract the top k elements along the specified axis\n        out = np.take_along_axis(x, idx, axis=axis)\n        return out, idx\n    else:\n        # Store the top k elements along the specified axis in the provided output\n        out[0] = np.take_along_axis(x, idx, axis=axis)\n        out[1] = idx\n        return out\n\n", "CodeBleu_score": 40.6032724277657}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    norm = np.sum(np.abs(x.reshape(-1))) if axis is None else np.sum(np.abs(x), axis=axis)\n    denorm = np.full_like(x, norm) if axis is None else norm / (np.abs(x) + EPSILON)\n    return np.divide(x, denorm, out=out)\n\n", "CodeBleu_score": 39.4249543198335}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        norm = np.sum(np.abs(x.reshape(-1)))\n    else:\n        norm = np.sum(np.abs(x), axis=axis)\n    if out is None:\n        out = np.divide(x, norm + EPSILON)\n    else:\n        np.divide(x, norm + EPSILON, out=out)\n    return out\n\n", "CodeBleu_score": 53.632420790688975}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    norm = np.sum(np.abs(x.reshape(-1))) if axis is None else np.sum(np.abs(x), axis=axis)\n    denorm = norm if axis is None else norm[..., np.newaxis]\n    return np.divide(x, denorm + EPS, out=out)\n\n", "CodeBleu_score": 36.6497784848261}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return _random_generator.beta(alpha, beta, shape=shape, device=device, dtype=dtype, seed=seed, out=out)\n\n", "CodeBleu_score": 42.554989694801016}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not isinstance(alpha, (float, np.ndarray)):\n        raise TypeError(f'alpha must be of type float or np.ndarray, got {type(alpha)}')\n    if not isinstance(beta, (float, np.ndarray)):\n        raise TypeError(f'beta must be of type float or np.ndarray, got {type(beta)}')\n    if not isinstance(shape, (ivy.NativeShape, Sequence, type(None))):\n        raise TypeError(f'shape must be of type ivy.NativeShape, Sequence[int] or None, got {type(shape)}')\n    if not isinstance(device, (str, type(None))):\n        raise TypeError(f'device must be of type str or None, got {type(device)}')\n    if not isinstance(dtype, (np.dtype, type(None))):\n        raise TypeError(f'dtype must be of type np.dtype or None, got {type(dtype)}')\n    if not isinstance(seed, (int, type(None))):\n        raise TypeError(f'seed must be of type int or None, got {type(seed)}')\n    if not isinstance(out, (np.ndarray, type(None))):\n        raise TypeError(f'out must be of type np.ndarray or None, got {type(out)}')\n    if not isinstance(alpha, np.ndarray):\n        alpha = np.array(alpha)\n    if not isinstance(beta, np.ndarray):\n        beta = np.array(beta)\n    if shape is not None:\n        shape = ivy.shape_to_tuple(shape)\n    if out is None:\n        out = np.random.beta(alpha, beta, shape)\n    else:\n        out = np.random.beta(alpha, beta, shape, out=out)\n    if seed is not None:\n        np.random.seed(seed)\n    if dtype is not None:\n        out = out.astype(dtype)\n    return out\n\n", "CodeBleu_score": 57.75109332034491}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if seed is not None:\n        ivy.set_seed(seed)\n    if shape is None:\n        shape = (1,)\n    if out is None:\n        out = ivy.array(np.random.beta(alpha, beta, shape), dtype=dtype, device=device)\n    else:\n        ivy.array(np.random.beta(alpha, beta, shape), dtype=dtype, device=device, out=out)\n    return out\n\n", "CodeBleu_score": 60.86293620381902}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    inverse = np.argsort(np.argsort(x))\n    return inverse\n\n", "CodeBleu_score": 24.240990182200417}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    inverse = np.argsort(np.argsort(x))\n    return inverse\n\n", "CodeBleu_score": 24.240990182200417}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    inverse = np.empty(len(x), int)\n    inverse[np.argsort(x)] = np.arange(len(x))\n    return inverse\n\n", "CodeBleu_score": 34.23132745679681}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\n    sns.set_style(\"white\")\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    fig.suptitle(\"Medical Charts\")\n\n    # Total vomit per month\n    total_vomit_per_month = df.groupby(\"Month\")[\"Vomit\"].sum()\n    axes[0, 0].plot(total_vomit_per_month.index, total_vomit_per_month.values)\n    axes[0, 0].set_title(\"Total Vomit per Month\")\n    axes[0, 0].set_xlabel(\"Month\")\n    axes[0, 0].set_ylabel(\"Total Vomit\")\n\n    # Days between vomit\n    days_between_vomit = df[df[\"Vomit\"] == 1][\"Days Between Vomit\"]\n    axes[0, 1].hist(days_between_vomit, bins=10, edgecolor=\"black\")\n    axes[0, 1].set_title(\"Days Between Vomit\")\n    axes[0, 1].set_xlabel(\"Days\")\n    axes[0, 1].set_ylabel(\"Frequency\")\n\n    # Days in daycare\n    days_in_daycare = df[\"Days In Daycare\"]\n    axes[1, 0].hist(days_in_daycare, bins=10, edgecolor=\"black\")\n    axes[1, 0].set_title(\"Days in Daycare\")\n    axes[1, 0].set_xlabel(\"Days\")\n    axes[1, 0].set_ylabel(\"Frequency\")\n\n    # Doctor visits\n    doctor_visits = df[\"Doctor Visits\"]\n    axes[1, 1].hist(doctor_visits, bins=10, edgecolor=\"black\")\n    axes[1, 1].set_title(\"Doctor Visits\")\n    axes[1, 1].set_xlabel(\"Visits\")\n    axes[1, 1].set_ylabel(\"Frequency\")\n\n    plt.tight_layout()\n    plt.savefig(\"medical_charts.png\")\n\n", "CodeBleu_score": 22.46043602816396}, {"code": "def plot_medical_charts() -> None\n\n    # Set the style of the plot\n    sns.set_style(\"whitegrid\")\n\n    # Create a figure with subplots\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n    # Plot total vomit per month\n    vomit_per_month = df.groupby(df[\"date\"].dt.month)[\"vomit\"].sum()\n    axes[0, 0].bar(vomit_per_month.index, vomit_per_month.values)\n    axes[0, 0].set_xlabel(\"Month\")\n    axes[0, 0].set_ylabel(\"Total Vomit\")\n    axes[0, 0].set_title(\"Total Vomit per Month\")\n\n    # Plot days between vomit\n    days_between_vomit = (df[\"date\"] - df[\"date\"].shift()).dt.days\n    axes[0, 1].hist(days_between_vomit.dropna(), bins=30)\n    axes[0, 1].set_xlabel(\"Days\")\n    axes[0, 1].set_ylabel(\"Frequency\")\n    axes[0, 1].set_title(\"Days Between Vomit\")\n\n    # Plot days in daycare\n    days_in_daycare = (df[\"date_out\"] - df[\"date_in\"]).dt.days\n    axes[1, 0].hist(days_in_daycare.dropna(), bins=30)\n    axes[1, 0].set_xlabel(\"Days\")\n    axes[1, 0].set_ylabel(\"Frequency\")\n    axes[1, 0].set_title(\"Days in Daycare\")\n\n    # Plot doctor visits\n    doctor_visits = df.groupby(df[\"date\"].dt.month)[\"doctor_visit\"].sum()\n    axes[1, 1].bar(doctor_visits.index, doctor_visits.values)\n    axes[1, 1].set_xlabel(\"Month\")\n    axes[1, 1].set_ylabel(\"Doctor Visits\")\n    axes[1, 1].set_title(\"Doctor Visits per Month\")\n\n    # Adjust the layout of the subplots\n    plt.tight_layout()\n\n    # Export the figure to a specified file location\n    plt.savefig(\"medical_charts.png\")\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 22.972382292259393}, {"code": "def plot_medical_charts() -> None\n    # Set the style of the plots\n    plt.style.use('seaborn')\n\n    # Create a figure with subplots\n    fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n\n    # Plot total vomit per month\n    df_vomit = df.groupby(df['date'].dt.to_period('M'))['vomit'].sum().reset_index()\n    ax[0, 0].bar(df_vomit['date'].astype(str), df_vomit['vomit'], color='red')\n    ax[0, 0].set_title('Total Vomit per Month')\n    ax[0, 0].set_xlabel('Month')\n    ax[0, 0].set_ylabel('Vomit')\n    ax[0, 0].tick_params(axis='x', rotation=45)\n\n    # Plot days between vomit\n    df_vomit_days = df[df['vomit'] == 1].groupby(df['date'].dt.to_period('M'))['date'].apply(lambda x: x.diff().dt.days.mean()).reset_index()\n    ax[0, 1].bar(df_vomit_days['date'].astype(str), df_vomit_days['date'], color='blue')\n    ax[0, 1].set_title('Days Between Vomit')\n    ax[0, 1].set_xlabel('Month')\n    ax[0, 1].set_ylabel('Days')\n    ax[0, 1].tick_params(axis='x', rotation=45)\n\n    # Plot days in daycare\n    df_daycare = df[df['daycare'] == 1].groupby(df['date'].dt.to_period('M'))['date'].count().reset_index()\n    ax[1, 0].bar(df_daycare['date'].astype(str), df_daycare['date'], color='green')\n    ax[1, 0].set_title('Days in Daycare')\n    ax[1, 0].set_xlabel('Month')\n    ax[1, 0].set_ylabel('Days')\n    ax[1, 0].tick_params(axis='x', rotation=45)\n\n    # Plot doctor visits\n    df_doctor = df[", "CodeBleu_score": 25.005874213326155}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\n    # Import and extract feeding data\n    feeding_data = parse_raw_data()\n\n    # Set up plot with dark grid style\n    sns.set_style(\"darkgrid\")\n\n    # Compute offset from birthday based on last solid feeding date\n    offset = (feeding_data[\"solid_feeding_date\"].max() - feeding_data[\"birthday\"]).days\n\n    # Create scatter plot of feeding times, using red for bottle feedings and blue for solid feedings\n    plt.scatter(\n        feeding_data[\"bottle_feeding_time\"] + offset,\n        [0] * len(feeding_data),\n        c=\"red\",\n        label=\"Bottle Feeding\",\n    )\n    plt.scatter(\n        feeding_data[\"solid_feeding_time\"] + offset,\n        [0.5] * len(feeding_data),\n        c=\"blue\",\n        label=\"Solid Feeding\",\n    )\n\n    # Add legend to distinguish between bottle and solid feedings\n    plt.legend(loc=\"upper left\")\n\n    # Calculate end date for the plot\n    end_date = feeding_data[\"solid_feeding_date\"].max() + pd.Timedelta(days=1)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    plt.xlim(offset, offset + 7)\n    plt.ylim(-0.5, 1)\n    plt.xlabel(\"Time (24-hour)\")\n    plt.ylabel(\"Feeding Type\")\n    plt.xticks(\n        range(offset, offset + 7),\n        [\n            (end_date - pd.Timedelta(days=i)).strftime(\"%a, %b %d\")\n            for i in range(7)\n        ],\n    )\n    plt.yticks([0, 0.5], [\"Bottle\", \"Solid\"])\n\n    # Export figure\n    export_figure(plt, \"feeding_24h\")\n\n", "CodeBleu_score": 28.189947352456514}, {"code": "def plot_feeding_24h_viz() -> None\n\n    # Import and extract feeding data\n    feeding_data = parse_raw_data.parse_feeding_data()\n\n    # Set up plot\n    sns.set_style(\"darkgrid\")\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Compute offset from birthday\n    last_solid_feeding_date = feeding_data.loc[feeding_data[\"feeding_type\"] == \"solid\", \"date\"].max()\n    offset = (last_solid_feeding_date - parse_raw_data.parse_birthday()).days\n\n    # Plot feeding times\n    ax.scatter(\n        feeding_data.loc[feeding_data[\"feeding_type\"] == \"bottle\", \"time\"],\n        feeding_data.loc[feeding_data[\"feeding_type\"] == \"bottle\", \"date\"] - offset,\n        color=\"red\",\n        label=\"Bottle\",\n    )\n    ax.scatter(\n        feeding_data.loc[feeding_data[\"feeding_type\"] == \"solid\", \"time\"],\n        feeding_data.loc[feeding_data[\"feeding_type\"] == \"solid\", \"date\"] - offset,\n        color=\"blue\",\n        label=\"Solid\",\n    )\n    ax.legend()\n\n    # Set end date for plot\n    end_date = feeding_data[\"date\"].max() + pd.Timedelta(days=1)\n\n    # Format plot\n    ax.set_xlim(0, 24)\n    ax.set_ylim(0, end_date - offset)\n    ax.set_xlabel(\"Time (hours)\")\n    ax.set_ylabel(\"Date\")\n    ax.set_title(\"24-Hour Feeding Data\")\n    ax.xaxis.set_major_locator(MultipleLocator(1))\n    ax.xaxis.set_minor_locator(MultipleLocator(0.5))\n    ax.yaxis.set_major_locator(mdates.DayLocator())\n    ax.yaxis.set_major_formatter(mdates.DateFormatter(\"%m/%d\"))\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=6)\n\n    # Export figure\n    export_figure(fig, \"feeding_24h_viz.png\")", "CodeBleu_score": 27.2404680202314}, {"code": "def plot_feeding_24h_viz() -> None\n    feeding_data = parse_raw_data(feeding_file)\n    fig, ax = plt.subplots(figsize=(12, 6))\n    sns.set_style(\"darkgrid\")\n    offset = (\n        feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"].max()\n        - feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"].min()\n    ).days\n    ax.scatter(\n        feeding_data[feeding_data[\"type\"] == \"bottle\"][\"date\"]\n        - feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"].min(),\n        feeding_data[feeding_data[\"type\"] == \"bottle\"][\"time\"],\n        color=\"red\",\n        label=\"Bottle\",\n    )\n    ax.scatter(\n        feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"]\n        - feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"].min(),\n        feeding_data[feeding_data[\"type\"] == \"solid\"][\"time\"],\n        color=\"blue\",\n        label=\"Solid\",\n    )\n    ax.legend()\n    end_date = feeding_data[feeding_data[\"type\"] == \"solid\"][\"date\"].max()\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n    ax.set_xlim(0, offset)\n    ax.set_ylim(0, 24)\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Time\")\n    ax.set_title(\"24-Hour Feeding Data\")\n    export_figure(fig, \"24h_feeding_data\")\n\n", "CodeBleu_score": 25.737958718277028}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        target_counts = self.get_target_counts(task)\n        sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax, **kwargs)\n        ax.set_xlabel(\"Target\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(\"Target Counts\")\n    else:\n        raise ValueError(f\"Invalid task: {task}\")\n", "CodeBleu_score": 40.05758437590149}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        counts = self.target_counts\n    elif task == ClassificationTask.Label:\n        counts = self.label_counts\n    else:\n        raise ValueError(f\"Invalid task: {task}\")\n    sns.barplot(x=counts.index, y=counts, ax=ax, **kwargs)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"{task} Counts\")\n\n", "CodeBleu_score": 33.45380163014427}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        counts = self.target_counts\n    elif task == ClassificationTask.Label:\n        counts = self.label_counts\n    else:\n        raise ValueError(f\"Unsupported task: {task}\")\n    sns.barplot(x=counts.index, y=counts, ax=ax, **kwargs)\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"{task.name} Counts\")\n    ax.tick_params(axis=\"x\", rotation=45)\n    return ax", "CodeBleu_score": 33.54917203313694}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n    from PIL import Image\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            rgb_image = np.frombuffer(canvas.tostring_rgb(), dtype=\"uint8\")\n            rgb_image = rgb_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            images.append(rgb_image)\n        images = np.stack(images)\n        return images\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        rgb_image = np.frombuffer(canvas.tostring_rgb(), dtype=\"uint8\")\n        rgb_image = rgb_image.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n        return rgb_image\n\n", "CodeBleu_score": 39.3442286241855}, {"code": "def render_to_rgb(figure)\n    # Create a canvas from the figure using plt_backend_agg.FigureCanvasAgg\n    canvas = FigureCanvasAgg(figure)\n    # Draw the figure on the canvas\n    canvas.draw()\n    # Get the RGBA buffer data from the canvas\n    buffer = canvas.buffer_rgba()\n    # Convert the buffer data to a NumPy array and reshape it into an image with width and height dimensions and 3 color channels (RGB)\n    image = np.frombuffer(buffer, dtype=np.uint8).reshape(figure.canvas.get_width_height()[::-1] + (4,))[:, :, :3]\n    # Optionally close the figure\n    if not plt.isinteractive():\n        plt.close(figure)\n    # Return the image in CHW format (channels, height, width)\n    return image.transpose(2, 0, 1)\n\n", "CodeBleu_score": 20.512753388060407}, {"code": "def render_to_rgb(figure)\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    rgb_data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n    width, height = figure.canvas.get_width_height()\n    image = rgb_data.reshape(height, width, 3)\n    if figure.get_figheight() == 1:\n        image = image.transpose(2, 0, 1)\n    if close:\n        plt.close(figure)\n    return image\n\n", "CodeBleu_score": 28.639487675164947}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n    log_dir = 'beholder_pytorch_example'\n    beholder = Beholder(log_dir)\n    for i in range(1000):\n        tensor_list = []\n        name_list = []\n        for j in range(2):\n            tensor = torch.rand(100, 100)\n            name = 'tensor_' + str(j)\n            tensor_list.append(tensor)\n            name_list.append(name)\n        beholder.update(tensor_list, name_list, i)\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 30.78648549417679}, {"code": "def beholder_pytorch()\n    log_dir = \"logs/beholder_pytorch\"\n    beholder = Beholder(log_dir)\n    for i in range(1000):\n        tensors = [torch.randn(3, 4, 5), torch.randn(3, 4, 5)]\n        names = [\"tensor1\", \"tensor2\"]\n        beholder.update(tensors, names, i)\n        time.sleep(0.001)\n        print(i)\n\n", "CodeBleu_score": 23.873480879729087}, {"code": "def beholder_pytorch()\n    log_dir = os.path.join(tempfile.gettempdir(), \"beholder_pytorch\")\n    beholder = Beholder(log_dir)\n\n    for i in range(1000):\n        frame = i\n        tensor_list = [\n            torch.rand(3, 4),\n            torch.rand(3, 4, 5),\n            torch.rand(3, 4, 5, 6),\n            torch.rand(3, 4, 5, 6, 7),\n            torch.rand(3, 4, 5, 6, 7, 8),\n        ]\n        name_list = [\"3x4\", \"3x4x5\", \"3x4x5x6\", \"3x4x5x6x7\", \"3x4x5x6x7x8\"]\n        beholder.update(tensor_list, name_list, frame)\n        time.sleep(0.001)\n        print(i)\n\n", "CodeBleu_score": 28.857659966546873}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    # Extract tensor data from the summary and reshape it based on its dimensions\n    tensor = summary.value[0].tensor\n    dims = tensor.tensor_shape.dim\n    if len(dims) == 4:\n        tensor = np.reshape(tensor.float_val, (dims[0].size, dims[1].size, dims[2].size, dims[3].size))\n    elif len(dims) == 2:\n        tensor = np.reshape(tensor.float_val, (dims[0].size, dims[1].size))\n    else:\n        tensor = np.reshape(tensor.float_val, (dims[0].size,))\n    # Compute thresholds as a list of values from 0 to 1 divided by the number of thresholds\n    thresholds = np.linspace(0, 1, num_thresholds)\n    # Split the tensor values into true positives (TP), false positives (FP), true negatives (TN), false negatives (FN), precision, and recall, flattening each into a list\n    tp = tensor[:, 0].flatten()\n    fp = tensor[:, 1].flatten()\n    tn = tensor[:, 2].flatten()\n    fn = tensor[:, 3].flatten()\n    precision = tensor[:, 4].flatten()\n    recall = tensor[:, 5].flatten()\n    # Store these values, along with the thresholds and tag, in a dictionary named pr_data\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"tag\": tag,\n    }\n    # Log the pr_data using the log_asset_data method with the provided tag and optional step\n    self.log_asset_data(pr_data, tag, step)\n", "CodeBleu_score": 37.48407049590199}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = summary.value[0].tensor.float_val\n    tensor_data = np.reshape(tensor_data, summary.value[0].tensor.tensor_shape.dim)\n    thresholds = np.linspace(0, 1, num_thresholds)\n    tp = tensor_data[:, 0]\n    fp = tensor_data[:, 1]\n    tn = tensor_data[:, 2]\n    fn = tensor_data[:, 3]\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    pr_data = {\n        \"thresholds\": thresholds.tolist(),\n        \"tp\": tp.tolist(),\n        \"fp\": fp.tolist(),\n        \"tn\": tn.tolist(),\n        \"fn\": fn.tolist(),\n        \"precision\": precision.tolist(),\n        \"recall\": recall.tolist(),\n    }\n    self.log_asset_data(tag, pr_data, step)\n", "CodeBleu_score": 26.946154680152322}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor = summary.value[0].tensor\n    tensor_data = tensor.float_val\n    tensor_shape = tensor.tensor_shape\n    tensor_dims = tensor_shape.dim\n    tensor_dims = [dim.size for dim in tensor_dims]\n    if len(tensor_dims) == 3:\n        tensor_data = np.reshape(tensor_data, tensor_dims)\n    else:\n        tensor_data = np.reshape(tensor_data, tensor_dims[1:])\n    thresholds = np.linspace(0, 1, num_thresholds)\n    tp, fp, tn, fn, precision, recall = [], [], [], [], [], []\n    for threshold in thresholds:\n        tp.append(np.sum(tensor_data[0, :, 0] >= threshold))\n        fp.append(np.sum(tensor_data[0, :, 1] >= threshold))\n        tn.append(np.sum(tensor_data[1, :, 0] >= threshold))\n        fn.append(np.sum(tensor_data[1, :, 1] >= threshold))\n        precision.append(tp[-1] / (tp[-1] + fp[-1]))\n        recall.append(tp[-1] / (tp[-1] + fn[-1]))\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(pr_data, tag, step)\n\n", "CodeBleu_score": 31.59150629455131}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n    with gzip.open(labels, 'rb') as lbpath:\n        lbpath.read(8)\n        buffer = lbpath.read(num)\n        labels = np.frombuffer(buffer, dtype=np.uint8)\n\n    with gzip.open(images, 'rb') as imgpath:\n        imgpath.read(16)\n        buffer = imgpath.read(num * 28 * 28)\n        images = np.frombuffer(buffer, dtype=np.uint8).reshape(num, 28, 28)\n\n    return images, labels\n", "CodeBleu_score": 38.030409117328766}, {"code": "def load_mnist(images, labels, num)\n    import gzip\n    import numpy as np\n\n    # Open the images and labels files\n    with gzip.open(labels, 'rb') as lbpath:\n        with gzip.open(images, 'rb') as imgpath:\n            # Read the magic number and the number of labels\n            lbpath.read(4)\n            num_labels = int.from_bytes(lbpath.read(4), 'big')\n            # Read the magic number and the number of images\n            imgpath.read(4)\n            num_images = int.from_bytes(imgpath.read(4), 'big')\n            # Read the number of rows and columns\n            rows = int.from_bytes(imgpath.read(4), 'big')\n            cols = int.from_bytes(imgpath.read(4), 'big')\n            # Initialize the arrays\n            data = np.zeros((num, rows * cols), dtype=np.uint8)\n            target = np.zeros((num,), dtype=np.uint8)\n            # Read the labels and images\n            for i in range(num):\n                if i % 10000 == 0:\n                    print('Loaded {}/{}'.format(i, num))\n                target[i] = int.from_bytes(lbpath.read(1), 'big')\n                for j in range(rows * cols):\n                    data[i, j] = int.from_bytes(imgpath.read(1), 'big')\n    return data, target\n", "CodeBleu_score": 42.109842081622844}, {"code": "def load_mnist(images, labels, num)\n    data = np.zeros((num, 28 * 28), dtype=np.float32)\n    target = np.zeros((num,), dtype=np.int64)\n    with gzip.open(labels, 'rb') as lbpath:\n        lbpath.read(8)\n        for i in range(num):\n            target[i] = ord(lbpath.read(1))\n    with gzip.open(images, 'rb') as imgpath:\n        imgpath.read(16)\n        for i in range(num):\n            data[i] = np.frombuffer(imgpath.read(28 * 28), dtype=np.uint8).astype(np.float32)\n    return data, target\n", "CodeBleu_score": 45.44564913556898}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.length = len(ldf)\n    for column in ldf.columns:\n        if pd.api.types.is_datetime64_any_dtype(ldf[column]):\n            self.unique_values[column.strftime(\"%Y-%m-%d\")] = list(ldf[column].unique())\n            self.cardinality[column.strftime(\"%Y-%m-%d\")] = len(self.unique_values[column.strftime(\"%Y-%m-%d\")])\n        elif pd.api.types.is_numeric_dtype(ldf[column]):\n            self.unique_values[column] = list(ldf[column].unique())\n            self.cardinality[column] = len(self.unique_values[column])\n            self.min_max[column] = (ldf[column].min(), ldf[column].max())\n        else:\n            self.unique_values[column] = list(ldf[column].unique())\n            self.cardinality[column] = len(self.unique_values[column])\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        self.unique_values[\"index\"] = list(ldf.index.unique())\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])\n", "CodeBleu_score": 38.82068752954978}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.length = len(ldf)\n    self.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    for column in ldf.columns:\n        if column == ldf.index.name:\n            continue\n        if pd.api.types.is_datetime64_any_dtype(ldf[column]):\n            self.unique_values[column] = ldf[column].astype(str).unique()\n            self.min_max[column] = (\n                ldf[column].astype(str).min(),\n                ldf[column].astype(str).max(),\n            )\n        else:\n            self.unique_values[column] = ldf[column].unique()\n            if pd.api.types.is_numeric_dtype(ldf[column]):\n                self.min_max[column] = (ldf[column].min(), ldf[column].max())\n        self.cardinality[column] = len(self.unique_values[column])\n    if not isinstance(ldf.index, pd.RangeIndex):\n        self.unique_values[ldf.index.name] = ldf.index.unique()\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n\n", "CodeBleu_score": 38.44517919275911}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.cardinality = {}\n    self.min_max = {}\n    self.unique_values = {}\n    self.length = len(ldf.data)\n    for attribute in ldf.columns:\n        if pd.api.types.is_datetime64_dtype(ldf.data[attribute]):\n            self.unique_values[attribute.to_string()] = ldf.data[attribute].unique()\n        elif pd.api_types.is_numeric_dtype(ldf.data[attribute]):\n            self.unique_values[attribute.to_string()] = ldf.data[attribute].unique()\n            self.min_max[attribute.to_string()] = [\n                ldf.data[attribute].min(),\n                ldf.data[attribute].max(),\n            ]\n        else:\n            self.unique_values[attribute.to_string()] = ldf.data[attribute].unique()\n        self.cardinality[attribute.to_string()] = len(self.unique_values[attribute.to_string()])\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        self.unique_values[\"index\"] = ldf.index.unique()\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])\n", "CodeBleu_score": 40.35438577778847}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    query = f\"SELECT * FROM {tbl.name} LIMIT {preview_size}\"\n    df = pd.read_sql(query, con=tbl.con)\n    return df\n\n", "CodeBleu_score": 19.27056278437402}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    # Check if the table is a LuxSQLTable object\n    if not isinstance(tbl, LuxSQLTable):\n        raise ValueError(\"tbl must be a LuxSQLTable object.\")\n\n    # Format the preview query with the table name and number of rows\n    query = f\"SELECT * FROM {tbl.name} LIMIT {preview_size}\"\n\n    # Execute the query and retrieve the data into a DataFrame\n    df = pd.read_sql(query, tbl.connection)\n\n    # Return the DataFrame\n    return df\n\n", "CodeBleu_score": 29.688716008666617}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    if tbl.connection is None:\n        raise Exception(\n            \"Connection to database not found. Please specify a connection in the constructor of the LuxSQLTable object.\"\n        )\n    query = f\"SELECT * FROM {tbl.name} LIMIT {preview_size}\"\n    df = pd.read_sql(query, tbl.connection)\n    return df\n\n", "CodeBleu_score": 28.593364283953004}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n    # retrieve sampling configuration parameters from lux.config\n    SAMPLE_FLAG = lux.config.get(\"SAMPLE_FLAG\")\n    SAMPLE_START = lux.config.get(\"SAMPLE_START\")\n    SAMPLE_CAP = lux.config.get(\"SAMPLE_CAP\")\n    SAMPLE_FRAC = lux.config.get(\"SAMPLE_FRAC\")\n    # execute a SQL query to determine the length of the table\n    tbl.execute_query(f\"SELECT COUNT(*) FROM {tbl.name}\")\n    # calculate a limit based on a fraction of the table length\n    limit = min(\n        SAMPLE_CAP,\n        max(\n            SAMPLE_START,\n            int(tbl.df.iloc[0, 0] * SAMPLE_FRAC),\n        ),\n    )\n    # format and execute a sample query to retrieve a sample of rows from the table\n    tbl.execute_query(\n        f\"SELECT * FROM {tbl.name} ORDER BY RANDOM() LIMIT {limit}\"\n    )\n    # store the sampled rows in the _sampled attribute of the tbl object\n    tbl._sampled = tbl.df.copy()\n\n", "CodeBleu_score": 27.483849886443913}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    if lux.config.SAMPLE_FLAG:\n        if lux.config.SAMPLE_START == 0:\n            lux.config.SAMPLE_START = 1\n        if lux.config.SAMPLE_CAP == 0:\n            lux.config.SAMPLE_CAP = 1000000\n        if lux.config.SAMPLE_FRAC == 0:\n            lux.config.SAMPLE_FRAC = 1\n        try:\n            length = tbl.length\n            if length > lux.config.SAMPLE_CAP:\n                limit = lux.config.SAMPLE_START + int(\n                    lux.config.SAMPLE_FRAC * (length - lux.config.SAMPLE_START)\n                )\n                if limit > lux.config.SAMPLE_CAP:\n                    limit = lux.config.SAMPLE_CAP\n            else:\n                limit = lux.config.SAMPLE_CAP\n            query = f\"SELECT * FROM {tbl.name} LIMIT {limit}\"\n            tbl._sampled = pd.read_sql(query, tbl.connection)\n            tbl._sampled.columns = tbl._sampled.columns.str.lower()\n        except Exception as e:\n            print(e)\n            print(f\"Failed to sample {tbl.name}\")\n\n", "CodeBleu_score": 27.688006946471333}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    SAMPLE_FLAG = lux.config.get(\"SAMPLE_FLAG\", False)\n    SAMPLE_START = lux.config.get(\"SAMPLE_START\", 0)\n    SAMPLE_CAP = lux.config.get(\"SAMPLE_CAP\", 1000000)\n    SAMPLE_FRAC = lux.config.get(\"SAMPLE_FRAC\", 0.01)\n\n    # Get the length of the table\n    tbl.execute_query(f\"SELECT COUNT(*) FROM {tbl.name}\")\n    tbl_len = tbl.result[0][0]\n\n    # Calculate the limit based on the fraction of the table length\n    limit = min(SAMPLE_CAP, max(SAMPLE_START, int(tbl_len * SAMPLE_FRAC)))\n\n    # Format and execute the sample query\n    query = f\"SELECT * FROM {tbl.name} LIMIT {limit}\"\n    tbl.execute_query(query)\n\n    # Store the sampled rows in the _sampled attribute\n    tbl._sampled = tbl.result\n\n", "CodeBleu_score": 27.50264805972159}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    if 'text' in params:\n        text = params['text']\n        x = params['x']\n        y = params['y']\n        del params['text']\n        del params['x']\n        del params['y']\n        params = {**text_defaults, **params}\n        bounds = background.bounds\n        x = np.interp(x, [0, 1], [bounds[0], bounds[2]])\n        y = np.interp(y, [0, 1], [bounds[1], bounds[3]])\n        plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 28.296196776881427}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    text = params['text']\n    x = params['x']\n    y = params['y']\n    # Override default text settings with provided parameters\n    params = {**DEFAULT_TEXT_PARAMS, **params}\n    # Extract the text, x, and y values\n    text = params['text']\n    x = params['x']\n    y = params['y']\n    # Retrieve the background's bounds\n    bounds = background.bounds\n    # Interpolate the x and y values to fit within these bounds\n    x = np.interp(x, (0, 1), (bounds[0], bounds[2]))\n    y = np.interp(y, (0, 1), (bounds[1], bounds[3]))\n    # Use the plt.text method to draw the text at the calculated position with the remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 18.30796582998864}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\n    # Extract the text, x, and y values from the params dictionary\n    text = params[\"text\"]\n    x = params[\"x\"]\n    y = params[\"y\"]\n\n    # Get the bounds of the background\n    bounds = background.bounds\n\n    # Interpolate the x and y values to fit within the bounds\n    x = bounds[0] + (bounds[2] - bounds[0]) * x\n    y = bounds[1] + (bounds[3] - bounds[1]) * y\n\n    # Override default text settings with provided parameters\n    params = {**DEFAULT_TEXT_PARAMS, **params}\n\n    # Draw the text at the calculated position with the remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 15.667833812493653}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n    # determine the average number of days between consecutive dates\n    if len(dates) > 1:\n        avg_days = sum( (dates[i+1]-dates[i]).days for i in range(len(dates)-1) ) / (len(dates)-1)\n    else:\n        avg_days = 1\n    # if datetime_format is provided, return this format\n    if datetime_format is not None:\n        return datetime_format\n    # if the average days between points is less than 0.33, indicating intraday data\n    if avg_days < 0.33:\n        # check if the data spans more than one day\n        if max(dates) - min(dates) > datetime.timedelta(days=1):\n            # set the format string to '%b %d, %H:%M'\n            return '%b %d, %H:%M'\n        else:\n            # set the format string to '%H:%M'\n            return '%H:%M'\n    # for daily or less frequent data\n    else:\n        # check if the data spans multiple years\n        if max(dates).year - min(dates).year > 0:\n            # set the format string to '%Y-%b-%d'\n            return '%Y-%b-%d'\n        else:\n            # set the format string to '%b %d'\n            return '%b %d'\n", "CodeBleu_score": 42.10490653198692}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    if datetime_format is None:\n        if len(dates) > 1:\n            avg_days = (dates[-1] - dates[0]).days / (len(dates) - 1)\n            if avg_days < 0.33:\n                if (dates[-1] - dates[0]).days > 1:\n                    datetime_format = '%b %d, %H:%M'\n                else:\n                    datetime_format = '%H:%M'\n            else:\n                if (dates[-1] - dates[0]).days > 365:\n                    datetime_format = '%Y-%b-%d'\n                else:\n                    datetime_format = '%b %d'\n        else:\n            datetime_format = '%Y-%b-%d'\n    return datetime_format\n", "CodeBleu_score": 47.677907617690266}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    if datetime_format is None:\n        # Calculate the average number of days between consecutive dates in the list\n        avg_days = np.mean(np.diff(dates)) / np.timedelta64(1, 'D')\n        # If the average days between points is less than 0.33, indicating intraday data\n        if avg_days < 0.33:\n            # Check if the data spans more than one day\n            if np.max(dates) - np.min(dates) > np.timedelta64(1, 'D'):\n                # Set the format string to '%b %d, %H:%M'\n                datetime_format = '%b %d, %H:%M'\n            else:\n                # Set the format string to '%H:%M'\n                datetime_format = '%H:%M'\n        # For daily or less frequent data\n        else:\n            # Check if the data spans multiple years\n            if np.max(dates) - np.min(dates) > np.timedelta64(365, 'D'):\n                # Set the format string to '%Y-%b-%d'\n                datetime_format = '%Y-%b-%d'\n            else:\n                # Set the format string to '%b %d'\n                datetime_format = '%b %d'\n    return datetime_format\n", "CodeBleu_score": 41.38347595475861}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n    if np.isscalar(c):\n        c = np.array([c])\n    if c.shape[0] == 4:\n        if c.dtype == np.uint8:\n            c = c / 255.0\n        elif c.dtype == np.uint16:\n            c = c / 65535.0\n        if alpha is not None:\n            c[3] = alpha\n        return tuple(c)\n    else:\n        return tuple(mpl.colors.to_rgba(c, alpha=alpha))\n\n", "CodeBleu_score": 21.968297768276127}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if c is None:\n        return None\n    if isinstance(c, str):\n        c = to_rgba(c)\n    if len(c) == 4:\n        if alpha is None:\n            alpha = c[3]\n        c = c[:3]\n    if isinstance(c, tuple):\n        c = np.array(c)\n    if c.dtype.kind == \"u\":\n        c = c.astype(float) / 255.0\n    if alpha is not None:\n        c = np.append(c, alpha)\n    return c\n\n", "CodeBleu_score": 35.7331369972507}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if alpha is None:\n        alpha = 1\n    if np.isscalar(c):\n        c = np.array([c])\n    if c.dtype == np.uint8:\n        c = c.astype(float) / 255\n    if c.ndim == 1:\n        c = np.array([c])\n    if c.shape[1] == 3:\n        c = np.concatenate([c, np.ones((c.shape[0], 1)) * alpha], axis=1)\n    elif c.shape[1] == 4:\n        c[:, 3] *= alpha\n    else:\n        raise ValueError(\"Invalid color array shape: {}\".format(c.shape))\n    return c\n\n", "CodeBleu_score": 26.65853680941347}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs) if'style' in kwargs or not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style(kwargs.pop('style',None))\n    result = self.figure.subplots(*args,**kwargs)\n    if isinstance(result,Axes):\n        result.style = self.mpfstyle\n    elif isinstance(result,np.ndarray):\n        for ax in result.flatten():\n            ax.style = self.mpfstyle\n    else:\n        raise TypeError('Unexpected result type')\n    return result\n", "CodeBleu_score": 34.64456373866341}, {"code": "def subplots(self,*args,**kwargs) if'style' in kwargs:\n        self._check_for_and_apply_style(kwargs['style'])\n        self.mpfstyle = kwargs['style']\n    else:\n        if not hasattr(self,'mpfstyle'):\n            self._check_for_and_apply_style()\n        self._check_for_and_apply_style(self.mpfstyle)\n    ax = self.add_subplot(*args,**kwargs)\n    if isinstance(ax,mpl.axes.Axes):\n        ax.set_mpfstyle(self.mpfstyle)\n    elif isinstance(ax,np.ndarray):\n        for axi in ax.flatten():\n            axi.set_mpfstyle(self.mpfstyle)\n    else:\n        raise TypeError(\"Unexpected result type from subplots: %s\"%type(ax))\n    return ax\n", "CodeBleu_score": 38.11522900488363}, {"code": "def subplots(self,*args,**kwargs) if'style' in kwargs or not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style(kwargs.get('style', self.mpfstyle))\n    result = self.figure.subplots(*args, **kwargs)\n    if isinstance(result, matplotlib.axes.Axes):\n        result.mpfstyle = self.mpfstyle\n    elif isinstance(result, np.ndarray):\n        for ax in result.flatten():\n            ax.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError('Unknown type of result from subplots: %s' % type(result))\n    return result\n", "CodeBleu_score": 38.836157526626344}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        fig.set_size_inches(fig.get_figwidth()*config['ratio'],fig.get_figheight()*config['ratio'])\n", "CodeBleu_score": 12.643971892629565}, {"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if 'figsize' not in config:\n        config['figsize'] = (fig.get_figwidth()/fig.get_figheight()*config['ratio'],config['ratio'])\n    fig.set_size_inches(config['figsize'])\n", "CodeBleu_score": 19.5251113796093}, {"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n        return\n    fig_ratio = fig.get_figheight()/fig.get_figwidth()\n    if fig_ratio < 0.5:\n        fig_ratio = 0.5\n    if fig_ratio > 2:\n        fig_ratio = 2\n    fig.set_size_inches(config['figsize'][0],config['figsize'][0]*fig_ratio)\n", "CodeBleu_score": 22.838412258601924}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    # check if moving averages are provided as a dictionary or a list of tuples\n    if isinstance(apmav, dict):\n        # extract the shift values and periods from the dictionary\n        shifts = list(apmav.keys())\n        periods = list(apmav.values())\n    else:\n        # assume the moving averages are provided as a list of tuples\n        shifts = [mav[0] for mav in apmav]\n        periods = [mav[1] for mav in apmav]\n\n    # ensure at most 7 moving averages are plotted\n    num_mavs = min(len(shifts), 7)\n\n    # calculate the rolling mean of the prices for each moving average\n    mav_prices = [prices.rolling(period).mean() for period in periods]\n\n    # apply any shifts to the moving average prices\n    shifted_mav_prices = [mav_price.shift(shift) for mav_price, shift in zip(mav_prices, shifts)]\n\n    # plot the moving averages on the axis\n    for i in range(num_mavs):\n        ax.plot(xdates, shifted_mav_prices[i], linewidth=config[\"linewidth\"], color=config[\"color_cycle\"][i])\n\n    return shifted_mav_prices\n", "CodeBleu_score": 26.596977380484432}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        apmav = config['mav']\n    if apwidth is None:\n        apwidth = config['width']\n    mavs = []\n    if isinstance(apmav,dict):\n        for k in apmav.keys():\n            mavs.append(apmav[k])\n    else:\n        mavs.append(apmav)\n    for mav in mavs[:7]:\n        if isinstance(mav,dict):\n            shift = mav['shift']\n            period = mav['period']\n        else:\n            shift = 0\n            period = mav\n        mav_prices = prices.rolling(period).mean()\n        mav_prices = mav_prices.shift(shift)\n        ax.plot(xdates,mav_prices,linewidth=apwidth,color=next(ax._get_lines.prop_cycler)['color'])\n    return mavs\n", "CodeBleu_score": 33.647464561964775}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    # Get the moving averages to plot\n    if apmav is None:\n        mav = config['mav']\n    else:\n        mav = apmav\n    # Ensure at most 7 moving averages are plotted\n    if len(mav) > 7:\n        mav = mav[:7]\n    # Get the line width and color cycle from the config\n    width = config['mavwidth']\n    color = config['mavcolor']\n    # Calculate the moving averages\n    mavs = []\n    for i in range(len(mav)):\n        if isinstance(mav[i],dict):\n            shift = mav[i]['shift']\n            period = mav[i]['period']\n        else:\n            shift = 0\n            period = mav[i]\n        mavs.append(prices.rolling(period).mean().shift(shift))\n    # Plot the moving averages on the axis\n    for i in range(len(mavs)):\n        ax.plot(xdates,mavs[i],linewidth=width,color=color[i])\n    # Return the calculated moving averages\n    return mavs\n", "CodeBleu_score": 29.208507348191446}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kernel_h, kernel_w = inputs[1].shape[2:]\n    in_channels = inputs[0].shape[1]\n    out_size = outputs[0].numel()\n    bias = 1 if opnode.bias is not None else 0\n    return out_size * (in_channels * kernel_h * kernel_w + bias)\n\n", "CodeBleu_score": 18.05594584775956}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kernel_h, kernel_w = inputs[0].shape[2:]\n    input_channels = inputs[0].shape[1]\n    output_elements = outputs[0].numel()\n    flops = output_elements * input_channels * kernel_h * kernel_w\n    if opnode.bias is not None:\n        flops += output_elements\n    return flops\n\n", "CodeBleu_score": 17.334574196909738}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kernel_h, kernel_w = inputs[1].shape[2:]\n    in_channels = inputs[0].shape[1]\n    out_elements = np.prod(outputs[0].shape[2:])\n    bias = 1 if opnode.bias is not None else 0\n    return out_elements * (in_channels * kernel_h * kernel_w + bias)\n\n", "CodeBleu_score": 22.149204865684055}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert mge_dtype in _dtype_to_ir_type, \"Unsupported dtype: {}\".format(mge_dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 28.425200685971642}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert mge_dtype.type is not None, f\"mge_dtype {mge_dtype} is not a NumPy data type\"\n    return _dtype_to_ir_type[mge_dtype.type](mge_dtype)\n\n", "CodeBleu_score": 26.72601141683197}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]\n\n", "CodeBleu_score": 20.00387635228036}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = canonicalize_numpy_array(x)\n    dtype = _numpy_dtype_to_ir_type(x.dtype)\n    if dtype == ir.F16Type():\n        x = x.astype(np.float16)\n    elif dtype == ir.BF16Type():\n        x = x.astype(np.float32)\n    elif dtype == ir.BoolType():\n        x = x.astype(np.bool_)\n    x = np.ascontiguousarray(x)\n    elements = np.ravel(x)\n    return ir.ConstantOp(elements, dtype).results\n\n", "CodeBleu_score": 37.46268450795432}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = canonicalize_dtype(x)\n    dtype = _dtype_to_ir_type(x.dtype)\n    if dtype == ir.F16Type():\n        x = x.astype(np.float16)\n    elif dtype == ir.BF16Type():\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    elements = _numpy_array_to_ir_elements(x)\n    return ir.ConstantOp(dtype, elements).results\n\n", "CodeBleu_score": 33.99226900809861}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be moved to the numpy dialect.\n    # TODO: This function should be", "CodeBleu_score": 8.263214670981661}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if (\n        not val.flags[\"C_CONTIGUOUS\"]\n        and not val.flags[\"F_CONTIGUOUS\"]\n        and val.size > 0\n    ):\n        # If the array has any zero strides and is non-empty, it identifies the zero-stride and non-zero-stride axes.\n        zero_strides = np.where(val.strides == 0)[0]\n        nonzero_strides = np.where(val.strides!= 0)[0]\n        # It collapses the array along the zero-stride axes and optionally canonicalizes its data type.\n        collapsed = val.reshape(val.shape[nonzero_strides])\n        if canonicalize_types:\n            collapsed = np.ascontiguousarray(collapsed)\n        # Then, it creates a broadcast operation for the collapsed array, matching the original shape and non-zero-stride axes, and returns the result.\n        return ir.Value.get_broadcast(\n            collapsed,\n            np.shape(val)[zero_strides],\n            np.shape(val)[nonzero_strides],\n        )\n    # If the array does not have zero strides, it processes the array using _numpy_array_constant with the given canonicalize_types flag.\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 29.625237112313417}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if np.any(val.strides == 0) and val.size!= 0:\n        # Identify the zero-stride and non-zero-stride axes\n        zero_stride_axes = np.where(val.strides == 0)[0]\n        non_zero_stride_axes = np.where(val.strides!= 0)[0]\n\n        # Collapse the array along the zero-stride axes\n        collapsed_val = val.reshape(\n            tuple(val.shape[i] for i in non_zero_stride_axes)\n        )\n\n        # Optionally canonicalize the data type\n        if canonicalize_types:\n            collapsed_val = collapsed_val.astype(canonicalize_types(collapsed_val.dtype))\n\n        # Create a broadcast operation for the collapsed array\n        collapsed_val = np.broadcast_to(\n            collapsed_val, val.shape\n        )  # Broadcast the collapsed array to the original shape\n\n        # Return the result\n        return [\n            ir.Constant(\n                val=collapsed_val,\n                type=ir.ArrayType(\n                    ir.DType(collapsed_val.dtype), tuple(collapsed_val.shape)\n                ),\n            )\n        ]\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 40.69315724631548}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if val.ndim > 0 and any(s == 0 for s in val.strides):\n        # collapse zero-stride axes\n        zero_stride_axes = [\n            i for i, s in enumerate(val.strides) if s == 0\n        ]  # zero-stride axes\n        non_zero_stride_axes = [\n            i for i, s in enumerate(val.strides) if s!= 0\n        ]  # non-zero-stride axes\n        collapsed_val = val.take(0, axis=zero_stride_axes)\n        if canonicalize_types:\n            collapsed_val = _canonicalize_type(collapsed_val)\n        collapsed_val = np.broadcast_to(\n            collapsed_val,\n            tuple(\n                val.shape[i] for i in non_zero_stride_axes\n            ),  # broadcast to original shape and non-zero-stride axes\n        )\n        return _numpy_array_constant(\n            collapsed_val, canonicalize_types=canonicalize_types\n        )\n    return _numpy_array_constant(val, canonicalize_types=canonicalize_types)\n\n", "CodeBleu_score": 36.27572113152506}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    return ir.DenseElementsAttr(\n        np.packbits(xs, bitorder=\"little\"),\n        ir.IntegerType.get_signless(1),\n        [len(xs)],\n    )\n\n", "CodeBleu_score": 24.250038760386595}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    return ir.DenseElementsAttr(\n        ir.DenseIntElementsAttr(\n            ir.IntegerType.get_signless(1),\n            ir.DenseIntElementsAttr.get_packed(\n                [int(x) for x in xs], ir.IntegerType.get_signless(1)\n            ),\n            shape=[len(xs)],\n        )\n    )\n\n", "CodeBleu_score": 27.22588298211385}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    return ir.DenseElementsAttr(\n        ir.DenseIntElementsAttr.get(\n            ir.DenseIntElementsAttr.get(\n                np.packbits(np.array(xs, dtype=np.bool_), bitorder=\"little\"),\n                ir.IntegerType.get_signless(8),\n            ),\n            ir.IntegerType.get_signless(1),\n            [len(xs)],\n        )\n    )\n\n", "CodeBleu_score": 29.498209892842326}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    var_in, var_out = ctx.vars_in[0], ctx.vars_out[0]\n    if var_in.data is not None:\n        var_out.data = var_in.data\n        ctx.module.set_output(var_out.name, var_out.data)\n    elif var_out.data is not None:\n        var_in.data = var_out.data\n        ctx.module.set_input(var_in.name, var_in.data)\n    else:\n        assert False, \"Only tensor creation from constants is supported.\"\n    assert var_in.shape == var_out.shape\n    return HLOTensor(\n        data=np.array(var_out.data, dtype=var_out.dtype),\n        shape=var_out.shape,\n        dtype=var_out.dtype,\n    )\n\n", "CodeBleu_score": 59.63374597131945}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    x = args[0]\n    if isinstance(x, HLOTensor):\n        x = x.data\n    if isinstance(x, (list, tuple)):\n        x = np.array(x, dtype=ctx.vars_out[0].dtype)\n    elif isinstance(x, np.ndarray):\n        x = np.array(x, dtype=x.dtype)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported.\")\n    if ctx.vars_in[0].data is not None:\n        ctx.vars_in[0].data = x\n    if ctx.vars_out[0].data is not None:\n        ctx.vars_out[0].data = x\n    assert ctx.vars_in[0].data.shape == ctx.vars_out[0].data.shape\n    return HLOTensor(ctx.vars_out[0].data, ctx.vars_out[0].dtype)\n\n", "CodeBleu_score": 44.22800798145628}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    in_var, out_var = ctx.vars_in[0], ctx.vars_out[0]\n    if in_var.data is not None:\n        ctx.set_binding(in_var, in_var.data)\n    if out_var.data is not None:\n        ctx.set_binding(out_var, out_var.data)\n    assert in_var.shape == out_var.shape\n    if in_var.data is not None:\n        data = np.array(in_var.data, dtype=out_var.dtype)\n    elif out_var.data is not None:\n        data = np.array(out_var.data, dtype=out_var.dtype)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported.\")\n    return HLOTensor(data, dtype=out_var.dtype)\n\n", "CodeBleu_score": 61.52344871176385}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n    return _device_put_array(np.array(x, dtype=_dtype(x)), device)\n\n", "CodeBleu_score": 15.581767678228287}, {"code": "def _device_put_scalar(x, device)\n    if isinstance(x, np.ndarray):\n        return _device_put_ndarray(x, device)\n    else:\n        if x is None:\n            return None\n        else:\n            return _device_put_ndarray(np.array(x, dtype=(np.dtype(dtype_or_type=x.dtype))), device)\n\n", "CodeBleu_score": 23.598356865228993}, {"code": "def _device_put_scalar(x, device)\n    if isinstance(x, np.ndarray) and x.size == 1:\n        return _device_put_ndarray(x, device)\n    x = _ensure_numpy_array(x)\n    return _device_put_ndarray(x, device)\n\n", "CodeBleu_score": 26.530676774919275}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if not isinstance(lhs_shape[0], int) or not isinstance(rhs_shape[0], int):\n        return False\n    if len(lhs_shape)!= len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i]!= rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 44.48567718968923}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if not lhs_shape and not rhs_shape:\n        return True\n    if isinstance(lhs_shape[0], int) and isinstance(rhs_shape[0], int):\n        if len(lhs_shape)!= len(rhs_shape):\n            return False\n        for i in range(len(lhs_shape)):\n            if lhs_shape[i]!= rhs_shape[i]:\n                return False\n        return True\n    return False\n", "CodeBleu_score": 42.34672493090517}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, tuple) or isinstance(lhs_shape, list)\n    assert isinstance(rhs_shape, tuple) or isinstance(rhs_shape, list)\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert isinstance(lhs_shape[0], int)\n    assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape)!= len(rhs_shape):\n        return False\n    for lhs, rhs in zip(lhs_shape, rhs_shape):\n        if lhs!= rhs:\n            return False\n    return True\n", "CodeBleu_score": 45.28504192046965}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, index_col=\"row_id\"\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, index_col=\"row_id\")\n    training_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, index_col=\"row_id\"\n    )\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, index_col=\"row_id\"\n    )\n    meta_dtypes[\"target\"] = dtypes[\"target\"]\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, index_col=\"row_id\"\n    )\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 26.39566068198858}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    train = pd.read_csv(\n        training_set_filename,\n        dtype=dtypes,\n    )\n    test = pd.read_csv(\n        test_set_filename,\n        dtype=dtypes,\n    )\n    train_meta = pd.read_csv(\n        training_set_metadata_filename,\n        dtype=meta_dtypes,\n    )\n    test_meta = pd.read_csv(\n        test_set_metadata_filename,\n        dtype=meta_dtypes,\n    )\n    test_meta[\"target\"] = np.nan\n    return train, test, train_meta, test_meta\n\n", "CodeBleu_score": 32.5367095286416}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    # Load training set\n    df_train = pd.read_csv(training_set_filename, dtype=dtypes)\n    # Load test set\n    df_test = pd.read_csv(test_set_filename, dtype=dtypes)\n    # Load training metadata\n    df_train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    # Load test metadata\n    meta_dtypes[\"target\"] = \"int8\"\n    df_test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes[\"target\"] = \"float32\"\n    return df_train, df_test, df_train_meta, df_test_meta\n\n", "CodeBleu_score": 32.385841568247535}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    with axes_style('white'):\n        fig, ax = plt.subplots(figsize=(10, 0.5))\n        ax.set_axis_off()\n        ax.pcolormesh(np.linspace(0, 1, 1000).reshape(1, 1000), cmap=cmap)\n    return fig, ax\n\n", "CodeBleu_score": 39.67023641451608}, {"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    ax = axes_style(style=\"white\")\n    fig, ax = plt.subplots(figsize=(10, 1))\n    ax.set_axis_off()\n    ax.pcolormesh(np.linspace(0, 1, 1000).reshape(1, 1000), cmap=cmap)\n    fig.show()\n", "CodeBleu_score": 35.61835664081475}, {"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    with axes_style(\"white\"):\n        fig, ax = plt.subplots(figsize=(6, 1))\n        fig.subplots_adjust(bottom=0.5)\n        ax.set_title(cmap.name + \" colormap\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(cmap.name)\n        ax.set_ylabel(\"0\")\n        ax.pcolormesh(np.arange(256).reshape(1, 256), cmap=cmap)\n        plt.show()\n", "CodeBleu_score": 38.171407654603804}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is not None and obj.get_transform()!= obj.axes.transData:\n        points = obj.axes.transData.inverted().transform(obj.get_transform().transform(zip(xdata, ydata)))\n        xdata, ydata = zip(*points)\n    return xdata, ydata\n\n", "CodeBleu_score": 55.810811825852944}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is not None:\n        if obj.get_transform()!= obj.axes.transData:\n            xdata, ydata = obj.axes.transData.inverted().transform_point((xdata, ydata))\n    return xdata, ydata\n", "CodeBleu_score": 41.22562054715086}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is not None and obj.get_transform()!= obj.axes.transData:\n        transform = obj.get_transform() + obj.axes.transData.inverted()\n        points = transform.transform(np.column_stack([xdata, ydata]))\n    else:\n        points = np.column_stack([xdata, ydata])\n    return points[:, 0], points[:, 1]\n\n", "CodeBleu_score": 58.32449492768447}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None and not isinstance(data2, list):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n", "CodeBleu_score": 60.260368718074375}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data=data, key='data')\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 59.533637919421814}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)", "CodeBleu_score": 61.70498034882521}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programas\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programando\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n    df[\"text\"] = df[\"", "CodeBleu_score": 11.278376185650218}, {"code": "def plot_most_used_words(df)\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programas\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programando\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programando\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programar\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_clean\"].str.replace(\"programa\", \"programar\")\n    df[\"text_clean\"] = df[\"text_", "CodeBleu_score": 12.120195722576588}, {"code": "def plot_most_used_words(df)\n\n    # correct \"programa\" to \"programar\"\n    df[\"text\"] = df[\"text\"].str.replace(\"programa\", \"programar\")\n\n    # filter tokens\n    tokens = df[\"text\"].str.findall(r\"[a-zA-Z]{2,}\").explode().to_frame()\n    tokens = tokens[~tokens[\"text\"].isin(stopwords.words(\"portuguese\"))]\n\n    # count words\n    words = tokens[\"text\"].value_counts().head(20)\n\n    # plot\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=words.index, y=words.values)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.xlabel(\"Palavra\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.title(\"Palavras mais frequentes\")\n    plt.tight_layout()\n    plt.savefig(\"most_used_words.png\")\n    plt.show()\n\n", "CodeBleu_score": 37.267290193796605}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    a = da.promote_types(a, [np.float64, np.float32, np.float16])\n    if min_count is None:\n        min_count = window\n    depth = {axis: -min_count + 1}\n    boundary = {axis: 0}\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    core_dims = [d for d in a.dims if d!= a.dims[axis]]\n    dsk = da.blockwise(\n        moving_func,\n        \"i\",\n        ag,\n        \"i\",\n        ag,\n        \"i\",\n        window=window,\n        min_count=min_count,\n        axis=axis,\n        dtype=a.dtype,\n        align_arrays=False,\n        concatenate=True,\n        meta=np.array(()),\n        adjust_chunks={axis: -min_count + 1},\n        new_axes=core_dims,\n    ).dask\n    return da.Array(dsk, name=ag.name, chunks=a.chunks, dtype=a.dtype)\n\n", "CodeBleu_score": 27.666971606584724}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    a = da.promote_types(a, [np.float64, np.float32, np.float16])\n    depth = (window - 1) // 2\n    if min_count is None:\n        min_count = window\n    ag = da.overlap.overlap(\n        a,\n        depth={axis: depth},\n        boundary={axis: 0},\n        trim=False,\n    )\n    ag_func = partial(moving_func, window=window, min_count=min_count, axis=axis)\n    ag_moving = da.map_blocks(ag_func, ag, dtype=a.dtype)\n    return da.overlap.trim_internal(ag_moving, depth={axis: depth})\n\n", "CodeBleu_score": 29.16143124443137}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    # Promote data type to handle edge cases\n    a = a.astype(np.float64)\n    # Set the fill value\n    if min_count is not None:\n        fill_value = np.nan\n    else:\n        fill_value = 0\n    # Calculate the depth of overlap for the specified axis\n    overlap = int(np.ceil((window - 1) / 2))\n    # Set the boundary fill values\n    if axis == -1:\n        ag = da.overlap.overlap(a, depth=overlap, boundary=fill_value)\n    else:\n        ag = da.overlap.overlap(a, depth={axis: overlap}, boundary=fill_value)\n    # Apply the moving_func to the overlapped array\n    b = da.map_blocks(moving_func, ag, window, min_count, axis=axis)\n    # Trim the overlapped regions\n    b = da.overlap.trim_internal(b, overlap, trim_axis=axis)\n    return b\n\n", "CodeBleu_score": 35.02314040987959}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    if skipna:\n        rhs = rhs.reshape(-1, rhs.shape[-1])\n        coeffs, residuals, rank, singular_values = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        return coeffs, residuals\n    else:\n        return da.linalg.lstsq(lhs, rhs, rcond=rcond)", "CodeBleu_score": 16.492975374765084}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    if skipna:\n        rhs = rhs.reshape(-1, 1)\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        coeffs = coeffs.reshape(-1)\n        residuals = residuals.reshape(-1)\n    else:\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n    return coeffs, residuals\n\n", "CodeBleu_score": 21.888528237341266}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    if skipna:\n        rhs = rhs.reshape(-1, 1)\n        poly_fit = da.polynomial.polyfit(lhs, rhs, 1, rcond=rcond)\n        return poly_fit[0], poly_fit[1]\n    else:\n        return da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n", "CodeBleu_score": 13.643395512103146}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        meta = np.ndarray\n    else:\n        meta = None\n    return da.from_array(data, chunks=chunks, meta=meta, **kwargs)\n\n", "CodeBleu_score": 56.29160971436005}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 60.97084684521099}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]    if isinstance(self.data, Array):\n        return self.data.__dask_postpersist__()\n    else:\n        raise AttributeError(\n            \"This method is only supported for dask arrays.\"\n        )\n\n", "CodeBleu_score": 35.46863044957816}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]    if not isinstance(self.data, da.Array):\n        raise AttributeError(\n            f\"Cannot post-persist {self.data.__class__.__name__} data.\"\n        )\n    func, args = self.data.__dask_postpersist__()\n    return finalize, (func, args)\n\n", "CodeBleu_score": 42.73450101940963}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]    if isinstance(self.data, Array):\n        return self.data.__dask_postpersist__()\n    else:\n        raise AttributeError(\n            \"This dataset does not have a dask array to post-persist.\"\n        )\n\n", "CodeBleu_score": 36.42072605941312}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(x) for x in args):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 47.555127095683915}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(x) for x in args):\n            dask = import_optional_dependency(dask_module, extra=name)\n            func = getattr(dask, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 54.49397896289383}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 47.70928545705178}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape!= arr2.shape:\n        return False\n    if hasattr(arr1, \"dask\"):\n        if hasattr(arr2, \"dask\"):\n            return arr1.dask.tokenize() == arr2.dask.tokenize()\n        return None\n    return None\n\n", "CodeBleu_score": 50.60959107411578}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape!= arr2.shape:\n        return False\n    if is_dask_collection(arr1) and is_dask_collection(arr2):\n        import dask\n        return dask.base.tokenize(arr1) == dask.base.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 53.894038009013634}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape!= arr2.shape:\n        return False\n    if hasattr(arr1, \"dask\") and hasattr(arr2, \"dask\"):\n        return arr1.dask.tokenize() == arr2.dask.tokenize()\n    return None\n\n", "CodeBleu_score": 46.86402263169809}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    if chunks is None:\n        return np.random.randn(*shape)\n    else:\n        return da.random.randn(*shape, chunks=chunks, seed=seed)\n\n", "CodeBleu_score": 12.51433375468756}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    if chunks is not None:\n        return da.random.normal(size=shape, chunks=chunks)\n    else:\n        return np.random.normal(size=shape)\n", "CodeBleu_score": 12.366725996961971}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    if chunks is None:\n        if frac_nan is None:\n            return np.random.randn(*shape)\n        else:\n            return np.random.randn(*shape) * (np.random.rand(*shape) > frac_nan)\n    else:\n        if frac_nan is None:\n            return da.random.randn(*shape, chunks=chunks, random_state=seed)\n        else:\n            return da.random.randn(*shape, chunks=chunks, random_state=seed) * (da.random.rand(*shape, chunks=chunks, random_state=seed) > frac_nan)\n\n", "CodeBleu_score": 23.982997884490338}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 90.57611571255117}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 90.57611571255117}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    # Validate the name\n    if name not in self._allowed_statements:\n        raise ValueError(\n            f\"Invalid financial statement name: {name}. Please choose from {self._allowed_statements}.\"\n        )\n    # Validate the timescale\n    if timescale not in self._allowed_periods:\n        raise ValueError(\n            f\"Invalid timescale: {timescale}. Please choose from {self._allowed_periods}.\"\n        )\n    # Create a financials table\n    try:\n        table = self._create_financials_table(name, timescale, proxy=proxy)\n    except Exception as e:\n        logger.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()\n    return table\n\n", "CodeBleu_score": 39.16530870846171}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.allowed_financials:\n        raise ValueError(\"Invalid financial statement name.\")\n    if timescale not in self.allowed_periods:\n        raise ValueError(\"Invalid period.\")\n    try:\n        table = self._create_financials_table(name, timescale, proxy)\n        return table\n    except Exception as e:\n        logging.error(f\"Error fetching {name} time series data: {e}\")\n        return pd.DataFrame()\n", "CodeBleu_score": 35.20568911640356}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.allowed_statements:\n        raise ValueError(\n            \"Invalid financial statement name. Allowed names are {}\".format(\n                self.allowed_statements\n            )\n        )\n    if timescale not in self.allowed_periods:\n        raise ValueError(\n            \"Invalid timescale. Allowed timescales are {}\".format(self.allowed_periods)\n        )\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        logging.error(\"Error fetching {} data for {}: {}\".format(name, timescale, e))\n        return pd.DataFrame()\n\n", "CodeBleu_score": 35.203882008239376}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)   data = data['ownershipGovernance']['fundOwnership']\n    if data:\n        data = data['fundOwnershipDetails']\n        for item in data:\n            del item['maxAge']\n        df = pd.DataFrame(data)\n        if not df.empty:\n            df['reportDate'] = pd.to_datetime(df['reportDate'])\n            df.rename(columns={'reportDate': 'Date', 'ownerName': 'Name', 'ownerType': 'Type', 'value': 'Value'}, inplace=True)\n        self._mutualfund = df\n", "CodeBleu_score": 36.14602423860438}, {"code": "def _parse_fund_ownership(self, data)   fund_ownership_data = data['ownershipGovernance']['fundOwnership']\n    fund_ownership_list = []\n    for owner in fund_ownership_data:\n        owner_data = {}\n        for key, value in owner.items():\n            if key =='maxAge':\n                continue\n            if key =='reportDate':\n                owner_data[key] = datetime.strptime(value, '%Y-%m-%d').date()\n            else:\n                owner_data[key] = value\n        fund_ownership_list.append(owner_data)\n    if fund_ownership_list:\n        self._mutualfund['fund_ownership'] = pd.DataFrame(fund_ownership_list)\n        self._mutualfund['fund_ownership']['reportDate'] = pd.to_datetime(self._mutualfund['fund_ownership']['reportDate'])\n        self._mutualfund['fund_ownership'].rename(columns={'fund': 'fund_name','reportDate':'report_date', 'ownerName': 'owner_name', 'ownerType': 'owner_type', 'ownerCusip': 'owner_cusip', 'value': 'value_in_millions', 'valueChange': 'value_change_in_millions', 'percentOfFund': 'percent_of_fund', 'percentOfCategory': 'percent_of_category'}, inplace=True)\n    else:\n        self._mutualfund['fund_ownership'] = pd.DataFrame()\n", "CodeBleu_score": 24.010861211776692}, {"code": "def _parse_fund_ownership(self, data)   ownership_list = data['ownershipGovernance']['fundOwnership']\n    if len(ownership_list) > 0:\n        for owner in ownership_list:\n            for key, value in owner.items():\n                if key!='maxAge':\n                    owner[key] = value[0]\n                else:\n                    del owner[key]\n        self._mutualfund = pd.DataFrame(ownership_list)\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'], format='%Y%m%d')\n        self._mutualfund.rename(columns={'reportDate': 'Report Date', 'nameOfIssuer': 'Name of Issuer', 'titleOfClass': 'Title of Class', 'cusip': 'CUSIP', 'value': 'Value','share': 'Share', 'changeInShare': 'Change in Share'}, inplace=True)\n    else:\n        self._mutualfund = pd.DataFrame()\n", "CodeBleu_score": 34.23963308206118}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n    if self.currency_ is None:\n      self.currency_ = self.get_metadata(self.ticker)['currency']\n    return self.currency_\n\n  def get_metadata(self, ticker, proxy=None):\n    \"\"\"The function get_metadata retrieves the metadata for a given ticker. It first checks if the metadata attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, assigns it to the metadata attribute, and then returns the metadata.\\n\"\"\"\n    if self.metadata_ is None:\n      self.metadata_ = self.get_history(ticker, proxy=proxy, metadata=True)\n    return self.metadata_\n\n  def get_history(self, ticker, proxy=None, metadata=False):\n    \"\"\"The function get_history retrieves the historical data for a given ticker. It first checks if the history attribute is already set; if so, it returns this value. If not, it fetches the historical data for the ticker using a method that can utilize a proxy if specified, assigns it to the history attribute, and then returns the history.\\n\"\"\"\n    if self.history_ is None:\n      self.history_ = self.get_history_from_yahoo(ticker, proxy=proxy, metadata=metadata)\n    return self.history_\n\n  def get_history_from_yahoo(self, ticker, proxy=None, metadata=False):\n    \"\"\"The function get_history_from_yahoo retrieves the historical data for a given ticker from Yahoo Finance. It first checks if the ticker is valid, and if not, it raises an exception. If the ticker is valid, it creates a URL for the historical data based on the ticker and the start and end dates, and then uses the requests library to retrieve the data from Yahoo Finance. If a proxy is specified, it sets the proxy settings for the requests session. It then reads the data from the response and parses it using the pandas.read_csv function, with the index_col parameter set to 'Date'. If the metadata parameter is True, it retrieves the metadata for the ticker using the get_metadata_from_yahoo function and adds it to the DataFrame as a new column. Finally, it returns the DataFrame.\\n\"\"\"\n    if not self.is_valid_ticker(ticker):\n      raise Exception('Invalid ticker')\n    url = self.get_url", "CodeBleu_score": 32.538831981536774}, {"code": "def currency(self)\n    if self._currency is None:\n      self._currency = self.get_metadata(proxy=self.proxy)['currency']\n    return self._currency\n\n  @property\n  def exchange(self):\n    \"\"\"The function exchange retrieves and returns the exchange information for a given ticker. It first checks if the exchange attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the exchange information from the metadata, assigns it to the exchange attribute, and then returns the exchange.\\n\"\"\"\n    if self._exchange is None:\n      self._exchange = self.get_metadata(proxy=self.proxy)['exchange']\n    return self._exchange\n\n  @property\n  def name(self):\n    \"\"\"The function name retrieves and returns the name information for a given ticker. It first checks if the name attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the name information from the metadata, assigns it to the name attribute, and then returns the name.\\n\"\"\"\n    if self._name is None:\n      self._name = self.get_metadata(proxy=self.proxy)['name']\n    return self._name\n\n  @property\n  def sector(self):\n    \"\"\"The function sector retrieves and returns the sector information for a given ticker. It first checks if the sector attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the sector information from the metadata, assigns it to the sector attribute, and then returns the sector.\\n\"\"\"\n    if self._sector is None:\n      self._sector = self.get_metadata(proxy=self.proxy)['sector']\n    return self._sector\n\n  @property\n  def industry(self):\n    \"\"\"The function industry retrieves and returns the industry information for a given ticker. It first checks if the industry attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the industry information from the metadata, assigns it to the industry attribute, and then returns the industry.\\n\"\"\"\n    if self._industry is None:\n      self._industry = self.get", "CodeBleu_score": 40.624719921959695}, {"code": "def currency(self)\n    if self.currency_ is None:\n      self.currency_ = self.metadata['currency']\n    return self.currency_\n\n  @property\n  def exchange(self):\n    \"\"\"The function exchange retrieves and returns the exchange information for a given ticker. It first checks if the exchange attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the exchange information from the metadata, assigns it to the exchange attribute, and then returns the exchange.\\n\"\"\"\n    if self.exchange_ is None:\n      self.exchange_ = self.metadata['exchange']\n    return self.exchange_\n\n  @property\n  def name(self):\n    \"\"\"The function name retrieves and returns the name information for a given ticker. It first checks if the name attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the name information from the metadata, assigns it to the name attribute, and then returns the name.\\n\"\"\"\n    if self.name_ is None:\n      self.name_ = self.metadata['name']\n    return self.name_\n\n  @property\n  def country(self):\n    \"\"\"The function country retrieves and returns the country information for a given ticker. It first checks if the country attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the country information from the metadata, assigns it to the country attribute, and then returns the country.\\n\"\"\"\n    if self.country_ is None:\n      self.country_ = self.metadata['country']\n    return self.country_\n\n  @property\n  def sector(self):\n    \"\"\"The function sector retrieves and returns the sector information for a given ticker. It first checks if the sector attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the sector information from the metadata, assigns it to the sector attribute, and then returns the sector.\\n\"\"\"\n    if self.sector_ is None:\n      self.sector_ = self.metadata['sector']\n    return self.sector_\n\n  @property\n  def industry(self):", "CodeBleu_score": 31.135012123796585}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame    if self._recommendations is None:\n      self._recommendations = recommendationTrend.get_recommendations(self.ticker, self.proxy)\n      if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n      else:\n        try:\n          self._recommendations = self._recommendations.rename(columns = {'Firm': 'firm', 'To Grade': 'toGrade', 'From Grade': 'fromGrade', 'Action': 'action', 'Date': 'date'})\n          self._recommendations['date'] = pd.to_datetime(self._recommendations['date'])\n          self._recommendations['fromGrade'] = self._recommendations['fromGrade'].replace('None', np.nan)\n          self._recommendations['toGrade'] = self._recommendations['toGrade'].replace('None', np.nan)\n          self._recommendations['action'] = self._recommendations['action'].replace('None', np.nan)\n          self._recommendations = self._recommendations.sort_values('date', ascending = False)\n        except:\n          raise YFDataException('Error parsing recommendation data.')\n    return self._recommendations\n\n  @property\n  def sustainability(self) -> pd.DataFrame:\n    \"\"\"The method sustainability returns a DataFrame of sustainability metrics.\\nIf the attribute _sustainability is None, it fetches data from a specified proxy with the module'sustainability'.\\nIf the fetch result is None, it sets _sustainability to an empty DataFrame.\\nOtherwise, it attempts to extract sustainability data from the fetched result and stores it in _sustainability as a DataFrame.\\nIf there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\\nThe method then returns the _sustainability DataFrame.\"\"\"\n    if self._sustainability is None:\n      self._sustainability = sustainability.get_sustainability(self.ticker, self.proxy)\n      if self._sustainability is None:\n        self._sustainability = pd.DataFrame()\n      else:\n        try:\n          self._sustainability = self._sustainability.rename(columns = {'Metric':'metric', 'Value': 'value', 'Date': 'date'})\n          self._sustainability['date'] = pd.to_datetime(self._sustainability['date'])\n          self", "CodeBleu_score": 40.67561706749011}, {"code": "def recommendations(self) -> pd.DataFrame    if self._recommendations is None:\n      result = recommendationTrend(self.ticker, proxy=self.proxy)\n      if result is None:\n        self._recommendations = pd.DataFrame()\n      else:\n        try:\n          self._recommendations = pd.DataFrame(result)\n        except:\n          raise YFDataException(f\"Error parsing recommendation data for {self.ticker}\")\n    return self._recommendations\n", "CodeBleu_score": 43.663441325962374}, {"code": "def recommendations(self) -> pd.DataFrame    if self._recommendations is None:\n      self._recommendations = recommendationTrend.get_recommendations(self.ticker)\n      if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n      else:\n        try:\n          self._recommendations = self._recommendations.set_index('date')\n          self._recommendations.index = pd.to_datetime(self._recommendations.index, format='%Y-%m-%d')\n          self._recommendations.index.name = 'date'\n        except Exception as e:\n          raise YFDataException(f'Error parsing recommendation data for {self.ticker}:\\n{e}')\n    return self._recommendations\n\n  @property\n  def sustainability(self) -> pd.DataFrame:\n    \"\"\"The method sustainability returns a DataFrame of sustainability metrics.\\nIf the attribute _sustainability is None, it fetches data from a specified proxy with the module'sustainability'.\\nIf the fetch result is None, it sets _sustainability to an empty DataFrame.\\nOtherwise, it attempts to extract sustainability metrics from the fetched result and stores it in _sustainability as a DataFrame.\\nIf there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\\nThe method then returns the _sustainability DataFrame.\"\"\"\n    if self._sustainability is None:\n      self._sustainability = sustainability.get_sustainability(self.ticker)\n      if self._sustainability is None:\n        self._sustainability = pd.DataFrame()\n      else:\n        try:\n          self._sustainability = self._sustainability.set_index('date')\n          self._sustainability.index = pd.to_datetime(self._sustainability.index, format='%Y-%m-%d')\n          self._sustainability.index.name = 'date'\n        except Exception as e:\n          raise YFDataException(f'Error parsing sustainability data for {self.ticker}:\\n{e}')\n    return self._sustainability\n\n  @property\n  def calendar(self) -> pd.DataFrame:\n    \"\"\"The method calendar returns a DataFrame of upcoming events.\\nIf the attribute _calendar is None, it fetches data from a specified proxy with the module 'calendar'.\\nIf the fetch result is None, it sets _calendar to an", "CodeBleu_score": 40.55218457644549}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise ValueError(\n            \"Clusterer must have a 'n_clusters' attribute set before \"\n            \"calling the elbow method\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n\n    # Create the figure\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Calculate the SSE for each cluster range\n    sse = []\n    time_ = []\n    for k in cluster_ranges:\n        print(f\"Clustering data for k={k}...\")\n        start = time.time()\n        clf.n_clusters = k\n        clf.fit(X)\n        end = time.time()\n        sse.append(clf.inertia_)\n        time_.append(end - start)\n\n    # Plot the SSE against cluster range\n    ax.plot(cluster_ranges, sse, marker='o')\n\n    # Set the axis labels and title\n    ax.set_xlabel(\"Number of clusters\")\n    ax.set_ylabel(\"Sum of Squared Errors\")\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Add a secondary y-axis for the clustering time\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, time_, color='r', marker='o')\n        ax2.set_ylabel(\"Clustering Duration (s)\", color='r', fontsize=text_fontsize)\n        ax2.tick_params(axis='y', labelcolor='r')\n\n    # Set the font sizes\n    ax.tick_params(axis='both', labelsize=text_fontsize)\n    ax.title.set_fontsize(title_fontsize)\n    if show_cluster_time:\n        ax2.tick_params(axis='both', labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 48.88335615437553}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise ValueError(\n            \"The clustering algorithm does not have a n_clusters parameter\")\n    if cluster_ranges is None:\n        cluster_ranges = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n    # \u805a\u7c7b\u8303\u56f4\uff0c\u6700\u5927\u503c\u4e0d\u8d85\u8fc7len(X) // 2\n    cluster_ranges = [\n        cr for cr in cluster_ranges\n        if cr <= len(X) // 2\n    ]\n    # \u5e76\u884c\u8ba1\u7b97\n    scores = Parallel(n_jobs=n_jobs)(\n        delayed(clf.set_params(n_clusters=cr).fit)(X).score(X)\n        for cr in cluster_ranges)\n    # \u7ed8\u5236\u56fe\u50cf\n    ax.plot(cluster_ranges, scores, zorder=2)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of Squared Errors\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    # \u7ed8\u5236\u56fe\u50cf\n    if show_cluster_time:\n        cluster_times = Parallel(n_jobs=n_jobs)(\n            delayed(time)(clf.set_params(n_clusters=cr).fit)(X)\n            for cr in cluster_ranges)\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_times, zorder=1)\n        ax2.set_ylabel(\"Clustering Duration [s]\", fontsize=text_fontsize)\n    # \u8bbe\u7f6e\u56fe\u50cf\u5e03\u5c40\n    fig.tight_layout()\n    return ax\n", "CodeBleu_score": 51.604937005635534}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError(\"%s has no attribute 'n_clusters'.\" % clf.__class__.__name__)\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n    else:\n        cluster_ranges = range(*cluster_ranges)\n    scores = []\n    times = []\n    for k in cluster_ranges:\n        start_time = time.time()\n        clf.set_params(n_clusters=k)\n        clf.fit(X)\n        scores.append(clf.score(X))\n        times.append(time.time() - start_time)\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.grid(True)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.plot(cluster_ranges, scores, 'b*-')\n    ax.set_xlim(min(cluster_ranges), max(cluster_ranges))\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, times, 'g^-')\n        ax2.set_ylabel('Clustering Time (s)', fontsize=text_fontsize)\n        ax2.grid(False)\n    return ax\n", "CodeBleu_score": 51.23076030428339}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    t0 = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    score = metrics.silhouette_score(X, clf.labels_, metric=\"euclidean\")\n    return score, time.time() - t0\n\n", "CodeBleu_score": 36.94851428368123}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    start_time = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    score = silhouette_score(X, clf.labels_)\n    time_taken = time.time() - start_time\n    return score, time_taken\n\n", "CodeBleu_score": 37.001210001284896}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    start_time = time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    return clf, time() - start_time\n\n", "CodeBleu_score": 34.135857068568754}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n    if not hasattr(clf, 'fit'):\n        raise TypeError('The clusterer must have a fit method')\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError('The clusterer must have a fit_predict method')\n    for method in ['plot_silhouette', 'plot_elbow_curve']:\n        if hasattr(clf, method):\n            warnings.warn(f'The clusterer already has a {method} method. Overriding it.')\n        setattr(clf, method, eval(method))\n    return clf\n\n", "CodeBleu_score": 26.3392738228896}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('clf must be a clustering object')\n    def plot_silhouette(X, y, ax=None):\n        if ax is None:\n            ax = plt.gca()\n        ax.set_xlim([-0.1, 1])\n        ax.set_ylim([0, len(X) + (len(set(y)) + 1) * 10])\n        sample_silhouette_values = silhouette_samples(X, y)\n        y_lower = 10\n        for i in range(len(set(y))):\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[y == i]\n            ith_cluster_silhouette_values.sort()\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n            color = cm.nipy_spectral(float(i) / len(set(y)))\n            ax.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n            y_lower = y_upper + 10\n        ax.set_title(\"The silhouette plot for the various clusters.\")\n        ax.set_xlabel(\"The silhouette coefficient values\")\n        ax.set_ylabel(\"Cluster label\")\n        ax.axvline(x=silhouette_score(X, y), color=\"red\", linestyle=\"--\")\n        ax.set_yticks([])\n        ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    def plot_elbow_curve(X, max_clusters=10):\n        distortions = []\n        for i in range(1, max_clusters + 1):\n            km = KMeans(n_clusters=i,\n                       ", "CodeBleu_score": 27.845397996551668}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"clf must be a valid clusterer instance\")\n\n    def plot_silhouette(X, y_pred, ax=None, **kwargs):\n        \"\"\"The function plot_silhouette takes the following arguments:\\nX: The input data.\\ny_pred: The predicted labels.\\nax: The matplotlib.axes.Axes object to use for plotting.\\n**kwargs: Additional keyword arguments to pass to the silhouette_score function.\\nThe function calculates the silhouette score for each sample in the input data X and the corresponding predicted labels y_pred.\\nIt then plots the silhouette values using a silhouette plot, with each sample represented as a line segment.\\nThe function returns the silhouette score and the matplotlib.axes.Axes object used for plotting.\"\"\"\n        silhouette_avg = silhouette_score(X, y_pred, **kwargs)\n        ax = ax or plt.gca()\n        silhouette_values = silhouette_samples(X, y_pred, **kwargs)\n        y_lower = 10\n        for i in range(len(np.unique(y_pred))):\n            ith_cluster_silhouette_values = silhouette_values[y_pred == i]\n            ith_cluster_silhouette_values.sort()\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n            color = plt.cm.nipy_spectral(float(i) / len(np.unique(y_pred)))\n            ax.fill_betweenx(\n                np.arange(y_lower, y_upper),\n                0,\n                ith_cluster_silhouette_values,\n                facecolor=color,\n                edgecolor=color,\n                alpha=0.7,\n            )\n            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n            y_lower = y_upper + 10\n        ax.set_title(\"The silhouette plot for the various clusters.\")\n        ax.set_xlabel(\"The silho", "CodeBleu_score": 27.28510851677569}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    if ax is None:\n        fig, ax = plt.subplots(1, figsize=figsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 77.43506953831141}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 79.31740369089935}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    else:\n        fig = ax.get_figure()\n    if cv is None:\n        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.grid(True)\n    return ax\n\n", "CodeBleu_score": 81.37985599520209}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\n            \"Expected clustering method \"\n            \"that implements 'n_clusters' \"\n            \"in its parameters. \"\n            \"The method received was \"\n            f\"{type(clf).__name__}.\"\n        )\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    scores = []\n    for i in cluster_ranges:\n        clf.n_clusters = i\n        clf.fit(X)\n        scores.append(abs(clf.score(X)))\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.plot(cluster_ranges, scores)\n    ax.grid(True)\n    ax.set_xlabel(\"Number of clusters\")\n    ax.set_ylabel(\"Score\")\n    ax.set_xticks(cluster_ranges)\n    ax.set_title(title, fontsize=title_fontsize)\n    plt.setp(ax.get_xticklabels(), fontsize=text_fontsize)\n    plt.setp(ax.get_yticklabels(), fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 55.70483009349876}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(f\"The classifier {clf} does not have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    classifiers = [clone(clf).set_params(n_clusters=n) for n in cluster_ranges]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.plot(cluster_ranges, scores)\n    ax.grid(True)\n    ax.set_xlabel(\"Number of clusters\")\n    ax.set_xticks(cluster_ranges)\n    ax.set_ylabel(\"Score (higher is better)\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 55.57569573996275}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The provided classifier does not have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    classifiers = [\n        clone(clf).set_params(n_clusters=n) for n in cluster_ranges\n    ]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.plot(cluster_ranges, scores)\n    ax.grid(True)\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_xticks(cluster_ranges)\n    ax.set_xticklabels(cluster_ranges)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax\n\n", "CodeBleu_score": 55.74921168591889}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    # Perform PCA transformation\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    # Set up the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n\n    # Plot the data points\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap)\n\n    # Label the axes\n    ax.set_xlabel(\"First principal component\")\n    ax.set_ylabel(\"Second principal component\")\n\n    # Add a legend\n    ax.legend(title=\"Labels\")\n\n    # Set the title\n    ax.set_title(title)\n\n    # Set the font sizes\n    ax.title.set_fontsize(title_fontsize)\n    ax.xaxis.label.set_fontsize(text_fontsize)\n    ax.yaxis.label.set_fontsize(text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    # Return the axis object\n    return ax\n\n", "CodeBleu_score": 35.907648072745175}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n    if title is not None:\n        ax.set_title(title, fontsize=title_fontsize)\n    # Perform PCA transformation\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    # Plot the data points\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, alpha=0.7)\n    # Label the axes\n    ax.set_xlabel(\"First principal component\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Second principal component\", fontsize=text_fontsize)\n    # Add a legend\n    ax.legend(loc=\"best\")\n    return ax\n\n", "CodeBleu_score": 38.93423483568237}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    # Apply PCA transformation\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    # Set up the plot\n    if figsize is None:\n        figsize = (8, 6)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n    # Plot the data\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap)\n    # Add labels and legend\n    ax.set_xlabel(\"First principal component\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Second principal component\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(handles=ax.collections, labels=clf.classes_)\n    # Return the axis object\n    return ax", "CodeBleu_score": 36.2701611031839}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n    # generate two clipping paths\n    clip_path1 = clip_path(ax)\n    clip_path2 = clip_path(ax)\n\n    # create data for 16 bars\n    x = np.arange(16)\n    y1 = np.random.rand(16)\n    y2 = np.random.rand(16)\n\n    # plot bars\n    ax.bar(x, y1, width=0.5, color='0.5', clip_path=clip_path1)\n    ax.bar(x, y2, width=0.5, color='0.75', clip_path=clip_path2)\n    ax.bar(x, y1, width=0.5, color='r', clip_path=clip_path1)\n    ax.bar(x, y2, width=0.5, color='r', clip_path=clip_path2)\n\n    # set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # add labels\n    ax.text(0.5, -0.1, 'Bar plot', transform=ax.transAxes, ha='center', va='top')\n    ax.text(0.5, -0.2, 'ax.bar()', transform=ax.transAxes, ha='center', va='top', color='blue', family='monospace')\n\n", "CodeBleu_score": 26.692970005238042}, {"code": "def barplot(ax)\n    # Generate clipping paths\n    clip_path1 = clip_path(ax)\n    clip_path2 = clip_path(ax, offset=0.5)\n    # Generate data for 16 bars\n    x = np.arange(16)\n    y1 = np.random.rand(16)\n    y2 = np.random.rand(16)\n    # Plot bars\n    ax.bar(x, y1, color='gray', clip_path=clip_path1)\n    ax.bar(x, y2, color='gray', clip_path=clip_path2)\n    ax.bar(x, y1, color='red', clip_path=clip_path1)\n    ax.bar(x, y2, color='red', clip_path=clip_path2)\n    # Set axis limits\n    ax.set_xlim(-1, 16)\n    # Add text labels\n    ax.text(0.5, 0.5, 'Bar plot', transform=ax.transAxes, ha='center', va='center', color='black')\n    ax.text(0.5, 0.4, 'ax.bar()', transform=ax.transAxes, ha='center', va='center', color='blue', fontfamily='monospace')\n", "CodeBleu_score": 26.216305832361662}, {"code": "def barplot(ax)\n    # Create two clipping paths\n    clip1 = clip_path(ax)\n    clip2 = clip_path(ax, offset=0.05)\n\n    # Generate data for 16 bars\n    X = np.arange(16)\n    Y1 = np.random.uniform(0, 1, size=16)\n    Y2 = np.random.uniform(0, 1, size=16)\n\n    # Plot bars\n    ax.bar(X, Y1, color='0.75', clip_path=clip1)\n    ax.bar(X, Y2, color='0.75', clip_path=clip2)\n    ax.bar(X, Y1, color='0.25', clip_path=clip1)\n    ax.bar(X, Y2, color='0.25', clip_path=clip2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add labels\n    ax.text(-0.5, -0.5, 'Bar plot', color='k', fontsize=18, ha='center', va='center')\n    ax.text(15.5, -0.5, 'ax.bar()', color='b', fontsize=18, ha='center', va='center', fontfamily='monospace')\n\n", "CodeBleu_score": 27.920276942437162}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n    width = 1.0 / n_segment\n    padding = width / 10\n    X0 = np.arange(0, 1.0, width + padding)\n    X1 = X0 + width\n    return X0, X1\n\n", "CodeBleu_score": 26.80297498470805}, {"code": "def split(n_segment)\n    X0 = np.zeros(n_segment)\n    X1 = np.zeros(n_segment)\n    width = 1.0/n_segment\n    for i in range(n_segment):\n        X0[i] = i*width\n        X1[i] = X0[i] + width\n    return X0, X1\n", "CodeBleu_score": 25.132686363648936}, {"code": "def split(n_segment)\n    x0 = []\n    x1 = []\n    width = 1 / n_segment\n    pad = width / 10\n    for i in range(n_segment):\n        x0.append(i * width + i * pad)\n        x1.append((i + 1) * width + i * pad)\n    return x0, x1\n\n", "CodeBleu_score": 23.840488495419688}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n    fig, ax = plt.subplots(figsize=(1, 1), dpi=dpi)\n    ax.text(0.5, 0.5, \"a\", va=\"center\", ha=\"center\", fontsize=75)\n    ax.set_axis_off()\n    canvas = fig.canvas\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    data = data.reshape(canvas.get_width_height()[::-1] + (4,))\n    data = np.roll(data, shift=3, axis=2)\n    return data\n\n", "CodeBleu_score": 47.51759023260236}, {"code": "def pixelated_text(dpi=100)\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.text(0.5, 0.5, \"a\", fontsize=75, ha=\"center\", va=\"center\")\n    ax.axis(\"off\")\n    canvas = FigureCanvas(fig)\n    canvas.draw()\n    img = np.array(canvas.renderer._renderer)\n    img = img.reshape(dpi, dpi, 4)\n    img = np.roll(img, 1, axis=-1)\n    return img\n\n", "CodeBleu_score": 50.9935330312213}, {"code": "def pixelated_text(dpi=100)\n    fig, ax = plt.subplots(figsize=(75 / dpi, 75 / dpi), dpi=dpi)\n    ax.text(0.5, 0.5, \"a\", ha=\"center\", va=\"center\", size=75)\n    ax.axis(\"off\")\n    canvas = FigureCanvas(fig)\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape(int(canvas.get_width_height()[::-1] + (4,)))\n    image = np.roll(image, shift=3, axis=2)\n    return image\n\n", "CodeBleu_score": 53.750927591688125}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n    # Generate a composite curve by summing multiple Gaussian-like components.\n    # The function randomly determines the number of components, their centers,\n    # widths, and scales. The widths and scales are normalized to ensure they\n    # sum to specific values. The function initializes an array of zeros and\n    # iteratively adds scaled Gaussian components based on the calculated\n    # parameters. Finally, it returns the resulting composite curve.\n\n    # Number of components\n    n = np.random.randint(1, 5)\n\n    # Centers of the components\n    centers = np.random.uniform(0, 1, n)\n\n    # Widths of the components\n    widths = np.random.uniform(0.01, 0.1, n)\n\n    # Scales of the components\n    scales = np.random.uniform(0.01, 0.1, n)\n\n    # Normalize the widths and scales\n    widths /= np.sum(widths)\n    scales /= np.sum(scales)\n\n    # Initialize the composite curve\n    curve = np.zeros(100)\n\n    # Add scaled Gaussian components to the curve\n    for i in range(n):\n        curve += scales[i] * np.exp(-(np.arange(100) - centers[i]) ** 2 / (2 * widths[i] ** 2))\n\n    return curve\n\n", "CodeBleu_score": 40.540092974187836}, {"code": "def curve()\n    n = np.random.randint(2, 10)\n    x = np.linspace(-1, 1, 100)\n    y = np.zeros(len(x))\n    for i in range(n):\n        center = np.random.uniform(-1, 1)\n        width = np.random.uniform(0.1, 0.5)\n        scale = np.random.uniform(0, 1)\n        y += scale * np.exp(-(x - center) ** 2 / (2 * width ** 2))\n    y /= y.sum()\n    return x, y\n\n", "CodeBleu_score": 36.69100532935426}, {"code": "def curve()\n    # Number of components\n    n_comp = np.random.randint(1, 5)\n    # Centers\n    centers = np.random.randint(0, 100, n_comp)\n    # Widths\n    widths = np.random.randint(1, 10, n_comp)\n    # Scales\n    scales = np.random.randint(1, 10, n_comp)\n    # Normalize scales\n    scales = scales / np.sum(scales)\n    # Initialize array of zeros\n    y = np.zeros(100)\n    # Add scaled Gaussian components\n    for i in range(n_comp):\n        y += scales[i] * np.exp(-(np.arange(100) - centers[i]) ** 2 / (2 * widths[i] ** 2))\n    return y\n\n", "CodeBleu_score": 32.9387756075175}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n    derivative += 1\n    radius = 2 ** accuracy * derivative\n    points = np.arange(-radius, radius + 1)\n    return np.linalg.inv(np.vander(points))[derivative] * math.factorial(derivative - 1), points\n", "CodeBleu_score": 42.59219061555122}, {"code": "def difference(derivative, accuracy=1)\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.arange(-radius, radius + 1)\n    vandermonde = np.vander(points)\n    coefficients = np.linalg.inv(vandermonde)[order] / np.math.factorial(order - 1)\n    return coefficients, points\n\n", "CodeBleu_score": 46.91937524133895}, {"code": "def difference(derivative, accuracy=1)\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.linspace(-radius, radius, 2 * radius + 1)\n    inverse_vandermonde = np.linalg.inv(np.vander(points))\n    coefficients = inverse_vandermonde[derivative] / np.math.factorial(derivative - 1)\n    return coefficients, points\n\n", "CodeBleu_score": 50.855145929425674}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Compute the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Normalize the cumulative distances to the range [0, 1]\n    normalized_distances = cumulative_distances / cumulative_distances[-1]\n    # Interpolate the X and Y coordinates at the positions specified by T\n    interpolated_X = np.interp(T, normalized_distances, X)\n    interpolated_Y = np.interp(T, normalized_distances, Y)\n    # Calculate the total distance covered by the original coordinates\n    total_distance = np.sum(distances)\n    return interpolated_X, interpolated_Y, total_distance\n", "CodeBleu_score": 26.62289737773948}, {"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X) ** 2 + np.diff(Y) ** 2)\n    # Calculate the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Interpolate the X and Y coordinates at the positions specified by T\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    # Calculate the total distance covered by the original coordinates\n    total_distance = np.sum(distances)\n    return interpolated_X, interpolated_Y, total_distance\n", "CodeBleu_score": 32.576772238759496}, {"code": "def interpolate(X, Y, T)\n    distances = np.sqrt(np.diff(X) ** 2 + np.diff(Y) ** 2)\n    cumulative_distances = np.cumsum(distances)\n    cumulative_distances = np.insert(cumulative_distances, 0, 0)\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    total_distance = cumulative_distances[-1]\n    return interpolated_X, interpolated_Y, total_distance\n", "CodeBleu_score": 31.828261090730877}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros_like(C, dtype=int)\n    Z = np.zeros_like(C, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter - 1] = 0\n    return Z, N\n\n", "CodeBleu_score": 68.58789768907857}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn).astype(np.float32)\n    Y = np.linspace(ymin, ymax, yn).astype(np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros_like(C, dtype=int)\n    Z = np.zeros_like(C)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 60.99525936332715}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros_like(C, dtype=int)\n    Z = np.zeros_like(C, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 66.2385614685601}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n    window_len = int(2 * np.ceil(3 * sigma) + 1)\n    if window_len % 2 == 0:\n        window_len += 1\n    A_smooth = smooth(A, window_len, window=\"hanning\")\n    A_smooth = A_smooth.T\n    A_smooth = smooth(A_smooth, window_len, window=\"hanning\")\n    A_smooth = A_smooth.T\n    return A_smooth\n\n", "CodeBleu_score": 21.868322337285}, {"code": "def smooth2d(A, sigma=3)\n    window_len = 2 * int(sigma) + 1\n    A1 = smooth1d(A, window_len, axis=0)\n    A2 = smooth1d(A1.T, window_len, axis=0)\n    return A2.T\n\n", "CodeBleu_score": 23.012765938578944}, {"code": "def smooth2d(A, sigma=3)\n    window_len = int(2*sigma+1)\n    if window_len % 2 == 0:\n        window_len += 1\n    A_smooth = smooth(A, window_len=window_len, window='hanning')\n    A_smooth = A_smooth.T\n    A_smooth = smooth(A_smooth, window_len=window_len, window='hanning')\n    A_smooth = A_smooth.T\n    return A_smooth\n", "CodeBleu_score": 21.560083855363533}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n    ox = self.ox / dpi\n    oy = self.oy / dpi\n    return np.roll(np.roll(padded_src, int(ox), axis=1), int(oy), axis=0)\n", "CodeBleu_score": 25.24793894344317}, {"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.ox, self.oy\n    # Convert the offsets to pixels\n    ox = int(ox * dpi / 25.4)\n    oy = int(oy * dpi / 25.4)\n    # Shift the image horizontally and vertically\n    shifted_img = np.roll(padded_src, ox, axis=1)\n    shifted_img = np.roll(shifted_img, oy, axis=0)\n    return shifted_img\n", "CodeBleu_score": 25.20865549437934}, {"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.offsets\n    return np.roll(np.roll(padded_src, int(ox * dpi / 25.4), axis=1), int(oy * dpi / 25.4), axis=0)\n", "CodeBleu_score": 29.452165610067016}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    shape = np.asarray(shape)\n    width = np.asarray(width)\n    center = np.asarray(center)\n    grid = np.meshgrid(*[np.linspace(-c, c, s) for c, s in zip(center, shape)])\n    grid = np.asarray(grid)\n    dist = np.sum((grid - center)**2 / width**2, axis=0)\n    return np.exp(-0.5 * dist)\n\n", "CodeBleu_score": 18.83501917505925}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    # Initialize shape, width, and center as tuples if they are provided as single values\n    if not isinstance(shape, tuple):\n        shape = (shape, shape)\n    if not isinstance(width, tuple):\n        width = (width, width)\n    if not isinstance(center, tuple):\n        center = (center, center)\n\n    # Create a grid for the specified shape\n    grid = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n\n    # Compute the squared distance of each grid point from the center, normalized by the width\n    squared_distance = ((grid[0] - center[0]) ** 2 + (grid[1] - center[1]) ** 2) / (width[0] * width[1])\n\n    # Return the Gaussian values computed using the exponential function applied to the negative half of the squared distances\n    return np.exp(-0.5 * squared_distance)\n", "CodeBleu_score": 27.480150348792737}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    if isinstance(shape, int):\n        shape = (shape,)\n    if isinstance(width, float):\n        width = (width,)\n    if isinstance(center, float):\n        center = (center,)\n    grid = np.meshgrid(*[np.arange(s) for s in shape])\n    dist = np.sum((np.array(grid) - np.array(center)) ** 2, axis=0)\n    return np.exp(-dist / (2 * width ** 2))\n\n", "CodeBleu_score": 29.92928195468728}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n    x = np.linspace(-1, 1, 100)\n    y = np.linspace(-1, 1, 100)\n    X, Y = np.meshgrid(x, y)\n    R = np.sqrt((X - position[0]) ** 2 + (Y - position[1]) ** 2)\n    return intensity * np.exp(-R / size)\n\n", "CodeBleu_score": 29.368726801095193}, {"code": "def stimulus(position, size, intensity)\n    # Convert polar coordinates to Cartesian coordinates\n    x = np.cos(position[0]) * np.sin(position[1])\n    y = np.sin(position[0]) * np.sin(position[1])\n    z = np.cos(position[1])\n    # Normalize the coordinates\n    x_norm = x / np.sqrt(x**2 + y**2 + z**2)\n    y_norm = y / np.sqrt(x**2 + y**2 + z**2)\n    z_norm = z / np.sqrt(x**2 + y**2 + z**2)\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    X, Y = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (X - x_norm)**2 + (Y - y_norm)**2\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / size**2) * intensity\n\n", "CodeBleu_score": 33.83307704792354}, {"code": "def stimulus(position, size, intensity)\n    # Convert polar coordinates to Cartesian coordinates\n    x, y = pol2cart(position[0], position[1])\n    # Normalize the coordinates\n    x = x / size\n    y = y / size\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    X, Y = np.meshgrid(np.arange(-1, 1, 0.01), np.arange(-1, 1, 0.01))\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (X - x) ** 2 + (Y - y) ** 2\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / (2 * size ** 2)) * intensity\n\n", "CodeBleu_score": 25.630511383278538}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xticklabels(['', ''])\n    ax.set_yticklabels(['', ''])\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=20)\n    ax.set_title(text, fontdict={'family':'serif', 'weight': 'bold'})\n\n", "CodeBleu_score": 29.853900036731567}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.text(\n        0.5,\n        0.5,\n        text,\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n        fontsize=20,\n        fontfamily=\"serif\",\n        fontweight=\"bold\",\n        color=\"#323232\",\n    )\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\n\n", "CodeBleu_score": 31.586286364072535}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.text(0.5, 0.5, text, horizontalalignment=\"center\", verticalalignment=\"center\", fontsize=20)\n    ax.set_title(\"plot() example\", fontdict={\"family\": \"serif\", \"weight\": \"bold\"}, loc=\"left\")\n\n", "CodeBleu_score": 33.0868630480247}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    return self.pipeline.predict_cumulative_hazard_function(X, **kwargs)\n", "CodeBleu_score": 10.556181377252734}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    return self.pipeline.predict_cumulative_hazard_function(X, **kwargs)\n\n  def predict_survival_function(self, X, **kwargs):\n    \"\"\"The function predict_survival_function predicts the survival function for input data X. It iteratively applies a series of transformations to X, excluding the final transformation, by calling the transform method of each transformation in the pipeline. After all transformations, it uses the final estimator in the pipeline to predict the survival function based on the transformed data and returns the prediction.\\n\"\"\"\n    return self.pipeline.predict_survival_function(X, **kwargs)\n\n  def predict_median(self, X, **kwargs):\n    \"\"\"The function predict_median predicts the median time to event for input data X. It iteratively applies a series of transformations to X, excluding the final transformation, by calling the transform method of each transformation in the pipeline. After all transformations, it uses the final estimator in the pipeline to predict the median time to event based on the transformed data and returns the prediction.\\n\"\"\"\n    return self.pipeline.predict_median(X, **kwargs)\n\n  def predict_percentile(self, X, p=0.5, **kwargs):\n    \"\"\"The function predict_percentile predicts the p-th percentile of the time to event for input data X. It iteratively applies a series of transformations to X, excluding the final transformation, by calling the transform method of each transformation in the pipeline. After all transformations, it uses the final estimator in the pipeline to predict the p-th percentile of the time to event based on the transformed data and returns the prediction.\\n\"\"\"\n    return self.pipeline.predict_percentile(X, p=p, **kwargs)\n\n  def predict_hazard_ratio(self, X, **kwargs):\n    \"\"\"The function predict_hazard_ratio predicts the hazard ratio for input data X. It iteratively applies a series of transformations to X, excluding the final transformation, by calling the transform method of each transformation in the pipeline. After all transformations, it uses the final estimator in the pipeline to predict the hazard ratio based on the transformed data and returns the prediction.\\n\"\"\"\n    return self.pipeline.predict_hazard_ratio(X, **kwargs)\n\n  def predict_log_hazard_ratio(self, X, **kwargs):\n    \"\"\"The function predict_log_hazard_ratio predicts", "CodeBleu_score": 18.993997235204397}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    return self.pipeline.predict_cumulative_hazard_function(X, **kwargs)\n", "CodeBleu_score": 10.556181377252734}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n    if self.pipeline is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n    return self.pipeline.predict_survival_function(X, **kwargs)\n", "CodeBleu_score": 22.611893288501513}, {"code": "def predict_survival_function(self, X, **kwargs)\n    # Iteratively apply a series of transformations to X using a pipeline of steps, excluding the final step\n    X_transformed = self.pipeline[:-1].transform(X)\n    # Use the final step in the pipeline to predict the survival function with the transformed data and any additional keyword arguments\n    return self.pipeline[-1].predict_survival_function(X_transformed, **kwargs)\n", "CodeBleu_score": 27.844803063300255}, {"code": "def predict_survival_function(self, X, **kwargs)\n    return self.pipeline[:-1].predict(X, **kwargs)\n", "CodeBleu_score": 9.193797180159054}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n    if results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.optimal:\n        pass\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.nearlyOptimal:\n        _log.warning(\"Solver terminated with a non-optimal exit status. The termination condition is %s. The optimality gap is %s.\", results.solver.termination_condition, results.solver.termination_condition_number)\n    elif results.solver.status == SolverStatus.aborted and results.solver.termination_condition == TerminationCondition.maxIterations:\n        _log.warning(\"Solver terminated with a non-optimal exit status. The termination condition is %s. The optimality gap is %s.\", results.solver.termination_condition, results.solver.termination_condition_number)\n    elif results.solver.status == SolverStatus.aborted and results.solver.termination_condition == TerminationCondition.infeasible:\n        raise RuntimeError(\"Solver terminated with a non-optimal exit status. The termination condition is %s. The optimality gap is %s.\" % (results.solver.termination_condition, results.solver.termination_condition_number))\n    elif results.solver.status == SolverStatus.aborted and results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded:\n        raise RuntimeError(\"Solver terminated with a non-optimal exit status. The termination condition is %s. The optimality gap is %s.\" % (results.solver.termination_condition, results.solver.termination_condition_number))\n    else:\n        raise RuntimeError(\"Solver terminated with a non-optimal exit status. The termination condition is %s. The optimality gap is %s.\" % (results.solver.termination_condition, results.solver.termination_condition_number))\n", "CodeBleu_score": 30.515533730765114}, {"code": "def _check_success(self, results)\n    if results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.optimal:\n        pass\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.nearlyOptimal:\n        _log.warning(\"Optimization terminated with a nearly optimal solution.\")\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.infeasible:\n        raise RuntimeError(\"Solver reported primal infeasibility.\")\n    elif results.solver.status == SolverStatus.ok and results.solver.termination_condition == TerminationCondition.unbounded:\n        raise RuntimeError(\"Solver reported dual infeasibility.\")\n    elif results.solver.status == SolverStatus.aborted and results.solver.termination_condition == TerminationCondition.maxIterations:\n        _log.warning(\"Maximum number of iterations reached.\")\n    else:\n        raise RuntimeError(\"Solver reported termination condition %s.\" % results.solver.termination_condition)\n", "CodeBleu_score": 36.71930880048098}, {"code": "def _check_success(self, results)\n    if results.solver.status == SolverStatus.ok:\n        if results.solver.termination_condition == TerminationCondition.optimal:\n            return\n        elif results.solver.termination_condition == TerminationCondition.nearlyOptimal:\n            _log.warning(\"Solver %s returned a nearly optimal solution. \"\n                         \"This may be due to a poorly scaled problem. \"\n                         \"Consider rescaling your problem.\",\n                         results.solver.name)\n            return\n        elif results.solver.termination_condition == TerminationCondition.maxIterations:\n            _log.warning(\"Solver %s reached the maximum number of \"\n                         \"iterations. The returned solution may not be \"\n                         \"optimal. Consider increasing the maximum number \"\n                         \"of iterations.\", results.solver.name)\n            return\n        elif results.solver.termination_condition == TerminationCondition.infeasible:\n            raise RuntimeError(\"Solver %s reported the problem is infeasible.\"\n                               % results.solver.name)\n        elif results.solver.termination_condition == TerminationCondition.unbounded:\n            raise RuntimeError(\"Solver %s reported the problem is unbounded.\"\n                               % results.solver.name)\n        else:\n            raise RuntimeError(\"Solver %s returned an unknown status: %s.\"\n                               % (results.solver.name,\n                                  results.solver.termination_condition))\n    else:\n        raise RuntimeError(\"Solver %s returned a bad status: %s.\"\n                           % (results.solver.name, results.solver.status))\n\n", "CodeBleu_score": 18.614475783224805}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n    # Compute the eigenvalues and eigenvectors of P\n    eigenvalues, eigenvectors = linalg.eigh(P)\n\n    # Identify the largest eigenvalue\n    largest_eigenvalue = np.max(eigenvalues)\n\n    # Calculate the condition number\n    condition_number = largest_eigenvalue * np.finfo(float).eps * P.shape[0]\n\n    # Filter out eigenvalues below the positive cutoff\n    eigenvalues = eigenvalues[eigenvalues > self.positive_cutoff]\n\n    # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    eigenvalues /= largest_eigenvalue\n\n    # Return the transposed decomposed matrix and the largest eigenvalue\n    return eigenvectors.T, largest_eigenvalue\n", "CodeBleu_score": 20.853352595857306}, {"code": "def _decompose(self, P)\n    # calculate eigenvalues and eigenvectors\n    eigvals, eigvecs = linalg.eigh(P)\n    # identify the largest eigenvalue\n    max_eigval = np.max(np.abs(eigvals))\n    # calculate condition number\n    cond_num = max_eigval * np.finfo(float).eps * P.shape[0]\n    # filter out eigenvalues below cutoffs\n    eigvals = eigvals[(eigvals > -self.eig_cutoff) & (eigvals < self.eig_cutoff)]\n    # normalize eigenvalues\n    eigvals = eigvals / max_eigval\n    # return transposed decomposed matrix and largest eigenvalue\n    return eigvecs.T, max_eigval\n\n", "CodeBleu_score": 18.702499919376486}, {"code": "def _decompose(self, P)\n    evals, evecs = linalg.eigh(P)\n    largest_eval = np.abs(evals).max()\n    condition_number = np.linalg.cond(P)\n    if not np.isfinite(largest_eval):\n        largest_eval = 0.\n    if not np.isfinite(condition_number):\n        condition_number = 0.\n    abs_evals = np.abs(evals)\n    positive_evals = evals[abs_evals > self.positive_cutoff]\n    positive_evals = positive_evals[abs_evals < self.negative_cutoff]\n    if len(positive_evals) == 0:\n        positive_evals = [1]\n    positive_evals = positive_evals / positive_evals.max()\n    decomposed_P = evecs @ np.diag(positive_evals) @ evecs.T\n    return decomposed_P, largest_eval\n", "CodeBleu_score": 31.262282872214424}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n    #print(\"D.shape: \", D.shape)\n    #print(\"coef.shape: \", coef.shape)\n    #print(\"self.coef_.shape: \", self.coef_.shape)\n    #print(\"self.coef_[:, 0].shape: \", self.coef_[:, 0].shape)\n    #print(\"self.coef_[:, 1].shape: \", self.coef_[:, 1].shape)\n    #print(\"self.coef_[:, 2].shape: \", self.coef_[:, 2].shape)\n    #print(\"self.coef_[:, 3].shape: \", self.coef_[:, 3].shape)\n    #print(\"self.coef_[:, 4].shape: \", self.coef_[:, 4].shape)\n    #print(\"self.coef_[:, 5].shape: \", self.coef_[:, 5].shape)\n    #print(\"self.coef_[:, 6].shape: \", self.coef_[:, 6].shape)\n    #print(\"self.coef_[:, 7].shape: \", self.coef_[:, 7].shape)\n    #print(\"self.coef_[:, 8].shape: \", self.coef_[:, 8].shape)\n    #print(\"self.coef_[:, 9].shape: \", self.coef_[:, 9].shape)\n    #print(\"self.coef_[:, 10].shape: \", self.coef_[:, 10].shape)\n    #print(\"self.coef_[:, 11].shape: \", self.coef_[:, 11].shape)\n    #print(\"self.coef_[:, 12].shape: \", self.coef_[:, 12].shape)\n    #print(\"self.coef_[:, 13].shape: \", self.coef_[:, 13].shape)\n    #print(\"self.coef_[:, 14].shape: \", self.coef_[:, 14].shape)\n    #print(\"self.coef_[:, 15].shape: \", self.coef_[:, 15].shape)\n    #print(\"self.coef_[:, 16].shape: \", self.coef_[:, 16].shape)\n    #print(\"self.coef_[:, 17].shape: \", self.coef_[:, 17].shape)\n    #print", "CodeBleu_score": 4.388888888888888}, {"code": "def _update_coef(self, coef, D)\n    # Get the indices of the elements in coef that are greater than a small threshold\n    indices = np.where(coef > 1e-15)\n    # Scale the selected coefficients by the corresponding elements in D\n    scaled_coef = coef[indices] * D[indices]\n    # Update the model's coefficient attribute with the scaled coefficients\n    self.coef_ = scaled_coef\n", "CodeBleu_score": 18.698881742086943}, {"code": "def _update_coef(self, coef, D)\n    # Select elements of the input coefficient array that are greater than a small threshold\n    # and then scale these selected coefficients by corresponding elements in another array\n    # The resulting product is stored in the model's coefficient attribute\n    #\n    # Parameters\n    # ----------\n    # coef: array_like\n    #     The input coefficient array\n    # D: array_like\n    #     The array containing the scaling factors\n    #\n    # Returns\n    # -------\n    # None\n    #\n    # Notes\n    # -----\n    # The input coefficient array is assumed to have the same shape as the model's coefficient attribute.\n    #\n    # The input array D is assumed to have the same shape as the model's coefficient attribute.\n    #\n    # The threshold for selecting coefficients is given by the model's threshold attribute.\n    #\n    # The scaling factors are given by the model's scaling attribute.\n    #\n    # The model's coefficient attribute is updated by selecting elements of the input coefficient array that are greater than the threshold and then scaling these selected coefficients by corresponding elements in the array D. The resulting product is stored in the model's coefficient attribute.\n    #\n    # The model's coefficient attribute is updated by selecting elements of the input coefficient array that are greater than the threshold and then scaling these selected coefficients by corresponding elements in the array D. The resulting product is stored in the model'threshold attribute.\n    #\n    # The model's coefficient attribute is updated by selecting elements of the input coefficient array that are greater than the threshold and then scaling these selected coefficients by corresponding elements in the array D. The resulting product is stored in the model's scaling attribute.\n    #\n    # Examples\n    # --------\n    # >>> from pyscf.pbc.cc.ccsd import _update_coef\n    # >>> import numpy as np\n    # >>> coef = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    # >>> D = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    # >>> _update_coef(coef, D)\n    # >>> coef\n    # array([ 1,  4,  9, 16, 25, 36, 49, 64, 81, 100])\n    #\n    # \"\"\"\n    #\n    # Select elements", "CodeBleu_score": 4.388888888888888}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    if sample_weight is None:\n        sample_weight = np.ones(len(y_true))\n    event_type = y_true[:, 0]\n    time = y_true[:, 1]\n    exp_raw_prediction = np.exp(raw_prediction)\n    gradient = -np.where(event_type == 1, exp_raw_prediction, 0) * sample_weight\n    return gradient\n\n", "CodeBleu_score": 38.5476595759703}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    # Check if sample weights are provided\n    if sample_weight is None:\n        # If no sample weights are provided, set them to 1\n        sample_weight = np.ones(y_true.shape[0])\n    # Compute the negative gradient of the Cox PH model\n    gradient = -np.where(y_true[:, 1] == 1, sample_weight, 0)\n    # Apply sample weights to the gradient if provided\n    if sample_weight is not None:\n        gradient *= sample_weight\n    # Return the weighted or unweighted gradient\n    return gradient", "CodeBleu_score": 52.80919878486898}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    if sample_weight is None:\n        return -np.where(y_true == 1, 1, 0) + np.where(y_true == 1, np.exp(raw_prediction), 0)\n    else:\n        return -sample_weight * np.where(y_true == 1, 1, 0) + sample_weight * np.where(y_true == 1, np.exp(raw_prediction), 0)\n\n", "CodeBleu_score": 23.83627854766924}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self._check_supports_missing(estimator_name):\n        if not np.isfinite(X.sum()):\n            raise ValueError(\n                \"Input contains NaN, infinity or a value too large for \"\n                \"dtype('float64').\"\n            )\n        return None\n    if np.isnan(X).any():\n        return np.isnan(X).any(axis=0)\n    return None\n\n", "CodeBleu_score": 22.546768318202226}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.missing_values_in_feature_supported:\n        if not np.all(np.isfinite(X)):\n            raise ValueError(\n                \"X contains np.inf or np.nan. If using \"\n                \"missing_values='number', then missing values are \"\n                \"not supported.\"\n            )\n        if np.sum(X)!= np.sum(X):\n            raise ValueError(\n                \"X contains np.nan. If using \"\n                \"missing_values='number', then missing values are \"\n                \"not supported.\"\n            )\n        return None\n    if estimator_name is None:\n        estimator_name = self.estimator_name\n    if estimator_name == \"classifier\":\n        missing_values_in_feature = self.missing_values_in_feature_classifier\n    elif estimator_name == \"regressor\":\n        missing_values_in_feature = self.missing_values_in_feature_regressor\n    else:\n        raise ValueError(\n            \"estimator_name must be either 'classifier' or \" \"'regressor'.\"\n        )\n    if missing_values_in_feature == \"number\":\n        if np.sum(X)!= np.sum(X):\n            raise ValueError(\n                \"X contains np.nan. If using \"\n                \"missing_values='number', then missing values are \"\n                \"not supported.\"\n            )\n        mask = np.isnan(X)\n    elif missing_values_in_feature == \"nan\":\n        mask = np.isnan(X)\n    elif missing_values_in_feature == \"zero\":\n        mask = np.isnan(X)\n    elif missing_values_in_feature == \"nan_if_invalid\":\n        mask = np.isnan(X)\n    else:\n        raise ValueError(\n            \"missing_values_in_feature must be one of \"\n            \"'number', 'nan', 'zero', or 'nan_if_invalid'.\"\n        )\n    return mask\n", "CodeBleu_score": 25.068833702354247}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.supports_missing_values:\n        if not np.isfinite(X.sum()):\n            raise ValueError(\"Input contains NaN, infinity or a value too \"\n                             \"large for dtype('float64').\")\n        return None\n    else:\n        if estimator_name is None:\n            estimator_name = self.__class__.__name__\n        if not hasattr(self,'missing_values_in_feature_mask_'):\n            self.missing_values_in_feature_mask_ = np.zeros(X.shape[1], dtype=np.bool)\n            for j in range(X.shape[1]):\n                if np.any(np.isnan(X[:, j])):\n                    self.missing_values_in_feature_mask_[j] = True\n                    warnings.warn(\"The feature %d (%s) of the %s is \"\n                                  \"invalid, setting to 0.\"\n                                  % (j, self.feature_names_[j], estimator_name),\n                                  UserWarning)\n                    X[:, j] = 0\n        return self.missing_values_in_feature_mask_\n\n", "CodeBleu_score": 29.43080641487828}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    check_is_fitted(self, \"is_fitted_\")\n    X = self._validate_data(X, check_input=check_input)\n    predictions = self.decision_tree.predict(X)\n    predictions = predictions[:, 1]\n    if return_array:\n        return predictions\n    else:\n        return StepFunction(predictions, self.unique_times_)\n\n", "CodeBleu_score": 49.015267442357256}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    check_is_fitted(self)\n    X = check_array(X, accept_sparse=True, dtype=np.float64)\n    predictions = self.decision_function(X)[:, 1]\n    if return_array:\n        return predictions\n    return self.predict_survival_function_from_array(predictions)\n\n", "CodeBleu_score": 30.951688075863892}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    self.check_is_fitted()\n    X = check_array(X, dtype=DTYPE, accept_sparse='csr')\n    predictions = self.decision_tree.predict(X)\n    if return_array:\n        return predictions[:, 1]\n    else:\n        return self._get_step_function(predictions[:, 1])\n\n", "CodeBleu_score": 23.456042303088182}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n    # Check the input data\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be a numpy array\")\n    if not isinstance(y, np.ndarray):\n        raise TypeError(\"y must be a numpy array\")\n    if X.ndim!= 2:\n        raise ValueError(\"X must be a 2-dimensional array\")\n    if y.ndim!= 1:\n        raise ValueError(\"y must be a 1-dimensional array\")\n    if X.shape[0]!= y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n    if X.shape[0] < 2:\n        raise ValueError(\"X must have at least 2 samples\")\n    if X.dtype!= np.float64:\n        X = X.astype(np.float64)\n    if y.dtype!= np.float64:\n        y = y.astype(np.float64)\n\n    # Separate events and times\n    events = y > 0\n    times = np.abs(y)\n\n    # Center the feature matrix\n    means = X.mean(axis=0)\n    X -= means\n\n    # Normalize the data\n    if self.normalize:\n        scales = np.sqrt((X ** 2).sum(axis=0))\n        scales[scales == 0] = 1\n        X /= scales\n\n    # Sort the data\n    order = np.argsort(times)[::-1]\n    X = X[order]\n    events = events[order]\n    times = times[order]\n\n    # Convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    events = np.asfortranarray(events)\n    times = np.asfortranarray(times)\n\n    return X, events, times, means, scales\n", "CodeBleu_score": 31.489449214520338}, {"code": "def _pre_fit(self, X, y)\n    # check if there is at least one sample\n    if X.shape[0] < 1:\n        raise ValueError('X must have at least 1 sample')\n\n    # convert input data to float64\n    X = X.astype(np.float64)\n\n    # separate event numbers and survival times\n    E = y[:, 0].astype(np.float64)\n    T = y[:, 1].astype(np.float64)\n\n    # center feature matrix\n    X_mean = np.mean(X, axis=0)\n    X = X - X_mean\n\n    # normalize feature matrix\n    if self.normalize:\n        X_scale = np.sqrt(np.sum(X ** 2, axis=0))\n        X /= X_scale\n\n    # sort data by survival time\n    idx = np.argsort(T)[::-1]\n    X = X[idx]\n    E = E[idx]\n    T = T[idx]\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    E = np.asfortranarray(E)\n    T = np.asfortranarray(T)\n\n    return X, E, T, X_mean, X_scale\n", "CodeBleu_score": 36.00527831515395}, {"code": "def _pre_fit(self, X, y)\n    X = check_array(X, dtype=np.float64, order=\"C\")\n    if X.shape[0] < 2:\n        raise ValueError(\"Need at least 2 samples.\")\n    if y is not None:\n        y = check_array(y, ensure_2d=False, dtype=None, order=\"C\")\n        if X.shape[0]!= y.shape[0]:\n            raise ValueError(\"X and y have incompatible shapes.\")\n    if self.center:\n        self.feature_means_ = np.mean(X, axis=0)\n        X -= self.feature_means_\n    if self.normalize:\n        self.scales_ = np.std(X, axis=0)\n        X /= self.scales_\n    if y is not None:\n        y = np.asarray(y, dtype=np.float64, order=\"C\")\n        y = y.reshape((-1, 2))\n        if y.shape[1]!= 2:\n            raise ValueError(\"y must be of shape (n_samples, 2).\")\n        y = y[np.argsort(y[:, 0])[::-1]]\n        X = X[np.argsort(y[:, 0])[::-1]]\n        return X, y[:, 1], y[:, 0], self.feature_means_, self.scales_\n    return X, None, None, self.feature_means_, self.scales_\n\n", "CodeBleu_score": 24.836680270168994}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n    alphas = self.alphas\n    if alphas is None:\n        alphas = np.linspace(0, 1, self.n_alphas)\n        new_alphas = True\n    else:\n        alphas = np.atleast_1d(alphas)\n        if len(alphas) == 0:\n            raise ValueError(\"alphas array is empty.\")\n        if len(alphas) > 1 and len(alphas)!= self.n_alphas:\n            warnings.warn(\"User-defined alphas not used and %d alphas\"\n                          \" instead used.\" % self.n_alphas, UserWarning)\n            alphas = np.linspace(0, 1, self.n_alphas)\n        if not np.isfinite(alphas).all():\n            raise ValueError(\"alphas must be finite.\")\n        if np.any(alphas < 0):\n            raise ValueError(\"alphas must be positive.\")\n        new_alphas = False\n    return alphas, new_alphas\n", "CodeBleu_score": 32.83272026621078}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        new_path = True\n        if not isinstance(self.n_alphas, int) or self.n_alphas < 1:\n            raise ValueError(\"n_alphas must be a positive integer\")\n        alphas = np.empty(self.n_alphas)\n    else:\n        new_path = False\n        alphas = np.asarray(self.alphas)\n        if alphas.ndim!= 1 or not np.isfinite(alphas).all() or (alphas < 0).any():\n            raise ValueError(\"alphas must be a 1D array of finite, non-negative values\")\n    return alphas, new_path\n", "CodeBleu_score": 45.337199298632825}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        if self.n_alphas <= 0:\n            raise ValueError(\"Invalid number of alphas: n_alphas=%d must be > 0\" % self.n_alphas)\n        alphas = np.linspace(self.alpha_min, self.alpha_max, self.n_alphas)\n        new_path = True\n    else:\n        alphas = np.array(self.alphas)\n        if len(alphas.shape) > 1:\n            raise ValueError(\"Invalid shape of alphas: alphas.shape=%s; expected 1D array\" % (alphas.shape,))\n        if not np.all(np.isfinite(alphas)):\n            raise ValueError(\"Invalid values in alphas: alphas=%s\" % (alphas,))\n        if np.any(alphas < 0):\n            raise ValueError(\"Invalid values in alphas: alphas=%s; expected non-negative\" % (alphas,))\n        new_path = False\n    return alphas, new_path\n", "CodeBleu_score": 38.62863342754493}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n    # Validate inputs\n    if not isinstance(event, np.ndarray):\n        raise TypeError(\"event must be a numpy array\")\n    if not isinstance(time, np.ndarray):\n        raise TypeError(\"time must be a numpy array\")\n    if len(event)!= len(time):\n        raise ValueError(\"event and time must have the same length\")\n    if not np.all(event.astype(int) == event):\n        raise ValueError(\"event must be a binary indicator\")\n    if not np.all(time.astype(int) == time):\n        raise ValueError(\"time must be a non-negative integer\")\n    # Calculate unique times, number of events, and number of subjects at risk\n    unique_times, counts = np.unique(time, return_counts=True)\n    events = np.bincount(time, weights=event)\n    at_risk = counts - events\n    # Compute the cumulative hazard\n    cumulative_hazard = np.cumsum(events / at_risk)\n    return unique_times, cumulative_hazard\n\n", "CodeBleu_score": 28.009004057748715}, {"code": "def nelson_aalen_estimator(event, time)\n    # Validate inputs\n    if not isinstance(event, np.ndarray):\n        raise ValueError(\"event must be a numpy array\")\n    if not isinstance(time, np.ndarray):\n        raise ValueError(\"time must be a numpy array\")\n    if len(event)!= len(time):\n        raise ValueError(\"event and time must have the same length\")\n    # Calculate unique times, number of events, and number of subjects at risk\n    unique_times = np.unique(time)\n    num_events = np.array([np.sum(event[time == t]) for t in unique_times])\n    num_subjects_at_risk = np.array([np.sum(time >= t) for t in unique_times])\n    # Compute cumulative hazard\n    cumulative_hazard = np.cumsum(num_events / num_subjects_at_risk)\n    return unique_times, cumulative_hazard\n", "CodeBleu_score": 28.13317919472847}, {"code": "def nelson_aalen_estimator(event, time)\n    # Validate inputs\n    if not isinstance(event, (list, np.ndarray)):\n        raise TypeError(\"event must be a list or numpy array.\")\n    if not isinstance(time, (list, np.ndarray)):\n        raise TypeError(\"time must be a list or numpy array.\")\n    if len(event)!= len(time):\n        raise ValueError(\"event and time must have the same length.\")\n\n    # Ensure inputs are consistent\n    event = np.array(event)\n    time = np.array(time)\n    if not np.all(event == 0) and not np.all(event == 1):\n        raise ValueError(\"event must contain only 0s and 1s.\")\n\n    # Calculate unique times, number of events, and number of subjects at risk\n    unique_times = np.unique(time)\n    n_events = np.array([np.sum(event[time == t]) for t in unique_times])\n    n_subjects = np.array([np.sum(time >= t) for t in unique_times])\n\n    # Compute cumulative hazard\n    cumulative_hazard = np.cumsum(n_events / n_subjects)\n\n    return unique_times, cumulative_hazard\n\n", "CodeBleu_score": 28.506711936046052}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n    if event.sum() == len(event):\n        return np.ones(len(event))\n    else:\n        km = KaplanMeierFitter()\n        km.fit(time, event_observed=event, timeline=np.unique(time))\n        km_survival = km.survival_function_\n        time_index = np.searchsorted(km_survival.index.values, time)\n        survival_prob = km_survival.values[time_index]\n        assert np.all(survival_prob > 0), \"All survival probabilities must be positive.\"\n        weights = 1. / survival_prob\n        return weights\n\n", "CodeBleu_score": 29.786630773683488}, {"code": "def ipc_weights(event, time)\n    if np.all(event == 1):\n        return np.ones(len(time))\n    else:\n        km = KaplanMeierFitter().fit(time, event, timeline=np.unique(time)[::-1])\n        surv = km.survival_function_.values\n        surv[surv <= 0] = np.nan\n        idx = np.searchsorted(np.unique(time), time[event == 1])\n        weights = 1 / surv[idx]\n        return weights\n\n", "CodeBleu_score": 37.650650877720544}, {"code": "def ipc_weights(event, time)\n    if np.all(event == 1):\n        return np.ones(event.shape)\n    else:\n        km = KaplanMeierFitter()\n        km.fit(time, event_observed=event, timeline=np.sort(np.unique(time)))\n        surv = np.array(km.survival_function_.KM_estimate)\n        assert np.all(surv > 0), \"Negative survival probabilities found.\"\n        idx = np.searchsorted(km.timeline, time)\n        weights = 1 / surv[idx]\n        return weights\n\n", "CodeBleu_score": 33.09122783001215}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        return [self.get_df_connector(dfs)]\n    elif isinstance(dfs, BaseConnector):\n        return [dfs]\n    elif isinstance(dfs, list):\n        return [\n            self.get_df_connector(df) if isinstance(df, pd.DataFrame) else df\n            for df in dfs\n        ]\n    else:\n        raise ValueError(\n            \"dfs must be a pandas.DataFrame, a connector, or a list of pandas.DataFrames or connectors\"\n        )\n\n    return connectors\n\n", "CodeBleu_score": 21.335104040063307}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\n            \"Invalid input type. Expected a pandas.DataFrame, a modin.DataFrame, a polars.DataFrame, a custom connector, or a list of these.\"\n        )\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connectors.append(PandasConnector(df))\n        elif isinstance(df, dd.DataFrame):\n            connectors.append(DaskConnector(df))\n        elif isinstance(df, pl.DataFrame):\n            connectors.append(PolarsConnector(df))\n        elif isinstance(df, BaseConnector):\n            connectors.append(df)\n        else:\n            raise ValueError(\n                \"Invalid input type. Expected a pandas.DataFrame, a modin.DataFrame, a polars.DataFrame, a custom connector, or a list of these.\"\n            )\n    return connectors\n\n", "CodeBleu_score": 45.39948234373775}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for df in dfs:\n            if not isinstance(df, (pd.DataFrame, BaseConnector)):\n                raise ValueError(\n                    \"Invalid input type. Input must be a pandas DataFrame, a connector object, or a list of pandas DataFrames and connector objects.\"\n                )\n    else:\n        raise ValueError(\n            \"Invalid input type. Input must be a pandas DataFrame, a connector object, or a list of pandas DataFrames and connector objects.\"\n        )\n\n    return [\n        Connector(df) if not isinstance(df, BaseConnector) else df for df in dfs\n    ]\n\n", "CodeBleu_score": 31.06368468861126}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n    if self._df is not None:\n        return self._df\n    elif self._df_filtered is not None:\n        return self._df_filtered\n    else:\n        self._df = self._fetch_data()\n        return self._df\n", "CodeBleu_score": 41.81462931435483}, {"code": "def execute(self) -> pd.DataFrame\n    if self._data is not None:\n        return self._data\n    if self._cache is not None:\n        return self._cache\n    self._data = self._fetch()\n    return self._data\n\n", "CodeBleu_score": 38.824009761256576}, {"code": "def execute(self) -> pd.DataFrame\n    if self.df is None:\n        self.fetch()\n    return self.df", "CodeBleu_score": 18.810966123938204}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_df()\n    return hashlib.sha256(str(sorted(self.columns.values.tolist())).encode() + self.formula.encode()).hexdigest()\n\n", "CodeBleu_score": 29.870418882820942}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_df()\n    column_names = \"|\".join(self.columns)\n    formula_string = self.formula.formula_string\n    return hashlib.sha256((column_names + \"|\" + formula_string).encode()).hexdigest()\n\n", "CodeBleu_score": 43.9875891898311}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.to_dataframe()\n    columns = \"|\".join(self.columns)\n    return hashlib.sha256(f\"{columns}|formula\".encode(\"utf-8\")).hexdigest()\n\n", "CodeBleu_score": 38.87862144918226}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = pd.DataFrame(df)\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    elif isinstance(df, (list, dict)):\n        df = pd.DataFrame(df)\n    else:\n        raise ValueError(\"Unsupported input type\")\n    return df\n\n", "CodeBleu_score": 37.3986695674639}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.DataFrame):\n        self._df = df\n    elif isinstance(df, pd.Series):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self._df = pd.read_csv(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        self._df = pd.DataFrame(df)\n    else:\n        raise ValueError(\"Unsupported input type\")\n\n", "CodeBleu_score": 41.034113148372796}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame.from_dict(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n    return df\n\n", "CodeBleu_score": 43.510953156737116}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    # If the DataFrame has fewer rows than the specified number, return a shuffled version of the entire DataFrame\n    if len(self.df) <= n:\n        return self.df.sample(frac=1)\n    # Otherwise, sample each column individually using a custom sampling method\n    samples = []\n    for col in self.df.columns:\n        # If the column is a string, sample a random string of the same length\n        if self.df[col].dtype == \"object\":\n            sample = self.random_string(len(self.df[col].iloc[0]))\n        # If the column is a numeric, sample a random number within the range of the column\n        elif self.df[col].dtype == \"float64\":\n            sample = np.random.uniform(self.df[col].min(), self.df[col].max())\n        # If the column is a date, sample a random date within the range of the column\n        elif self.df[col].dtype == \"datetime64[ns]\":\n            sample = self.random_date(self.df[col].min(), self.df[col].max())\n        # If the column is a boolean, sample a random boolean value\n        elif self.df[col].dtype == \"bool\":\n            sample = np.random.choice([True, False])\n        # If the column is a category, sample a random category from the column\n        elif self.df[col].dtype.name == \"category\":\n            sample = np.random.choice(self.df[col].cat.categories)\n        # If the column is a numeric, sample a random number within the range of the column\n        elif self.df[col].dtype == \"int64\":\n            sample = np.random.randint(self.df[col].min(), self.df[col].max())\n        # If the column is a category, sample a random category from the column\n        else:\n            sample = np.random.choice(self.df[col].unique())\n        # Append the sampled value to the list of samples\n        samples.append(sample)\n    # Create a new DataFrame with the sampled values\n    sampled_df = pd.DataFrame([samples], columns=self.df.columns)\n    # Anonymize the first few rows of the sampled DataFrame\n    anonymized_df = self.anonymize(sampled_df)\n    # Return the anonym", "CodeBleu_score": 36.818624679027565}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > len(self.df):\n        return self.df.sample(frac=1)\n    else:\n        samples = []\n        for col in self.df.columns:\n            if self.df[col].dtype == \"object\":\n                samples.append(self.df[col].sample(n=n, replace=True))\n            elif self.df[col].dtype == \"float64\":\n                samples.append(self.df[col].sample(n=n, replace=True))\n            elif self.df[col].dtype == \"int64\":\n                samples.append(self.df[col].sample(n=n, replace=True))\n            else:\n                samples.append(self.df[col].sample(n=n, replace=True))\n        sampled_df = pd.concat(samples, axis=1)\n        sampled_df.iloc[0, :] = \"Anonymized\"\n        return sampled_df\n\n", "CodeBleu_score": 30.9390066257924}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if self.df.shape[0] <= n:\n        return self.df.sample(frac=1)\n    else:\n        sample_df = pd.DataFrame()\n        for col in self.df.columns:\n            if self.df[col].dtype == \"object\":\n                sample_df[col] = self.df[col].sample(n, replace=True)\n            else:\n                sample_df[col] = self.df[col].sample(n, replace=True)\n        sample_df.iloc[:3, :] = self.anonymize(sample_df.iloc[:3, :])\n        return sample_df\n\n", "CodeBleu_score": 42.06998955126384}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    if df.empty:\n        return \"\"\n\n    # Add the opening tag with name and description if available\n    tag = f\"<DataFrame name=\\\"{df.name}\\\" description=\\\"{extras['description']}\\\">\"\n\n    # Add the dimensions of the DataFrame in CSV format\n    tag += f\"<dimensions rows=\\\"{df.shape[0]}\\\" columns=\\\"{df.shape[1]}\\\">\"\n\n    # Add the content of the DataFrame in CSV format\n    tag += f\"<content>{df.to_csv(index=False)}</content>\"\n\n    # Add the closing tag\n    tag += \"</DataFrame>\"\n\n    return tag\n", "CodeBleu_score": 32.723711589799116}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    # Create a list of column names\n    columns = list(df.columns)\n    # Create a list of column types\n    types = list(df.dtypes)\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col, \"\") for col in columns]\n    # Create a list of column units\n    units = [extras.get(col + \"_unit\", \"\") for col in columns]\n    # Create a list of column descriptions\n    descriptions = [extras.get(col,", "CodeBleu_score": 29.08573280254237}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    if \"name\" in extras:\n        name = extras[\"name\"]\n    else:\n        name = \"\"\n    if \"description\" in extras:\n        description = extras[\"description\"]\n    else:\n        description = \"\"\n    if \"date\" in extras:\n        date = extras[\"date\"]\n    else:\n        date = \"\"\n    if \"source\" in extras:\n        source = extras[\"source\"]\n    else:\n        source = \"\"\n    if \"unit\" in extras:\n        unit = extras[\"unit\"]\n    else:\n        unit = \"\"\n    if \"frequency\" in extras:\n        frequency = extras[\"frequency\"]\n    else:\n        frequency = \"\"\n    if \"geography\" in extras:\n        geography = extras[\"geography\"]\n    else:\n        geography = \"\"\n    if \"notes\" in extras:\n        notes = extras[\"notes\"]\n    else:\n        notes = \"\"\n    if \"notes_url\" in extras:\n        notes_url = extras[\"notes_url\"]\n    else:\n        notes_url = \"\"\n    if \"notes_file\" in extras:\n        notes_file = extras[\"notes_file\"]\n    else:\n        notes_file = \"\"\n    if \"notes_date\" in extras:\n        notes_date = extras[\"notes_date\"]\n    else:\n        notes_date = \"\"\n    if \"notes_source\" in extras:\n        notes_source = extras[\"notes_source\"]\n    else:\n        notes_source = \"\"\n    if \"notes_geography\" in extras:\n        notes_geography = extras[\"notes_geography\"]\n    else:\n        notes_geography = \"\"\n    if \"notes_frequency\" in extras:\n        notes_frequency = extras[\"notes_frequency\"]\n    else:\n        notes_frequency = \"\"\n    if \"notes_unit\" in extras:\n        notes_unit = extras[\"notes_unit\"]\n    else:\n        notes_unit = \"\"\n    if \"notes_other\" in extras:\n        notes_other = extras[\"notes_other\"]\n    else:\n        notes_other = \"\"\n    if \"notes_other_url\" in extras:\n        notes_other_url = extras[\"notes_other_url\"]\n    else:\n        notes_other_url = \"\"\n    if \"notes_other_file\" in extras:\n        notes_other_file = extras[\"notes_other_file\"]\n    else:\n        notes_other_file = \"\"\n    if \"notes_other_date\" in extras:", "CodeBleu_score": 23.521082898972328}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n    def __init__(self, df: DataFrame, table_name: str, description: Optional[str] = None) -> None:\n        self.df = df\n        self.table_name = table_name\n        self.description = description\n\n    def __call__(self) -> str:\n        html = f\"<table class='table table-bordered table-striped' id='{self.table_name}'><thead><tr>\"\n        for column in self.df.columns:\n            html += f\"<th>{column}</th>\"\n        html += \"</tr></thead><tbody>\"\n        html += self.df.head().to_csv(index=False, line_terminator=\"</tr><tr>\")\n        html += \"</tr></tbody></table>\"\n        if self.description is not None:\n            html += f\"<p>{self.description}</p>\"\n        return html\n", "CodeBleu_score": 35.137424470432656}, {"code": "def convert_df_sql_connector_to_str\n    def __init__(self, df: DataFrame, table_name: str, description: str = None) -> str:\n        \"\"\"Initialize the DataFrame to HTML converter.\\n\\nParameters:\\n\\tdf (DataFrame): The DataFrame to be converted.\\n\\ttable_name (str): The name of the table to be created in the HTML string.\\n\\tdescription (str, optional): The description of the table. Defaults to None.\\n\"\"\"\n        self.df = df\n        self.table_name = table_name\n        self.description = description\n\n    def __call__(self) -> str:\n        \"\"\"Convert the DataFrame to an HTML table string.\\n\\nReturns:\\n\\tstr: The HTML table string.\\n\"\"\"\n        if self.description is None:\n            html_string = f\"<table id='{self.table_name}'>\"\n        else:\n            html_string = f\"<table id='{self.table_name}'><caption>{self.description}</caption>\"\n        html_string += self.df.head().to_csv(index=False)\n        html_string += \"</table>\"\n        return html_string\n\n", "CodeBleu_score": 36.033925196090536}, {"code": "def convert_df_sql_connector_to_str\n    def __init__(self, df: DataFrame, table_name: str, description: str = None) -> None:\n        \"\"\"Initializes the object with the DataFrame, table name, and optional description.\"\"\"\n        self.df = df\n        self.table_name = table_name\n        self.description = description\n\n    def convert_df_sql_connector_to_str(self) -> str:\n        \"\"\"Converts the DataFrame to an HTML table string with an optional description.\\nReturns the HTML string.\"\"\"\n        # Convert the DataFrame to an HTML table string\n        html_table = self.df.to_html(\n            classes=\"table table-striped table-hover table-sm table-bordered\",\n            border=0,\n            justify=\"left\",\n            index=False,\n        )\n\n        # Add a description if it exists\n        if self.description:\n            html_table = f\"<p>{self.description}</p>{html_table}\"\n\n        # Add the table name and description to the HTML table string\n        html_table = f\"<h3>{self.table_name}</h3>{html_table}\"\n\n        return html_table\n\n", "CodeBleu_score": 32.925654193744144}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = self.convert_df_to_json(df, extras)\n    yaml_str = json.dumps(json.loads(json_str), indent=2)\n    if extras.get(\"is_direct_sql\", False):\n        yaml_str = f\"<table><tr><td><pre>{yaml_str}</pre></td></tr></table>\"\n    return yaml_str\n", "CodeBleu_score": 53.15983120684861}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = self.convert_df_to_json(df)\n    yaml_str = json.dumps(json.loads(json_str), indent=2, sort_keys=True)\n    if extras.get(\"is_direct_sql\", False):\n        yaml_str = f\"<table><tr><td><pre>{yaml_str}</pre></td></tr></table>\"\n    return yaml_str\n", "CodeBleu_score": 46.02330761437895}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    # Convert the DataFrame to a JSON string\n    json_str = self.convert_df_to_json(df, extras)\n    # Convert the JSON string to a YAML string\n    yaml_str = json.loads(json_str)\n    # If the extras dictionary contains the key \"is_direct_sql\" with a value of True, wrap the YAML string in HTML table tags\n    if extras.get(\"is_direct_sql\", False):\n        yaml_str = f\"<table>{yaml_str}</table>\"\n    return yaml_str", "CodeBleu_score": 50.931040209574185}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n    if not expected_type:\n        return True\n\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n\n    if expected_type == \"string\":\n        return isinstance(self.value, str)\n\n    if expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n\n    if expected_type == \"plot\":\n        if isinstance(self.value, dict):\n            return \"path\" in self.value and \"name\" in self.value\n        return isinstance(self.value, str)\n\n    return False\n", "CodeBleu_score": 56.86481516417945}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, str) and re.match(r\"^.*\\.(png|jpg|jpeg|pdf|svg)$\", self.value) or isinstance(self.value, dict)\n    else:\n        return False\n", "CodeBleu_score": 57.07531680450914}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    else:\n        raise ValueError(f\"Invalid expected_type: {expected_type}\")\n\n", "CodeBleu_score": 53.81706687602234}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n      if self.metric in [\"dtw\", \"edr\", \"erp\", \"msm\", \"lcss\", \"wdtw\", \"wddtw\"]:\n        return pairwise_distance(X, X2, metric=self.metric, **self.metric_params)\n      else:\n        sig = signature(self.metric)\n        if \"X\" in sig.parameters and \"X2\" in sig.parameters:\n          return self.metric(X, X2, **self.metric_params)\n        else:\n          return self.metric(X, **self.metric_params)\n    else:\n      return self.metric(X, X2, **self.metric_params)\n\n  def _fit(self, X, y=None):\n    \"\"\"The function _fit is used to fit the model to a dataset X. If the distance metric is a string, it initializes the distance function with the specified parameters. If the distance metric is a function, it directly uses the given function. The function returns the fitted model.\\n\"\"\"\n    if isinstance(self.metric, str):\n      self.metric = get_distance(self.metric, **self.metric_params)\n    return self\n\n  def _predict(self, X):\n    \"\"\"The function _predict is used to predict the labels for a given dataset X. It first calculates the distance between X and the training dataset using the _distance function. It then finds the indices of the k nearest neighbors in the training dataset and their corresponding labels. If the problem is a classification task, it finds the most common label among the k nearest neighbors and returns it as the predicted label. If the problem is a regression task, it calculates the mean of the labels of the k nearest neighbors and returns it as the predicted value. The function returns the predicted labels or values.\\n\"\"\"\n    dist = self._distance(X, self.X_train_)\n    if self.problem_type == \"classification\":\n      inds = np.argsort(dist, axis=1)[:, :self.k]\n      labels = self.y_train_[inds]\n      return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], 1, labels)\n    else:\n      inds = np.argsort(dist, axis=1)[:, :self.k]\n      values = self.y_train", "CodeBleu_score": 31.522887161534392}, {"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n      # If X2 is None, check if the distance function can handle a single dataset\n      if inspect.signature(self._distance_func).parameters[\"X2\"].default is not inspect.Parameter.empty:\n        # If the distance function can handle a single dataset, call it with X2=X\n        return self._distance_func(X, X=X, **self._distance_params)\n      else:\n        # If the distance function cannot handle a single dataset, call it with X2=None\n        return self._distance_func(X, X2=None, **self._distance_params)\n    else:\n      # If X2 is not None, call the distance function with X2\n      return self._distance_func(X, X2=X2, **self._distance_params)\n  def _get_distance_params(self, distance_params):\n    \"\"\"The function _get_distance_params takes in a dictionary of distance parameters and returns a dictionary with the same keys and values, unless the key is'metric', in which case it returns a dictionary with the key'metric' and the value of the'metric' key from the distance_params dictionary.\\n\"\"\"\n    if'metric' in distance_params:\n      return {'metric': distance_params['metric']}\n    else:\n      return distance_params\n  def _get_distance_func(self, distance_func):\n    \"\"\"The function _get_distance_func takes in a distance function and returns the same function, unless the distance function is a string, in which case it returns the corresponding distance function from sktime.distances.\\n\"\"\"\n    if isinstance(distance_func, str):\n      return get_distance(distance_func)\n    else:\n      return distance_func\n  def _get_distance_func_name(self, distance_func):\n    \"\"\"The function _get_distance_func_name takes in a distance function and returns its name, unless the distance function is a string, in which case it returns the string itself.\\n\"\"\"\n    if isinstance(distance_func, str):\n      return distance_func\n    else:\n      return distance_func.__name__\n  def _get_distance_params_name(self, distance_params):\n    \"\"\"The function _get_distance_params_name takes in a dictionary of distance parameters and returns a string representation of the dictionary, unless the dictionary is empty, in", "CodeBleu_score": 27.112132366301623}, {"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n        if inspect.signature(self.distance).parameters.get(\"X2\", None):\n            return self.distance(X, X2, **self.distance_params)\n        else:\n            return self.distance(X, **self.distance_params)\n    else:\n        return self.distance(X, X2, **self.distance_params)\n\n", "CodeBleu_score": 13.277678872646382}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        X = X.copy()\n        X.reset_index(inplace=True, drop=True)\n        X = X.T.fillna(0).values\n        X = np.concatenate((X, np.array([len(x) for x in X])), axis=1)\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        X = X.copy()\n        X = X.reshape(-1, 1)\n        X = np.concatenate((X, np.array([len(x) for x in X])), axis=1)\n    elif isinstance(X, list):\n        X = np.array(X)\n        X = np.concatenate((X, np.array([len(x) for x in X])), axis=1)\n    else:\n        raise ValueError(\"X must be a pandas DataFrame, numpy array, or list\")\n    return X\n\n", "CodeBleu_score": 32.53243071588579}, {"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        X = X.reset_index(drop=True)\n        X = X.pivot(columns=X.columns[0], values=X.columns[1])\n        X = X.fillna(0).values\n    elif isinstance(X, pd.Series):\n        X = X.reset_index(drop=True)\n        X = X.fillna(0).values\n    elif isinstance(X, np.ndarray):\n        X = X.reshape(-1, 1)\n    else:\n        raise ValueError(\n            \"X must be a 2D numpy array, a pandas DataFrame, or a pandas Series\"\n        )\n    return X\n\n", "CodeBleu_score": 33.57682324786211}, {"code": "def _convert_X_to_sklearn(self, X)\n    if isinstance(X, pd.DataFrame):\n        X_converted = X.reset_index().pivot(index='index', columns='id', values='value').fillna(0).to_numpy()\n        lengths = np.array([len(X[X['id'] == id]) for id in X['id'].unique()])\n        X_converted = np.concatenate((lengths[:, None], X_converted), axis=1)\n    elif isinstance(X, np.ndarray):\n        X_converted = X.flatten()\n    else:\n        raise ValueError(f\"Input data type {type(X)} is not supported.\")\n    return X_converted\n\n", "CodeBleu_score": 41.16344492077443}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n    return inspect.isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 15.617278133242513}, {"code": "def is_sklearn_estimator(obj)\n    if not inspect.isclass(obj):\n        return False\n    if not issubclass(obj, BaseObject):\n        return False\n    if issubclass(obj, BaseObject) and not issubclass(obj, SklearnBaseEstimator):\n        return False\n    return True\n\n", "CodeBleu_score": 28.10415669360281}, {"code": "def is_sklearn_estimator(obj)\n    return (\n        inspect.isclass(obj)\n        and issubclass(obj, SklearnBaseEstimator)\n        and not issubclass(obj, BaseObject)\n    )\n\n", "CodeBleu_score": 19.841382284839394}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(\n            regressor=self.regressor,\n            transformers=other * self.transformers,\n        )\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 28.066438045535584}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(\n            regressor=self.regressor,\n            transformers=other * self.transformers,\n        )\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 28.066438045535584}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(\n            self.regressor, TransformerPipeline(other, *self.transformers)\n        )\n    return NotImplemented\n\n", "CodeBleu_score": 23.501072912376884}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    if not all([isinstance(col, str) for col in df.columns]):\n        raise ValueError(\"Not all columns are strings.\")\n    return df\n\n", "CodeBleu_score": 47.51841507239094}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 40.667516976634914}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    if any([not isinstance(col, str) for col in df.columns]):\n        raise ValueError(\"All column names must be strings.\")\n    return df\n\n", "CodeBleu_score": 46.29996749767669}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n    random_state = np.random.RandomState(self.random_state)\n    probas = self.predict_proba(X)\n    n_samples = X.shape[0]\n    n_classes = self.classes_.shape[0]\n    classes = self.classes_\n    y_pred = np.empty(n_samples, dtype=classes.dtype)\n    for i in range(n_samples):\n      y_pred[i] = classes[random_state.choice(n_classes, p=probas[i])]\n    return y_pred\n", "CodeBleu_score": 32.50039486487421}, {"code": "def predict(self, X)\n    # Initialize a random state\n    random_state = np.random.RandomState(self.random_state)\n    # Get the predicted probabilities\n    predicted_probabilities = self.predict_proba(X)\n    # Get the indices of the maximum probability values\n    max_probability_indices = np.argmax(predicted_probabilities, axis=1)\n    # Randomly choose one among the indices\n    chosen_indices = random_state.choice(\n        max_probability_indices.shape[0], size=max_probability_indices.shape[0]\n    )\n    # Map the chosen indices to the corresponding class labels\n    predicted_labels = self.classes_[max_probability_indices[chosen_indices]]\n    # Return the predicted labels\n    return predicted_labels\n\n", "CodeBleu_score": 20.80381620222674}, {"code": "def predict(self, X)\n    random_state = np.random.RandomState(self.random_state)\n    return np.array([self.classes_[np.argmax(random_state.multinomial(1, proba))] for proba in self.predict_proba(X)])\n", "CodeBleu_score": 25.879813265032325}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_transformed = np.hstack([pca.transform(X[:, g]) for pca, g in zip(pcas, groups)])\n    X_transformed[np.isnan(X_transformed)] = 0\n    y_prob = clf.predict_proba(X_transformed)\n    if len(y_prob[0]) == 2:\n        return y_prob\n    else:\n        y_prob_new = np.zeros((y_prob.shape[0], 2))\n        y_prob_new[:, 0] = y_prob[:, 0]\n        y_prob_new[:, 1] = np.sum(y_prob[:, 1:], axis=1)\n        return y_prob_new\n\n", "CodeBleu_score": 31.923063555023788}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_transformed = []\n    for pca, group in zip(pcas, groups):\n        group_idx = [i for i, j in enumerate(self.groups) if j == group]\n        X_transformed.append(pca.transform(X[:, group_idx]))\n    X_transformed = np.hstack(X_transformed)\n    X_transformed[np.isnan(X_transformed)] = 0\n    if self.n_classes_!= len(clf.classes_):\n        extra_classes = np.setdiff1d(range(self.n_classes_), clf.classes_)\n        probs = np.hstack([clf.predict_proba(X_transformed), np.zeros((len(X), len(extra_classes)))])\n        probs[:, clf.classes_] = clf.predict_proba(X_transformed)\n    else:\n        probs = clf.predict_proba(X_transformed)\n    return probs", "CodeBleu_score": 35.315555817834394}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    X_transformed = []\n    for group, pca in zip(groups, pcas):\n        X_transformed.append(pca.transform(X[:, group]))\n    X_transformed = np.hstack(X_transformed)\n    X_transformed[np.isnan(X_transformed)] = 0\n    y_prob = clf.predict_proba(X_transformed)\n    if y_prob.shape[1]!= len(self.classes_):\n        y_prob = np.hstack(\n            [\n                y_prob,\n                np.zeros((y_prob.shape[0], len(self.classes_) - y_prob.shape[1])),\n            ]\n        )\n    return y_prob\n\n", "CodeBleu_score": 35.77668183366116}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n    check_is_fitted(self)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.ndim == 3 and X.shape[1] == 1:\n        X = X.reshape(X.shape[0], X.shape[2])\n    if X.ndim!= 2:\n        raise ValueError(\n            \"X must be a 2D array with a single feature or a 3D array with a single feature and a single sample.\"\n        )\n    X = check_array(X, accept_sparse=True, allow_nan=True)\n    proba = []\n    for x in X:\n        proba.append(self.root.predict_proba(x))\n    return np.array(proba)\n\n", "CodeBleu_score": 11.407820787807745}, {"code": "def predict_proba(self, X)\n    if not self.is_fitted_:\n      raise ValueError(\"The model is not fitted.\")\n    if X.shape[1] == 1:\n      X = X.reshape(X.shape[0], )\n    elif X.ndim == 3 and X.shape[1] == 1:\n      X = X.reshape(X.shape[0], X.shape[2])\n    else:\n      raise ValueError(\"Invalid input shape. Expected 1D or 2D array, got {0}.\".format(X.shape))\n    X = self._validate_input(X)\n    proba = np.zeros((X.shape[0], self.n_classes_))\n    for i, x in enumerate(X):\n      proba[i] = self.root_node_.predict_proba(x)\n    return proba\n", "CodeBleu_score": 35.17109147106021}, {"code": "def predict_proba(self, X)\n    if not self.is_fitted_:\n      raise NotFittedError(\n          \"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" % type(self).__name__\n      )\n    if X.ndim == 3 and X.shape[1] == 1:\n      X = X.reshape(X.shape[0], X.shape[2])\n    elif X.ndim!= 2:\n      raise ValueError(\"X must be a 2D array, but got a %dD array instead.\" % X.ndim)\n    if not self.is_fitted_:\n      raise NotFittedError(\n          \"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" % type(self).__name__\n      )\n    X = check_array(X, accept_sparse=False, allow_nd=False, dtype=None, force_all_finite=False)\n    return np.array([self.root.predict_proba(X[i]) for i in range(X.shape[0])])\n\n  def predict(self, X):\n    \"\"\"This function predicts class labels for the given input data.\\nIt first checks if the model is fitted and raises an error if not.\\nFor a single class, it returns the class label for all inputs.\\nIt reshapes the input if it is a 3D numpy array with a single feature, or raises an error for invalid input types or shapes.\\nThe input data is validated, allowing NaN values.\\nIt calculates the predicted class labels by iterating over each input sample and using the root node's predict method.\\nFinally, it returns the computed class labels.\"\"\"\n    if not self.is_fitted_:\n      raise NotFittedError(\n          \"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" % type(self).__name__\n      )\n    if X.ndim == 3 and X.shape[1] == 1:\n      X = X.reshape(X.shape[0], X.shape[2])\n    elif X.ndim!= 2:\n      raise ValueError(\"X must be a 2D array, but got a %dD array instead.\" % X.ndim)\n    if not self.is_fitted_:\n      raise NotFittedError(\n          \"This %s", "CodeBleu_score": 41.15414257894481}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        raise TypeError(\"The transformers_ attribute is None. Please fit the transformer first.\")\n\n    if self.transformers_.scitype == \"Primitives\":\n        return X.to_numpy()\n    elif self.transformers_.scitype == \"Series\":\n        return X.to_numpy().flatten()\n    else:\n        raise TypeError(\n            f\"Unexpected output type: {self.transformers_.scitype}. Expected output types are 'Primitives' and 'Series'.\"\n        )\n\n", "CodeBleu_score": 22.77591402129886}, {"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        raise TypeError(\"No transformers have been fitted. Please fit the transformer first.\")\n    scitype = self.transformers_.scitype\n    if scitype == \"Primitives\":\n        X = convert_to_table(X)\n    elif scitype == \"Series\":\n        X = convert_to_flat_numpy(X)\n    else:\n        raise TypeError(f\"Unexpected scitype: {scitype}.\")\n    return X\n\n", "CodeBleu_score": 28.99176215749107}, {"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        raise ValueError(\"The transformers_ attribute is None. Please fit the transformer first.\")\n    scitype = self.transformers_.get_scitype()\n    if scitype == \"Primitives\":\n        return X.to_numpy()\n    elif scitype == \"Series\":\n        return X.to_numpy().reshape(-1, 1)\n    else:\n        raise TypeError(f\"Unexpected output type {scitype}.\")\n\n", "CodeBleu_score": 26.591323448110483}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n    if not isinstance(random_state, (int, np.random.RandomState)):\n        raise ValueError(\n            f\"random_state must be an integer or a numpy.random.RandomState, got {type(random_state)}\"\n        )\n    random_state_params = {}\n    for param_name, param_value in estimator.get_params().items():\n        if \"random_state\" in param_name:\n            random_state_params[param_name] = random_state\n    if len(random_state_params) > 0:\n        estimator.set_params(**random_state_params)\n    return estimator\n\n", "CodeBleu_score": 27.6563601663224}, {"code": "def set_random_state(estimator, random_state=0)\n    random_state = check_random_state(random_state)\n    params = {}\n    for param, value in estimator.get_params().items():\n        if \"random_state\" in param:\n            params[param] = random_state.randint(100000)\n    if len(params) > 0:\n        estimator.set_params(**params)\n\n", "CodeBleu_score": 32.88133975741745}, {"code": "def set_random_state(estimator, random_state=0)\n    # ensure random_state is a valid random state object\n    if not isinstance(random_state, (int, np.random.RandomState)):\n        raise ValueError(\n            \"random_state must be an int or a RandomState object\"\n        )\n    # initialize an empty dictionary to hold parameters that need updating\n    params = {}\n    # iterate over the estimator's parameters\n    for param, value in estimator.get_params().items():\n        # check for parameters related to random state\n        if \"random_state\" in param:\n            # assign a random integer value\n            params[param] = random_state.randint(10000)\n    # update the estimator with the new random state values\n    if len(params) > 0:\n        estimator.set_params(**params)\n\n", "CodeBleu_score": 32.893920356460335}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n    transformed = np.empty((n_instances, n_intervals * 3))\n    for i, (start, stop) in enumerate(intervals):\n        slice_ = X[:, start:stop]\n        mean = slice_.mean(axis=1)\n        std = slice_.std(axis=1)\n        slope = np.gradient(mean)\n        transformed[:, i * 3] = mean\n        transformed[:, i * 3 + 1] = std\n        transformed[:, i * 3 + 2] = slope\n    return transformed.T\n\n", "CodeBleu_score": 30.33584187592092}, {"code": "def _transform(X, intervals)\n    n, d = X.shape\n    k = len(intervals)\n    X_new = np.zeros((d, k))\n    for i in range(k):\n        X_new[:, i] = np.mean(X[:, intervals[i][0]:intervals[i][1]], axis=1)\n        X_new[:, i] = np.std(X[:, intervals[i][0]:intervals[i][1]], axis=1)\n        X_new[:, i] = np.mean(X[:, intervals[i][0]:intervals[i][1]], axis=1)\n    return X_new.T\n\n", "CodeBleu_score": 22.38554237165623}, {"code": "def _transform(X, intervals)\n    n, m = X.shape\n    k = len(intervals)\n    X_trans = np.zeros((m, k))\n    for i, (l, r) in enumerate(intervals):\n        X_trans[i, :] = np.mean(X[:, l:r], axis=1)\n        X_trans[i + k, :] = np.std(X[:, l:r], axis=1)\n        X_trans[i + 2 * k, :] = (X[:, r - 1] - X[:, l]) / (r - l)\n    return X_trans.T\n\n", "CodeBleu_score": 19.97275880331006}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    if annotation_params is None:\n        annotation_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if annotate_params is None:\n        annotate_params = {}\n    if plot_params is None:\n        plot_params = {}\n    if configuration is None:\n        configuration = {}\n    if annotation_func is None:\n        annotation_func = []\n    if len(annotation_func) == 0:\n        annotation_func = []\n    if plot == \"bar\":\n        if \"x\" not in plot_params:\n            plot_params[\"x\"] = \"x\"\n        if \"y\" not in plot_params:\n            plot_params[\"y\"] = \"y\"\n        if \"data\" not in plot_params:\n            plot_params[\"data\"] = self.data\n        if \"ax\" not in plot_params:\n            plot_params[\"ax\"] = self.ax\n        if \"color\" not in plot_params:\n            plot_params[\"color\"] = \"black\"\n        if \"orient\" not in plot_params:\n            plot_params[\"orient\"] = \"v\"\n        if \"order\" not in plot_params:\n            plot_params[\"order\"] = None\n        if \"hue\" not in plot_params:\n            plot_params[\"hue\"] = None\n        if \"hue_order\" not in plot_params:\n            plot_params[\"hue_order\"] = None\n        if \"palette\" not in plot_params:\n            plot_params[\"palette\"] = None\n        if \"legend\" not in plot_params:\n            plot_params[\"legend\"] = True\n        if \"legend_out\" not in plot_params:\n            plot_params[\"legend_out\"] = True\n        if \"legend_kws\" not in plot_params:\n            plot_params[\"legend_kws\"] = {}\n        if \"sharex\" not in plot_params:\n            plot_params[\"sharex\"] = True\n        if \"sharey\" not in plot_params:\n            plot_params[\"sharey\"] = True\n        if \"margin_titles\" not in plot_params:\n            plot_params[\"margin_titles\"] = False\n        if \"facet_kws\" not in plot_params:\n            plot_params[\"facet_kws\"] = {}\n        if \"size\"", "CodeBleu_score": 33.08842629101124}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    # initialize annotation and operation parameters\n    if annotation_params is None:\n        annotation_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    # create the plot with given parameters\n    ax = self.plot(plot, plot_params, **kwargs)\n    # apply pre-plot axis operations\n    for op in ax_op_before:\n        self.apply_ax_op(op, ax)\n    # configure and annotate the plot based on provided configuration\n    self.configure_plot(ax, configuration)\n    # annotate the plot based on provided annotation function\n    self.annotate(ax, annotation_func, annotation_params, *args)\n    # apply post-plot axis operations\n    for op in ax_op_after:\n        self.apply_ax_op(op, ax)\n    # return the output of the plotting process\n    return ax\n\n", "CodeBleu_score": 47.400710047477226}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if annotation_func is None:\n        annotation_func = []\n    if isinstance(annotation_func, str):\n        annotation_func = [annotation_func]\n    if isinstance(ax_op_before, str):\n        ax_op_before = [ax_op_before]\n    if isinstance(ax_op_after, str):\n        ax_op_after = [ax_op_after]\n    if isinstance(annotation_func, list):\n        annotation_func = [annotation_func]\n    if isinstance(ax_op_before, list):\n        ax_op_before = [ax_op_before]\n    if isinstance(ax_op_after, list):\n        ax_op_after = [ax_op_after]\n    if isinstance(annotate_params, dict):\n        annotate_params = [annotate_params]\n    if isinstance(annotation_params, dict):\n        annotation_params = [annotation_params]\n    if isinstance(plot_params, dict):\n        plot_params = [plot_params]\n    if plot_params is None:\n        plot_params = {}\n    if isinstance(plot_params, list):\n        plot_params = [plot_params]\n    if configuration is None:\n        configuration = {}\n    if isinstance(configuration, dict):\n        configuration = [configuration]\n    if isinstance(plot, str):\n        plot = [plot]\n    if isinstance(plot, list):\n        plot = [plot]\n    if isinstance(configuration, list):\n        configuration = [configuration]\n    if isinstance(plot_params, list):\n        plot_params = [plot_params]\n    if isinstance(annotation_func, list):\n        annotation_func = [annotation_func]\n    if isinstance(ax_op_before, list):\n        ax_op_before = [ax_op_before]\n    if isinstance(ax_op_after, list):\n        ax_op_after = [ax_op_after]\n    if isinstance(annotate_params, list):\n        annotate_params = [annotate_params]\n    if isinstance(annotation_params, list):\n        annotation_params = [annotation_", "CodeBleu_score": 37.28412731980737}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n      raise Exception('Model not fitted.')\n    X = self._validate_input(X)\n    if X.shape[1]!= self._feature_count:\n      raise Exception('Feature count mismatch. Expected %d, got %d' % (self._feature_count, X.shape[1]))\n    scores = self._program.execute(X)\n    probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    return probabilities\n", "CodeBleu_score": 43.553111922448494}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n      raise RuntimeError('Model has not been fitted yet.')\n    X = check_array(X)\n    if X.shape[1]!= self.feature_count_:\n      raise ValueError('Expected {} features, got {}.'.format(self.feature_count_, X.shape[1]))\n    scores = self._program.execute(X)\n    probs = np.exp(scores) / (1 + np.exp(scores))\n    return np.c_[1 - probs, probs]\n", "CodeBleu_score": 39.8453734999622}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n      raise Exception('Model not fitted.')\n\n    X = self._validate_input(X)\n\n    if X.shape[1]!= self.n_features_:\n      raise ValueError('Number of features of the model must match the input. Model n_features is {} and input n_features is {}'.format(self.n_features_, X.shape[1]))\n\n    scores = self._program.predict(X)\n    probs = np.exp(scores) / (1 + np.exp(scores))\n\n    return np.column_stack((1 - probs, probs))\n\n  def predict(self, X):\n    \"\"\"The function predict computes class predictions for the input data X. It first checks if the model has been fitted by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature count is checked against the model's expected feature count, raising an error if they do not match. The function then executes the program to obtain scores, transforms these scores into probabilities, and constructs a 1D array with the predicted class for each input example. It returns this array of class predictions.\\n\"\"\"\n    if not hasattr(self, '_program'):\n      raise Exception('Model not fitted.')\n\n    X = self._validate_input(X)\n\n    if X.shape[1]!= self.n_features_:\n      raise ValueError('Number of features of the model must match the input. Model n_features is {} and input n_features is {}'.format(self.n_features_, X.shape[1]))\n\n    scores = self._program.predict(X)\n    probs = np.exp(scores) / (1 + np.exp(scores))\n\n    return np.array([1 if p >= 0.5 else 0 for p in probs])\n\n  def score(self, X, y):\n    \"\"\"The function score computes the accuracy score for the input data X and its corresponding labels y. It first checks if the model has been fitted by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature count is checked against the model's expected feature count, raising an error if they do not match. The function then executes the program to obtain scores, transforms these scores into probabilities, and compares the predicted class for each input example with the corresponding label in y", "CodeBleu_score": 47.71711248766495}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n    program = copy.deepcopy(self)\n    nodes_to_mutate = []\n    for node in program.nodes:\n        if random_state.uniform() < self.mutation_prob:\n            nodes_to_mutate.append(node)\n    for node in nodes_to_mutate:\n        if node.is_function:\n            node.function = random_state.choice(\n                [f for f in program.function_set if f.arity == node.function.arity]\n            )\n        else:\n            if random_state.uniform() < 0.5:\n                node.value = random_state.uniform(\n                    program.constant_range[0], program.constant_range[1]\n                )\n            else:\n                node.value = random_state.choice(program.feature_names)\n    return program, nodes_to_mutate\n\n", "CodeBleu_score": 22.05465703715123}, {"code": "def point_mutation(self, random_state)\n    # Create a copy of the program\n    program = copy.deepcopy(self)\n    # Identify nodes to modify based on a uniform random distribution and a probability threshold\n    modified_nodes = []\n    for i, node in enumerate(program.nodes):\n        if random_state.uniform() < self.mutation_probability:\n            modified_nodes.append(i)\n    # For each node to be mutated, if it is a function, it finds a valid replacement function with the same arity and updates the node.\n    # If the node is a terminal, it replaces it with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features.\n    for node_idx in modified_nodes:\n        node = program.nodes[node_idx]\n        if node.is_function:\n            # Find a valid replacement function with the same arity\n            replacement_function = random_state.choice(\n                [f for f in self.function_set if f.arity == node.arity]\n            )\n            # Update the node\n            program.nodes[node_idx] = replacement_function\n        else:\n            # Replace the terminal with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features\n            if random_state.uniform() < 0.5:\n                # Replace with a constant\n                program.nodes[node_idx] = Constant(\n                    random_state.uniform(self.constant_range[0], self.constant_range[1])\n                )\n            else:\n                # Replace with a variable\n                program.nodes[node_idx] = Variable(random_state.randint(0, self.num_features))\n    # Return the mutated program and the list of modified node indices\n    return program, modified_nodes\n\n", "CodeBleu_score": 24.897393269889456}, {"code": "def point_mutation(self, random_state)\n    # Copy the program\n    mutated_program = copy.deepcopy(self.program)\n    # Identify nodes to modify based on a uniform random distribution and a probability threshold\n    nodes_to_modify = [\n        node_index\n        for node_index in range(len(mutated_program))\n        if random_state.random() < self.mutation_probability\n    ]\n    # For each node to be mutated, if it is a function, it finds a valid replacement function with the same arity and updates the node\n    for node_index in nodes_to_modify:\n        if mutated_program[node_index].arity > 0:\n            # Find a valid replacement function with the same arity\n            while True:\n                replacement_function = random_state.choice(\n                    self.function_set[mutated_program[node_index].arity]\n                )\n                if replacement_function!= mutated_program[node_index].function:\n                    break\n            # Update the node\n            mutated_program[node_index].function = replacement_function\n    # If the node is a terminal, it replaces it with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features\n    for node_index in nodes_to_modify:\n        if mutated_program[node_index].arity == 0:\n            if random_state.random() < 0.5:\n                # Replace with a constant\n                mutated_program[node_index].value = random_state.uniform(\n                    self.constant_range[0], self.constant_range[1]\n                )\n            else:\n                # Replace with a variable\n                mutated_program[node_index].value = random_state.randint(0, self.n_features)\n    return mutated_program, nodes_to_modify\n\n", "CodeBleu_score": 26.313730573272707}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate inputs\n    if not isinstance(close, pd.Series):\n        raise TypeError(\"Input 'close' must be a pandas Series\")\n    if length is None:\n        length = 10\n    if mamode is None:\n        mamode = \"EMA\"\n    if offset is None:\n        offset = 0\n    # Calculate the moving average\n    if mamode == \"SMA\":\n        ma = close.rolling(length).mean()\n    elif mamode == \"EMA\":\n        ma = close.ewm(span=length, adjust=False).mean()\n    elif mamode == \"WMA\":\n        ma = close.rolling(length).apply(lambda x: np.average(x, weights=np.arange(1, length + 1)))\n    elif mamode == \"DEMA\":\n        ma = 2 * close.ewm(span=length, adjust=False).mean() - close.ewm(span=length, adjust=False).mean().ewm(span=length, adjust=False).mean()\n    elif mamode == \"TEMA\":\n        ma = 3 * close.ewm(span=length, adjust=False).mean() - 3 * close.ewm(span=length, adjust=False).mean().ewm(span=length, adjust=False).mean() + close.ewm(span=length, adjust=False).mean().ewm(span=length, adjust=False).mean().ewm(span=length, adjust=False).mean()\n    elif mamode == \"TRIMA\":\n        ma = close.rolling(length).apply(lambda x: np.average(x, weights=np.tri(length)))\n    elif mamode == \"KAMA\":\n        er = abs(close - close.shift(1)) / close\n        sc = er * (2.0 / (length + 1) - 2.0 / (length + 1) ** 2 + (length - 1) / (length + 1) ** 2 + (length - 1) / (length + 1) ** 3)\n        sc = sc.replace(np.nan, 0)\n        sc = sc.replace(np.inf, 0)\n        sc = sc.replace(-np.inf, 0)\n        fsc = sc.ewm(", "CodeBleu_score": 27.51970066319468}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    if length is None:\n        length = 12\n    if mamode is None:\n        mamode ='sma'\n    if offset is None:\n        offset = 0\n    # Validate inputs\n    close = ta.utils.check_series(close)\n    length = int(length)\n    if length < 2:\n        raise ValueError(\"Input 'length' must be greater than 1\")\n    mamode = mamode.lower()\n    if mamode not in ['sma', 'ema', 'wma', 'dema', 't3', 'tema', 'trima', 'wma']:\n        raise ValueError(\"Input'mamode' must be one of:'sma', 'ema', 'wma', 'dema', 't3', 'tema', 'trima', 'wma'\")\n    # Calculate moving average\n    if mamode =='sma':\n        ma = ta.sma(close, length)\n    elif mamode == 'ema':\n        ma = ta.ema(close, length)\n    elif mamode == 'wma':\n        ma = ta.wma(close, length)\n    elif mamode == 'dema':\n        ma = ta.dema(close, length)\n    elif mamode == 't3':\n        ma = ta.t3(close, length)\n    elif mamode == 'tema':\n        ma = ta.tema(close, length)\n    elif mamode == 'trima':\n        ma = ta.trima(close, length)\n    elif mamode == 'wma':\n        ma = ta.wma(close, length)\n    # Calculate bias\n    bias = (close - ma) / ma\n    bias = ta.utils.fillna(bias)\n    bias = ta.utils.remove_lookahead_bias(bias)\n    # Offset\n    if offset!= 0:\n        bias = ta.utils.df_shift(bias, offset)\n        bias = ta.utils.fillna(bias)\n    # Name & Category\n    bias.name = f\"BIAS_{length}_{mamode.upper()}\"\n    bias.category ='momentum'\n    return bias\n", "CodeBleu_score": 34.05611346333418}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate inputs\n    close = security.array(close)\n    length = int(length) if length and length > 0 else 10\n    mamode = mamode.lower() if mamode else \"sma\"\n    offset = int(offset) if offset and offset!= 0 else 0\n    # Calculate moving average\n    ma = security.moving_average(close, length, mamode)\n    # Calculate bias\n    bias = (close - ma) / ma\n    # Shift and fill\n    bias = bias.shift(offset, fill_method=\"ffill\")\n    # Name and category the result\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n    return bias\n\n", "CodeBleu_score": 28.266873526325952}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        if broker == 'ok':\n            url = 'https://www.okex.com/api/spot/v3/instruments/%s-usdt/candles?granularity=%s&limit=%s' % (code, ktype, size)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data, columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n            if ktype == 'D':\n                df['date'] = pd.to_datetime(df['date'], unit='ms')\n            elif ktype == '60':\n                df['date'] = pd.to_datetime(df['date'], unit='s')\n            else:\n                df['date'] = pd.to_datetime(df['date'], unit='ms')\n            return df\n        elif broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period=%smin&size=%s&symbol=%susdt' % (ktype, size, code)\n            data = requests.get(url).json()['data']\n            df = pd.DataFrame(data, columns=['id', 'open', 'close', 'low', 'high', 'amount', 'vol', 'count'])\n            df['id'] = pd.to_datetime(df['id'], unit='ms')\n            df['date'] = df['id']\n            df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n            df['date'] = pd.to_datetime(df['date'])\n            df = df[['date', 'open', 'high', 'low', 'close', 'vol']]\n            return df\n        elif broker == 'binance':\n            url = 'https://api.binance.com/api/v3/klines?symbol=%susdt&interval=%smin&limit=%s' % (code, ktype, size)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data, columns=['id', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore'])\n            df['id'] = pd.to", "CodeBleu_score": 35.28774805245642}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        if broker == 'okex':\n            url = 'https://www.okex.com/api/spot/v3/instruments/{}/candles?granularity={}&limit={}'.format(code, ktype, size)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data, columns=['id', 'open', 'high', 'low', 'close', 'volume'])\n            df['id'] = pd.to_datetime(df['id'], unit='ms')\n        elif broker == 'binance':\n            url = 'https://api.binance.com/api/v3/klines?symbol={}&interval={}&limit={}'.format(code, ktype, size)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data, columns=['id', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])\n            df['id'] = pd.to_datetime(df['id'], unit='ms')\n        elif broker == 'huobi':\n            url = 'https://api.huobi.pro/market/history/kline?period={}&size={}&symbol={}'.format(ktype, size, code)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data['data'], columns=['id', 'open', 'close', 'low', 'high', 'volume'])\n            df['id'] = pd.to_datetime(df['id'], unit='ms')\n        elif broker == 'bitfinex':\n            url = 'https://api.bitfinex.com/v2/candles/trade:{}:{}/hist?limit={}'.format(ktype, code, size)\n            data = requests.get(url).json()\n            df = pd.DataFrame(data, columns=['id', 'open', 'close', 'high', 'low', 'volume'])\n            df['id'] = pd.to_datetime(df['id'], unit='ms')\n        elif broker == 'bitmex':\n            url = 'https://www.bitmex.com/api/v1/trade/bucketed?bin", "CodeBleu_score": 29.6300167273177}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        if broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period={}&size={}&symbol={}'.format(ktype, size, code.upper())\n            df = pd.DataFrame(requests.get(url).json()['data'])\n            df.columns = ['amount', 'vol', 'count', 'open', 'close', 'low', 'high', 'id']\n            df['date'] = pd.to_datetime(df['id'], unit='ms')\n        elif broker == 'binance':\n            url = 'https://api.binance.com/api/v3/klines?symbol={}&interval={}&limit={}'.format(code.upper(), ktype, size)\n            df = pd.DataFrame(requests.get(url).json())\n            df.columns = ['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'trades_num', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n            df['date'] = pd.to_datetime(df['open_time'], unit='ms')\n        elif broker == 'bitmex':\n            url = 'https://www.bitmex.com/api/v1/trade/bucketed?binSize={}&partial=false&symbol={}&count={}&reverse=true'.format(ktype, code.upper(), size)\n            df = pd.DataFrame(requests.get(url).json())\n            df['date'] = pd.to_datetime(df['timestamp'])\n        elif broker == 'bitfinex':\n            url = 'https://api-pub.bitfinex.com/v2/candles/trade:{}:t{}/hist?limit={}'.format(ktype, code.upper(), size)\n            df = pd.DataFrame(requests.get(url).json())\n            df.columns = ['mts', 'open', 'close', 'high', 'low', 'volume']\n            df['date'] = pd.to_datetime(df['mts'], unit='ms')\n        elif broker == 'okex':\n            url = 'https://www.okex.com/api/v5/market/candles?instId={}&bar", "CodeBleu_score": 33.241175759848964}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        data = _get_data(broker, code, size)\n        if broker == 'ok':\n            timestamp = data['timestamp']\n            ask_data = pd.DataFrame(data['asks'])\n            bid_data = pd.DataFrame(data['bids'])\n        elif broker == 'hb':\n            timestamp = data['ts']\n            ask_data = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid_data = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n        else:\n            raise ValueError('Broker not supported.')\n        ask_data['timestamp'] = timestamp\n        bid_data['timestamp'] = timestamp\n        data = {'ask': ask_data, 'bid': bid_data}\n        return pd.Panel(data)\n    except Exception:\n        traceback.print_exc()\n\n", "CodeBleu_score": 42.77706273661204}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        data = _get_data(broker, code, size)\n        if broker == 'ok':\n            data['asks'] = data['asks'][::-1]\n            data['bids'] = data['bids'][::-1]\n            data['asks'] = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            data['bids'] = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            data['asks']['timestamp'] = data['timestamp']\n            data['bids']['timestamp'] = data['timestamp']\n        elif broker == 'hb':\n            data['asks'] = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            data['bids'] = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            data['asks']['timestamp'] = data['ts']\n            data['bids']['timestamp'] = data['ts']\n        elif broker == 'zb':\n            data['asks'] = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            data['bids'] = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            data['asks']['timestamp'] = data['timestamp']\n            data['bids']['timestamp'] = data['timestamp']\n        else:\n            print('Broker not supported')\n            return None\n        return pd.Panel(data)\n    except Exception as e:\n        print(traceback.format_exc())\n        return None\n\n", "CodeBleu_score": 34.459125651460965}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        data = _fetch_data(broker, code, size)\n        if broker == 'okex':\n            timestamp = int(data['data']['timestamp'])\n        elif broker == 'huobi':\n            timestamp = int(data['data']['ts']) / 1000\n        else:\n            timestamp = int(data['ts'])\n        ask = pd.DataFrame(data['tick']['asks'], columns=['price', 'volume'])\n        bid = pd.DataFrame(data['tick']['bids'], columns=['price', 'volume'])\n        ask['timestamp'] = timestamp\n        bid['timestamp'] = timestamp\n        return pd.Panel({'ask': ask, 'bid': bid})\n    except Exception:\n        traceback.print_exc()\n\n", "CodeBleu_score": 35.59350090735154}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n    url = 'https://api.huobi.pro/market/history/trade?symbol={0}usdt&size=1000'.format(code)\n    if broker == 'ok':\n        url = 'https://www.okex.com/api/spot/v3/instruments/{0}-usdt/trades?limit=1000'.format(code)\n    if broker == 'chbtc':\n        url = 'https://www.chbtc.com/api/data/v1/trades?symbol={0}usdt&size=1000'.format(code)\n    data = requests.get(url).json()\n    if data is not None:\n        if broker == 'hb':\n            data = pd.DataFrame(data['data'])\n            data = data[['id', 'ts', 'price', 'amount']]\n            data['datetime'] = data['ts'].apply(lambda x: datetime.datetime.fromtimestamp(x / 1000))\n            data = data[['id', 'datetime', 'price', 'amount']]\n        if broker == 'ok':\n            data = pd.DataFrame(data['data'])\n            data = data[['trade_id', 'timestamp', 'price','size']]\n            data['datetime'] = data['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x / 1000))\n            data = data[['trade_id', 'datetime', 'price','size']]\n        if broker == 'chbtc':\n            data = pd.DataFrame(data['data'])\n            data = data[['tid', 'date', 'price', 'amount']]\n            data['datetime'] = data['date'].apply(lambda x: datetime.datetime.fromtimestamp(x / 1000))\n            data = data[['tid', 'datetime', 'price', 'amount']]\n        data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n        data['type'] = 'buy'\n        return data\n\n", "CodeBleu_score": 42.387882292481876}, {"code": "def coins_trade(broker='hb', code='btc')\n    # Fetch data from a URL based on the broker and cryptocurrency code\n    if broker == 'hb':\n        url = 'https://api.huobi.pro/market/history/trade?symbol=%susdt&size=5000' % code\n    elif broker == 'ok':\n        url = 'https://www.okex.com/api/v1/trades.do?symbol=%susdt' % code\n    elif broker == 'chbtc':\n        url = 'https://www.chbtc.com/api/trade?symbol=%susdt' % code\n    else:\n        raise ValueError('Unsupported broker')\n    data = pd.read_json(url)\n    # Process data based on the broker\n    if broker == 'hb':\n        data = data.loc[data['status'] == 'ok', ['data']].iloc[0, 0]\n        data = pd.DataFrame(data)\n        data['amount'] = data['amount'].astype(float)\n        data['price'] = data['price'].astype(float)\n        data['type'] = data['direction'].map({'buy': 'bid','sell': 'ask'})\n        data = data.loc[:, ['id', 'ts', 'price', 'amount', 'type']]\n        data['datetime'] = pd.to_datetime(data['ts'], unit='ms')\n        data = data.drop(['ts'], axis=1)\n    elif broker == 'ok':\n        data = data.loc[data['result'] == True, ['data']].iloc[0, 0]\n        data = pd.DataFrame(data)\n        data['amount'] = data['amount'].astype(float)\n        data['price'] = data['price'].astype(float)\n        data['type'] = data['type'].map({'buy-market': 'bid','sell-market': 'ask'})\n        data = data.loc[:, ['tid', 'date', 'price', 'amount', 'type']]\n        data['datetime'] = pd.to_datetime(data['date'], unit='ms')\n        data = data.drop(['date'], axis=1)\n    elif broker == 'chbtc':\n        data = data.loc[data['result'] == True, ['data']].iloc[0, 0]\n        data = pd.DataFrame(data)\n        data['amount'] = data['amount'].astype(float)\n        data", "CodeBleu_score": 36.739446545249535}, {"code": "def coins_trade(broker='hb', code='btc')\n    data = None\n    if broker == 'hb':\n        data = requests.get('https://api.huobi.pro/market/history/trade?symbol={0}usdt&size=2000'.format(code)).json()\n    elif broker == 'ok':\n        data = requests.get('https://www.okex.com/api/v1/trades.do?symbol={0}usdt&size=2000'.format(code)).json()\n    elif broker == 'chbtc':\n        data = requests.get('https://www.chbtc.com/api/trades?symbol={0}usdt'.format(code)).json()\n    if data is not None:\n        if broker == 'hb':\n            data = pd.DataFrame(data['data'])\n            data = data[['id', 'ts', 'price', 'amount']]\n            data.columns = ['tid', 'datetime', 'price', 'amount']\n            data['datetime'] = data['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n        elif broker == 'ok':\n            data = pd.DataFrame(data['trades'])\n            data = data[['tid', 'date', 'price', 'amount']]\n            data.columns = ['tid', 'datetime', 'price', 'amount']\n            data['datetime'] = data['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n        elif broker == 'chbtc':\n            data = pd.DataFrame(data)\n            data = data[['tid', 'date', 'price', 'amount']]\n            data.columns = ['tid', 'datetime', 'price', 'amount']\n            data['datetime'] = data['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n        data['type'] = data['amount'].apply(lambda x: 'buy' if x > 0 else'sell')\n        data['amount'] = data['amount'].apply(lambda x: abs(x))\n        data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n        return data\n\n", "CodeBleu_score": 45.03605738490196}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n    print(\"Parsing fund data from: {}\".format(url))\n    try:\n        response = requests.get(url)\n    except Exception as e:\n        print(\"Error: {}\".format(e))\n        return None\n    try:\n        text = response.content.decode(\"utf-8\")\n    except Exception as e:\n        print(\"Error: {}\".format(e))\n        return None\n    try:\n        data = json.loads(text)\n    except Exception as e:\n        print(\"Error: {}\".format(e))\n        return None\n    if fund_type == 'open':\n        df = pd.DataFrame(data['open'], columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n    elif fund_type == 'close':\n        df = pd.DataFrame(data['close'], columns=['Date', 'Close'])\n    df.fillna(0, inplace=True)\n    return df\n", "CodeBleu_score": 31.084797601222647}, {"code": "def _parse_fund_data(url, fund_type='open')\n    print(\"Parsing data from %s\" % url)\n    try:\n        response = requests.get(url)\n        if response.encoding == 'ISO-8859-1':\n            response.encoding = 'utf-8'\n        data = response.text.splitlines()\n        data = [d.strip() for d in data if d.strip()!= '']\n        data = [d.split(';') for d in data]\n        data = [d for d in data if d[0]!= '']\n        data = [[d[0], float(d[1].replace(',', '.'))] for d in data]\n        if fund_type == 'open':\n            df = pd.DataFrame(data, columns=['date', 'open'])\n        elif fund_type == 'close':\n            df = pd.DataFrame(data, columns=['date', 'close'])\n        df = df.fillna(0)\n        return df\n    except Exception as e:\n        print(e)\n\n", "CodeBleu_score": 36.04802400410932}, {"code": "def _parse_fund_data(url, fund_type='open')\n    print(\"Retrieving fund data from \" + url)\n    try:\n        response = requests.get(url)\n        response.encoding = 'utf-8'\n        text = response.text\n        json_text = text.replace(\"'\", '\"')\n        json_text = json_text.replace('u\"', '\"')\n        json_data = json.loads(json_text)\n        df = pd.DataFrame(json_data['funds'], columns=['fund_id', 'fund_name', 'fund_type', 'fund_category', 'fund_currency', 'fund_value', 'fund_change', 'fund_change_pct', 'fund_url'])\n        df = df.fillna(0)\n        return df\n    except Exception as e:\n        print(\"Error: \" + str(e))\n        return None\n", "CodeBleu_score": 31.85493614590598}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if start == None:\n        start = date.today()\n    if end == None:\n        end = date.today()\n    if market == 'CFFEX':\n        market = 'cffex'\n    elif market == 'DCE':\n        market = 'dce'\n    elif market == 'SHFE':\n        market ='shfe'\n    elif market == 'CZCE':\n        market = 'czce'\n    else:\n        return None\n    start = start.strftime('%Y%m%d')\n    end = end.strftime('%Y%m%d')\n    data = []\n    for i in range((end - start).days + 1):\n        d = (start + timedelta(days=i)).strftime('%Y%m%d')\n        try:\n            df = get_daily(d, market)\n            data.append(df)\n        except:\n            pass\n    if len(data) > 0:\n        return pd.concat(data)\n    else:\n        return None\n", "CodeBleu_score": 41.80113709291826}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if start is None:\n        start = datetime.datetime.now()\n    if end is None:\n        end = datetime.datetime.now()\n    if market == 'CFFEX':\n        func = get_future_daily_cffex\n    elif market == 'DCE':\n        func = get_future_daily_dce\n    elif market == 'SHFE':\n        func = get_future_daily_shfe\n    elif market == 'CZCE':\n        func = get_future_daily_czce\n    else:\n        raise ValueError('Market not supported.')\n    data = []\n    for date in pd.date_range(start, end):\n        date_str = date.strftime('%Y%m%d')\n        data.append(func(date_str))\n    return pd.concat(data, axis=0)\n", "CodeBleu_score": 40.333778948092146}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    if start is None:\n        start = datetime.today().strftime('%Y-%m-%d')\n    if end is None:\n        end = datetime.today().strftime('%Y-%m-%d')\n    df_list = []\n    for date in pd.date_range(start, end):\n        if market == 'CFFEX':\n            df = get_future_daily_cffex(date)\n        elif market == 'SHFE':\n            df = get_future_daily_shfe(date)\n        elif market == 'DCE':\n            df = get_future_daily_dce(date)\n        elif market == 'CZCE':\n            df = get_future_daily_czce(date)\n        else:\n            raise ValueError('Market not supported.')\n        if not df.empty:\n            df_list.append(df)\n    if df_list:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()\n\n", "CodeBleu_score": 40.92569260758005}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)    url = 'https://www.boxofficemojo.com/chart/realtime/world/'\n    for _ in range(retry_count):\n        try:\n            response = requests.get(url)\n            if response.ok:\n                data = response.json()\n                df = pd.DataFrame(data['data2'])\n                df = df.drop(columns=['MovieImg','mId'])\n                df['timestamp'] = datetime.now()\n                return df\n        except:\n            time.sleep(pause)", "CodeBleu_score": 30.19906552174057}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)    for _ in range(retry_count):\n        try:\n            response = requests.get(realtime_boxoffice_url)\n            response.raise_for_status()\n            data = response.json()\n            if data['data2']:\n                df = pd.DataFrame(data['data2'])\n                df = df.drop(['MovieImg','mId'], axis=1)\n                df['timestamp'] = datetime.now()\n                return df\n        except requests.exceptions.HTTPError as errh:\n            print(\"Http Error:\",errh)\n        except requests.exceptions.ConnectionError as errc:\n            print(\"Error Connecting:\",errc)\n        except requests.exceptions.Timeout as errt:\n            print(\"Timeout Error:\",errt)\n        except requests.exceptions.RequestException as err:\n            print(\"OOps: Something Else\",err)\n        time.sleep(pause)\n", "CodeBleu_score": 37.9687188229305}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)    for _ in range(retry_count):\n        try:\n            response = requests.get(realtime_boxoffice_url)\n            response.raise_for_status()\n            data = response.json()\n            df = pd.DataFrame(data['data2'])\n            df = df.drop(columns=['MovieImg','mId'])\n            df['timestamp'] = datetime.now()\n            return df\n        except requests.exceptions.RequestException:\n            print(f'Error fetching realtime box office data, retrying in {pause} seconds...')\n            time.sleep(pause)\n    return None\n", "CodeBleu_score": 33.926583305632704}]}
