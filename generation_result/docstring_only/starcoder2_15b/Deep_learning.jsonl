{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n    # Load from cache if available\n    if self.config.get(\"use_cache\", False):\n        model = self.load_from_cache(half_attention)\n        if model is not None:\n            return model\n\n    # Load from path\n    model = self.load_from_path(half_attention)\n\n    # Configure model\n    model = self.configure_model(model)\n\n    # Move to device\n    model = model.to(self.device)\n\n    # Set model to eval mode\n    model.eval()\n\n    # Cache model if needed\n    if self.config.get(\"use_cache\", False):\n        self.cache_model(model, half_attention)\n\n    return model\n\n", "CodeBleu_score": 19.648827395584682}, {"code": "def load_model_from_config(self, half_attention)\n    if self.config.get(\"cache_dir\", None) is not None:\n        cache_dir = os.path.join(self.config[\"cache_dir\"], self.config[\"model_name\"])\n        os.makedirs(cache_dir, exist_ok=True)\n        cache_file = os.path.join(cache_dir, \"model.safetensors\")\n        if os.path.exists(cache_file):\n            if os.path.getmtime(cache_file) > os.path.getmtime(self.config[\"model_path\"]):\n                print(\"Loading model from cache...\")\n                return torch.load(cache_file, map_location=self.device)\n    print(\"Loading model from file...\")\n    if self.config[\"model_path\"].endswith(\".safetensors\"):\n        model = torch.load_safetensors(self.config[\"model_path\"], device=self.device)\n    else:\n        model = torch.load(self.config[\"model_path\"], map_location=self.device)\n    if \"config\" in model:\n        model_config = model[\"config\"]\n    else:\n        model_config = model.config\n    model = model[\"model\"]\n    model.eval()\n    model.to(self.device)\n    if half_attention:\n        model.half()\n    if self.config.get(\"optimize_memory\", False):\n        model.to(memory_format=torch.channels_last)\n    if self.config.get(\"cache_dir\", None) is not None:\n        torch.save(model, cache_file)\n    return model", "CodeBleu_score": 38.82153422737026}, {"code": "def load_model_from_config(self, half_attention)\n    if self.config_path is None:\n        raise ValueError(\"Please specify a config path\")\n    if self.model_path is None:\n        raise ValueError(\"Please specify a model path\")\n    if self.cache_path is None:\n        raise ValueError(\"Please specify a cache path\")\n    if self.cache_path.exists():\n        if self.cache_path.stat().st_mtime > self.config_path.stat().st_mtime and self.cache_path.stat().st_mtime > self.model_path.stat().st_mtime:\n            with open(self.cache_path, \"rb\") as f:\n                model = torch.load(f)\n            print(\"Loaded model from cache\")\n            return model\n    with open(self.config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    model = load_model(config, self.model_path, device=self.device, half_attention=half_attention)\n    model.eval()\n    if self.cache_path.exists():\n        if self.cache_path.stat().st_mtime > self.config_path.stat().st_mtime and self.cache_path.stat().st_mtime > self.model_path.stat().st_mtime:\n            with open(self.cache_path, \"wb\") as f:\n                torch.save(model, f)\n            print(\"Cached model\")\n    return model\n", "CodeBleu_score": 31.1666938186945}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    return sampler.sample(\n        steps, batch_size, shape, cond, eta, callback=callback,\n        normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n        temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n", "CodeBleu_score": 34.39774110511743}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(\"eta:\", eta, \"steps:\", steps)\n    samples, values = sampler.sample(\n        steps=steps,\n        batch_size=batch_size,\n        shape=shape,\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n    return samples, values\n", "CodeBleu_score": 49.46050868098518}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    # This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler.\n    # It initializes the sampler with the provided model, extracts the batch size from the shape parameter, and prints the eta and steps values.\n    # The function then calls the sample method of the DDIMSampler instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t.\n    # Finally, it returns the generated samples and intermediate values.\n    # This function is used to generate samples from a diffusion model using the DDIM (Denoising Diffusion Implicit Models) sampler.\n    # Parameters:\n    # model: The diffusion model to use for sampling.\n    # cond: The conditioning data to use for the sampling process.\n    # steps: The number of sampling steps to perform.\n    # shape: The shape of the generated samples.\n    # eta: The scaling factor for the sampling process.\n    # callback: A callback function to be called during the sampling process.\n    # normals_sequence: A sequence of normal random variables to use for the sampling process.\n    # mask: A mask to apply to the generated samples.\n    # x0: The initial value for the sampling process.\n    # quantize_x0: A flag indicating whether to quantize the initial value.\n    # temperature: The temperature to use for the sampling process.\n    # score_corrector: A score corrector to use for the sampling process.\n    # corrector_kwargs: Additional keyword arguments for the score corrector.\n    # x_t: The final value for the sampling process.\n    # Returns:\n    # The generated samples and intermediate values.\n    # This function is used to generate samples from a diffusion model using the DDIM (Denoising Diffusion Implicit Models) sampler.\n    # Parameters:\n    # model: The diffusion model to use for sampling.\n    # cond: The conditioning data to use for the sampling process.\n    # steps: The number of sampling steps to perform.\n    # shape: The shape of the generated samples.\n    # eta: The scaling factor for the sampling process.\n    # callback: A callback function to be called during the sampling process.\n    # normals_sequence: A sequence of normal random variables", "CodeBleu_score": 11.151981997576597}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if self.ema_enabled:\n      if context is not None:\n        print('Using EMA weights')\n      self.model.set_weights(self.ema_weights)\n    else:\n      if context is not None:\n        print('Using model weights')\n    yield\n    if self.ema_enabled:\n      if context is not None:\n        print('Restoring model weights')\n      self.model.set_weights(self.model_weights)\n", "CodeBleu_score": 50.43612851727873}, {"code": "def ema_scope(self, context=None)\n    if self.ema_decay is not None:\n      if context is not None:\n        print('Using EMA weights')\n      self.model.set_weights(self.ema_weights)\n      yield\n      if context is not None:\n        print('Restoring weights')\n      self.model.set_weights(self.ema_weights)\n    else:\n      yield\n\n  def train(self, x, y, batch_size=32, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False):\n    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\\n\"\"\"\n    if self.ema_decay is not None:\n      self.ema_weights = self.model.get_weights()\n    if self.ema_decay is not None:\n      callbacks = callbacks or []\n      callbacks.append(self.ema_callback)\n    self.model.fit(x, y, batch_size=batch_size, epochs=epochs, verbose=verbose, callbacks=callbacks, validation_split=validation_split, validation_data=validation_data, shuffle=shuffle, class_weight=class_weight, sample_weight=sample_weight, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, validation_freq=validation_freq, max_queue_size=max_queue_size, workers=workers, use_multiprocessing=use_multiprocessing)\n\n  def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False):\n    \"\"\"Returns the loss value & metrics values for the model in test mode.\\n\"\"\"\n    if self.ema_decay is not None:\n      callbacks = callbacks or []\n      callbacks.append(self.ema_callback)\n    return self.model.evaluate(x, y, batch_size=batch_size, verbose=verbose, sample_", "CodeBleu_score": 36.35168623707205}, {"code": "def ema_scope(self, context=None)\n    if self.ema_enabled:\n      if context is not None:\n        context.print_info(\"Switching to EMA weights\")\n      self.model.set_weights(self.ema_weights)\n    yield\n    if self.ema_enabled:\n      if context is not None:\n        context.print_info(\"Restoring original weights\")\n      self.model.set_weights(self.ema_weights_backup)\n\n  def _get_ema_weights(self):\n    \"\"\"This function returns the current EMA weights of the model.\\nIf EMA is not enabled, it returns the model's current weights.\\n\"\"\"\n    if self.ema_enabled:\n      return self.model.get_weights()\n    else:\n      return self.ema_weights\n\n  def _set_ema_weights(self, weights):\n    \"\"\"This function sets the EMA weights of the model to the given weights.\\nIf EMA is not enabled, it sets the model's weights to the given weights.\\n\"\"\"\n    if self.ema_enabled:\n      self.ema_weights = weights\n    else:\n      self.model.set_weights(weights)\n\n  def _update_ema_weights(self):\n    \"\"\"This function updates the EMA weights of the model.\\nIf EMA is not enabled, it does nothing.\\n\"\"\"\n    if self.ema_enabled:\n      self.ema_weights = [self.ema_decay * w + (1 - self.ema_decay) * w_ema for w, w_ema in zip(self.model.get_weights(), self.ema_weights)]\n\n  def _reset_ema_weights(self):\n    \"\"\"This function resets the EMA weights of the model to the model's current weights.\\nIf EMA is not enabled, it does nothing.\\n\"\"\"\n    if self.ema_enabled:\n      self.ema_weights = self.model.get_weights()\n\n  def _update_ema_weights_backup(self):\n    \"\"\"This function updates the EMA weights backup of the model.\\nIf EMA is not enabled, it does nothing.\\n\"\"\"\n    if self.ema_enabled:\n      self.ema_weights_backup = self.ema_weights\n\n  def _reset_ema_weights_backup(self):\n    \"\"\"This function resets the EMA weights backup of the model to the model's current weights.\\nIf EMA is not enabled, it", "CodeBleu_score": 40.37837710421368}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path, map_location=self.device)\n    state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    print(f\"Missing keys: {missing_keys}\")\n    print(f\"Unexpected keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 30.96428113118667}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path, map_location=\"cpu\")\n    state_dict_keys = list(state_dict.keys())\n    missing_keys = []\n    unexpected_keys = []\n    for key in state_dict_keys:\n        if key in ignore_keys:\n            del state_dict[key]\n        elif key not in self.state_dict().keys():\n            missing_keys.append(key)\n        elif self.state_dict()[key].size()!= state_dict[key].size():\n            unexpected_keys.append(key)\n    self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing keys: \", missing_keys)\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected keys: \", unexpected_keys)", "CodeBleu_score": 39.472303284152346}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del self.state_dict()[k]\n    self.load_state_dict(self.state_dict(), strict=False)\n    print('Missing keys: {}'.format(self.missing_keys))\n    print('Unexpected keys: {}'.format(self.unexpected_keys))\n\n", "CodeBleu_score": 21.45102427083316}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n    # Encode input\n    encoded_input = self.encoder(input)\n    # Decode encoded input\n    decoded_output = self.decoder(encoded_input)\n    # Calculate difference metric\n    difference = torch.abs(input - decoded_output)\n    difference_metric = torch.mean(difference)\n    if return_pred_indices:\n      return decoded_output, difference_metric, self.prediction_indices\n    return decoded_output, difference_metric\n\n  def fit(self, train_dataloader, val_dataloader, epochs=100, lr=0.001, print_every=10, save_every=10, save_path=None, save_name='autoencoder', save_best=False):\n    \"\"\"This function trains the autoencoder on the training data and validates it on the validation data.\\nIt prints the training progress and saves the model at the specified intervals.\\n\"\"\"\n    # Initialize optimizer and loss function\n    optimizer = optim.Adam(self.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n    # Initialize variables for best validation loss and epoch\n    best_val_loss = float('inf')\n    best_epoch = 0\n    # Initialize lists for training and validation loss\n    train_losses = []\n    val_losses = []\n    # Iterate over specified number of epochs\n    for epoch in range(epochs):\n      # Initialize lists for training and validation loss for current epoch\n      train_loss = []\n      val_loss = []\n      # Train the model on the training data\n      self.train()\n      for batch in train_dataloader:\n        # Move batch to GPU if available\n        if torch.cuda.is_available():\n          batch = batch.cuda()\n        # Reset gradients\n        optimizer.zero_grad()\n        # Calculate loss\n        loss = loss_fn(self.forward(batch)[0], batch)\n        # Backpropagate loss\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        # Add loss to list for current epoch\n        train_loss.append(loss.item())\n      # Calculate average training loss for current epoch\n      train_loss = np.mean(train_loss)\n      # Add average training loss for current epoch to list of training losses\n      train_losses.append(train_loss)\n      # Validate the model on the validation data\n      self.eval()\n      for batch in val_dataloader:\n        # Move batch to GPU if available\n        if", "CodeBleu_score": 23.7506124731135}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode the input\n    encoded_input = self.encoder(input)\n    # Decode the encoded input\n    decoded_output = self.decoder(encoded_input)\n    # Compute the difference between the input and the decoded output\n    difference = torch.abs(input - decoded_output)\n    # If return_pred_indices is True, compute the prediction indices\n    if return_pred_indices:\n      pred_indices = torch.argmax(difference, dim=1)\n      return decoded_output, difference, pred_indices\n    # Otherwise, return only the decoded output and the difference\n    return decoded_output, difference\n", "CodeBleu_score": 20.56010957487178}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode\n    encoded = self.encode(input)\n\n    # Decode\n    decoded = self.decode(encoded)\n\n    # Calculate difference metric\n    difference = torch.sum(torch.abs(input - decoded))\n\n    # Return\n    if return_pred_indices:\n      return decoded, difference, self.pred_indices\n    else:\n      return decoded, difference\n\n  def train(self, input, return_pred_indices=False):\n    \"\"\"This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\\nOptionally, it can also return prediction indices if specified.\\n\"\"\"\n    # Encode\n    encoded = self.encode(input)\n\n    # Decode\n    decoded = self.decode(encoded)\n\n    # Calculate difference metric\n    difference = torch.sum(torch.abs(input - decoded))\n\n    # Return\n    if return_pred_indices:\n      return decoded, difference, self.pred_indices\n    else:\n      return decoded, difference\n\n  def predict(self, input):\n    \"\"\"This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\\nOptionally, it can also return prediction indices if specified.\\n\"\"\"\n    # Encode\n    encoded = self.encode(input)\n\n    # Decode\n    decoded = self.decode(encoded)\n\n    # Return\n    return decoded\n\n  def save(self, path):\n    \"\"\"This function saves the model to a specified path.\"\"\"\n    # Save model\n    torch.save(self.state_dict(), path)\n\n  def load(self, path):\n    \"\"\"This function loads the model from a specified path.\"\"\"\n    # Load model\n    self.load_state_dict(torch.load(path))", "CodeBleu_score": 31.047018249169646}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n    x = batch[k]\n    if x.dim() == 2:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    if self.resize:\n        x = F.interpolate(x, size=(self.size, self.size), mode=\"bicubic\", align_corners=False)\n    x = x.to(self.device, dtype=torch.float32)\n    x = x.detach()\n    return x\n", "CodeBleu_score": 32.57932342162947}, {"code": "def get_input(self, batch, k)\n    if self.batch_resizing:\n        input = batch[k].detach().float().unsqueeze(0).permute(0, 3, 1, 2).contiguous()\n        input = F.interpolate(input, scale_factor=self.resize_factor, mode=\"bicubic\", align_corners=False)\n    else:\n        input = batch[k].detach().float().unsqueeze(0).permute(0, 3, 1, 2).contiguous()\n    return input\n", "CodeBleu_score": 26.749424530996652}, {"code": "def get_input(self, batch, k)\n    return batch[k].permute(0, 3, 1, 2).contiguous().float().detach()\n", "CodeBleu_score": 1.9223044314801256}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    z = self.encoder(x)\n    x_hat = self.decoder(z)\n\n    if optimizer_idx == 0:\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    elif optimizer_idx == 1:\n        real_pred = self.discriminator(x)\n        fake_pred = self.discriminator(x_hat.detach())\n        real_loss = F.mse_loss(real_pred, torch.ones_like(real_pred))\n        fake_loss = F.mse_loss(fake_pred, torch.zeros_like(fake_pred))\n        loss = (real_loss + fake_loss) / 2\n        self.log(\"train_loss_disc\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n", "CodeBleu_score": 20.287021248593273}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x_hat, mu, logvar = self(x)\n    # Reconstruction loss\n    recon_loss = F.mse_loss(x_hat, x)\n    # KL divergence\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    # Total loss\n    loss = recon_loss + kl_loss\n    # Log metrics\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    self.log(\"train_recon_loss\", recon_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    self.log(\"train_kl_loss\", kl_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n", "CodeBleu_score": 22.484095183107573}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, _ = batch\n    x_hat = self.forward(x)\n\n    if optimizer_idx == 0:\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n        return loss\n\n    elif optimizer_idx == 1:\n        real_preds = self.discriminator(x)\n        fake_preds = self.discriminator(x_hat.detach())\n        real_loss = F.binary_cross_entropy_with_logits(real_preds, torch.ones_like(real_preds))\n        fake_loss = F.binary_cross_entropy_with_logits(fake_preds, torch.zeros_like(fake_preds))\n        loss = (real_loss + fake_loss) / 2\n        self.log(\"train_loss_disc\", loss, on_epoch=True, on_step=False)\n        return loss\n", "CodeBleu_score": 18.83507313827481}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if self.args.log_images and self.args.log_images_interval!= 0 and self.global_step % self.args.log_images_interval == 0:\n        if self.args.log_images_only_inputs:\n            self.log_images_only_inputs(batch)\n        else:\n            self.log_images_inputs_and_reconstructions(batch, plot_ema=plot_ema, **kwargs)\n\n", "CodeBleu_score": 16.30111015868827}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if only_inputs:\n        images = batch['inputs']\n    else:\n        images = torch.cat([batch['inputs'], batch['reconstructions']], dim=0)\n    if self.channels == 1:\n        images = images.repeat(1, 3, 1, 1)\n    if self.log_ema and plot_ema:\n        ema_images = torch.cat([batch['inputs'], batch['ema_reconstructions']], dim=0)\n        if self.channels == 1:\n            ema_images = ema_images.repeat(1, 3, 1, 1)\n        images = torch.cat([images, ema_images], dim=0)\n    self.logger.experiment.add_images(f'{self.name}/images', images, self.global_step, dataformats='NCHW')\n\n", "CodeBleu_score": 28.662882720946087}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if self.log_images_freq == 0:\n        return\n    if self.log_images_freq > 0 and self.global_step % self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self.log_images_freq < 0 and self.global_step % -self.log_images_freq!= 0:\n        return\n    if self", "CodeBleu_score": 13.001820200683008}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n", "CodeBleu_score": 34.197735971701654}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = (pred - target)**2\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise Exception('Loss type not supported')\n    return loss\n", "CodeBleu_score": 45.291741199297554}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss.sum()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            return loss.mean()\n        else:\n            return loss.sum()\n    else:\n        raise ValueError('Unsupported loss type: {}'.format(self.loss_type))\n\n", "CodeBleu_score": 40.18973280199508}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n    x = x.clone()\n    t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    if self.conditioning_type == \"concat\":\n      c = self.conditioning_network(c)\n      x = torch.cat([x, c], dim=1)\n    elif self.conditioning_type == \"add\":\n      c = self.conditioning_network(c)\n      x = x + c\n    elif self.conditioning_type == \"mlp\":\n      c = self.conditioning_network(c)\n      x = torch.cat([x, c], dim=1)\n    else:\n      raise NotImplementedError(f\"Conditioning type {self.conditioning_type} not implemented\")\n    return self.model(x, t, *args, **kwargs)\n", "CodeBleu_score": 34.63994429561286}, {"code": "def forward(self, x, c, *args, **kwargs)\n    if not self.training:\n      return self.forward_with_ema(x, c, *args, **kwargs)\n    if self.training_steps == 0:\n      self.training_steps = self.config.n_training_steps\n    t = self.training_steps\n    self.training_steps -= 1\n    if self.config.timestep_embedding_size is not None:\n      x = torch.cat([x, self.timestep_embedding(t)], dim=1)\n    if self.config.conditioning_embedding_size is not None:\n      c = self.conditioning_embedding(c)\n    return self.model(x, c, t, *args, **kwargs)\n  def forward_with_ema(self, x, c, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model using exponential moving average (EMA) weights.\\nIt applies the EMA weights to the model's parameters and computes the loss for the given inputs and conditioning.\\nFinally, it returns the loss value.\\n\"\"\"\n    with torch.no_grad():\n      with self.ema_scope():\n        if self.config.timestep_embedding_size is not None:\n          x = torch.cat([x, self.timestep_embedding(self.training_steps)], dim=1)\n        if self.config.conditioning_embedding_size is not None:\n          c = self.conditioning_embedding(c)\n        return self.model(x, c, self.training_steps, *args, **kwargs)\n  def train(self, mode=True):\n    \"\"\"This function sets the model to training mode and updates the training steps.\\nIt also sets the model's parameters to require gradients and updates the EMA weights.\\n\"\"\"\n    super().train(mode)\n    if mode:\n      self.training_steps = self.config.n_training_steps\n    self.model.train(mode)\n    self.model.require_grad_(mode)\n    self.update_ema()\n  def eval(self):\n    \"\"\"This function sets the model to evaluation mode and disables gradient tracking.\\nIt also sets the model's parameters to not require gradients and updates the EMA weights.\\n\"\"\"\n    super().eval()\n    self.model.eval()\n    self.model.require_grad_(False)\n    self.update_ema()\n ", "CodeBleu_score": 38.73534208488596}, {"code": "def forward(self, x, c, *args, **kwargs)\n    return self.forward_with_ema(x, c, *args, **kwargs)\n\n  def forward_with_ema(self, x, c=None, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    if self.training:\n      return self.forward_train(x, c, *args, **kwargs)\n    else:\n      return self.forward_eval(x, c, *args, **kwargs)\n\n  def forward_train(self, x, c=None, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    if self.training:\n      return self.forward_train(x, c, *args, **kwargs)\n    else:\n      return self.forward_eval(x, c, *args, **kwargs)\n\n  def forward_eval(self, x, c=None, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    if self.training:\n      return self.forward_train(x, c, *args, **kwargs)\n    else:\n      return self.forward_eval(x, c, *args, **kwargs)\n\n  def forward_train(self, x, c=None, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    if self.training:\n     ", "CodeBleu_score": 20.68196609534809}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    indices = indices.view(-1)\n    if self.remap_indices:\n      indices = self.index_remapping[indices]\n    return self.codebook_entries.index_select(0, indices).view(shape)\n", "CodeBleu_score": 17.600013641596547}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_indices is not None:\n      indices = self.remap_indices[indices]\n    indices = indices.reshape(-1)\n    return self.codebook[indices].reshape(shape)\n", "CodeBleu_score": 26.567525163260846}, {"code": "def get_codebook_entry(self, indices, shape)\n    indices = indices.view(-1)\n    indices = self.map_ids(indices)\n    latent_vectors = self.codebook.embedding.weight[indices]\n    latent_vectors = latent_vectors.view(shape)\n    return latent_vectors\n", "CodeBleu_score": 12.316102877311383}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    m = int(math.sqrt(dimension))\n    n = dimension // m\n    while m > n:\n        m -= 1\n        n = dimension // m\n    while factor > 0 and m > factor:\n        m -= 1\n        n = dimension // m\n    return m, n\n\n", "CodeBleu_score": 32.75048139335195}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension//factor)\n        else:\n            raise ValueError(\"The specified factor does not divide the dimension.\")\n    else:\n        factor = math.ceil(math.sqrt(dimension))\n        while dimension % factor!= 0:\n            factor -= 1\n        return (factor, dimension//factor)\n", "CodeBleu_score": 24.41438223167019}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        factors = [i for i in range(1, dimension + 1) if dimension % i == 0]\n        if factor > 0:\n            factors = [i for i in factors if i <= factor]\n        factors.sort(key=lambda x: abs(x - dimension / x))\n        return factors[0], dimension // factors[0]\n", "CodeBleu_score": 27.567856656530754}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n    weight = orig_weight.to(updown.weight.device).to(updown.weight.dtype)\n    weight = weight.view(-1)\n    updown_weight = updown.weight.view(-1)\n    combined_weight = (\n        weight * self.input_weight_scale + updown_weight * self.updown_weight_scale\n    )\n    combined_weight = combined_weight / combined_weight.norm()\n    combined_weight = combined_weight * self.combined_weight_scale\n    return combined_weight - weight\n\n", "CodeBleu_score": 16.397578971895992}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if self.use_cuda:\n        orig_weight = orig_weight.cuda()\n        updown = updown.cuda()\n    else:\n        orig_weight = orig_weight.cpu()\n        updown = updown.cpu()\n    updown = updown.float()\n    orig_weight = orig_weight.float()\n    updown = updown.unsqueeze(1)\n    updown = updown.unsqueeze(1)\n    updown = updown.unsqueeze(1)\n    updown = updown.unsqueeze(1)\n    updown = updown.repeat(1, 3, 1, 1)\n    updown = updown.float()\n    if self.use_cuda:\n        updown = updown.cuda()\n    else:\n        updown = updown.cpu()\n    updown = updown.float()\n    orig_weight = orig_weight.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown = updown.float()\n    updown =", "CodeBleu_score": 16.23261602658613}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if self.weight_decompose_mode == \"none\":\n        return orig_weight\n    if self.weight_decompose_mode == \"add\":\n        combined_weight = orig_weight + updown\n    elif self.weight_decompose_mode == \"multiply\":\n        combined_weight = orig_weight * updown\n    else:\n        raise ValueError(\"Invalid weight decompose mode: {}\".format(self.weight_decompose_mode))\n    combined_weight = combined_weight.to(orig_weight.device)\n    combined_weight = combined_weight.type(orig_weight.dtype)\n    combined_weight = combined_weight / combined_weight.norm()\n    combined_weight = combined_weight * self.weight_decompose_scale\n    adjusted_weight = combined_weight - orig_weight\n    return adjusted_weight\n", "CodeBleu_score": 27.904253846444842}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n      ex_bias = ex_bias * self.scalar\n    if self.decomp_type is not None:\n      updown = self.decomp_fn(updown, self.decomp_type)\n    if updown.shape.num_elements() == orig_weight.shape.num_elements():\n      updown = tf.reshape(updown, orig_weight.shape)\n    else:\n      updown = tf.reshape(updown, output_shape)\n    return updown, ex_bias\n", "CodeBleu_score": 35.55425834036527}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n      if self.scale is not None:\n        updown = updown * self.scale\n      updown = updown + ex_bias\n    updown = tf.reshape(updown, output_shape)\n    if orig_weight is not None and tf.size(orig_weight) == tf.size(updown):\n      updown = tf.reshape(updown, tf.shape(orig_weight))\n    if self.bias is not None:\n      updown = updown + self.bias\n    if self.scale is not None:\n      updown = self.decompose(updown)\n    return updown, ex_bias\n", "CodeBleu_score": 45.512848356648796}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scaling_factor is not None:\n        updown = self.decompose_weight(updown)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    return updown, ex_bias", "CodeBleu_score": 34.859452198199804}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n    if self.bias is not None:\n      bias = self.bias.to(orig_weight.device)\n    else:\n      bias = None\n    return self._calc_updown(orig_weight, self.weight.to(orig_weight.device), bias)\n\n  def _calc_updown(self, orig_weight, weight, bias):\n    \"\"\"This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the other calc_updown function, and is implemented by the user.\\n\"\"\"\n    raise NotImplementedError(\"calc_updown is not implemented.\")\n\n  def _update_weight(self, weight, orig_weight):\n    \"\"\"This function updates the weight tensor of the module based on the original weight tensor and the updated weight tensor.\\nIt is called by the other update_weight function, and is implemented by the user.\\n\"\"\"\n    raise NotImplementedError(\"update_weight is not implemented.\")\n\n  def update_weight(self, orig_weight):\n    \"\"\"This function updates the weight tensor of the module based on the original weight tensor and the updated weight tensor.\\nIt transfers the updated weight tensor to the device of the original weight tensor, then calls another function to finalize the weight update.\\n\"\"\"\n    self._update_weight(self.weight.to(orig_weight.device), orig_weight)\n\n  def _update_bias(self, bias, orig_bias):\n    \"\"\"This function updates the bias tensor of the module based on the original bias tensor and the updated bias tensor.\\nIt is called by the other update_bias function, and is implemented by the user.\\n\"\"\"\n    raise NotImplementedError(\"update_bias is not implemented.\")\n\n  def update_bias(self, orig_bias):\n    \"\"\"This function updates the bias tensor of the module based on the original bias tensor and the updated bias tensor.\\nIt transfers the updated bias tensor to the device of the original bias tensor, then calls another function to finalize the bias update.\\n\"\"\"\n    self._update_bias(self.bias.to(orig_bias.device), orig_bias)\n\n  def _update_running_mean(self, running_mean, orig_running_mean):\n    \"\"\"This function updates the running mean tensor of the module based on the original running mean tensor and the updated running mean tensor.\\nIt is called by the other update_running_mean function, and is implemented by the user", "CodeBleu_score": 38.62481927863538}, {"code": "def calc_updown(self, orig_weight)\n    return self.calc_updown_helper(orig_weight, self.weight.to(orig_weight.device), self.bias)\n\n  def calc_updown_helper(self, orig_weight, cur_weight, cur_bias):\n    \"\"\"This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt calls another function to finalize the updated weights.\\n\"\"\"\n    return self.calc_updown_helper_helper(orig_weight, cur_weight, cur_bias)\n\n  def calc_updown_helper_helper(self, orig_weight, cur_weight, cur_bias):\n    \"\"\"This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt does the actual calculation of the updated weights.\\n\"\"\"\n    return orig_weight\n\n  def set_updown(self, orig_weight, updown):\n    \"\"\"This function sets the current weight tensor and optional bias based on the original weight tensor and an updated weight tensor.\\nIt transfers the updated weight tensor and optional bias to the device of the original weight tensor, then calls another function to finalize the updated weights.\\n\"\"\"\n    return self.set_updown_helper(orig_weight, updown, self.weight.to(orig_weight.device), self.bias)\n\n  def set_updown_helper(self, orig_weight, updown, cur_weight, cur_bias):\n    \"\"\"This function sets the current weight tensor and optional bias based on the original weight tensor and an updated weight tensor.\\nIt calls another function to finalize the updated weights.\\n\"\"\"\n    return self.set_updown_helper_helper(orig_weight, updown, cur_weight, cur_bias)\n\n  def set_updown_helper_helper(self, orig_weight, updown, cur_weight, cur_bias):\n    \"\"\"This function sets the current weight tensor and optional bias based on the original weight tensor and an updated weight tensor.\\nIt does the actual setting of the updated weights.\\n\"\"\"\n    return\n\n  def calc_updown_bias(self, orig_weight, orig_bias):\n    \"\"\"This function calculates and returns an updated bias tensor based on the original weight tensor and the original bias tensor.\\nIt calls another function to finalize the updated bias.\\n\"\"\"\n    return self.calc_updown_bias_helper(", "CodeBleu_score": 32.25007678781597}, {"code": "def calc_updown(self, orig_weight)\n    if self.bias is not None:\n      bias = self.bias.to(orig_weight.device)\n    else:\n      bias = None\n    return self._calc_updown(orig_weight, self.weight.to(orig_weight.device), bias)\n\n  def _calc_updown(self, orig_weight, cur_weight, bias):\n    \"\"\"This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the calc_updown function.\\n\"\"\"\n    raise NotImplementedError\n\n  def _calc_updown_bias(self, orig_weight, cur_weight, bias):\n    \"\"\"This function calculates and returns an updated bias tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the calc_updown function.\\n\"\"\"\n    raise NotImplementedError\n\n  def _calc_updown_weight(self, orig_weight, cur_weight, bias):\n    \"\"\"This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the calc_updown function.\\n\"\"\"\n    raise NotImplementedError\n\n  def _calc_updown_weight_bias(self, orig_weight, cur_weight, bias):\n    \"\"\"This function calculates and returns an updated weight tensor and bias tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the calc_updown function.\\n\"\"\"\n    raise NotImplementedError\n\n  def _calc_updown_weight_bias_helper(self, orig_weight, cur_weight, bias, orig_weight_grad, cur_weight_grad, bias_grad, orig_bias_grad, cur_bias_grad, orig_weight_grad_norm, cur_weight_grad_norm, orig_bias_grad_norm, cur_bias_grad_norm, orig_weight_grad_norm_sum, cur_weight_grad_norm_sum, orig_bias_grad_norm_sum, cur_bias_grad_norm_sum):\n    \"\"\"This function calculates and returns an updated weight tensor and bias tensor based on the original weight tensor and the current weight tensor.\\nIt is called by the _calc_updown_weight_bias function.\\n\"\"\"\n    raise NotImplementedError\n\n  def _calc_updown_weight_bias_helper_no_bias(self, orig_weight, cur_weight, bias", "CodeBleu_score": 38.60612917965817}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n    if len(networks) > max_networks_in_memory:\n        while len(networks) > max_networks_in_memory:\n            networks.popitem(last=False)\n        gc.collect()\n\n", "CodeBleu_score": 32.152354444712394}, {"code": "def purge_networks_from_memory()\n    global network_list\n    global network_count\n    global network_limit\n    if network_count > network_limit:\n        for i in range(network_count - network_limit):\n            network_list.pop(0)\n        network_count = network_limit\n        gc.collect()\n\n", "CodeBleu_score": 22.52593940864675}, {"code": "def purge_networks_from_memory()\n    global _networks\n    if len(_networks) > _max_networks_in_memory:\n        for network in _networks:\n            if network.network_id not in _networks_in_use:\n                del _networks[network.network_id]\n        gc.collect()\n\n", "CodeBleu_score": 29.7334693049527}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.backup_weights is not None and self.backup_biases is not None:\n        self.weight.data.copy_(self.backup_weights)\n        if self.bias is not None:\n            self.bias.data.copy_(self.backup_biases)\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.data.copy_(self.backup_weights)\n            if self.in_proj_bias is not None:\n                self.in_proj_bias.data.copy_(self.backup_biases)\n", "CodeBleu_score": 28.795132323290034}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, \"weight_backup\") and hasattr(self, \"bias_backup\"):\n        if self.weight_backup is not None:\n            self.weight = self.weight_backup\n            self.weight_backup = None\n        if self.bias_backup is not None:\n            self.bias = self.bias_backup\n            self.bias_backup = None\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"in_proj_weight_backup\") and hasattr(self, \"in_proj_bias_backup\"):\n            if self.in_proj_weight_backup is not None:\n                self.in_proj_weight = self.in_proj_weight_backup\n                self.in_proj_weight_backup = None\n            if self.in_proj_bias_backup is not None:\n                self.in_proj_bias = self.in_proj_bias_backup\n                self.in_proj_bias_backup = None\n        if hasattr(self, \"out_proj_weight_backup\") and hasattr(self, \"out_proj_bias_backup\"):\n            if self.out_proj_weight_backup is not None:\n                self.out_proj_weight = self.out_proj_weight_backup\n                self.out_proj_weight_backup = None\n            if self.out_proj_bias_backup is not None:\n                self.out_proj_bias = self.out_proj_bias_backup\n                self.out_proj_bias_backup = None\n", "CodeBleu_score": 35.332999083459896}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, \"weight_backup\") and hasattr(self, \"bias_backup\"):\n        self.weight.data.copy_(self.weight_backup)\n        if hasattr(self, \"bias\") and self.bias is not None:\n            self.bias.data.copy_(self.bias_backup)\n        else:\n            self.bias = None\n        del self.weight_backup, self.bias_backup\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        for i in range(self.num_heads):\n            self.in_proj_weight[i * 3:(i + 1) * 3].data.copy_(self.in_proj_weight_backup[i * 3:(i + 1) * 3])\n            self.in_proj_bias[i * 3:(i + 1) * 3].data.copy_(self.in_proj_bias_backup[i * 3:(i + 1) * 3])\n        del self.in_proj_weight_backup, self.in_proj_bias_backup\n", "CodeBleu_score": 38.82217120156256}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n    if not loaded_networks:\n        return original_forward(input)\n    device = next(org_module.parameters()).device\n    org_module.load_state_dict(org_module.state_dict())\n    org_module.reset_parameters()\n    output = original_forward(input.to(device))\n    for network in loaded_networks:\n        if network.layer == org_module.__class__.__name__:\n            output = network.forward(output)\n    return output\n\n", "CodeBleu_score": 33.94837327244007}, {"code": "def network_forward(org_module, input, original_forward)\n    global loaded_networks\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n    device = next(org_module.parameters()).device\n    for network in loaded_networks:\n        network.set_device(device)\n        network.restore_weights()\n        network.reset_weights()\n    output = original_forward(input)\n    for network in loaded_networks:\n        if network.check_layer(org_module):\n            output = network.apply(output)\n    return output\n\n", "CodeBleu_score": 36.80965079188074}, {"code": "def network_forward(org_module, input, original_forward)\n    if len(loaded_networks) == 0:\n        return original_forward(org_module, input)\n    input = input.to(device)\n    org_module.load_state_dict(org_module.state_dict())\n    org_module.zero_grad()\n    output = original_forward(org_module, input)\n    for network in loaded_networks:\n        if network.layer_name == org_module.__class__.__name__:\n            output = network.forward(output)\n    return output\n", "CodeBleu_score": 41.19986385453062}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n    trainables = []\n    for layer in self.model.modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 36.70289292577784}, {"code": "def trainables(self)\n    trainables = []\n    for layer in self.model.children():\n      if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n        trainables.extend(layer.parameters())\n    return trainables\n\n  def train(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, weight_decay, num_warmup_steps, num_training_steps, device, seed, save_dir):\n    \"\"\"This function trains the model on the given training dataset, and evaluates the model on the given validation dataset using the specified hyperparameters. It also saves the model with the best validation accuracy to the specified directory.\\nIt sets the seed for reproducibility, sets the model to training mode, and initializes the AdamW optimizer with the specified learning rate and weight decay.\\nIt then iterates through the specified number of epochs, and for each epoch, it performs the following steps:\\n- It initializes the dataloader for the training dataset with the specified batch size and shuffles the data.\\n- It initializes the dataloader for the validation dataset with the specified batch size.\\n- It initializes the progress bar for the training dataset.\\n- It iterates through the batches in the training dataset, and for each batch:\\n- It moves the input and target tensors to the specified device.\\n- It performs a forward pass through the model, calculates the loss, and backpropagates the loss.\\n- It updates the model parameters using the optimizer.\\n- It updates the progress bar with the loss.\\n- It initializes the progress bar for the validation dataset.\\n- It iterates through the batches in the validation dataset, and for each batch:\\n- It moves the input and target tensors to the specified device.\\n- It performs a forward pass through the model, calculates the loss, and backpropagates the loss.\\n- It updates the model parameters using the optimizer.\\n- It updates the progress bar with the loss.\\n- It calculates the accuracy of the model on the validation dataset.\\n- It prints the epoch number, training loss, validation loss, and validation accuracy.\\n- It saves the model with the best validation accuracy to the specified directory.\\n\"\"\"\n    torch.manual_seed(seed)\n    self.model.train()\n    optimizer = AdamW(self.trainables(), lr=learning_rate, weight_decay=weight_decay)\n   ", "CodeBleu_score": 33.28658969983618}, {"code": "def trainables(self)\n    trainables = []\n    for layer in self.model.modules():\n        if isinstance(layer, nn.Linear) or isinstance(layer, nn.LayerNorm):\n            trainables.append(layer.weight)\n            if layer.bias is not None:\n                trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 34.520274456264424}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n    global loaded_hypernetworks\n    loaded_hypernetworks = []\n    for name in names:\n        if name in loaded_hypernetworks:\n            print(f\"Hypernetwork {name} already loaded.\")\n            continue\n        try:\n            hypernetwork = Hypernetwork.load(name)\n        except:\n            print(f\"Hypernetwork {name} not found.\")\n            continue\n        if multipliers is not None:\n            if name in multipliers:\n                hypernetwork.set_multiplier(multipliers[name])\n            else:\n                print(f\"No multiplier for hypernetwork {name}.\")\n        loaded_hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 26.07748437116714}, {"code": "def load_hypernetworks(names, multipliers=None)\n    global loaded_hypernetworks\n    loaded_hypernetworks = []\n    for name in names:\n        if name in loaded_hypernetworks:\n            loaded_hypernetworks.remove(name)\n    for name in names:\n        if name in loaded_hypernetworks:\n            loaded_hypernetworks.remove(name)\n        if name in hypernetworks:\n            hypernetwork = hypernetworks[name]\n        else:\n            hypernetwork = Hypernetwork(name)\n            hypernetworks[name] = hypernetwork\n        if multipliers:\n            hypernetwork.multiplier = multipliers[name]\n        loaded_hypernetworks.append(hypernetwork)\n", "CodeBleu_score": 30.488934503007414}, {"code": "def load_hypernetworks(names, multipliers=None)\n    global loaded_hypernetworks\n    loaded_hypernetworks = []\n    for name in names:\n        if name in loaded_hypernetworks:\n            print(f'Hypernetwork \"{name}\" already loaded, skipping...')\n        else:\n            try:\n                hypernetwork = get_hypernetwork(name)\n                if multipliers is not None:\n                    hypernetwork.set_multiplier(multipliers[names.index(name)])\n                loaded_hypernetworks.append(hypernetwork)\n            except:\n                print(f'Hypernetwork \"{name}\" not found, skipping...')\n    print(f'Loaded {len(loaded_hypernetworks)} hypernetworks.')\n", "CodeBleu_score": 30.88092713949508}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 72.69978727224338}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n", "CodeBleu_score": 54.02529865145008}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 72.69978727224338}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        context = x\n    x_shape = x.shape\n    context_shape = context.shape\n    batch_size = x_shape[0]\n    d_model = self.d_model\n    n_heads = self.n_heads\n    d_k = d_model // n_heads\n    d_v = d_model // n_heads\n    # Project and reshape from (batch_size, seq_length, d_model) to (batch_size, n_heads, seq_length, d_k)\n    query = self.query_proj(x).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n    key = self.key_proj(context).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n    value = self.value_proj(context).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n    # Apply hypernetworks to the context\n    if self.hyper_query:\n        query = self.hyper_query(context).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n    if self.hyper_key:\n        key = self.hyper_key(context).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n    if self.hyper_value:\n        value = self.hyper_value(context).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n    # Scaled dot-product attention\n    attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n    attn_probs = F.softmax(attn_scores, dim=-1)\n    attn_output = torch.matmul(attn_probs, value)\n    # Reshape and apply final linear layer\n    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n    return", "CodeBleu_score": 31.369953754391076}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        context = x\n    q = self.q_proj(x)\n    k = self.k_proj(context)\n    v = self.v_proj(context)\n    if self.hypernet is not None:\n        k = self.hypernet(context, k)\n        v = self.hypernet(context, v)\n    q = self.reshape_qkv(q, self.num_heads)\n    k = self.reshape_qkv(k, self.num_heads)\n    v = self.reshape_qkv(v, self.num_heads)\n    attn_output = scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=self.dropout_p, is_causal=self.is_causal)\n    attn_output = attn_output.view(x.size(0), -1, self.all_head_size)\n    attn_output = self.out_proj(attn_output)\n    return attn_output\n\n", "CodeBleu_score": 31.14187584544957}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        context = x\n    if mask is None:\n        mask = torch.ones(x.shape[:2], dtype=torch.bool, device=x.device)\n    q = self.q_proj(x)\n    k = self.k_proj(context)\n    v = self.v_proj(context)\n    q = self.q_proj_hypernet(context)\n    k = self.k_proj_hypernet(context)\n    v = self.v_proj_hypernet(context)\n    q = q.view(x.shape[0], x.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    k = k.view(context.shape[0], context.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    v = v.view(context.shape[0], context.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    attn_scores = attn_scores.masked_fill(~mask[:, None, None, :], float(\"-inf\"))\n    attn = torch.softmax(attn_scores, dim=-1)\n    attn = self.attn_dropout(attn)\n    x = torch.matmul(attn, v)\n    x = x.transpose(1, 2).contiguous().view(x.shape[0], x.shape[1], self.num_heads * self.head_dim)\n    return self.out_proj(x)\n\n", "CodeBleu_score": 36.264507666852985}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    original_hypernetwork_name = hypernetwork.name\n    original_hypernetwork_checkpoint = hypernetwork.checkpoint\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.checkpoint = checkpoint\n    try:\n        hypernetwork.save(filename)\n    except:\n        hypernetwork.name = original_hypernetwork_name\n        hypernetwork.checkpoint = original_hypernetwork_checkpoint\n        raise\n", "CodeBleu_score": 30.309035593320584}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    if hypernetwork_name not in checkpoint.hypernetworks:\n        checkpoint.hypernetworks[hypernetwork_name] = {}\n    original_hypernetwork = checkpoint.hypernetworks[hypernetwork_name]\n    checkpoint.hypernetworks[hypernetwork_name] = hypernetwork\n    try:\n        save(checkpoint, filename)\n    except:\n        checkpoint.hypernetworks[hypernetwork_name] = original_hypernetwork\n        raise\n    else:\n        checkpoint.hypernetworks[hypernetwork_name] = original_hypernetwork\n", "CodeBleu_score": 24.499488046046036}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    original_hypernetwork_checkpoint = hypernetwork.checkpoint\n    hypernetwork.checkpoint = checkpoint\n    try:\n        hypernetwork.save(filename)\n    except:\n        hypernetwork.checkpoint = original_hypernetwork_checkpoint\n        raise\n    hypernetwork.checkpoint = original_hypernetwork_checkpoint\n", "CodeBleu_score": 24.240349927989463}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if not self.ema_enabled:\n      return context\n    if context is None:\n      context = {}\n    if self.ema_scope_debug:\n      self.log('Switching to EMA weights...')\n    context['ema_scope_weights'] = self.get_weights()\n    self.set_weights(self.ema_weights)\n    return context\n\n  def ema_scope_exit(self, context):\n    \"\"\"This function restores the original model parameters after exiting the EMA scope.\\nIf EMA is enabled, it restores the original model parameters and optionally logs this action.\\n\"\"\"\n    if not self.ema_enabled:\n      return\n    if self.ema_scope_debug:\n      self.log('Restoring original weights...')\n    self.set_weights(context['ema_scope_weights'])\n\n  def ema_scope_enter(self, context=None):\n    \"\"\"This function manages the use of Exponential Moving Average (EMA) weights for a model, temporarily switching to EMA weights and restoring the original weights after execution.\\nIf EMA is enabled, it stores the current model parameters, copies EMA weights to the model, and optionally logs this action.\\nAfter the code block execution, it restores the original weights and optionally logs the restoration.\\n\"\"\"\n    if not self.ema_enabled:\n      return context\n    if context is None:\n      context = {}\n    if self.ema_scope_debug:\n      self.log('Switching to EMA weights...')\n    context['ema_scope_weights'] = self.get_weights()\n    self.set_weights(self.ema_weights)\n    return context\n\n  def ema_scope_exit(self, context):\n    \"\"\"This function restores the original model parameters after exiting the EMA scope.\\nIf EMA is enabled, it restores the original model parameters and optionally logs this action.\\n\"\"\"\n    if not self.ema_enabled:\n      return\n    if self.ema_scope_debug:\n      self.log('Restoring original weights...')\n    self.set_weights(context['ema_scope_weights'])\n\n  def ema_scope_enter(self, context=None):\n    \"\"\"This function manages the use of Exponential Moving Average (EMA) weights for a model, temporarily switching to EMA weights and restoring the original weights after execution.\\nIf EMA is enabled, it stores", "CodeBleu_score": 32.591814117535954}, {"code": "def ema_scope(self, context=None)\n    if self.ema_on:\n      self.model.store_weights()\n      self.model.load_ema_weights()\n      if self.verbose:\n        print(f\"Using EMA weights for {self.model.name}\")\n    yield\n    if self.ema_on:\n      self.model.load_weights()\n      if self.verbose:\n        print(f\"Restored original weights for {self.model.name}\")\n", "CodeBleu_score": 38.00794680867619}, {"code": "def ema_scope(self, context=None)\n    if self.ema_enable:\n      if self.ema_enable_logging:\n        self.logger.info(f'Switching to EMA weights for model {self.model.name}')\n      self.model.set_weights(self.ema_model.get_weights())\n    try:\n      yield\n    finally:\n      if self.ema_enable:\n        if self.ema_enable_logging:\n          self.logger.info(f'Restoring original weights for model {self.model.name}')\n        self.model.set_weights(self.ema_weights)\n\n  def ema_update(self, context=None):\n    \"\"\"This function updates the EMA weights of the model.\\nIt calculates the decayed EMA weights using the decay rate and the current EMA weights, and then updates the EMA weights of the model using the decayed weights.\\nIf EMA is enabled, it calculates the decayed EMA weights using the decay rate and the current EMA weights, and then updates the EMA weights of the model using the decayed weights.\\n\"\"\"\n    if self.ema_enable:\n      decay = self.ema_decay\n      weights = [w.numpy() for w in self.model.get_weights()]\n      ema_weights = [w.numpy() for w in self.ema_model.get_weights()]\n      ema_weights = [decay * w + (1 - decay) * w_ for w, w_ in zip(weights, ema_weights)]\n      self.ema_model.set_weights(ema_weights)\n\n  def ema_update_schedule(self, context=None):\n    \"\"\"This function updates the EMA weights of the model.\\nIt calculates the decayed EMA weights using the decay rate and the current EMA weights, and then updates the EMA weights of the model using the decayed weights.\\nIf EMA is enabled, it calculates the decayed EMA weights using the decay rate and the current EMA weights, and then updates the EMA weights of the model using the decayed weights.\\n\"\"\"\n    if self.ema_enable:\n      decay = self.ema_decay\n      weights = [w.numpy() for w in self.model.get_weights()]\n      ema_weights = [w.numpy() for w in self.ema_model.get_weights()]\n      ema_weights = [", "CodeBleu_score": 37.64127983593099}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if self.config.model_config.denoising_fn == \"ddpm\":\n        pred_noise = model_output\n    else:\n        pred_noise = model_output[:, : self.config.model_config.num_channels]\n    if clip_denoised:\n        denoised_pred = x - self.sqrt_alphas_cumprod[t] * pred_noise\n        denoised_pred = denoised_pred.clamp(-1.0, 1.0)\n    else:\n        denoised_pred = x\n    model_mean = self.sqrt_one_minus_alphas_cumprod[t] * denoised_pred + self.sqrt_alphas_cumprod[t] * pred_noise\n    posterior_variance = self.posterior_variance_explicit(t)\n    posterior_log_variance_clipped = self.posterior_log_variance_clipped_explicit(t)\n    return model_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 38.616545020147285}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if self.config.model_config.noise_schedule == \"fixed\":\n      pred_noise = model_output\n    else:\n      pred_noise = model_output[:, :self.config.model_config.num_channels]\n    if self.config.model_config.parameterization == \"epsilon\":\n      pred_noise = self.config.model_config.rescale_timestep(1.0 - t) * pred_noise\n    elif self.config.model_config.parameterization == \"v_prediction\":\n      pred_noise = 0.5 * torch.erf(pred_noise / math.sqrt(2.0))\n    elif self.config.model_config.parameterization == \"v_prediction_sqrt\":\n      pred_noise = torch.exp(0.5 * pred_noise)\n    else:\n      raise ValueError(f\"Unknown noise prediction parameterization {self.config.model_config.parameterization}\")\n    if clip_denoised:\n      pred_x0 = x - pred_noise\n      pred_x0 = pred_x0.clamp(-1.0, 1.0)\n    else:\n      pred_x0 = x\n    return pred_x0, pred_noise, model_output\n", "CodeBleu_score": 36.160584943462545}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if self.config.model_config.noise_schedule.pred_noise_scheduler_name == \"ddim\":\n      # DDIM uses the predicted noise as the denoised image\n      pred_noise = model_output[:, 1, :, :]\n      mean = x - pred_noise\n    else:\n      # Otherwise, the denoised image is reconstructed from the predicted noise\n      pred_noise = model_output[:, 1, :, :]\n      noise = torch.randn_like(pred_noise)\n      if self.config.model_config.noise_schedule.noise_type == \"gaussian\":\n        pred_sample = pred_noise * noise\n      else:\n        pred_sample = pred_noise * noise.sign()\n      mean = x - pred_sample\n    if clip_denoised:\n      mean = mean.clamp(-1, 1)\n    model_mean, posterior_variance, posterior_log_variance = self.config.model_config.noise_schedule.convert_to_model_mean_and_variance(mean, t)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 37.8698292621372}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n\n", "CodeBleu_score": 34.197735971701654}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss\n    else:\n        raise ValueError(f'Loss type {self.loss_type} not recognized')\n    return loss\n", "CodeBleu_score": 49.089045469960524}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError('Unsupported loss type')\n    return loss\n", "CodeBleu_score": 62.027009717215286}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones([batch_size], dtype=torch.long, device=self.device) * (self.num_timesteps - 1)\n    mean, logvar = self.predict(x_start, t)\n    kl = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1)\n    return torch.mean(kl) / math.log(2)\n", "CodeBleu_score": 25.390621419955362}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, dtype=torch.long, device=self.device)\n    mean, logvar = self.model.predict(x_start, t)\n    kl = -0.5 * (1 + logvar - mean ** 2 - logvar.exp()).sum(dim=-1)\n    return torch.mean(kl) / math.log(2)\n", "CodeBleu_score": 28.500342298982368}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps, dtype=torch.long, device=self.device)\n    mean, logvar = self.forward(x_start, t)\n    kl = -0.5 * (1 + logvar - mean ** 2 - logvar.exp())\n    kl = kl.sum(dim=-1)\n    prior_bpd = (kl / (np.log(2) * np.prod(x_start.shape[1:]))).mean()\n    return prior_bpd\n", "CodeBleu_score": 29.536705144686138}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_dir = Path(os.environ[\"CACHE_DIR\"])\n    cache_dir.mkdir(exist_ok=True)\n    cache_path = cache_dir / f\"{subsection}.cache\"\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 48.781678435396515}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        os.path.join(CACHE_DIR, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 57.51519189749161}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache = diskcache.Cache(\n        os.path.join(CACHE_DIR, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n    return cache\n\n", "CodeBleu_score": 57.633260370743876}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if not os.path.exists(cache_dir):\n            os.mkdir(cache_dir)\n        if not os.path.isfile(cache_filename(subsection)):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 49.13891732833151}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        cache_filename = os.path.join(cache_dir, subsection + \".cache\")\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data(subsection)\n        if subsection in caches:\n            return caches[subsection]\n        caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 45.711230964050486}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    global cache_dir\n    global cache_filename\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        cache_dir = os.path.join(os.path.dirname(__file__), \"cache\")\n        cache_filename = os.path.join(cache_dir, subsection + \".pickle\")\n        if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 42.95496068805594}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title]['mtime'] > mtime:\n        data = cache[title]['data']\n    else:\n        data = func()\n        cache[title] = {'mtime': mtime, 'data': data}\n        dump_cache()\n    return data\n\n", "CodeBleu_score": 19.003925393377923}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache:\n        if mtime <= cache[title]['mtime']:\n            return cache[title]['data']\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 22.822315358916555}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title]['mtime'] >= mtime:\n        return cache[title]['data']\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 17.20191147211874}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    print(\"***\", message, file=sys.stderr)\n    if exc_info:\n        print(\"    \" + traceback.format_exc().rstrip(), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 47.6303254517083}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 40.35401246483239}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 40.35401246483239}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        prompt = style.extract_prompt(prompt)\n    except Exception as e:\n        print(f\"Failed to extract prompt style: {e}\")\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = style.extract_prompt(negative_prompt)\n    except Exception as e:\n        print(f\"Failed to extract negative prompt style: {e}\")\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 49.156860094622246}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        try:\n            prompt = extract_style_text(style.prompt, prompt)\n        except ValueError:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        try:\n            negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n        except ValueError:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 54.836120523820256}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    # Extract style text from both the positive and negative prompts\n    try:\n        prompt = extract_style_text(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 52.87739229230648}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n      self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n", "CodeBleu_score": 55.6187172935094}, {"code": "def random_partial(self, count, n_frames)\n    if self.utterances is None:\n      self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n", "CodeBleu_score": 50.12500088981666}, {"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n", "CodeBleu_score": 55.89879214825755}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set both increase_only and decrease_only to True\")\n\n    dBFS_change = target_dBFS - wav.dBFS\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n\n    return wav.apply_gain(dBFS_change)\n\n", "CodeBleu_score": 43.83012184874045}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set both increase_only and decrease_only to True\")\n\n    dBFS_change = target_dBFS - get_dBFS(wav)\n\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 54.667237640396905}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set to True.\")\n\n    db_change = target_dBFS - audioop.rms(wav, 2)\n    if db_change < 0 and increase_only or db_change > 0 and decrease_only:\n        return wav\n    return audioop.mul(wav, 2, int(math.pow(10, db_change / 20)))\n\n", "CodeBleu_score": 44.83557580948316}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath}, trained for {_model.training_step} steps\")\n\n", "CodeBleu_score": 33.77337877761994}, {"code": "def load_model(weights_fpath: Path, device=None)\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(\n        embedding_size=256,\n        hidden_size=768,\n        num_layers=3,\n        num_classes=5994,\n        dropout=0.1,\n        device=device,\n    )\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath}, trained to step {get_step_from_path(weights_fpath)}\")\n    return _model\n\n", "CodeBleu_score": 49.232434603131445}, {"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained for {get_step_from_fname(weights_fpath)} steps\")\n\n", "CodeBleu_score": 41.061010874853984}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeddings\n", "CodeBleu_score": 69.77480336620489}, {"code": "def embed_frames_batch(frames_batch)\n    # Check if the model is loaded\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n\n    # Convert the input frames_batch from a NumPy array to a PyTorch tensor\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    with torch.no_grad():\n        embeddings = _model(frames_batch)\n\n    # Detach the result from the computation graph, move it to the CPU, and convert it back to a NumPy array\n    embeddings = embeddings.detach().cpu().numpy()\n\n    return embeddings\n\n", "CodeBleu_score": 62.39161247866224}, {"code": "def embed_frames_batch(frames_batch)\n    if not _model:\n        raise ValueError(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 50.83692039972233}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 <= min_pad_coverage <= 1, \"min_pad_coverage must be in the range [0, 1]\"\n    assert 0 <= overlap <= 1, \"overlap must be in the range [0, 1]\"\n    assert partial_utterance_n_frames >= 1, \"partial_utterance_n_frames must be at least 1\"\n    # Compute the number of frames and the frame step based on the sampling rate and overlap\n    n_frames = int(np.ceil(n_samples / (sampling_rate * (1 - overlap))))\n    frame_step = int(np.ceil(n_samples / n_frames))\n    # Compute wav and mel slices using a loop, considering the steps required based on the frames and frame step\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames * sampling_rate\n        wav_slices.append(wav[start:end])\n        mel_slices.append(melspectrogram(wav[start:end], sr=sampling_rate))\n    # Evaluate the need for extra padding by checking the coverage of the last slice\n    if n_frames > 1:\n        last_slice_coverage = (\n            len(wav_slices[-1]) / (sampling_rate * partial_utterance_n_frames)\n        )\n        if last_slice_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 37.55001897817025}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0.0 < overlap < 1.0, \"Overlap must be in (0, 1.0)\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum pad coverage must be in (0, 1.0)\"\n    assert partial_utterance_n_frames > 0, \"Number of frames must be greater than 0\"\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    n_frames = (n_samples - partial_utterance_n_frames) // frame_step + 1\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start_frame = i * frame_step\n        end_frame = start_frame + partial_utterance_n_frames\n        wav_slices.append(np.arange(start_frame, end_frame))\n        mel_slices.append(np.arange(start_frame // 2, end_frame // 2))\n    if n_frames > 1:\n        last_coverage = (wav_slices[-1][-1] - wav_slices[-1][0]) / n_samples\n        if last_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.071013779556395}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 <= overlap <= 1, \"Overlap must be within the range [0, 1].\"\n    assert 0 <= min_pad_coverage <= 1, \"Minimum pad coverage must be within the range [0, 1].\"\n    frame_step = int(partials_n_frames * (1 - overlap))\n    n_frames = (n_samples - partials_n_frames) // frame_step + 1\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start = i * frame_step\n        end = start + partials_n_frames\n        wav_slice = wav[start:end]\n        mel_slice = melspectrogram(\n            wav_slice,\n            sr=sr,\n            n_mels=n_mels,\n            hop_length=hop_length,\n            win_length=win_length,\n            window=window,\n        )\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    last_coverage = (n_samples - start) / partials_n_frames\n    if last_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 36.863856082969825}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=\"plasma\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(im, ax=ax)\n    ax.set_title(title)\n\n", "CodeBleu_score": 53.078389436754016}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=\"viridis\", vmin=color_range[0], vmax=color_range[1])\n    plt.colorbar(im, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 53.16822962246312}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    else:\n        height = shape[0]\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=\"coolwarm\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar()\n\n", "CodeBleu_score": 53.608463909451274}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n    # get number of speakers per batch\n    n_spks = embeds.shape[0]\n    # get number of utterances per speaker\n    n_utts = embeds.shape[1]\n    # compute centroids with current embedding included\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    # compute centroids with current embedding excluded\n    centroids_excl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    centroids_excl = centroids_excl.repeat(1, n_utts, 1)\n    centroids_excl = (n_utts * centroids_incl - centroids_excl) / (n_utts - 1)\n    # initialize similarity matrix\n    sim_matrix = torch.zeros(n_spks * n_utts, n_spks * n_utts).to(embeds.device)\n    # initialize mask matrix\n    mask = torch.ones(n_spks * n_utts, n_spks * n_utts).to(embeds.device)\n    # compute similarity matrix\n    for i in range(n_spks):\n        # compute similarity scores\n        sim_scores_incl = torch.matmul(embeds[i], centroids_incl[i].t())\n        sim_scores_excl = torch.matmul(embeds[i], centroids_excl[i].t())\n        # update similarity matrix\n        sim_matrix[i * n_utts:(i + 1) * n_utts, i * n_utts:(i + 1) * n_utts] = sim_scores_incl\n        sim_matrix[i * n_utts:(i + 1) * n_utts, :i * n_utts] = sim_scores_excl\n        sim_matrix[i * n_utts:(i + 1) * n_utts, (i + 1) * n_utts:] = sim_scores_excl\n        # update mask matrix\n        mask[i * n_ut", "CodeBleu_score": 28.151123537447543}, {"code": "def similarity_matrix(self, embeds)\n    num_speakers_per_batch = embeds.shape[0]\n    num_utterances_per_speaker = embeds.shape[1]\n\n    centroids_incl = torch.zeros((num_speakers_per_batch, self.embed_dim), dtype=embeds.dtype, device=embeds.device)\n    centroids_excl = torch.zeros((num_speakers_per_batch, self.embed_dim), dtype=embeds.dtype, device=embeds.device)\n\n    for i in range(num_speakers_per_batch):\n        centroids_incl[i] = torch.mean(F.normalize(embeds[i], p=2, dim=1), dim=0)\n        centroids_excl[i] = torch.mean(F.normalize(torch.cat([embeds[i, :i], embeds[i, i+1:]], dim=0), p=2, dim=1), dim=0)\n\n    sim_matrix = torch.zeros((num_speakers_per_batch * num_utterances_per_speaker, num_speakers_per_batch * num_utterances_per_speaker), dtype=embeds.dtype, device=embeds.device)\n    mask = torch.ones_like(sim_matrix)\n    mask = mask.triu(diagonal=1)\n\n    for i in range(num_speakers_per_batch):\n        sim_matrix[i*num_utterances_per_speaker:(i+1)*num_utterances_per_speaker, i*num_utterances_per_speaker:(i+1)*num_utterances_per_speaker] = torch.mm(embeds[i], centroids_incl[i].unsqueeze(1))\n        sim_matrix[i*num_utterances_per_speaker:(i+1)*num_utterances_per_speaker, i*num_utterances_per_speaker+1:(i+1)*num_utterances_per_speaker] = torch.mm(embeds[i, :-1], centroids_excl[i].unsqueeze(1))\n        sim_matrix[i*num_utterances_per_speaker+1:(i+1)*num_utterances_per_speaker, i*num_utterances_per_speaker", "CodeBleu_score": 31.582450289302987}, {"code": "def similarity_matrix(self, embeds)\n    num_spks_per_batch = embeds.shape[0]\n    num_utts_per_spk = embeds.shape[1]\n    centroids_incl = embeds.sum(dim=1) / num_utts_per_spk\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = embeds.sum(dim=1) / (num_utts_per_spk - 1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n    sim_mat = torch.zeros(num_spks_per_batch * num_utts_per_spk, num_spks_per_batch * num_utts_per_spk).to(embeds.device)\n    mask = torch.eye(num_spks_per_batch * num_utts_per_spk).to(embeds.device)\n    for i in range(num_spks_per_batch):\n        for j in range(num_utts_per_spk):\n            idx1 = i * num_utts_per_spk + j\n            sim_mat[idx1] = torch.matmul(embeds[i][j], centroids_incl)\n            sim_mat[idx1] += torch.matmul(embeds[i][j], centroids_excl)\n            sim_mat[idx1] /= 2\n            sim_mat[idx1] *= (num_utts_per_spk - 1)\n            sim_mat[idx1] *= self.scale\n            sim_mat[idx1] += self.bias\n            sim_mat[idx1] *= (1 - mask[idx1])\n    return sim_mat\n", "CodeBleu_score": 28.17933745619448}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n    n_speakers, n_utterances = embeds.shape[:2]\n    embeds = embeds.reshape(n_speakers * n_utterances, -1)\n    sim_mat = torch.matmul(embeds, embeds.T)\n    mask = torch.eye(n_speakers * n_utterances, dtype=torch.bool).to(self.device)\n    labels = mask.reshape(n_speakers * n_utterances)\n    loss = self.criterion(sim_mat, labels)\n    sim_mat = sim_mat.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels, sim_mat)\n    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n    return loss, eer", "CodeBleu_score": 33.682653750200934}, {"code": "def loss(self, embeds)\n    n_speakers, n_utterances = embeds.shape[:2]\n    embeds = embeds.reshape(n_speakers * n_utterances, -1)\n    sim_mat = torch.matmul(embeds, embeds.T)\n    mask = torch.eye(n_speakers * n_utterances, dtype=torch.bool).to(embeds.device)\n    sim_mat = sim_mat.masked_fill(mask, -np.inf)\n    sim_mat = sim_mat / self.temperature\n    labels = torch.arange(n_speakers * n_utterances).to(embeds.device)\n    loss = self.ce_loss(sim_mat, labels)\n    sim_mat = sim_mat.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    fpr, tpr, _ = roc_curve(labels, sim_mat)\n    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n    return loss, eer", "CodeBleu_score": 35.930289594799994}, {"code": "def loss(self, embeds)\n    # get the number of speakers and utterances per speaker\n    n_speakers, n_utterances = embeds.shape[:2]\n    # compute similarity matrix\n    sim_matrix = torch.matmul(embeds, embeds.T)\n    # reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(n_speakers, n_utterances, n_speakers, n_utterances)\n    # generate ground truth labels\n    labels = torch.arange(n_speakers).repeat_interleave(n_utterances)\n    # calculate loss\n    loss = F.cross_entropy(sim_matrix.reshape(-1, n_speakers), labels)\n    # compute EER without backpropagation\n    with torch.no_grad():\n        # compute predicted labels\n        predicted_labels = sim_matrix.argmax(dim=1).flatten()\n        # compute ROC curve\n        fpr, tpr, thresholds = roc_curve(labels, predicted_labels)\n        # find the threshold where the false positive rate equals the false negative rate\n        eer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\n    return loss, eer_threshold\n", "CodeBleu_score": 34.01750356906983}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n    if self.log_params_disabled:\n      return\n    self.params_string = \"\"\n    for i in self.model_params:\n      self.params_string += i + \": \" + str(self.model_params[i]) + \"\\n\"\n    for i in self.data_params:\n      self.params_string += i + \": \" + str(self.data_params[i]) + \"\\n\"\n    self.params_string += \"\\n\"\n    self.params_string += \"Epochs: \" + str(self.epochs) + \"\\n\"\n    self.params_string += \"Batch Size: \" + str(self.batch_size) + \"\\n\"\n    self.params_string += \"Learning Rate: \" + str(self.learning_rate) + \"\\n\"\n    self.params_string += \"Loss Function: \" + str(self.loss_function) + \"\\n\"\n    self.params_string += \"Optimizer: \" + str(self.optimizer) + \"\\n\"\n    self.params_string += \"Metrics: \" + str(self.metrics) + \"\\n\"\n    self.params_string += \"Callbacks: \" + str(self.callbacks) + \"\\n\"\n    self.params_string += \"Verbose: \" + str(self.verbose) + \"\\n\"\n    self.params_string += \"Shuffle: \" + str(self.shuffle) + \"\\n\"\n    self.params_string += \"Validation Split: \" + str(self.validation_split) + \"\\n\"\n    self.params_string += \"Cross Validation Folds: \" + str(self.cross_validation_folds) + \"\\n\"\n    self.params_string += \"Cross Validation Folds Shuffle: \" + str(self.cross_validation_folds_shuffle) + \"\\n\"\n    self.params_string += \"Cross Validation Folds Random State: \" + str(self.cross_validation_folds_random_state) + \"\\n\"\n    self.params_string += \"Cross Validation Folds Verbose: \" + str(self.cross_validation_folds_verbose) + \"\\n\"\n    self.params_string += \"Cross Validation Folds Return Train Score: \" + str(self.cross_validation_folds_return_train_score) + \"\\n\"\n    self.params", "CodeBleu_score": 17.020500111554355}, {"code": "def log_params(self)\n    if self.log_params_disabled == False:\n      # Import parameter configurations\n      from.configs import params\n      # Initialize formatted string\n      params_str = \"\"\n      # Iterate through model parameters\n      for name, value in params[\"model\"].items():\n        params_str += \"{}: {}\\n\".format(name, value)\n      # Iterate through data parameters\n      for name, value in params[\"data\"].items():\n        params_str += \"{}: {}\\n\".format(name, value)\n      # Display formatted string\n      self.visualize(params_str, title=\"Parameters\")\n\n  def log_train_data(self, data, title=\"Train Data\"):\n    \"\"\"The function logs the training data if not disabled.\\nIt imports the data visualization tool, then displays the data using the tool, with the title \"Train Data\".\"\"\"\n    if self.log_train_data_disabled == False:\n      # Import data visualization tool\n      from.visualization import visualize_data\n      # Display data\n      visualize_data(data, title=title)\n\n  def log_test_data(self, data, title=\"Test Data\"):\n    \"\"\"The function logs the testing data if not disabled.\\nIt imports the data visualization tool, then displays the data using the tool, with the title \"Test Data\".\"\"\"\n    if self.log_test_data_disabled == False:\n      # Import data visualization tool\n      from.visualization import visualize_data\n      # Display data\n      visualize_data(data, title=title)\n\n  def log_train_loss(self, loss, title=\"Train Loss\"):\n    \"\"\"The function logs the training loss if not disabled.\\nIt imports the loss visualization tool, then displays the loss using the tool, with the title \"Train Loss\".\"\"\"\n    if self.log_train_loss_disabled == False:\n      # Import loss visualization tool\n      from.visualization import visualize_loss\n      # Display loss\n      visualize_loss(loss, title=title)\n\n  def log_test_loss(self, loss, title=\"Test Loss\"):\n    \"\"\"The function logs the testing loss if not disabled.\\nIt imports the loss visualization tool, then displays the loss using the tool, with the title \"Test Loss\".\"\"\"\n    if self.log_test_loss_disabled == False:\n      # Import loss visualization tool\n      from.visualization import visualize_loss\n      # Display loss\n      visualize_loss(loss, title=title)\n\n  def log_train_metrics", "CodeBleu_score": 25.462278001804872}, {"code": "def log_params(self)\n    if self.log_params_disable is False:\n      self.log_params_disable = True\n      self.log_params_str = \"\"\n      self.log_params_str += \"Model Parameters:\\n\"\n      for key, value in self.model_params.items():\n        self.log_params_str += f\"{key}: {value}\\n\"\n      self.log_params_str += \"\\nData Parameters:\\n\"\n      for key, value in self.data_params.items():\n        self.log_params_str += f\"{key}: {value}\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n      self.log_params_str += \"\\n\"\n     ", "CodeBleu_score": 11.543976753359381}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.args.disable_plots:\n        return\n    if out_fpath is not None:\n        out_fpath = os.path.join(out_fpath, f\"projections_{step}.png\")\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    speakers = np.repeat(np.arange(len(utterances_per_speaker)), utterances_per_speaker)\n    reducer = umap.UMAP(random_state=self.args.seed)\n    projections = reducer.fit_transform(embeds)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(projections[:, 0], projections[:, 1], c=speakers, cmap=\"tab20\", s=10)\n    ax.set_title(f\"Step {step}\")\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath)\n        plt.close()\n", "CodeBleu_score": 47.87908328194784}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.config.use_cuda:\n        embeds = embeds.cpu()\n    embeds = embeds.detach().numpy()\n    embeds = embeds.reshape(-1, self.config.embed_dim)\n    speakers = []\n    for i in range(len(utterances_per_speaker)):\n        speakers += [i] * utterances_per_speaker[i]\n    speakers = np.array(speakers)\n    if len(speakers) > max_speakers:\n        speakers = speakers[:max_speakers]\n        embeds = embeds[:max_speakers * utterances_per_speaker[0]]\n    umap_embeds = umap.UMAP(n_neighbors=5, min_dist=0.1, metric=\"cosine\").fit_transform(embeds)\n    if out_fpath is None:\n        fig, ax = plt.subplots()\n        ax.scatter(umap_embeds[:, 0], umap_embeds[:, 1], s=1, c=speakers, cmap=\"tab20\")\n        ax.set_title(f\"Step {step}\")\n        plt.show()\n    else:\n        fig, ax = plt.subplots()\n        ax.scatter(umap_embeds[:, 0], umap_embeds[:, 1], s=1, c=speakers, cmap=\"tab20\")\n        ax.set_title(f\"Step {step}\")\n        plt.savefig(out_fpath)\n        plt.close()\n", "CodeBleu_score": 43.952271563115595}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if not self.args.visualize_speakers:\n        return\n    if self.args.speakers_per_batch < 10000:\n        print('Warning: visualization of speaker embeddings is only supported for large speaker batches.')\n        return\n    if self.args.speakers_per_batch > 100000:\n        print('Warning: visualization of speaker embeddings is only supported for small speaker batches.')\n        return\n    if not out_fpath:\n        out_fpath = os.path.join(self.args.output_dir,'speakers_step_{:07d}.png'.format(step))\n    if len(embeds) < 100000:\n        embeds = torch.cat(embeds, dim=0).detach().cpu().numpy()\n    else:\n        embeds = torch.cat(embeds[:100000], dim=0).detach().cpu().numpy()\n    embeds = embeds.reshape(-1, self.args.embedding_dim)\n    speakers = []\n    for utterances in utterances_per_speaker:\n        speakers.extend([utterances[0].speaker] * len(utterances))\n    speakers = np.array(speakers[:100000])\n    if len(embeds) > 10000:\n        embeds = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='cosine').fit_transform(embeds)\n    if max_speakers > 0:\n        speakers = speakers[:max_speakers]\n        embeds = embeds[:max_speakers]\n    plt.figure(figsize=(10, 10))\n    plt.title('Speaker embeddings at step {}'.format(step))\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=speakers, cmap='tab20')\n    if out_fpath:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 37.51490266510843}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, _ = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = speaker_embedding.shape[-1]\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = tf.repeat(\n        speaker_embedding, repeats=sequence_length, axis=0\n    )\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = tf.reshape(\n        speaker_embedding,\n        (batch_size, sequence_length, speaker_embedding_dim),\n    )\n    speaker_embedding = tf.transpose(speaker_embedding, perm=[0, 2, 1])\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n", "CodeBleu_score": 19.552189299568624}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, hidden_size = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_index = len(x.shape) - 1\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = tf.repeat(speaker_embedding, repeats=sequence_length, axis=speaker_embedding_index)\n    # Reshape the speaker embedding to match the input tensor\n    speaker_embedding = tf.reshape(speaker_embedding, (batch_size, sequence_length, hidden_size))\n    # Transpose the speaker embedding to match the input tensor\n    speaker_embedding = tf.transpose(speaker_embedding, perm=[0, 2, 1])\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n", "CodeBleu_score": 19.275328005302185}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    x_shape = x.shape\n    x_shape_names = [\"batch\", \"time\", \"features\"]\n    speaker_embedding_dim = speaker_embedding.shape[-1]\n    speaker_embedding_index = x_shape_names.index(\"features\")\n    speaker_embedding = tf.repeat(speaker_embedding, x_shape[1], axis=0)\n    speaker_embedding = tf.reshape(speaker_embedding, x_shape[:-1] + [speaker_embedding_dim])\n    speaker_embedding = tf.transpose(speaker_embedding, [0, 2, 1])\n    return tf.concat([x, speaker_embedding], axis=speaker_embedding_index)\n", "CodeBleu_score": 26.044744054410845}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n    device = self.parameters().__next__().device\n    self.cum_attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n    self.attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n\n", "CodeBleu_score": 21.203836069048858}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = self.parameters().__next__().device\n    batch_size, seq_len = encoder_seq_proj.size(0), encoder_seq_proj.size(1)\n    self.cum_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n", "CodeBleu_score": 22.591448029222015}, {"code": "def init_attention(self, encoder_seq_proj)\n    self.device = next(self.parameters()).device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cumulative_attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n", "CodeBleu_score": 25.357551810427122}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n", "CodeBleu_score": 35.95634239481868}, {"code": "def load(self, path, optimizer=None)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n", "CodeBleu_score": 36.797024185946505}, {"code": "def load(self, path, optimizer=None)\n    if not os.path.isfile(path):\n        raise FileNotFoundError(\"Checkpoint file not found: {}\".format(path))\n    if not path.endswith(\".pt\"):\n        raise ValueError(\"Checkpoint file is not a PyTorch checkpoint file: {}\".format(path))\n    if self.device is None:\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.model.load_state_dict(checkpoint[\"model\"])\n    if optimizer is not None and \"optimizer\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n", "CodeBleu_score": 44.22285004657509}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n    if optimizer:\n        torch.save({\n           'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n", "CodeBleu_score": 55.153054763366185}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n      torch.save(self.state_dict(), path)\n    else:\n      torch.save({\n         'model_state_dict': self.state_dict(),\n          'optimizer_state_dict': optimizer.state_dict(),\n      }, path)\n\n  def load(self, path, optimizer=None):\n    \"\"\"The function loads the model state from a specified path.\\nIf an optimizer is provided, it loads both the model state and the optimizer state.\\nIf no optimizer is provided, it loads only the model state.\"\"\"\n    if optimizer is None:\n      self.load_state_dict(torch.load(path))\n    else:\n      checkpoint = torch.load(path)\n      self.load_state_dict(checkpoint['model_state_dict'])\n      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n", "CodeBleu_score": 40.9478972364548}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is not None:\n        torch.save({\n           'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n    else:\n        torch.save({\n           'model_state_dict': self.state_dict(),\n        }, path)\n", "CodeBleu_score": 69.2737224301243}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n    text = text.lower()\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"1\", \" one \", text)\n    text = re.sub(r\"2\", \" two \", text)\n    text = re.sub(r\"3\", \" three \", text)\n    text = re.sub(r\"4\", \" four \", text)\n    text = re.sub(r\"5\", \" five \", text)\n    text = re.sub(r\"6\", \" six \", text)\n    text = re.sub(r\"7\", \" seven \", text)\n    text = re.sub(r\"8\", \" eight \", text)\n    text = re.sub(r\"9\", \" nine \", text)\n    text = re.sub(r\"0\", \" zero \", text)\n    text = re.sub(r\" a \", \" a. \", text)\n    text = re.sub(r\" b \", \" b. \", text)\n    text = re.sub(r\" c \", \" c. \", text)\n    text = re.sub(r\" d \", \" d. \", text)\n    text = re.sub(r\" e \", \" e. \", text)\n    text = re.sub(r\" f \", \" f. \", text)\n    text = re.sub(r\" g \", \" g. \", text)\n    text = re.sub(r\" h \", \" h. \", text)\n    text = re.sub(r\" i \", \" i. \", text)\n    text = re.sub(r\" j \", \" j. \", text)\n    text = re.sub(r\" k \", \" k. \", text)\n    text = re.sub(r\" l \", \" l. \", text)\n    text = re.sub(r\" m \", \" m. \", text)\n    text = re.sub(r\" n \", \" n. \", text)\n    text = re.sub(r\" o \", \" o. \", text)\n    text = re.sub(r\" p \", \" p. \", text)\n    text = re.sub(r\" q \", \" q. \", text)\n    text = re.sub(r\" r \", \" r. \", text)\n    text = re.sub(r\" s \", \" s. \",", "CodeBleu_score": 18.270308492124983}, {"code": "def english_cleaners(text)\n    text = text.lower()\n    text = text.replace(\"'\", \"\")\n    text = text.replace(\"`\", \"\")\n    text = text.replace(\"?\", \"\")\n    text = text.replace(\"!\", \"\")\n    text = text.replace(\",\", \"\")\n    text = text.replace(\".\", \"\")\n    text = text.replace(\";\", \"\")\n    text = text.replace(\":\", \"\")\n    text = text.replace(\"\\\"\", \"\")\n    text = text.replace(\"\u201c\", \"\")\n    text = text.replace(\"\u201d\", \"\")\n    text = text.replace(\"\u2019\", \"\")\n    text = text.replace(\"\u2018\", \"\")\n    text = text.replace(\"(\", \"\")\n    text = text.replace(\")\", \"\")\n    text = text.replace(\"[\", \"\")\n    text = text.replace(\"]\", \"\")\n    text = text.replace(\"{\", \"\")\n    text = text.replace(\"}\", \"\")\n    text = text.replace(\"\u2014\", \"\")\n    text = text.replace(\"-\", \"\")\n    text = text.replace(\"\u2013\", \"\")\n    text = text.replace(\"=\", \"\")\n    text = text.replace(\"+\", \"\")\n    text = text.replace(\"*\", \"\")\n    text = text.replace(\"/\", \"\")\n    text = text.replace(\"\\\\\", \"\")\n    text = text.replace(\"|\", \"\")\n    text = text.replace(\"@\", \"\")\n    text = text.replace(\"#\", \"\")\n    text = text.replace(\"$\", \"\")\n    text = text.replace(\"%\", \"\")\n    text = text.replace(\"^\", \"\")\n    text = text.replace(\"&\", \"\")\n    text = text.replace(\"~\", \"\")\n    text = text.replace(\"\u00a9\", \"\")\n    text = text.replace(\"\u00ae\", \"\")\n    text = text.replace(\"\u2122\", \"\")\n    text = text.replace(\"\u00ab\", \"\")\n    text = text.replace(\"\u00bb\", \"\")\n    text = text.replace(\"\u2026\", \"\")\n    text = text.replace(\"\u00a1\", \"\")\n    text = text.replace(\"\u00bf\", \"\")\n    text = text.replace(\"\u00b0\", \"\")\n    text = text.replace(\"\u00a2\", \"\")\n    text = text.replace(\"\u00a3\", \"\")\n    text = text.replace(\"\u00a5\", \"\")\n    text = text.replace(\"\u20ac\", \"\")\n    text = text.replace(\"\u00a6\", \"\")\n    text = text.replace(\"\u00ac\", \"\")\n    text = text.replace(\"\u00af\", \"\")\n    text = text.replace(\"\u00b1\", \"\")\n    text = text", "CodeBleu_score": 20.199085933571475}, {"code": "def english_cleaners(text)\n    text = text.lower()\n    text = text.replace('\\u2019', \"'\")\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \"! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re", "CodeBleu_score": 21.126407010045376}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents separately\n    while len(text) > 0:\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(text[:m.start()], cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = text[m.end():]\n    return sequence + [_eos]\n\n", "CodeBleu_score": 61.93070646942409}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    for symbol in text:\n        if symbol == '{':\n            symbol = '{ '\n        elif symbol == '}':\n            symbol ='}'\n        sequence += _symbols_to_sequence(symbol, cleaner_names)\n    if sequence[-1]!= _eos_id:\n        sequence.append(_eos_id)\n    return sequence\n\n", "CodeBleu_score": 22.44547730147405}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents separately\n    for match in re.findall(r'\\{(.+?)\\}|\\S+', text):\n        if match[0] == '':\n            sequence += _symbols_to_sequence(_clean_text(match[1], cleaner_names))\n        else:\n            sequence += _arpabet_to_sequence(match)\n    # Append EOS token\n    sequence.append(_cleaners['eos'])\n    return sequence\n", "CodeBleu_score": 29.6027777822204}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n    result = \"\"\n    for element in sequence:\n        if element == 0:\n            break\n        elif element == 1:\n            result += \"{\"\n        elif element == 2:\n            result += \"}\"\n        else:\n            result += int_to_char[element]\n    result = result.replace(\"{}\", \" \")\n    return result\n\n", "CodeBleu_score": 28.7303908680228}, {"code": "def sequence_to_text(sequence)\n    result = \"\"\n    for index in sequence:\n        if index == 0:\n            continue\n        result += \"{\" + int_to_arpabet[index] + \"}\"\n    result = re.sub(r\"(\\{[^\\}]+\\})(\\{[^\\}]+\\})\", r\"\\1 \\2\", result)\n    return result\n\n", "CodeBleu_score": 27.134326336859}, {"code": "def sequence_to_text(sequence)\n    result = ''\n    for id in sequence:\n        if id == 0:\n            break\n        symbol = int_to_symbol[id]\n        if symbol[0] == '{':\n            result +='' + symbol\n        else:\n            result += symbol\n    return result.replace(' {', '{')\n\n", "CodeBleu_score": 35.081428152175725}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    if encoder_model_fpath:\n        encoder_model = load_model(encoder_model_fpath)\n    else:\n        encoder_model = load_model(ENCODER_MODEL_FPATH)\n    for fpath in fpaths:\n        wav = preprocess_wav(fpath)\n        embed = compute_embedding(encoder_model, wav)\n        np.save(fpath.replace('.wav', '.npy'), embed)\n\n", "CodeBleu_score": 31.289271619033965}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    global encoder_model\n    if encoder_model is None:\n        encoder_model = load_model(encoder_model_fpath)\n    for fpath in fpaths:\n        wav = load_wav(fpath)\n        preprocessed_wav = preprocess_wav(wav)\n        embed = embed_utterance_from_wav(preprocessed_wav)\n        np.save(fpath + \".npy\", embed)\n", "CodeBleu_score": 28.562475987496533}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    global encoder_model\n    if encoder_model is None:\n        encoder_model = load_model(encoder_model_fpath)\n    for fpath in fpaths:\n        waveform, sample_rate = load_wav_to_torch(fpath)\n        if waveform.shape[0] < 10000:\n            waveform = torch.cat([waveform, waveform], dim=0)\n        if waveform.shape[0] > 10000:\n            waveform = waveform[:10000]\n        embedding = encoder_model.embed_utterance(waveform)\n        torch.save(embedding, fpath.replace('.wav', '.pt'))\n\n", "CodeBleu_score": 29.356710221230152}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # Check if the synthesizer root directory exists\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"The synthesizer root directory '{synthesizer_root}' does not exist.\")\n\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"The encoder model file '{encoder_model_fpath}' does not exist.\")\n\n    # Check if the embeddings directory exists\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    if not embeddings_dir.exists():\n        raise FileNotFoundError(f\"The embeddings directory '{embeddings_dir}' does not exist.\")\n\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"The metadata file '{metadata_fpath}' does not exist.\")\n\n    # Read the metadata file\n    metadata_df = pd.read_csv(metadata_fpath)\n\n    # Prepare file paths for audio and embeddings\n    audio_fpaths = [synthesizer_root / \"wavs\" / f\"{row['id']}.wav\" for _, row in metadata_df.iterrows()]\n    embeddings_fpaths = [embeddings_dir / f\"{row['id']}.npy\" for _, row in metadata_df.iterrows()]\n\n    # Create a multiprocessing pool\n    pool = multiprocessing.Pool(n_processes)\n\n    # Define a function to generate embeddings for a single audio file\n    def generate_embedding(audio_fpath, embedding_fpath, encoder_model_fpath):\n        # Load the audio file\n        y, sr = librosa.load(audio_fpath, sr=22050)\n\n        # Load the encoder model\n        encoder = Encoder(encoder_model_fpath)\n\n        # Generate the embedding\n        embedding = encoder.embed_utterance(y)\n\n        # Save the embedding to a file\n        np.save(embedding_fpath, embedding)\n\n    # Use the multiprocessing pool to generate embeddings for all audio files\n    with tqdm(total=len(audio_fpaths)) as pbar:\n        for _ in pool.imap_unordered(\n            generate_embedding, audio_fpaths, embeddings_fpaths, repeat(encoder_model_fpath), chunksize=1\n        ):\n            pbar.update()\n\n    # Close", "CodeBleu_score": 32.41748278622389}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Check if the synthesizer root exists\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"Synthesizer root '{synthesizer_root}' not found.\")\n\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"Encoder model '{encoder_model_fpath}' not found.\")\n\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"Metadata file '{metadata_fpath}' not found.\")\n\n    # Check if the embeddings directory exists\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    if not embeddings_dir.exists():\n        raise FileNotFoundError(f\"Embeddings directory '{embeddings_dir}' not found.\")\n\n    # Read metadata\n    metadata = pd.read_csv(metadata_fpath)\n\n    # Get audio and embeddings paths\n    audio_paths = [synthesizer_root / row[\"path\"] for _, row in metadata.iterrows()]\n    embeddings_paths = [embeddings_dir / f\"{audio_path.stem}.npy\" for audio_path in audio_paths]\n\n    # Create a multiprocessing pool\n    pool = mp.Pool(n_processes)\n\n    # Create a progress bar\n    progress_bar = tqdm(total=len(audio_paths), desc=\"Creating embeddings\")\n\n    # Create embeddings\n    for audio_path, embeddings_path in zip(audio_paths, embeddings_paths):\n        pool.apply_async(\n            create_embedding,\n            args=(audio_path, embeddings_path, encoder_model_fpath),\n            callback=lambda _: progress_bar.update(),\n        )\n\n    # Wait for all processes to finish\n    pool.close()\n    pool.join()\n\n    # Close the progress bar\n    progress_bar.close()\n\n", "CodeBleu_score": 33.51003066287222}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Create the output directory if it doesn't exist\n    output_dir = Path(synthesizer_root, \"embeddings\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Read metadata\n    metadata = pd.read_csv(Path(synthesizer_root, \"metadata.csv\"))\n    metadata[\"path\"] = metadata[\"path\"].apply(lambda x: Path(synthesizer_root, x))\n\n    # Prepare file paths for audio and embeddings\n    audio_paths = metadata[\"path\"].tolist()\n    embedding_paths = [\n        Path(output_dir, f\"{Path(p).stem}.npy\") for p in audio_paths\n    ]\n\n    # Create a multiprocessing pool\n    pool = mp.Pool(n_processes)\n\n    # Define a function to generate embeddings for a single audio file\n    def generate_embedding(args):\n        audio_path, embedding_path = args\n        if not embedding_path.exists():\n            print(f\"Generating embedding for {audio_path.stem}\")\n            audio = load_audio(audio_path)\n            embedding = generate_embedding_from_audio(audio, encoder_model_fpath)\n            np.save(embedding_path, embedding)\n\n    # Use the multiprocessing pool to generate embeddings for all audio files\n    with tqdm(total=len(audio_paths)) as pbar:\n        for _ in pool.imap_unordered(generate_embedding, zip(audio_paths, embedding_paths)):\n            pbar.update()\n\n    # Close the multiprocessing pool\n    pool.close()\n\n", "CodeBleu_score": 35.478493188966766}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Get the predicted and target mel spectrograms\n    predicted_mel = np.squeeze(mel_prediction.numpy())\n    target_mel = np.squeeze(target_spectrogram.numpy())\n\n    # Get the attention plot\n    attention_plot = np.squeeze(attention.numpy())\n\n    # Get the input sequence\n    input_sequence = input_seq.numpy()\n\n    # Get the time\n    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n    # Get the model type\n    model_type = hparams.model_type\n\n    # Create the plot title\n    plot_title = f\"{model_type} - {current_time} - Step {step} - Loss {loss:.4f}\"\n\n    # Create the plot\n    plot_mel_spectrogram(predicted_mel, target_mel, attention_plot, plot_title, plot_dir)\n\n    # Save the predicted mel spectrogram to the specified directory\n    save_mel_spectrogram(predicted_mel, mel_output_dir, sample_num, step)\n\n    # Reconstruct the waveform from the predicted mel spectrogram\n    reconstructed_waveform = reconstruct_waveform(predicted_mel, hparams)\n\n    # Save the reconstructed waveform to the specified directory\n    save_waveform(reconstructed_waveform, wav_dir, sample_num, step)\n\n    # Print the input sequence at the current step\n    print(f\"Input sequence at step {step}: {input_sequence}\")\n\n", "CodeBleu_score": 25.957765091179642}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    print('Step:', step)\n    print('Input:', input_seq)\n    print('Loss:', loss)\n    if attention is not None:\n        plot_attention(attention, step, plot_dir, hparams)\n    plot_spectrogram(mel_prediction.T, step, plot_dir, hparams)\n    plot_spectrogram(target_spectrogram.T, step, plot_dir, hparams)\n    plot_spectrogram(mel_prediction.T, step, mel_output_dir, hparams)\n    save_wav(wav_from_spectrogram(mel_prediction.T), step, wav_dir, hparams)\n    plot_spectrogram(mel_prediction.T, step, plot_dir, hparams, title=f'Predicted Mel Spectrogram\\n{hparams.model_type} Model\\n{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\nStep: {step}\\nLoss: {loss:.4f}')\n    plot_spectrogram(target_spectrogram.T, step, plot_dir, hparams, title=f'Target Mel Spectrogram\\n{hparams.model_type} Model\\n{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\nStep: {step}\\nLoss: {loss:.4f}')\n    plt.close('all')\n\n", "CodeBleu_score": 27.21487138493644}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # attention\n    attention = attention.cpu().detach().numpy()\n    attention = np.array(attention)\n    attention = np.squeeze(attention, axis=0)\n    attention = np.squeeze(attention, axis=0)\n    attention = np.transpose(attention, (1, 0))\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    im = ax.imshow(attention, aspect='auto', origin='lower',\n                   interpolation='none')\n    fig.colorbar(im, ax=ax)\n    xlabel = 'Decoder timestep'\n    plt.xlabel(xlabel)\n    ylabel = 'Encoder timestep'\n    plt.ylabel(ylabel)\n    plt.tight_layout()\n    plt.savefig(plot_dir + '/{}_{}_{}_attention.png'.format(\n        hparams.model_type, time.time(), step))\n    plt.close()\n\n    # mel\n    mel_prediction = mel_prediction.cpu().detach().numpy()\n    mel_prediction = np.array(mel_prediction)\n    mel_prediction = np.squeeze(mel_prediction, axis=0)\n    mel_prediction = np.transpose(mel_prediction, (1, 2, 0))\n    fig = plt.figure(figsize=(10, 8))\n    ax1 = fig.add_subplot(311)\n    ax1.set_title(hparams.model_type)\n    im = ax1.imshow(np.rot90(mel_prediction), aspect='auto',\n                    interpolation='none')\n    plt.colorbar(mappable=im, shrink=0.65, orientation='horizontal',\n                 ax=ax1)\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    ax2 = fig.add_subplot(312)\n    ax2.set_title('Target Mel Spectrogram')\n    im = ax2.imshow(np.rot90(target_spectrogram), aspect='auto',\n                    interpolation='none')\n    plt.colorbar(mappable=im, shrink=0.65, orientation='horizontal',\n                 ax=ax2)\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    ax3 = fig.add_subplot(313)\n    ax3.set_title('Residual')\n    im = ax", "CodeBleu_score": 31.015852479313534}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n", "CodeBleu_score": 5.637300874658717}, {"code": "def num_params(self)\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n", "CodeBleu_score": 5.637300874658717}, {"code": "def num_params(self)\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"The model has {num_params / 1e6:.3f}M trainable parameters.\")\n", "CodeBleu_score": 14.144869488247508}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'before':\n        return torch.cat((x, torch.zeros(pad, *x.shape[1:], device=x.device)), dim=0)\n    elif side == 'after':\n        return torch.cat((torch.zeros(pad, *x.shape[1:], device=x.device), x), dim=0)\n    elif side == 'both':\n        return torch.cat((torch.zeros(pad // 2, *x.shape[1:], device=x.device), x, torch.zeros(pad // 2, *x.shape[1:], device=x.device)), dim=0)\n    else:\n        raise ValueError('Invalid side value.')\n", "CodeBleu_score": 24.017486459426106}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'before':\n        return torch.cat([x.new_zeros(x.size(0), x.size(1), pad), x], dim=2)\n    elif side == 'after':\n        return torch.cat([x, x.new_zeros(x.size(0), x.size(1), pad)], dim=2)\n    elif side == 'both':\n        return torch.cat([x.new_zeros(x.size(0), x.size(1), pad), x, x.new_zeros(x.size(0), x.size(1), pad)], dim=2)\n    else:\n        raise ValueError('Side must be one of \"before\", \"after\", or \"both\"')\n", "CodeBleu_score": 23.314632668841945}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'before':\n        padded_tensor = torch.zeros(x.size(0), x.size(1), pad, device=x.device)\n        padded_tensor[:, :, pad:] = x\n    elif side == 'after':\n        padded_tensor = torch.zeros(x.size(0), x.size(1), pad, device=x.device)\n        padded_tensor[:, :, :pad] = x\n    elif side == 'both':\n        padded_tensor = torch.zeros(x.size(0), x.size(1), pad*2, device=x.device)\n        padded_tensor[:, :, pad:] = x[:, :, :x.size(2)-pad]\n        padded_tensor[:, :, :pad] = x[:, :, -pad:]\n    return padded_tensor\n", "CodeBleu_score": 31.314776327835236}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(20, 10))\n    plt.imshow(attn, cmap=\"gray\")\n    plt.savefig(path + \".png\")\n    plt.close(fig)\n\n", "CodeBleu_score": 48.71599461318888}, {"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.tick_params(labelsize=12)\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 52.453649671045774}, {"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path + '.png')\n    plt.close()\n\n", "CodeBleu_score": 50.32526426348214}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n    fig = plt.figure(figsize=(15, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"x\", fontsize=20, color=\"red\")\n    ax.set_ylabel(\"y\", fontsize=20, color=\"red\")\n    ax.tick_params(axis=\"both\", labelcolor=\"red\", labelsize=20)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 22.99601386339014}, {"code": "def plot(array)\n    fig = plt.figure(figsize=(12, 5))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"Time\", color=\"black\", fontsize=14)\n    ax.set_ylabel(\"Value\", color=\"black\", fontsize=14)\n    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=12)\n    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=12)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 28.39343622871093}, {"code": "def plot(array)\n    plt.figure(figsize=(12, 8))\n    ax = plt.subplot(111)\n    ax.set_xlabel(\"x\", fontsize=16, color=\"green\")\n    ax.set_ylabel(\"y\", fontsize=16, color=\"red\")\n    ax.tick_params(axis=\"x\", colors=\"green\")\n    ax.tick_params(axis=\"y\", colors=\"red\")\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 23.178940905616678}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.writer(f)\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writerow(data.keys())\n        writer.writerow(data.values())\n\n", "CodeBleu_score": 43.98026187670323}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 60.413545715102444}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    data = {\"image\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    if not os.path.isfile(CSV_FILE_NAME):\n        with open(CSV_FILE_NAME, \"w\") as file:\n            writer = csv.DictWriter(file, fieldnames=data.keys())\n            writer.writeheader()\n    with open(CSV_FILE_NAME, \"a\") as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        writer.writerow(data)\n\n", "CodeBleu_score": 51.50660041368889}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    if not file.endswith(\".pt\"):\n        file += \".pt\"\n    if not os.path.exists(os.path.dirname(file)):\n        os.makedirs(os.path.dirname(file))\n    log(prefix + \" Exporting model to TorchScript format...\")\n    if optimize:\n        log(prefix + \" Optimizing for mobile use...\")\n    log(prefix + \" Saving model to file...\")\n    if optimize:\n        traced = torch.jit.trace(model, im)\n        traced.save(file)\n        traced.save(os.path.join(os.path.dirname(file), \"traced.pt\"))\n        log(prefix + \" Saved traced model to file.\")\n        log(prefix + \" Saving model details...\")\n        with open(os.path.join(os.path.dirname(file), \"model.txt\"), \"w\") as f:\n            f.write(str(model))\n        log(prefix + \" Saved model details to file.\")\n    else:\n        torch.jit.save(torch.jit.script(model), file)\n        log(prefix + \" Saved model to file.\")\n    log(prefix + \" Model export complete.\")\n    return file, None\n\n", "CodeBleu_score": 31.691506595921865}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    if not isinstance(model, nn.Module):\n        return None\n    if not isinstance(im, torch.Tensor):\n        return None\n    if not isinstance(file, str):\n        return None\n    if not isinstance(optimize, bool):\n        return None\n    if not isinstance(prefix, str):\n        return None\n    # start\n    print(prefix + \" Exporting TorchScript model to file: \" + file)\n    # export\n    if optimize:\n        traced = torch.jit.trace(model, im)\n        traced.save(file)\n        print(prefix + \" TorchScript model saved to file: \" + file)\n        print(prefix + \" TorchScript model exported for mobile use\")\n    else:\n        traced = torch.jit.trace(model, im)\n        traced.save(file)\n        print(prefix + \" TorchScript model saved to file: \" + file)\n        print(prefix + \" TorchScript model exported for desktop use\")\n    # return\n    return file\n", "CodeBleu_score": 27.815701915470225}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    # Log start of export\n    log(prefix + \"Exporting model to TorchScript...\")\n    # Trace the model with the given input\n    model = torch.jit.trace(model, im)\n    # Include extra files with model details\n    model.save(file)\n    model.save(file + \".pt\")\n    model.save(file + \".pth\")\n    # If optimization is specified, save the model for mobile use\n    if optimize:\n        log(prefix + \"Optimizing model for mobile use...\")\n        model.save(file + \".pt\", _export_constants=True)\n        model.save(file + \".pth\", _export_constants=True)\n    # Log end of export\n    log(prefix + \"Model exported to TorchScript.\")\n    return file, None\n\n", "CodeBleu_score": 25.373908623935375}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    if not check_coremltools():\n        return None, None\n\n    print(prefix + \"Exporting to CoreML...\")\n    start = time.time()\n\n    if nms:\n        model = iOSModel(model, im)\n\n    # Trace the model\n    trace = torch.jit.trace(model, torch.rand(1, 3, im, im))\n\n    # Convert to CoreML\n    coreml = ct.convert(trace, inputs=[ct.ImageType(name=\"input.1\", shape=trace.graph.inputs[0].type.sizes)], minimum_deployment_target=ct.target.iOS13)\n\n    # Apply quantization\n    if int8:\n        coreml = ct.convert(trace, inputs=[ct.ImageType(name=\"input.1\", shape=trace.graph.inputs[0].type.sizes)], minimum_deployment_target=ct.target.iOS13, converter_kwargs={\"mlmodel_path_or_mlmodel\": coreml, \"convert_to\": \"int8\"})\n    elif half:\n        coreml = ct.convert(trace, inputs=[ct.ImageType(name=\"input.1\", shape=trace.graph.inputs[0].type.sizes)], minimum_deployment_target=ct.target.iOS13, converter_kwargs={\"mlmodel_path_or_mlmodel\": coreml, \"convert_to\": \"fp16\"})\n\n    # Save the CoreML model\n    file = file.replace(\".onnx\", \".mlmodel\")\n    coreml.save(file)\n\n    print(prefix + \"Exporting to CoreML...Done, %g seconds\" % (time.time() - start))\n\n    return file, coreml\n", "CodeBleu_score": 42.360888083264655}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):    if not coremltools_available:\n        raise RuntimeError(\"The 'coremltools' package is not available. Install it with 'pip install coremltools'.\")\n    print(f\"{prefix} Exporting to CoreML format...\")\n    if nms:\n        model = iOSModel(model, nms_threshold=nms)\n    model.eval()\n    model.to(im.device)\n    model.half() if half else model.float()\n    model.forward(im)\n    model.eval()\n    model.float()\n    model = torch.jit.trace(model, im)\n    mlmodel = ct.convert(model, inputs=[ct.ImageType(name=\"input_1\", shape=im.shape)])\n    if int8:\n        if ct.utils.macos_version() >= (10, 15):\n            mlmodel = ct.utils.quantization.quantize_weights(mlmodel, nbits=8)\n            print(f\"{prefix} Quantized model to int8.\")\n        else:\n            print(f\"{prefix} Skipping int8 quantization. macOS version is too old.\")\n    mlmodel.save(file)\n    print(f\"{prefix} Done. Saved to {file}\")\n    return file, mlmodel\n", "CodeBleu_score": 40.30213115114437}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):    if not coremltools:\n        print(f\"{prefix} The coremltools package is required for this function.\")\n        return None, None\n    print(f\"{prefix} Exporting to CoreML...\")\n    if nms:\n        print(f\"{prefix} Wrapping model with iOSModel for NMS support...\")\n        model = iOSModel(model)\n    print(f\"{prefix} Tracing model...\")\n    traced = torch.jit.trace(model, im)\n    print(f\"{prefix} Converting to CoreML...\")\n    mlmodel = ct.convert(traced, inputs=[ct.ImageType(name=\"input\", shape=im.shape)])\n    if int8 or half:\n        print(f\"{prefix} Applying quantization...\")\n        if int8:\n            mlmodel = ct.quantization.quantize_weights(mlmodel, nbits=8)\n        if half:\n            mlmodel = ct.quantization.quantize_weights(mlmodel, nbits=16)\n    print(f\"{prefix} Saving model...\")\n    file = file.replace(\".mlmodel\", \"\") + \".mlmodel\"\n    mlmodel.save(file)\n    print(f\"{prefix} Done. Saved model to {file}.\")\n    return file, mlmodel\n", "CodeBleu_score": 37.99449991366084}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule == 'discrete':\n        if t >= self.T:\n            return 0\n        else:\n            return np.log(self.alpha_0 + (self.alpha_t - self.alpha_0) * t / self.T)\n    elif self.schedule == 'linear':\n        if t <= self.T:\n            return np.log(self.alpha_0 + (self.alpha_t - self.alpha_0) * t / self.T)\n        else:\n            return np.log(self.alpha_t)\n    elif self.schedule == 'cosine':\n        if t <= self.T:\n            cosine_part = np.cos((np.pi * t) / self.T)\n            return np.log(self.alpha_0 + (self.alpha_t - self.alpha_0) * (1 + cosine_part) / 2)\n        else:\n            return np.log(self.alpha_t)\n    else:\n        raise ValueError(\"The schedule type is not recognized.\")\n", "CodeBleu_score": 38.07859662760158}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n      return self.log_alpha_0 - t / (self.total_steps - 1) * (self.log_alpha_0 - self.log_alpha_final)\n    elif self.schedule_type == 'linear':\n      return self.log_alpha_final + (self.log_alpha_0 - self.log_alpha_final) * t / self.total_steps\n    elif self.schedule_type == 'cosine':\n      s = np.cos((t / self.total_steps + self.s_offset) * np.pi) * (-0.5) + 0.5\n      return self.log_alpha_final + (self.log_alpha_0 - self.log_alpha_final) * s\n    else:\n      raise NotImplementedError\n\n  def marginal_log_prob(self, t, x0, x1):\n    \"\"\"The function computes the log of the marginal probability of the label x1 given the label x0 at a given continuous-time label t. It computes the log of the mean coefficient alpha_t using the marginal_log_mean_coeff function, and then applies the formula for the log of the marginal probability.\"\"\"\n    log_mean_coeff = self.marginal_log_mean_coeff(t)\n    return log_mean_coeff + self.log_prob(x0, x1) - self.log_mean_coeff(t)\n\n  def log_prob(self, x0, x1):\n    \"\"\"The function computes the log of the conditional probability of the label x1 given the label x0. It checks if the labels are valid, and if they are not, it returns a large negative value. Otherwise, it computes the log of the conditional probability using the log_probs attribute and returns the result.\"\"\"\n    if x0 < 0 or x0 >= self.num_classes or x1 < 0 or x1 >= self.num_classes:\n      return -1e9\n    else:\n      return self.log_probs[x0, x1]\n\n  def log_mean_coeff(self, t):\n    \"\"\"The function computes the log of the mean coefficient alpha_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'", "CodeBleu_score": 36.70008458326201}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n      return self.log_alpha_0 - np.log(self.num_timesteps) + t * self.log_alpha_t_minus_alpha_0\n    elif self.schedule_type == 'linear':\n      return self.log_alpha_0 + t * (self.log_alpha_t_minus_alpha_0 - self.log_alpha_0)\n    elif self.schedule_type == 'cosine':\n      return self.log_alpha_0 + (self.log_alpha_t_minus_alpha_0 - self.log_alpha_0) * 0.5 * (1.0 + np.cos(np.pi * t / self.num_timesteps))\n    else:\n      raise ValueError('Unknown schedule type %s' % self.schedule_type)\n\n  def marginal_log_prob(self, t, x):\n    \"\"\"The function computes the log of the marginal probability of a given sample x at a given continuous-time label t.\\nIt uses the log of the mean coefficient alpha_t computed by the function marginal_log_mean_coeff() and the log of the standard deviation sigma_t.\\nIt returns the computed log(p_t(x)).\"\"\"\n    return self.marginal_log_mean_coeff(t) - 0.5 * self.log_one_minus_alpha_t[t] - 0.5 * self.log_one_minus_alpha_t[t] * x**2\n\n  def q_sample(self, x_start, x_t, t):\n    \"\"\"The function computes a sample from the conditional distribution q(x_{t-1}|x_t,t) using the reparameterization trick.\\nIt uses the log of the standard deviation sigma_t and the log of the mean coefficient alpha_t computed by the function marginal_log_mean_coeff().\\nIt returns the computed sample.\"\"\"\n    posterior_mean = self.sqrt_one_minus_alpha_t[t] * x_start + self.sqrt_alpha_t[t] * x_t\n    posterior_variance = 1. - self.alpha_t[t]\n    noise = torch.randn_like(x_t)\n    return posterior_mean + self.sqrt_one_minus_", "CodeBleu_score": 37.580951814078766}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n      t = self.inverse_linear_lambda(lamb)\n    elif self.schedule == \"discrete\":\n      t = self.inverse_discrete_lambda(lamb)\n    else:\n      t = self.inverse_non_linear_lambda(lamb)\n    return t\n\n  def inverse_linear_lambda(self, lamb):\n    \"\"\"The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t using a linear schedule.\\nIt calculates t using beta values and a temporary variable.\"\"\"\n    lamb_t = lamb / self.beta\n    t = (1.0 / (1.0 - self.beta)) * (lamb_t - self.beta * self.T)\n    return t\n\n  def inverse_discrete_lambda(self, lamb):\n    \"\"\"The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t using a discrete schedule.\\nIt interpolates log_alpha to find t.\"\"\"\n    t = np.interp(lamb, self.log_alpha, self.t)\n    return t\n\n  def inverse_non_linear_lambda(self, lamb):\n    \"\"\"The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t using a non-linear schedule.\\nIt computes log_alpha and uses an arccos function to derive t.\"\"\"\n    log_alpha = self.log_alpha_from_lambda(lamb)\n    t = self.T * (1.0 - np.arccos(np.exp(log_alpha)) / np.pi)\n    return t\n\n  def log_alpha_from_lambda(self, lamb):\n    \"\"\"The function computes the log_alpha value from a given half-logSNR lambda_t using a non-linear schedule.\\nIt uses an arccos function to derive log_alpha.\"\"\"\n    log_alpha = np.log(np.cos(np.pi * lamb / self.T))\n    return log_alpha\n\n  def log_alpha_from_t(self, t):\n    \"\"\"The function computes the log_alpha value from a given continuous-time label t using a non-linear schedule.\\nIt uses an arccos function to derive log_alpha.\"\"\"\n    log_alpha = np.log(np.cos(np.pi * (", "CodeBleu_score": 31.75084544836386}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n      if lamb < 0 or lamb > self.beta_s[-1]:\n        return None\n      t_interp = np.interp(lamb, self.beta_s, np.arange(self.T + 1))\n      return int(t_interp)\n    elif self.schedule == \"discrete\":\n      if lamb < 0 or lamb > self.log_alpha_s[-1]:\n        return None\n      log_alpha_interp = np.interp(lamb, self.log_alpha_s, np.arange(self.T + 1))\n      return int(log_alpha_interp)\n    else:\n      if lamb < 0 or lamb > self.log_alpha_s[-1]:\n        return None\n      t = np.arccos(lamb / self.log_alpha_s[-1]) / (np.pi / (self.T + 1))\n      return int(t)\n\n  def forward(self, x):\n    \"\"\"The function computes the forward pass of the model.\\nIt computes the log-likelihood of the data under the prior distribution.\\nIt computes the log-likelihood of the data under the posterior distribution.\\nIt computes the log-likelihood of the data under the variational distribution.\\nIt computes the KL divergence between the variational distribution and the prior distribution.\\nIt computes the ELBO loss.\\nIt computes the log-likelihood of the data under the posterior distribution.\\nIt computes the log-likelihood of the data under the variational distribution.\\nIt computes the KL divergence between the variational distribution and the prior distribution.\\nIt computes the ELBO loss.\"\"\"\n    self.log_p_x = self.log_p_x_fn(x)\n    self.log_p_z = self.log_p_z_fn(self.z_mean, self.z_log_var)\n    self.log_q_z = self.log_q_z_fn(self.z_mean, self.z_log_var)\n    self.kl_z = self.kl_div_fn(self.z_mean, self.z_log_var)\n    self.elbo = tf.reduce_mean(self.log_p_x + self.log_p_z - self.log_q_z)\n    self.log_p_x", "CodeBleu_score": 30.20967538265195}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n      t = self.T * (1 - np.exp(-lamb / self.beta)) / (1 - np.exp(-self.beta))\n    elif self.schedule == \"discrete\":\n      lamb = np.maximum(lamb, self.log_alpha[0])\n      lamb = np.minimum(lamb, self.log_alpha[-1])\n      t = np.interp(lamb, self.log_alpha, self.t_values)\n    else:\n      t = self.T * np.arccos(np.exp(-lamb / self.beta)) / np.pi\n    return t\n\n  def forward(self, x, t):\n    \"\"\"The function computes the forward process of the diffusion model.\\nIt first computes the standard deviation sigma_t from the label t, then adds Gaussian noise to the input x using the standard deviation.\\nFinally, it returns the noisy image and the standard deviation.\"\"\"\n    sigma_t = self.sigma(t)\n    x_t = x + torch.randn_like(x) * sigma_t\n    return x_t, sigma_t\n\n  def reverse(self, x_t, t):\n    \"\"\"The function computes the reverse process of the diffusion model.\\nIt first computes the standard deviation sigma_t from the label t, then computes the mean mu_t using the forward process of the diffusion model.\\nFinally, it returns the mean mu_t and the standard deviation sigma_t.\"\"\"\n    sigma_t = self.sigma(t)\n    mu_t = (x_t - sigma_t ** 2 / self.sigma(t - 1) * self.forward(x_t, t - 1)[0]) / (1 - sigma_t ** 2 / self.sigma(t - 1) ** 2)\n    return mu_t, sigma_t\n\n  def forward_diffusion(self, x, t):\n    \"\"\"The function computes the forward process of the diffusion model.\\nIt first computes the standard deviation sigma_t from the label t, then adds Gaussian noise to the input x using the standard deviation.\\nFinally, it returns the noisy image and the standard deviation.\"\"\"\n    sigma_t = self.sigma(t)\n    x_t = x + torch.randn_like(x) * sigma_t\n    return x_t, sigma_t", "CodeBleu_score": 31.547353538702545}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    t = get_time_steps(t_continuous, x.shape[0])\n    t = t.expand(x.shape[0], -1)\n    if model_type == \"unet\":\n        out = model(x, t, cond)\n    elif model_type == \"resnet\":\n        out = model(x, t)\n    else:\n        raise ValueError(f\"model_type {model_type} not supported\")\n    if noise_schedule == \"linear\":\n        out = linear_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"cosine\":\n        out = cosine_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"scaled_linear\":\n        out = scaled_linear_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"not_scaled_linear\":\n        out = not_scaled_linear_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"cosine_0.008\":\n        out = cosine_0008_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"cosine_0.008_st\":\n        out = cosine_0008_st_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"not_scaled_cosine\":\n        out = not_scaled_cosine_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"not_scaled_cosine_0.008\":\n        out = not_scaled_cosine_0008_beta_schedule(t, out, beta_start, beta_end)\n    elif noise_schedule == \"not_scaled_cosine_0.008_st\":\n        out = not_scaled_cosine_0008_st_beta_schedule(\n            t, out, beta_start, beta_end\n        )\n    elif noise_schedule == \"ddim\":\n        out = ddim_beta_schedule(t, out, beta_start, beta_end)\n    else:\n        raise ValueError(f\"noise_schedule {noise", "CodeBleu_score": 23.298136061704927}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if t_continuous is not None:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    if cond is not None:\n        cond = cond.expand(x.shape[0], -1, -1, -1)\n    if model_type == \"unet\":\n        pred = model(x, t_continuous, cond)\n    elif model_type == \"resnet\":\n        pred = model(x, t_continuous)\n    else:\n        raise ValueError(f\"model_type given as {model_type} is not supported\")\n    if noise_type == \"learned\":\n        pred_noise = 0.5 * pred\n    elif noise_type == \"fixed\":\n        pred_noise = torch.sqrt(t_continuous) * pred\n    else:\n        raise ValueError(f\"noise_type given as {noise_type} is not supported\")\n    return pred_noise\n\n", "CodeBleu_score": 28.92890062691911}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    # Expand time input to match batch size\n    t_continuous = expand_to_batch_size(t_continuous, x)\n    # Process the input through the model\n    if cond is not None:\n        pred = model([x, t_continuous, cond], training=False)\n    else:\n        pred = model([x, t_continuous], training=False)\n    # Apply different transformations to the model output using noise schedule parameters\n    if model_type == \"ddim\":\n        pred = noise_pred_from_ddim(pred, t_continuous)\n    elif model_type == \"plms\":\n        pred = noise_pred_from_plms(pred, t_continuous)\n    elif model_type == \"heun\":\n        pred = noise_pred_from_heun(pred, t_continuous)\n    elif model_type == \"euler\":\n        pred = noise_pred_from_euler(pred, t_continuous)\n    elif model_type == \"dpm\":\n        pred = noise_pred_from_dpm(pred, t_continuous)\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n    return pred\n\n", "CodeBleu_score": 22.072899630746345}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    x = torch.flatten(x0, start_dim=1)\n    x = torch.abs(x)\n    x = torch.sort(x, dim=1, descending=True)[0]\n    x = x.mean(dim=1, keepdim=True)\n    x = x.unsqueeze(dim=2)\n    x = x.unsqueeze(dim=3)\n    x = x.unsqueeze(dim=4)\n    x = x.unsqueeze(dim=5)\n    x = x.expand(x0.shape)\n    if t is None:\n        t = self.threshold\n    x = torch.clamp(x0, -t * x, t * x)\n    x = x / t\n    return x\n\n", "CodeBleu_score": 24.111388106417557}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    x0_abs = torch.abs(x0)\n    x0_abs_reshape = x0_abs.view(x0_abs.shape[0], -1)\n    x0_abs_q = torch.quantile(x0_abs_reshape, self.quantile, dim=1)\n    x0_abs_q = x0_abs_q.view(x0_abs_q.shape[0], 1, 1, 1)\n    x0_abs_q = x0_abs_q.repeat(1, x0_abs.shape[1], x0_abs.shape[2], x0_abs.shape[3])\n    x0_abs_q = x0_abs_q.to(x0.device)\n    x0_abs_q = torch.max(x0_abs_q, self.threshold)\n    x0_abs_q = x0_abs_q.unsqueeze(1)\n    x0_abs_q = x0_abs_q.repeat(1, x0.shape[1], 1, 1, 1)\n    x0_abs_q = x0_abs_q.to(x0.device)\n    x0_abs_q = x0_abs_q.permute(0, 1, 3, 4, 2)\n    x0_abs_q = x0_abs_q.reshape(x0_abs.shape)\n    x0_clamp = torch.clamp(x0, -x0_abs_q, x0_abs_q)\n    x0_clamp = x0_clamp / x0_abs_q\n    return x0_clamp\n", "CodeBleu_score": 13.563455809413854}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    x = x0.reshape(x0.shape[0], -1)\n    q = self.quantile(x, dim=1)\n    if t is None:\n      t = self.threshold\n    s = (1.0 - t) / (1.0 - q)\n    x = torch.clamp(x, min=0.0, max=s)\n    return x.reshape(x0.shape) * self.scale\n", "CodeBleu_score": 26.369211971705475}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n    # Dynamic thresholding\n    threshold = self.threshold_ratio * self.max_threshold\n    x = torch.clamp(x, min=-threshold, max=threshold)\n    # Normalization\n    x = x / threshold\n    return x\n\n", "CodeBleu_score": 9.7419412241509}, {"code": "def data_prediction_fn(self, x, t)\n    # Calculate the threshold value\n    threshold = self.threshold_ratio * self.max_threshold\n    # Clamp the input tensor values within the threshold\n    x = torch.clamp(x, -threshold, threshold)\n    # Normalize the clamped values\n    x = x / threshold\n    return x\n\n", "CodeBleu_score": 9.461559739680823}, {"code": "def data_prediction_fn(self, x, t)\n    # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n    threshold = self.threshold_ratio * self.max_threshold\n    # Clamp the input tensor values within this threshold\n    x = torch.clamp(x, min=-threshold, max=threshold)\n    # Normalize the clamped values\n    x = x / threshold\n    # Return the processed tensor\n    return x\n", "CodeBleu_score": 9.7419412241509}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape)\n    # Scale the input\n    input = input / (sigma_in + 1e-10)\n    # Compute the prediction by adjusting the model output with the scaled input and sigma\n    prediction = model(input, **kwargs) + input * sigma_in\n    return prediction\n\n", "CodeBleu_score": 23.56234012118069}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma = sigma_in.reshape(input.shape)\n    scaled_input = scale(input, sigma, **kwargs)\n    return model(scaled_input) + scaled_input * sigma\n\n", "CodeBleu_score": 20.685536990174874}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma = sigma_in.reshape(input.shape)\n    input = scale_input(input)\n    prediction = model.predict(input, **kwargs)\n    return prediction + sigma * input\n", "CodeBleu_score": 20.970047337774794}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = torch.sigmoid(beta_0 + beta_1 * taus)\n    log_alpha = torch.log(alpha)\n    # Setting the required gradients for taus\n    taus.requires_grad = True\n    # Cloning the tensor to perform backpropagation\n    alpha_clone = alpha.clone()\n    log_alpha_clone = log_alpha.clone()\n    # Performing backpropagation\n    log_alpha_clone.backward(torch.ones_like(log_alpha_clone))\n    # Calculating the integrand\n    integrand = alpha_clone * taus.grad\n    return integrand, alpha, log_alpha\n\n", "CodeBleu_score": 32.85731250659435}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = torch.exp(taus)\n    log_alpha = torch.log(alpha)\n    # calculate the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha), retain_graph=True)\n    # calculate the integrand\n    integrand = (\n        alpha\n        * torch.exp(\n            -0.5 * (beta_0 + beta_1 * log_alpha) ** 2\n            - 0.5 * (log_alpha ** 2)\n            - 0.5 * log_alpha\n        )\n    )\n    return integrand\n\n", "CodeBleu_score": 23.865722845301356}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = torch.exp(-beta_0 - beta_1 * taus)\n    log_alpha = -beta_0 - beta_1 * taus\n    grad_log_alpha = torch.autograd.grad(log_alpha, taus, torch.ones_like(log_alpha), retain_graph=True, create_graph=True)[0]\n    return alpha * grad_log_alpha\n", "CodeBleu_score": 25.382846604659477}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = model.get_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    for i, sigma in enumerate(sigmas):\n        if disable is not None and i in disable:\n            continue\n        x = model.denoise(x, sigma, extra_args=extra_args)\n        dx = model.get_grad(x, sigma, extra_args=extra_args)\n        x = x - eta * dx\n        if callback is not None:\n            callback(x, sigma)\n        if i < len(sigmas) - 1:\n            x = noise_sampler.add_noise(x, sigma, s_noise)\n    return x\n", "CodeBleu_score": 29.39208395260954}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = model.get_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler(x)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None and i not in disable:\n            callback(x, i)\n        x = model.denoise(x, sigma, extra_args)\n        x = x + eta * (model.get_grad(x, sigma, extra_args) - x / sigma**2)\n        if noise_sampler is not None and i not in disable:\n            x = x + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 27.426652442559842}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = model.get_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler(x, s_noise)\n    for sigma in sigmas:\n        x = denoise(model, x, sigma, extra_args=extra_args)\n        dx = model.get_ancestral_step(x, sigma, extra_args=extra_args)\n        x = x - eta * dx\n        if callback is not None:\n            callback(x, sigma)\n        if noise_sampler is not None:\n            x = x + noise_sampler(sigma)\n    return x\n\n", "CodeBleu_score": 24.95065462317426}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n    if order > i:\n        raise ValueError(\"Order cannot be higher than the step index.\")\n    def product_term(x):\n        \"\"\"The function calculates the product term for the coefficient, excluding the j-th term.\"\"\"\n        prod = 1\n        for k in range(order):\n            if k!= j:\n                prod *= x - t[i-k]\n        return prod\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n", "CodeBleu_score": 37.40807669507371}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if order >= i+1:\n        raise ValueError(\"Order must be less than the step index.\")\n    def product_term(k):\n        \"\"\"The function calculates the product term for a given k, excluding the j-th term.\"\"\"\n        if k == j:\n            return 1\n        return t[i]-t[k]\n    def f(x):\n        \"\"\"The function computes the product term for a given x, excluding the j-th term.\"\"\"\n        return product_term(x)\n    return integrate.quad(f, i, i+1)[0]\n", "CodeBleu_score": 22.886554573009498}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if order > i:\n        raise ValueError(f\"The order {order} is too high for the step index {i}.\")\n\n    def product_term(k):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\\nIt returns 1 if k is equal to j, and 0 otherwise.\"\"\"\n        if k == j:\n            return 1\n        return 0\n\n    return integrate.quad(lambda x: product_term(x), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 26.928493692483578}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = partial(torch.randn_like, dtype=x.dtype, device=x.device)\n    for sigma in sigmas:\n        if callback is not None:\n            callback(x, sigma)\n        if sigma > 0:\n            x = model(x, sigma, **extra_args) + sigma * noise_sampler()\n        else:\n            x = model(x, sigma, **extra_args)\n    return x\n\n", "CodeBleu_score": 28.09567908504638}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = NoiseSampler(model.device)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(x, i, sigma)\n        if sigma > 0:\n            x = x + noise_sampler.get_noise(x, sigma)\n        x = model(x, sigma=sigma, **extra_args)\n    return x\n\n", "CodeBleu_score": 27.951340502206417}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = get_default_extra_args(model)\n    if noise_sampler is None:\n        noise_sampler = get_default_noise_sampler(model)\n    for i, sigma in enumerate(sigmas):\n        if disable is not None and i in disable:\n            continue\n        x = model(x, sigma=sigma, **extra_args)\n        if i < len(sigmas) - 1 and sigmas[i + 1] > 0:\n            x = x + noise_sampler(x, sigma=sigmas[i + 1])\n        if callback is not None:\n            callback(x, i, sigma)\n    return x\n\n", "CodeBleu_score": 35.25880168854306}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)    if len(x.shape) >= target_dims:\n        raise ValueError(\"The number of target dimensions must be greater than the current dimensions.\")\n    else:\n        n_dims_to_append = target_dims - len(x.shape)\n        x = x.expand(x.shape + (1,) * n_dims_to_append)\n        if x.device.type == \"mps\":\n            x = x.detach().clone()\n        return x\n\n", "CodeBleu_score": 37.70876925792865}, {"code": "def append_dims(x, target_dims)    if len(x.shape) >= target_dims:\n        raise ValueError(f\"The number of dimensions ({len(x.shape)}) is greater than or equal to the target number of dimensions ({target_dims}).\")\n    return x.expand(*x.shape, *((1,) * (target_dims - len(x.shape)))).detach().clone() if x.device.type == \"mps\" else x.expand(*x.shape, *((1,) * (target_dims - len(x.shape))))\n\n", "CodeBleu_score": 29.58596959739647}, {"code": "def append_dims(x, target_dims)    if len(x.shape) >= target_dims:\n        raise ValueError(f'The current dimensions of the tensor ({len(x.shape)}) are greater than or equal to the target dimensions ({target_dims}).')\n    return x.expand(*x.shape, *((1,) * (target_dims - len(x.shape)))).detach().clone() if x.device.type =='mps' else x.expand(*x.shape, *((1,) * (target_dims - len(x.shape))))\n\n", "CodeBleu_score": 29.679090809174607}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filename = os.path.basename(url)\n    filepath = os.path.join(path, filename)\n    if not os.path.exists(filepath):\n        print(f\"Downloading {filename}...\")\n        r = requests.get(url, stream=True)\n        with open(filepath, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n    if digest:\n        print(f\"Checking {filename}...\")\n        if not check_digest(filepath, digest):\n            raise ValueError(f\"Invalid digest for {filename}\")\n    return filepath\n\n", "CodeBleu_score": 39.39411218308139}, {"code": "def download_file(path, url, digest=None)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(os.path.join(path, os.path.basename(url))):\n        print(\"Downloading {} to {}\".format(url, path))\n        urllib.request.urlretrieve(url, os.path.join(path, os.path.basename(url)))\n    if digest:\n        if hashlib.sha256(open(os.path.join(path, os.path.basename(url)), 'rb').read()).hexdigest()!= digest:\n            raise ValueError(\"Hash mismatch for file {}!\".format(os.path.basename(url)))\n    return os.path.join(path, os.path.basename(url))\n\n", "CodeBleu_score": 32.75817954319995}, {"code": "def download_file(path, url, digest=None)\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    if not os.path.exists(path):\n        print(\"Downloading {} to {}\".format(url, path))\n        r = requests.get(url, stream=True)\n        with open(path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n    if digest:\n        assert hashlib.sha256(open(path, 'rb').read()).hexdigest() == digest\n    return path\n\n", "CodeBleu_score": 46.66677982965336}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    yield\n    for module in model.modules():\n        if isinstance(module, torch.nn.BatchNorm2d):\n            module.train(mode)\n\n", "CodeBleu_score": 32.571797182719244}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    yield\n    for module in model.modules():\n        if isinstance(module, nn.BatchNorm2d):\n            module.train(mode)\n\n", "CodeBleu_score": 31.65743492844355}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    previous_modes = []\n    for module in model.modules():\n        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n            previous_modes.append(module.training)\n            if mode:\n                module.eval()\n            else:\n                module.train()\n    yield\n    for module, previous_mode in zip(model.modules(), previous_modes):\n        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n            module.train(previous_mode)\n\n", "CodeBleu_score": 29.720410874964465}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        averaged_params.data.mul_(decay).add_((1 - decay) * current_params.data)\n    for current_buffers, averaged_buffers in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffers.data.copy_(current_buffers.data)\n\n", "CodeBleu_score": 30.41122860745564}, {"code": "def ema_update(model, averaged_model, decay)\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n    for current_buffers, averaged_buffers in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffers.data.mul_(decay).add_(current_buffers.data, alpha=1 - decay)\n", "CodeBleu_score": 32.85556074447027}, {"code": "def ema_update(model, averaged_model, decay)\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.84036159017947}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = log_logistic_cdf(min_value, loc, scale)\n    max_cdf = log_logistic_cdf(max_value, loc, scale)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    logit_samples = torch.log(uniform_samples / (1 - uniform_samples))\n    return torch.exp(logit_samples) * scale + loc\n\n", "CodeBleu_score": 40.186346910553475}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = log_logistic_cdf(min_value, loc, scale)\n    max_cdf = log_logistic_cdf(max_value, loc, scale)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return log_logistic_ppf(uniform_samples, loc, scale)\n\n", "CodeBleu_score": 38.09352335655617}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid(log_logistic_cdf(min_value, loc, scale))\n    max_cdf = torch.sigmoid(log_logistic_cdf(max_value, loc, scale))\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    logit_samples = torch.log(uniform_samples / (1. - uniform_samples))\n    log_logistic_samples = torch.exp(logit_samples) * scale + loc\n    return log_logistic_samples\n\n", "CodeBleu_score": 40.337110943740704}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU(alpha=1.0)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 44.514487154683096}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n    if antialias:\n        return Activation1d(activation_fn)\n\n", "CodeBleu_score": 44.72662587271558}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU(alpha=1.0)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n    if antialias:\n        return Activation1d(activation)\n    return activation\n\n", "CodeBleu_score": 48.27840967329017}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    t = t.to(torch.float32) if t.dtype == torch.float16 else t\n    freqs = freqs.to(torch.float32) if freqs.dtype == torch.float16 else freqs\n    assert len(t.shape) == len(freqs.shape) == 3, \"The input tensor and frequency tensor must have the same rank.\"\n    assert t.shape[-1] == freqs.shape[-1], \"The last dimension of the input tensor and frequency tensor must be the same.\"\n    assert t.shape[-2] % 2 == 0, \"The second-to-last dimension of the input tensor must be even.\"\n    assert freqs.shape[-2] % 2 == 0, \"The second-to-last dimension of the frequency tensor must be even.\"\n    assert t.shape[-2] == freqs.shape[-2], \"The second-to-last dimension of the input tensor and frequency tensor must be the same.\"\n    t_rotated = torch.zeros_like(t)\n    t_rotated[..., ::2] = torch.cos(freqs * scale) * t[..., ::2] - torch.sin(freqs * scale) * t[..., 1::2]\n    t_rotated[..., 1::2] = torch.sin(freqs * scale) * t[..., ::2] + torch.cos(freqs * scale) * t[..., 1::2]\n    return torch.cat([t, t_rotated], dim = -1)\n", "CodeBleu_score": 26.363783521471245}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    t = t.float() if t.dtype == torch.float16 else t\n    freqs = freqs.float() if freqs.dtype == torch.float16 else freqs\n    if t.ndim == 3:\n        t = t.unsqueeze(1)\n    if t.ndim == 4:\n        t = t.unsqueeze(2)\n    if t.ndim == 5:\n        t = t.unsqueeze(3)\n    t_shape = t.shape\n    freqs_shape = freqs.shape\n    t = t.reshape(-1, t_shape[-1])\n    freqs = freqs.reshape(-1, freqs_shape[-1])\n    assert t.shape[-1] == freqs.shape[-1]\n    freqs = freqs * scale\n    freqs = torch.cat((freqs, freqs), dim = -1)\n    t = torch.cat((t, t), dim = -1)\n    t = t * freqs\n    t = torch.stack((torch.cos(t), torch.sin(t)), dim = -1)\n    t = t.reshape(t_shape)\n    return t\n", "CodeBleu_score": 27.520069997860443}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    t = t.to(torch.float32) if t.dtype == torch.float16 else t\n    freqs = freqs.to(torch.float32) if freqs.dtype == torch.float16 else freqs\n    assert len(t.shape) == 3, \"The function only supports 3D tensors.\"\n    assert t.shape[-1] % 2 == 0, \"The function only supports tensors with even last dimension.\"\n    assert t.shape[-2] == len(freqs), \"The function only supports tensors with last dimension equal to the length of the frequency tensor.\"\n    t_rotated = t[..., ::2] * torch.cos(freqs) - t[..., 1::2] * torch.sin(freqs)\n    t_unrotated = t[..., 1::2] * torch.cos(freqs) + t[..., ::2] * torch.sin(freqs)\n    return torch.cat((t_rotated, t_unrotated), dim = -1) * scale\n", "CodeBleu_score": 29.494562491854047}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n    return self.codebook(idx).movedim(dim, -1)", "CodeBleu_score": 17.627927836145286}, {"code": "def idx2vq(self, idx, dim=-1)\n    return self.codebook.embedding(idx).permute(0, 2, 1) if dim == -1 else self.codebook.embedding(idx).permute(0, 2, 1).permute(0, 2, 1)\n", "CodeBleu_score": 29.7786240053978}, {"code": "def idx2vq(self, idx, dim=-1)\n    return self.codebook(idx).movedim(-1, dim)\n\n", "CodeBleu_score": 19.460246063668617}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n    if quantize:\n      encoded, indices, loss = self.quantizer(self.encoder(x))\n      return encoded, indices, loss\n    else:\n      return self.encoder(x)\n\n  def decode(self, x, quantize=False):\n    \"\"\"The function decodes the input x using a series of blocks.\\nIf quantization is enabled, it applies a vector quantizer to the decoded output and returns the quantized output, original output, and combined loss.\\nIf quantization is not enabled, it returns the decoded output.\"\"\"\n    if quantize:\n      decoded, loss = self.quantizer(self.decoder(x))\n      return decoded, loss\n    else:\n      return self.decoder(x)\n\n  def forward(self, x, quantize=False):\n    \"\"\"The function encodes the input x, decodes the encoded output, and returns the decoded output.\\nIf quantization is enabled, it applies a vector quantizer to the encoded and decoded outputs and returns the quantized encoded and decoded outputs, original encoded and decoded outputs, indices, and combined losses.\"\"\"\n    if quantize:\n      encoded, indices, enc_loss = self.encode(x, quantize=True)\n      decoded, dec_loss = self.decode(encoded, quantize=True)\n      return encoded, decoded, indices, enc_loss + dec_loss\n    else:\n      return self.decode(self.encode(x, quantize=False))\n\n  def get_loss(self, x, quantize=False):\n    \"\"\"The function encodes the input x, decodes the encoded output, and returns the combined losses.\\nIf quantization is enabled, it applies a vector quantizer to the encoded and decoded outputs and returns the combined losses.\"\"\"\n    if quantize:\n      encoded, indices, enc_loss = self.encode(x, quantize=True)\n      decoded, dec_loss = self.decode(encoded, quantize=True)\n      return enc_loss + dec_loss\n    else:\n      return self.get_reconstruction_loss(x)\n\n  def get_reconstruction_loss(self, x):\n    \"\"\"The function encodes the input x, decodes the encoded output, and returns the reconstruction loss.\"\"\"\n    return self.loss_fn(x, self.decode(self.encode(x, quantize=False)))\n\n  def get_quantization_loss(self, x):\n    \"\"\"The", "CodeBleu_score": 27.569885273122146}, {"code": "def encode(self, x, quantize=False)\n    if self.quantize:\n      quantized, indices, combined_loss = self.vector_quantizer(x)\n      return self.blocks(x), quantized, indices, combined_loss\n    else:\n      return self.blocks(x)\n", "CodeBleu_score": 28.909933309058744}, {"code": "def encode(self, x, quantize=False)\n    if self.quantize:\n      z, indices, loss = self.encoder.encode(x, quantize)\n      return z, indices, loss\n    else:\n      z = self.encoder.encode(x)\n      return z\n\n  def decode(self, z):\n    \"\"\"The function decodes the encoded input z using a series of blocks and returns the decoded output.\"\"\"\n    x_hat = self.decoder.decode(z)\n    return x_hat\n\n  def forward(self, x):\n    \"\"\"The function applies the encode and decode functions to the input x and returns the decoded output.\"\"\"\n    z = self.encode(x)\n    x_hat = self.decode(z)\n    return x_hat\n\n  def loss(self, x):\n    \"\"\"The function computes the loss for the input x by applying the encode and decode functions and calculating the reconstruction loss.\"\"\"\n    x_hat = self.forward(x)\n    loss = self.loss_fn(x_hat, x)\n    return loss\n\n  def train(self, train_loader, val_loader, epochs=100, batch_size=128, learning_rate=1e-3, weight_decay=1e-5, save_dir='saved_models', save_all=False):\n    \"\"\"The function trains the model using the specified training and validation data loaders, number of epochs, batch size, learning rate, weight decay, and save directory.\\nIt also allows for saving all models during training or just the best one.\\nIt returns the trained model and a dictionary containing the training and validation losses for each epoch.\"\"\"\n    # Set up optimizer and scheduler\n    optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    # Set up loss lists\n    train_losses = []\n    val_losses = []\n\n    # Set up best loss and save directory\n    best_loss = float('inf')\n    save_dir = Path(save_dir)\n    save_dir.mkdir(exist_ok=True)\n\n    # Set up progress bar\n    pbar = tqdm(range(epochs))\n\n    # Train model\n    for epoch in pbar:\n      # Set model to train mode\n      self.train()\n\n      # Initialize running loss\n      running_loss = 0.0\n\n      #", "CodeBleu_score": 28.571094856385294}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if self.ema_decay is None:\n      return contextlib.nullcontext()\n    self.ema_scope_enter(context)\n    try:\n      yield\n    finally:\n      self.ema_scope_exit(context)\n\n  def ema_scope_enter(self, context=None):\n    \"\"\"The function applies EMA weights to the model parameters, optionally logging the switch.\"\"\"\n    if self.ema_decay is None:\n      return\n    self.ema_scope_active = True\n    self.ema_scope_context = context\n    self.ema_scope_log_enter()\n    self.ema_apply()\n\n  def ema_scope_exit(self, context=None):\n    \"\"\"The function restores the original model parameters, optionally logging the restoration.\"\"\"\n    if self.ema_decay is None:\n      return\n    self.ema_scope_active = False\n    self.ema_scope_context = None\n    self.ema_scope_log_exit()\n    self.ema_restore()\n\n  def ema_scope_log_enter(self):\n    \"\"\"The function logs the switch to EMA weights.\"\"\"\n    if self.ema_scope_context is None:\n      return\n    self.ema_scope_context.logger.info(\"Switching to EMA weights.\")\n\n  def ema_scope_log_exit(self):\n    \"\"\"The function logs the restoration of original model parameters.\"\"\"\n    if self.ema_scope_context is None:\n      return\n    self.ema_scope_context.logger.info(\"Restoring original parameters.\")\n\n  def ema_apply(self):\n    \"\"\"The function applies EMA weights to the model parameters.\"\"\"\n    if self.ema_decay is None:\n      return\n    self.ema_apply_weights()\n    self.ema_apply_biases()\n\n  def ema_apply_weights(self):\n    \"\"\"The function applies EMA weights to the model weights.\"\"\"\n    if self.ema_decay is None:\n      return\n    for layer in self.layers:\n      layer.ema_apply_weights()\n\n  def ema_apply_biases(self):\n    \"\"\"The function applies EMA weights to the model biases.\"\"\"\n    if self.ema_decay is None:\n      return\n    for layer in self.layers:\n      layer.ema_apply_biases()\n\n  def ema_restore(self):\n    \"\"\"The function restores the original model", "CodeBleu_score": 26.390994222500204}, {"code": "def ema_scope(self, context=None)\n    if self.ema_decay is None:\n      return\n    self.ema_scope_on = True\n    if context is not None:\n      context.log(f\"Switching to EMA weights with decay {self.ema_decay}\")\n    self.save_params()\n    self.load_ema_params()\n\n  def ema_scope_off(self, context=None):\n    \"\"\"The function manages the application of Exponential Moving Average (EMA) weights in a context manager.\\nIf EMA is used, it stores the current parameters and replaces them with EMA weights, optionally logging the switch.\\nUpon exiting the context, it restores the original parameters and logs the restoration if a context is provided.\"\"\"\n    if self.ema_decay is None:\n      return\n    if context is not None:\n      context.log(f\"Switching to non-EMA weights\")\n    self.load_params()\n    self.ema_scope_on = False\n\n  def load_ema_params(self):\n    \"\"\"The function loads the parameters from the EMA model.\"\"\"\n    if self.ema_decay is None:\n      return\n    self.ema_scope_on = True\n    for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n      p.data.copy_(ema_p.data)\n\n  def load_params(self):\n    \"\"\"The function loads the parameters from the non-EMA model.\"\"\"\n    if self.ema_decay is None:\n      return\n    self.ema_scope_on = False\n    for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n      p.data.copy_(ema_p.data)\n\n  def save_params(self):\n    \"\"\"The function saves the parameters of the non-EMA model.\"\"\"\n    if self.ema_decay is None:\n      return\n    for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n      ema_p.data.copy_(p.data)\n\n  def ema_parameters(self):\n    \"\"\"The function returns the parameters of the EMA model.\"\"\"\n    if self.ema_decay is None:\n      return\n    for module in self.modules():\n      if hasattr(module, \"ema_parameters\"):\n        yield from module.ema_parameters()\n\n  def parameters(self):\n    \"\"\"The function returns the parameters of the", "CodeBleu_score": 32.650920585943766}, {"code": "def ema_scope(self, context=None)\n    if self.ema_decay is not None:\n      with tf.name_scope('ema_scope'):\n        self.ema_apply_op = self.ema.apply(self.ema_variables)\n        self.ema_apply_op_shadow = self.ema_shadow.apply(self.ema_variables_shadow)\n        self.ema_apply_op_shadow_2 = self.ema_shadow_2.apply(self.ema_variables_shadow_2)\n        self.ema_apply_op_shadow_3 = self.ema_shadow_3.apply(self.ema_variables_shadow_3)\n        self.ema_apply_op_shadow_4 = self.ema_shadow_4.apply(self.ema_variables_shadow_4)\n        self.ema_apply_op_shadow_5 = self.ema_shadow_5.apply(self.ema_variables_shadow_5)\n        self.ema_apply_op_shadow_6 = self.ema_shadow_6.apply(self.ema_variables_shadow_6)\n        self.ema_apply_op_shadow_7 = self.ema_shadow_7.apply(self.ema_variables_shadow_7)\n        self.ema_apply_op_shadow_8 = self.ema_shadow_8.apply(self.ema_variables_shadow_8)\n        self.ema_apply_op_shadow_9 = self.ema_shadow_9.apply(self.ema_variables_shadow_9)\n        self.ema_apply_op_shadow_10 = self.ema_shadow_10.apply(self.ema_variables_shadow_10)\n        self.ema_apply_op_shadow_11 = self.ema_shadow_11.apply(self.ema_variables_shadow_11)\n        self.ema_apply_op_shadow_12 = self.ema_shadow_12.apply(self.ema_variables_shadow_12)\n        self.ema_apply_op_shadow_13 = self.ema_shadow_13.apply(self.ema_variables_shadow_13)\n        self.ema_apply_op_shadow_14 = self.ema_shadow_14.apply(self.ema_variables_shadow_", "CodeBleu_score": 28.86391671873001}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = self.mod_mlp(x, shift_mlp, scale_mlp)\n    return self.mlp(x, gate_mlp)\n\n", "CodeBleu_score": 33.19385007171004}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = self.shift_mlp(x) + self.scale_mlp(x)\n    return self.gate_mlp(x) * gate_mlp", "CodeBleu_score": 32.616581487339715}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = self.shift_mlp(x) + self.scale_mlp(x)\n    return x * gate_mlp\n", "CodeBleu_score": 31.875224482806374}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n    if context_block is not None:\n        context, context_intermediates = context_block(context, c)\n    if x_block is not None:\n        x, x_intermediates = x_block(x, c)\n    qkv = torch.cat([context, x], dim=1)\n    qkv = context.optimized_attention(qkv, c)\n    context, x = qkv.split([context.shape[1], x.shape[1]], dim=1)\n    if context_block is not None:\n        context, _ = context_block(context, c, intermediates=context_intermediates)\n    if x_block is not None:\n        x, _ = x_block(x, c, intermediates=x_intermediates)\n    return context, x\n\n", "CodeBleu_score": 30.841011984834658}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    if context_block is not None:\n        q, k, v, context_intermediates = context_block(context, c)\n    else:\n        q, k, v = None, None, None\n    if x_block is not None:\n        q_x, k_x, v_x, x_intermediates = x_block(x, c)\n    else:\n        q_x, k_x, v_x = None, None, None\n    if q is not None and q_x is not None:\n        q = torch.cat([q, q_x], dim=1)\n    if k is not None and k_x is not None:\n        k = torch.cat([k, k_x], dim=1)\n    if v is not None and v_x is not None:\n        v = torch.cat([v, v_x], dim=1)\n    if q is not None and k is not None and v is not None:\n        q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\n        q = q.view(q.size(0), -1, q.size(-1))\n        k = k.view(k.size(0), -1, k.size(-1))\n        v = v.view(v.size(0), -1, v.size(-1))\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        if context_block is not None:\n            context_intermediates[\"q\"] = q[:, :context_intermediates[\"q\"].size(1), :]\n            context_intermediates[\"k\"] = k[:, :context_intermediates[\"k\"].size(1), :]\n            context_intermediates[\"v\"] = v[:, :context_intermediates[\"v\"].size(1), :]\n        if x_block is not None:\n            x_intermediates[\"q\"] = q[:, context_intermediates[\"q\"].size(1):, :]\n            x_intermediates[\"k\"] = k[:, context_intermediates[\"k\"].size(1):, :]\n            x_intermediates[\"v\"] = v[:, context_intermediates[\"v\"].size(1):, :]\n        q, k,", "CodeBleu_score": 28.0756795555276}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Process context through its pre-attention block\n    if context_block is not None:\n        context, context_intermediates = context_block(context, c)\n    else:\n        context_intermediates = None\n    # Process x through its pre-attention block\n    if x_block is not None:\n        x, x_intermediates = x_block(x, c)\n    else:\n        x_intermediates = None\n    # Concatenate qkv tensors\n    qkv = torch.cat([context, x], dim=1)\n    # Apply optimized attention\n    qkv = _optimized_attention(qkv, c)\n    # Split the attention results back into context and x parts\n    context, x = qkv.split([context.shape[1], x.shape[1]], dim=1)\n    # Process context through its post-attention block\n    if context_block is not None:\n        context = context_block.post_attention_block(context, context_intermediates, c)\n    # Process x through its post-attention block\n    if x_block is not None:\n        x = x_block.post_attention_block(x, x_intermediates, c)\n    return context, x\n\n", "CodeBleu_score": 34.346781351743495}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert embedding_dim % 2 == 0, \"The embedding dimension must be an even number.\"\n    timesteps = tf.convert_to_tensor(timesteps, dtype=tf.float32)\n    assert timesteps.ndim == 1, \"The timesteps input must be one-dimensional.\"\n    timesteps = tf.expand_dims(timesteps, axis=-1)\n    half_dim = embedding_dim // 2\n    exponents = tf.range(start=0, limit=half_dim, delta=1, dtype=tf.float32)\n    exponents = tf.pow(10000, -exponents / half_dim)\n    exponents = tf.expand_dims(exponents, axis=0)\n    timesteps = tf.expand_dims(timesteps, axis=-1)\n    div_term = tf.matmul(timesteps, exponents)\n    sin_embedding = tf.sin(div_term)\n    cos_embedding = tf.cos(div_term)\n    embedding = tf.concat([sin_embedding, cos_embedding], axis=-1)\n    embedding = tf.cast(embedding, tf.float32)\n    if embedding_dim % 2 == 1:\n        embedding = tf.pad(embedding, [[0, 0], [0, 0], [0, 1]])\n    return embedding\n\n", "CodeBleu_score": 43.24809725522808}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert embedding_dim % 2 == 0, \"The embedding dimension must be even.\"\n    timesteps = timesteps.reshape(-1, 1)\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps * emb\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), \"constant\", 0)\n    return emb\n\n", "CodeBleu_score": 66.7562802310274}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert isinstance(timesteps, torch.Tensor)\n    assert timesteps.ndim == 1\n    assert isinstance(embedding_dim, int)\n    assert embedding_dim > 0\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), \"constant\", 0)\n    return emb\n\n", "CodeBleu_score": 69.73490364462185}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n    try:\n        q, k, v = q.reshape(q.shape[0], -1, q.shape[-1]), k.reshape(k.shape[0], -1, k.shape[-1]), v.reshape(v.shape[0], -1, v.shape[-1])\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n        return F.scaled_dot_product_attention(q, k, v, None, dropout_p=0, is_causal=False)[0].transpose(1, 2).reshape(q.shape)\n    except:\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 26.01749790685232}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape and transpose tensors to prepare for attention computation\n        q = q.reshape(q.shape[0], -1, q.shape[-1]).permute(1, 0, 2)\n        k = k.reshape(k.shape[0], -1, k.shape[-1]).permute(1, 0, 2)\n        v = v.reshape(v.shape[0], -1, v.shape[-1]).permute(1, 0, 2)\n        # Compute attention using PyTorch's scaled dot-product attention\n        attention_output = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(k.shape[-1])\n        attention_output = torch.softmax(attention_output, dim=-1)\n        attention_output = torch.bmm(attention_output, v)\n        # Reshape attention output back to original dimensions\n        attention_output = attention_output.permute(1, 0, 2).reshape(q.shape[0], -1, q.shape[-1])\n        return attention_output\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            # Switch to slice attention as a fallback\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 30.92368086665086}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape and transpose tensors to prepare for attention computation\n        q = q.view(q.shape[0], -1, q.shape[-1]).transpose(1, 2)\n        k = k.view(k.shape[0], -1, k.shape[-1]).transpose(1, 2)\n        v = v.view(v.shape[0], -1, v.shape[-1]).transpose(1, 2)\n\n        # Compute attention using PyTorch's scaled dot-product attention\n        attn_output, _ = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n\n        # Reshape attention output back to original dimensions\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            attn_output.shape[0], -1, attn_output.shape[-1]\n        )\n\n        return attn_output\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            # If OOM exception occurs, switch to slice attention\n            return slice_attention(q, k, v)\n        else:\n            # Raise the exception if it is not an OOM exception\n            raise e\n\n", "CodeBleu_score": 39.21414373609002}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            h.add_control(control, name)\n        except Exception as e:\n            logging.warning(f\"Failed to add control {name} to header: {e}\")\n    return h\n\n", "CodeBleu_score": 23.132347487669314}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            h.add_control(control, name)\n        except Exception as e:\n            logger.warning(\"Could not add control %s to %s: %s\", name, h.id, e)\n    return h\n\n", "CodeBleu_score": 28.140086853598767}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            h.add_control(control, name)\n        except Exception as e:\n            logger.warning(f'Failed to add control {control} to {h.name}: {e}')\n    return h\n\n", "CodeBleu_score": 23.234532822267923}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    codebook_entry = self.embedding(indices)\n    if shape is not None:\n        codebook_entry = codebook_entry.view(shape + (-1,))\n        codebook_entry = codebook_entry.permute((0, 3, 1, 2))\n    return codebook_entry\n", "CodeBleu_score": 29.030350948189426}, {"code": "def get_codebook_entry(self, indices, shape)\n    indices = tf.cast(indices, tf.int32)\n    depth = self.embedding_weights.shape[0]\n    indices_one_hot = tf.one_hot(indices, depth)\n    quantized = tf.matmul(indices_one_hot, self.embedding_weights)\n    if shape is None:\n        return quantized\n    else:\n        quantized = tf.reshape(quantized, shape)\n        return tf.transpose(quantized, self.transpose_perm)\n\n", "CodeBleu_score": 23.776942815457737}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.is_quantized:\n        if self.is_permuted:\n            indices = self.permute_indices(indices)\n        one_hot = tf.one_hot(indices, self.codebook_size, axis=-1)\n        quantized_vectors = tf.matmul(one_hot, self.codebook_weights)\n        if shape is not None:\n            quantized_vectors = tf.reshape(quantized_vectors, shape)\n        return quantized_vectors\n    else:\n        return indices\n\n", "CodeBleu_score": 36.90441554224185}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n    noise = torch.randn(4, 4, self.device)\n    self.noises = [noise]\n    for i in range(3, self.log_size):\n        for _ in range(2):\n            noise = torch.cat([noise, noise.flip([2])], 2)\n            noise = torch.cat([noise, noise.flip([3])], 3)\n            self.noises.append(noise)\n    return self.noises\n", "CodeBleu_score": 34.51231325665014}, {"code": "def make_noise(self)\n    noise_list = []\n    for i in range(3, self.log_size + 1):\n      size = 2 ** i\n      noise_list.append(torch.randn(size, size, device=self.device))\n      noise_list.append(torch.randn(size, size, device=self.device))\n    return noise_list\n", "CodeBleu_score": 36.09651066753484}, {"code": "def make_noise(self)\n    device = self.device\n    noise = torch.randn(4, 4, device=device)\n    noises = [noise]\n    for i in range(3, self.log_size):\n        for _ in range(2):\n            noises.append(noise.clone().detach())\n            noises[-1] = noises[-1].resize_(noise.size(0) * 2, noise.size(1) * 2).normal_()\n    return noises\n", "CodeBleu_score": 38.55669593753822}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n    latent = torch.randn((num_latent, self.latent_size), device=self.device)\n    latent = self.style(latent)\n    latent = latent.mean(0, keepdim=True)\n    return latent\n", "CodeBleu_score": 30.88174556052554}, {"code": "def mean_latent(self, num_latent)\n    latents = torch.randn(num_latent, self.latent_dim, device=self.device)\n    mean_latent = self.style(latents).mean(0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 29.70147627684106}, {"code": "def mean_latent(self, num_latent)\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style(latent_in)\n    return latent_out.mean(0)\n", "CodeBleu_score": 27.83029491606018}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n    jitter = np.random.randint(-shift, shift, size=(3,))\n    img_jitter = np.zeros_like(img)\n    for i in range(3):\n        img_jitter[:, :, i] = img[:, :, i] + jitter[i]\n    img_jitter = np.clip(img_jitter, 0, 1)\n    return img_jitter\n\n", "CodeBleu_score": 36.56333282294554}, {"code": "def color_jitter(img, shift)\n    # Get the image dimensions\n    height, width, channels = img.shape\n    # Create a random jitter value for each RGB channel\n    jitter = np.random.randint(-shift, shift, size=3)\n    # Add the jitter value to the image\n    img = img + jitter\n    # Clip the image values to ensure pixel values remain between 0 and 1\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 53.06744328263292}, {"code": "def color_jitter(img, shift)\n    jitter = torch.zeros_like(img)\n    for i in range(3):\n        jitter[:, i, :, :] = torch.rand(img.shape[2], img.shape[3]) * 2 * shift - shift\n    jitter = jitter.to(img.device)\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 45.27401614508649}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n    if data is None:\n      return\n    if self.device == 'cuda':\n      data = data.cuda()\n    self.lq = data\n    if self.opt['phase'] == 'train':\n      self.gt = data.clone()\n    if self.opt['data_type'] == 30:\n      self.leye = data[:, 0:2, :, :].clone()\n      self.reye = data[:, 2:4, :, :].clone()\n      self.mouth = data[:, 4:6, :, :].clone()\n\n  def forward(self):\n    \"\"\"The function performs the forward propagation of the network.\"\"\"\n    self.lq_fea = self.netG(self.lq)\n\n  def test(self):\n    \"\"\"The function performs the forward propagation of the network.\"\"\"\n    self.forward()\n\n  def get_current_log(self):\n    \"\"\"The function returns the current log of the network.\"\"\"\n    return self.log_dict\n\n  def get_current_visuals(self, need_GT=True):\n    \"\"\"The function returns the current visuals of the network.\"\"\"\n    out_dict = OrderedDict()\n    out_dict['LQ'] = self.lq.detach()[0].float().cpu()\n    out_dict['SR'] = self.lq_fea.detach()[0].float().cpu()\n    if need_GT:\n      out_dict['GT'] = self.gt.detach()[0].float().cpu()\n    if self.opt['data_type'] == 30:\n      out_dict['LE'] = self.leye.detach()[0].float().cpu()\n      out_dict['RE'] = self.reye.detach()[0].float().cpu()\n      out_dict['MO'] = self.mouth.detach()[0].float().cpu()\n    return out_dict\n\n  def get_current_visuals_test(self, need_GT=True):\n    \"\"\"The function returns the current visuals of the network.\"\"\"\n    out_dict = OrderedDict()\n    out_dict['LQ'] = self.lq.detach()[0].float().cpu()\n    out_dict['SR'] = self.lq_fea.detach()[0].float().cpu()\n    if need_GT:\n      out_dict['GT'] = self.gt.detach()[0].float().cpu()\n    if self.opt['data_type", "CodeBleu_score": 24.998358817073402}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n      self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n      self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n      self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data:\n      self.mouth = data['mouth'].to(self.device)\n", "CodeBleu_score": 56.98608226755201}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n      self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n      self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n      self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data:\n      self.mouth = data['mouth'].to(self.device)\n\n  def get_lq(self):\n    \"\"\"The function returns the low-quality image.\"\"\"\n    return self.lq\n\n  def get_gt(self):\n    \"\"\"The function returns the ground-truth image.\"\"\"\n    return self.gt\n\n  def get_left_eye(self):\n    \"\"\"The function returns the left eye location.\"\"\"\n    return self.left_eye\n\n  def get_right_eye(self):\n    \"\"\"The function returns the right eye location.\"\"\"\n    return self.right_eye\n\n  def get_mouth(self):\n    \"\"\"The function returns the mouth location.\"\"\"\n    return self.mouth\n\n  def get_device(self):\n    \"\"\"The function returns the device on which the data is stored.\"\"\"\n    return self.device\n\n  def get_input_data(self):\n    \"\"\"The function returns the input data.\"\"\"\n    return self.lq\n\n  def get_output_data(self):\n    \"\"\"The function returns the output data.\"\"\"\n    return self.gt\n\n  def get_input_shape(self):\n    \"\"\"The function returns the shape of the input data.\"\"\"\n    return self.lq.shape\n\n  def get_output_shape(self):\n    \"\"\"The function returns the shape of the output data.\"\"\"\n    return self.gt.shape\n\n  def get_input_size(self):\n    \"\"\"The function returns the size of the input data.\"\"\"\n    return self.lq.size\n\n  def get_output_size(self):\n    \"\"\"The function returns the size of the output data.\"\"\"\n    return self.gt.size\n\n  def get_input_dtype(self):\n    \"\"\"The function returns the data type of the input data.\"\"\"\n    return self.lq.dtype\n\n  def get_output_dtype(self):\n    \"\"\"The function returns the data type of the output data.\"\"\"\n    return self.gt.dtype\n\n  def get_input_device(self):\n    \"\"\"", "CodeBleu_score": 45.024753259733146}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n    img_pyramid = [self.gt_img]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n", "CodeBleu_score": 18.6234098638856}, {"code": "def construct_img_pyramid(self)\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid", "CodeBleu_score": 20.489604414459468}, {"code": "def construct_img_pyramid(self)\n    pyramid = []\n    for i in range(self.n_scales):\n      pyramid.insert(0, self.downscale_image(self.img_gt, i))\n    return pyramid", "CodeBleu_score": 16.510304710653944}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n    # save generator\n    torch.save(self.G.state_dict(), f\"./checkpoints/{self.name}/G_{epoch}_{current_iter}.pt\")\n    # save discriminator\n    torch.save(self.D.state_dict(), f\"./checkpoints/{self.name}/D_{epoch}_{current_iter}.pt\")\n    # save EMA generator\n    torch.save(self.G_ema.state_dict(), f\"./checkpoints/{self.name}/G_ema_{epoch}_{current_iter}.pt\")\n    # save discriminators\n    if self.facial_discriminators:\n        torch.save(self.D_left.state_dict(), f\"./checkpoints/{self.name}/D_left_{epoch}_{current_iter}.pt\")\n        torch.save(self.D_right.state_dict(), f\"./checkpoints/{self.name}/D_right_{epoch}_{current_iter}.pt\")\n        torch.save(self.D_mouth.state_dict(), f\"./checkpoints/{self.name}/D_mouth_{epoch}_{current_iter}.pt\")\n    # save training state\n    torch.save(self.state, f\"./checkpoints/{self.name}/state_{epoch}_{current_iter}.pt\")\n\n  def load(self, epoch, current_iter):\n    \"\"\"The function loads the state of the model at a given epoch and iteration.\\nIt loads the generator network, its EMA version, and the discriminator network.\\nIf facial discriminators are used, it also loads the discriminators for the left eye, right eye, and mouth.\\nFinally, it loads the overall training state.\"\"\"\n    # load generator\n    self.G.load_state_dict(torch.load(f\"./checkpoints/{self.name}/G_{epoch}_{current_iter}.pt\"))\n    # load discriminator\n    self.D.load_state_dict(torch.load(f\"./checkpoints/{self.name}/D_{epoch}_{current_iter}.pt\"))\n    # load EMA generator\n    self.G_ema.load_state_dict(torch.load(f\"./checkpoints/{self.name}/G_ema_{epoch}_{current_iter}.pt\"))\n    # load discriminators\n    if self.facial_discriminators:\n        self.D_left", "CodeBleu_score": 32.818277328125035}, {"code": "def save(self, epoch, current_iter)\n    # Save the generator\n    torch.save(self.netG.state_dict(), os.path.join(self.args.outf, 'netG_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the generator EMA\n    torch.save(self.netG_ema.state_dict(), os.path.join(self.args.outf, 'netG_ema_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the discriminator\n    torch.save(self.netD.state_dict(), os.path.join(self.args.outf, 'netD_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the left eye discriminator\n    torch.save(self.netD_left_eye.state_dict(), os.path.join(self.args.outf, 'netD_left_eye_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the right eye discriminator\n    torch.save(self.netD_right_eye.state_dict(), os.path.join(self.args.outf, 'netD_right_eye_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the mouth discriminator\n    torch.save(self.netD_mouth.state_dict(), os.path.join(self.args.outf, 'netD_mouth_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n    # Save the training state\n    torch.save(self.state, os.path.join(self.args.outf,'state_epoch_%d_iter_%d.pth' % (epoch, current_iter)))\n", "CodeBleu_score": 42.35561277284769}, {"code": "def save(self, epoch, current_iter)\n    # Save the generator and its EMA version\n    torch.save(self.G.state_dict(), f\"./models/{self.model_name}/G_{epoch}_{current_iter}.pth\")\n    torch.save(self.G_ema.state_dict(), f\"./models/{self.model_name}/G_ema_{epoch}_{current_iter}.pth\")\n    # Save the discriminator\n    torch.save(self.D.state_dict(), f\"./models/{self.model_name}/D_{epoch}_{current_iter}.pth\")\n    # Save the discriminators for the left eye, right eye, and mouth if they are used\n    if self.facial_discriminators:\n        torch.save(self.D_left_eye.state_dict(), f\"./models/{self.model_name}/D_left_eye_{epoch}_{current_iter}.pth\")\n        torch.save(self.D_right_eye.state_dict(), f\"./models/{self.model_name}/D_right_eye_{epoch}_{current_iter}.pth\")\n        torch.save(self.D_mouth.state_dict(), f\"./models/{self.model_name}/D_mouth_{epoch}_{current_iter}.pth\")\n    # Save the training state\n    torch.save(self.state, f\"./models/{self.model_name}/state_{epoch}_{current_iter}.pth\")\n", "CodeBleu_score": 36.85407743623968}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 67.27675337828248}, {"code": "def clean_folder(folder)\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.27675337828248}, {"code": "def clean_folder(folder)\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 67.27675337828248}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.experiences):\n      sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [e for e in self.experiences if e not in batch]\n    return batch\n\n  def add_experience(self, experience):\n    \"\"\"The function adds a single experience to the list of experiences.\"\"\"\n    self.experiences.append(experience)\n\n  def add_experiences(self, experiences):\n    \"\"\"The function adds a list of experiences to the list of experiences.\"\"\"\n    self.experiences.extend(experiences)\n\n  def get_experiences(self):\n    \"\"\"The function returns the list of experiences.\"\"\"\n    return self.experiences\n\n  def clear_experiences(self):\n    \"\"\"The function clears the list of experiences.\"\"\"\n    self.experiences = []\n\n  def get_size(self):\n    \"\"\"The function returns the number of experiences in the list.\"\"\"\n    return len(self.experiences)\n", "CodeBleu_score": 43.342941380842134}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [item for item in self.experiences if item not in batch]\n    return batch\n", "CodeBleu_score": 65.25895837697294}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.experiences):\n      sample_size = len(self.experiences)\n    sample = random.sample(self.experiences, sample_size)\n    self.experiences = [x for x in self.experiences if x not in sample]\n    return sample\n", "CodeBleu_score": 72.62126067962913}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n    flat_configs = np.array([list(flatten_dict(config).values()) for config in self.all_configs])\n    estimates = self.cost_model.predict(flat_configs)\n    if self.metric == \"latency\":\n        top_indices = np.argsort(estimates)\n    elif self.metric == \"throughput\":\n        top_indices = np.argsort(-estimates)\n    return top_indices\n\n", "CodeBleu_score": 21.668851772723603}, {"code": "def find_estimated_top_configs(self)\n    # Flatten each configuration in all_configs\n    flat_configs = []\n    for config in self.all_configs:\n      flat_configs.append(config.flatten())\n    flat_configs = np.array(flat_configs)\n\n    # Predict estimates for the flattened configurations\n    estimates = self.cost_model.predict(flat_configs)\n\n    # Sort the configurations based on the specified metric\n    if self.metric == \"latency\":\n      top_indices = np.argsort(estimates)\n    else:\n      top_indices = np.argsort(-estimates)\n\n    return top_indices\n\n", "CodeBleu_score": 25.770980679590977}, {"code": "def find_estimated_top_configs(self)\n    # Flatten all configurations\n    flattened_configs = [self.flatten_config(config) for config in self.all_configs]\n    # Collect numerical values\n    numerical_values = [self.get_numerical_values(config) for config in flattened_configs]\n    # Convert to numpy array\n    numerical_values = np.array(numerical_values)\n    # Predict estimates for the configurations\n    estimates = self.cost_model.predict(numerical_values)\n    # Sort the configurations based on the specified metric\n    if self.metric == \"cost\":\n        sorted_indices = np.argsort(estimates)\n    elif self.metric == \"reward\":\n        sorted_indices = np.argsort(-estimates)\n    else:\n        raise ValueError(\"Invalid metric: {}\".format(self.metric))\n    # Return the indices of the top configurations\n    return sorted_indices[:self.top_k]", "CodeBleu_score": 26.337340614954247}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 78.2184919550726}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 78.2184919550726}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 78.2184919550726}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n    if self.optimal_cmd:\n      print('Running after tuning...')\n      print(' '.join(self.optimal_cmd))\n      p = subprocess.Popen(self.optimal_cmd)\n      p.wait()\n      print('Done')\n    else:\n      print('No optimal command found')\n", "CodeBleu_score": 50.291277775561284}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd:\n        print(\"Running the optimal command: \", self.optimal_cmd)\n        p = subprocess.Popen(self.optimal_cmd, shell=True)\n        p.wait()\n        print(\"Command completed successfully.\")\n    else:\n        print(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 49.83457877692244}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n      self.logger.info(\"Running optimal command: \" + self.optimal_cmd)\n      subprocess.Popen(self.optimal_cmd, shell=True).wait()\n      self.logger.info(\"Success!\")\n    else:\n      self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 34.254808259672906}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n    # Set the master port and result directory for the experiment\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n    # Update user arguments based on argument mappings\n    if \"argument_mappings\" in exp:\n        for mapping in exp[\"argument_mappings\"]:\n            if mapping[\"type\"] == \"env\":\n                os.environ[mapping[\"name\"]] = mapping[\"value\"]\n            elif mapping[\"type\"] == \"arg\":\n                exp[\"args\"][mapping[\"name\"]] = mapping[\"value\"]\n    # Start the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n    # Store the running experiment details with a timestamp\n    self.running_experiments[time.time()] = {\n        \"exp\": exp,\n        \"reservations\": reservations,\n        \"thread\": thread\n    }\n    self.logger.info(\"Experiment started: %s\", exp[\"name\"])\n\n", "CodeBleu_score": 28.243359015879125}, {"code": "def run_job(self, exp: dict, reservations)\n    try:\n        if self.debug:\n            print(f\"Running job {exp['name']} with args {exp['args']}\")\n        exp['master_port'] = self.master_port\n        exp['result_dir'] = self.result_dir\n        exp['args'] = self.update_args(exp['args'], exp['arg_map'])\n        exp['args'] = self.update_args(exp['args'], self.arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args'] = self.update_args(exp['args'], self.global_arg_map)\n        exp['args']", "CodeBleu_score": 17.332092618627275}, {"code": "def run_job(self, exp: dict, reservations)\n    exp_id = exp[\"id\"]\n    exp_name = exp[\"name\"]\n    exp_script = exp[\"script\"]\n    exp_args = exp[\"args\"]\n    exp_port = exp[\"port\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_user_args = exp[\"user_args\"]\n    exp_user_args_dict = exp[\"user_args_dict\"]\n    exp_user_args_list = exp[\"user_args_list\"]\n    exp_user_args_list_dict = exp[\"user_args_list_dict\"]\n    exp_user_args_list_dict_list = exp[\"user_args_list_dict_list\"]\n    exp_user_args_list_list = exp[\"user_args_list_list\"]\n    exp_user_args_list_list_dict = exp[\"user_args_list_list_dict\"]\n    exp_user_args_list_list_list = exp[\"user_args_list_list_list\"]\n    exp_user_args_list_list_list_dict = exp[\"user_args_list_list_list_dict\"]\n    exp_user_args_list_list_list_list = exp[\"user_args_list_list_list_list\"]\n    exp_user_args_list_list_list_list_dict = exp[\"user_args_list_list_list_list_dict\"]\n    exp_user_args_list_list_list_list_list = exp[\"user_args_list_list_list_list_list\"]\n    exp_user_args_list_list_list_list_list_dict = exp[\"user_args_list_list_list_list_list_dict\"]\n    exp_user_args_list_list_list_list_list_list = exp[\"user_args_list_list_list_list_list_list\"]\n    exp_user_args_list_list_list_list_list_list_dict = exp[\"user_args_list_list_list_list_list_list_dict\"]\n    exp_user_args_list_list_list_list_list_list_list = exp[\"user_args_list_list_list_list_list_list_list\"]\n    exp_user_args_list_list", "CodeBleu_score": 9.079350390242382}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed_training_initialized:\n        if self.rank == 0:\n            message = self.add_rank_to_message(message)\n            self.append_message_to_file(message, path)\n    else:\n        message = self.add_rank_to_message(message)\n        self.append_message_to_file(message, path)\n", "CodeBleu_score": 19.120640677403312}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.dist_initialized:\n        ranks = ranks if ranks is not None else [self.rank]\n        message = self.add_rank_to_message(message, ranks)\n    if self.log_permission:\n        if path is None:\n            path = self.log_path\n        with open(path, 'a') as f:\n            json.dump(message, f)\n            f.write(os.linesep)\n", "CodeBleu_score": 29.156814270006937}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.dist_initialized:\n        message = f'Rank {ranks}: {message}'\n    if self.logging_permitted:\n        if path is None:\n            path = self.log_path\n        with open(path, 'a') as f:\n            f.write(f'{message}\\n')", "CodeBleu_score": 18.109855409023794}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    # check if there are any layer files\n    if not self.layer_files:\n        return {}\n    # verify the layer index\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index: {}\".format(layer_index))\n    # retrieve files with a specific prefix for the given layer\n    layer_prefix = self.layer_files[layer_index].split(\"_\")[0]\n    layer_files = [f for f in self.layer_files if f.startswith(layer_prefix)]\n    # partition these files based on tp_degree\n    layer_files_map = {}\n    for f in layer_files:\n        tp_degree = int(f.split(\"_\")[-1])\n        if tp_degree not in layer_files_map:\n            layer_files_map[tp_degree] = []\n        layer_files_map[tp_degree].append(f)\n    # create and return a dictionary mapping partition indices to their corresponding file lists\n    return layer_files_map\n", "CodeBleu_score": 38.10096621292513}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    if not self.layer_files:\n        return None\n    if layer_index < 0 or layer_index >= self.num_layers:\n        return None\n    prefix = self.layer_files[layer_index][0].split(\"_\")\n    prefix = prefix[0] + \"_\" + prefix[1] + \"_\"\n    files = [f for f in self.layer_files[layer_index] if f.startswith(prefix)]\n    if not files:\n        return None\n    files = sorted(files)\n    tp_degree = int(files[0].split(\"_\")[-1].split(\".\")[0])\n    if tp_degree <= 0:\n        return None\n    return {i: files[i::tp_degree] for i in range(tp_degree)}\n\n", "CodeBleu_score": 33.95000374915242}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self._get_layer_files(layer_index)\n    if not layer_files:\n        return None\n    if layer_index >= len(self.layer_list):\n        return None\n    tp_other_layer_map = {}\n    for file in layer_files:\n        tp_degree = int(file.split(\"_\")[-1].split(\".\")[0])\n        if tp_degree not in tp_other_layer_map:\n            tp_other_layer_map[tp_degree] = []\n        tp_other_layer_map[tp_degree].append(file)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 35.79085196822388}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n    info = {}\n    info['os'] = platform.system()\n    info['architecture'] = platform.architecture()[0]\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python'] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 43.519059975764584}, {"code": "def system_info()\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture()[0],\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 76.04343095580735}, {"code": "def system_info()\n    return {\n        \"operating_system\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 82.38149591942839}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n    try:\n        import torch\n        if torch.cuda.is_available():\n            cuda_avail = True\n            device_name = torch.cuda.get_device_name(0)\n            device_id = torch.cuda.current_device()\n            cuda_version = torch.version.cuda\n            print(f\"CUDA is available! Device name: {device_name}, Device ID: {device_id}, CUDA version: {cuda_version}\")\n        else:\n            cuda_avail = False\n            print(\"CUDA is not available\")\n    except ModuleNotFoundError:\n        print(\"PyTorch is not installed\")\n    return cuda_avail, device_id, device_name, cuda_version\n", "CodeBleu_score": 36.597769568134794}, {"code": "def cuda_info()\n    cuda_available = torch.cuda.is_available()\n    if cuda_available:\n        cuda_version = torch.version.cuda\n        cuda_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    else:\n        cuda_version = None\n        cuda_devices = None\n    return cuda_available, cuda_version, cuda_devices\n\n", "CodeBleu_score": 45.585139460713876}, {"code": "def cuda_info()\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    cuda_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    return cuda_available, cuda_version, cuda_devices\n\n", "CodeBleu_score": 45.87352074370546}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    # Retrieve the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\", None)\n\n    # Initialize the accuracy counters for each class\n    class_accuracies = {}\n\n    # Iterate over each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file and the class name from the item\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_id_to_name is not None:\n            predicted_class = encoder_manager.predict_class_label(embedding)\n        else:\n            predicted_class = None\n\n        # Record the accuracy for the current class\n        if class_name not in class_accuracies:\n            class_accuracies[class_name] = {\"count\": 0, \"correct\": 0}\n\n        class_accuracies[class_name][\"count\"] += 1\n        if predicted_class == class_name:\n            class_accuracies[class_name][\"correct\"] += 1\n\n    # Calculate the accuracy for each class and the average accuracy across all classes\n    total_count = 0\n    total_correct = 0\n    for class_name, accuracy in class_accuracies.items():\n        class_accuracy = accuracy[\"correct\"] / accuracy[\"count\"]\n        total_count += accuracy[\"count\"]\n        total_correct += accuracy[\"correct\"]\n        print(f\"Accuracy for class {class_name}: {class_accuracy:.4f}\")\n\n    average_accuracy = total_correct / total_count\n    print(f\"Average accuracy across all classes: {average_accuracy:.4f}\")\n\n", "CodeBleu_score": 34.76928561863583}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config.get(\"class_name_key\")\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\")\n    class_name_to_id_mapping = encoder_manager.config.get(\"class_name_to_id_mapping\")\n    class_id_key = encoder_manager.config.get(\"class_id_key\")\n    class_name_key = encoder_manager.config.get(\"class_name_key\")\n    class_accuracy = {}\n    total_samples = 0\n    total_correct = 0\n    for item in dataset_items:\n        audio_file = item[0]\n        class_id = item[1]\n        class_name = item[2]\n        embedding = encoder_manager.get_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict(embedding)\n        if class_id_key and class_name_key:\n            predicted_class_name = class_id_to_name_mapping.get(\n                str(predicted_class_id)\n            )\n            if predicted_class_name is None:\n                predicted_class_name = class_name_to_id_mapping.get(\n                    str(predicted_class_id)\n                )\n            if predicted_class_name is None:\n                predicted_class_name = str(predicted_class_id)\n        else:\n            predicted_class_name = str(predicted_class_id)\n        if class_name not in class_accuracy:\n            class_accuracy[class_name] = {\"correct\": 0, \"total\": 0}\n        class_accuracy[class_name][\"total\"] += 1\n        if predicted_class_name == class_name:\n            class_accuracy[class_name][\"correct\"] += 1\n        total_samples += 1\n        if predicted_class_name == class_name:\n            total_correct += 1\n    for class_name in class_accuracy:\n        accuracy = class_accuracy[class_name][\"correct\"] / class_accuracy[class_name][\n            \"total\"\n        ]\n        print(f\"{class_name}: {accuracy:.2f}\")\n    average_accuracy = total_correct / total_samples\n    print(f\"Average accuracy: {average_accuracy:.2f", "CodeBleu_score": 31.400815847873524}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\", {})\n\n    # Initialize the accuracy counters\n    class_accuracies = defaultdict(int)\n    class_counts = defaultdict(int)\n\n    # For each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if encoder_manager.config[\"class_name_key\"] and encoder_manager.config[\"class_id_to_name\"]:\n            predicted_label = encoder_manager.predict(embedding)\n            true_label = item[class_name_key]\n\n            # Record the accuracy for each class by comparing the true class name with the predicted label\n            class_accuracies[true_label] += int(predicted_label == true_label)\n            class_counts[true_label] += 1\n\n    # Calculate and print the accuracy for each class and the average accuracy across all classes\n    total_accuracy = 0\n    for class_name, accuracy in class_accuracies.items():\n        class_accuracy = accuracy / class_counts[class_name]\n        total_accuracy += class_accuracy\n        print(f\"Class: {class_name}, Accuracy: {class_accuracy}\")\n    print(f\"Average Accuracy: {total_accuracy / len(class_accuracies)}\")\n\n", "CodeBleu_score": 28.066038252507237}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n    # Adjust the output path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the file already exists and force is not specified\n    if os.path.exists(output_path) and not force:\n        print(f\"File already exists: {output_path}\")\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Remove silence from the audio using specified parameters\n    audio_without_silence = pydub.AudioSegment.from_file(audio_path).split_on_silence(\n        min_silence_len=min_silence_len,\n        silence_thresh=silence_thresh,\n        keep_silence=keep_silence,\n    )\n\n    # Check if speech was detected\n    speech_detected = len(audio_without_silence) > 0\n\n    # Save the processed audio\n    if speech_detected:\n        audio_without_silence.export(output_path, format=\"wav\")\n        print(f\"Saved file: {output_path}\")\n    else:\n        print(f\"No speech detected: {output_path}\")\n\n    return output_path, speech_detected\n\n", "CodeBleu_score": 41.73931636749037}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    # Adjust the output path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the file already exists and force is not specified\n    if os.path.isfile(output_path) and not force:\n        print(f\"Skipping {audio_path} as it already exists at {output_path}\")\n        return output_path, False\n\n    # Create the necessary directory structure\n    output_dir = os.path.dirname(output_path)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Remove silence from the audio using specified parameters\n    audio_segment = AudioSegment.from_file(audio_path)\n    audio_segment = remove_silence(audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n\n    # Save the processed audio\n    audio_segment.export(output_path, format=\"wav\")\n\n    # Return the output path and a flag indicating if speech was detected\n    return output_path, audio_segment.duration_seconds > 0\n\n", "CodeBleu_score": 38.73134585902293}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\n    # Check if the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        # If not, create the directory\n        os.makedirs(OUTPUT_DIR)\n\n    # Get the output path by replacing the input directory with the output directory\n    output_path = audio_path.replace(INPUT_DIR, OUTPUT_DIR)\n\n    # Check if the file already exists and force is not specified\n    if os.path.exists(output_path) and not force:\n        # If so, return the output path and a False flag\n        return output_path, False\n\n    # Get the directory of the output path\n    output_dir = os.path.dirname(output_path)\n\n    # Check if the directory exists\n    if not os.path.exists(output_dir):\n        # If not, create the directory\n        os.makedirs(output_dir)\n\n    # Remove silence from the audio using specified parameters\n    audio_clip = AudioSegment.from_file(audio_path)\n    audio_clip = remove_silence(audio_clip, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n\n    # Check if speech was detected\n    if len(audio_clip) == 0:\n        # If not, return the output path and a False flag\n        return output_path, False\n\n    # Save the processed audio\n    audio_clip.export(output_path, format=\"wav\")\n\n    # Return the output path and a True flag\n    return output_path, True\n\n", "CodeBleu_score": 41.759256969014714}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if not output_dir:\n        output_dir = input_dir\n    copy_dir(input_dir, output_dir)\n    files = get_files(output_dir, file_ext)\n    with tqdm(total=len(files), desc=\"Resampling files\") as pbar:\n        Parallel(n_jobs=n_jobs)(delayed(resample_file)(file, output_sr, pbar) for file in files)\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 25.30590482672139}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if output_dir is None:\n        output_dir = input_dir\n    copy_tree(input_dir, output_dir)\n    audio_files = get_audio_files(output_dir, file_ext)\n    with Pool(n_jobs) as p:\n        for _ in tqdm(\n            p.imap_unordered(resample_audio, audio_files), total=len(audio_files)\n        ):\n            pass\n    print(\"Done!\")\n\n", "CodeBleu_score": 28.43757421068179}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if output_dir is None:\n        output_dir = input_dir\n    shutil.copytree(input_dir, output_dir)\n    file_list = get_file_list(output_dir, file_ext)\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(file_list)) as pbar:\n            for _ in p.imap_unordered(partial(resample_file, output_sr=output_sr), file_list):\n                pbar.update()\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 34.003820606560986}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            inputs = inputs.reshape(-1, 1, 28, 28)\n            labels = labels.reshape(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            losses.append(loss.item())\n            if batch_idx == 0:\n                umap_plot(outputs, labels, global_step)\n    avg_loss = sum(losses) / len(losses)\n    logger.info(f\"Evaluation Loss at step {global_step}: {avg_loss}\")\n    return avg_loss\n\n", "CodeBleu_score": 21.655227468981757}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs = inputs.reshape(inputs.shape[0] * inputs.shape[1], inputs.shape[2], inputs.shape[3], inputs.shape[4])\n            labels = labels.reshape(labels.shape[0] * labels.shape[1])\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            if batch_idx == 0:\n                embeddings = outputs.detach().cpu()\n                labels = labels.detach().cpu()\n            else:\n                embeddings = torch.cat((embeddings, outputs.detach().cpu()), dim=0)\n                labels = torch.cat((labels, labels.detach().cpu()), dim=0)\n        eval_loss /= len(data_loader)\n        logger.info(\"Evaluation loss: {:.4f}\".format(eval_loss))\n        # Generate UMAP plot\n        if embeddings.shape[1] == 2:\n            umap_plot(embeddings, labels, global_step)\n        else:\n            logger.info(\"Embeddings dimension is not 2, skipping UMAP plot.\")\n    return eval_loss\n\n", "CodeBleu_score": 40.1701363120235}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    # Set the model to evaluation mode\n    model.eval()\n    # Initialize variables to store the evaluation loss and number of samples\n    eval_loss = 0.0\n    num_samples = 0\n    # Initialize a list to store the embeddings from the last batch\n    embeddings = []\n    # Initialize a list to store the labels from the last batch\n    labels = []\n    # Iterate over the batches in the data loader\n    for batch_idx, (inputs, targets) in enumerate(data_loader):\n        # Reshape the inputs and labels to group samples of each class\n        inputs = inputs.reshape(len(targets), -1)\n        targets = targets.reshape(len(targets), -1)\n        # Dispatch the inputs and labels to the GPU if available\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass through the model\n        outputs = model(inputs)\n        # Compute the loss for the batch\n        loss = criterion(outputs, targets)\n        # Accumulate the evaluation loss and number of samples\n        eval_loss += loss.item() * inputs.size(0)\n        num_samples += inputs.size(0)\n        # If this is the last batch, store the embeddings and labels\n        if batch_idx == len(data_loader) - 1:\n            embeddings.extend(outputs.detach().cpu().numpy())\n            labels.extend(targets.detach().cpu().numpy())\n    # Calculate the average evaluation loss\n    eval_loss = eval_loss / num_samples\n    # Log evaluation statistics\n    logger.info(f\"Evaluation loss: {eval_loss:.4f}\")\n    # Generate and log a UMAP plot of the embeddings from the last batch\n    plot_embeddings(embeddings, labels, global_step, \"eval\")\n    # Return the average evaluation loss\n    return eval_loss\n\n", "CodeBleu_score": 31.85627131910896}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n    if model_name == \"xtts\":\n        try:\n            from xtts.config import XTTSConfig as config\n        except ModuleNotFoundError:\n            from xtts.config import XTTSConfig as config\n    else:\n        config_class = model_name.title() + \"Config\"\n        for path in [\"coqpit.configs\", \"configs\"]:\n            try:\n                config = getattr(import_module(path), config_class)\n            except ModuleNotFoundError:\n                continue\n            else:\n                break\n        else:\n            raise ModuleNotFoundError(f\"Configuration class {config_class} not found.\")\n    return config\n\n", "CodeBleu_score": 38.86011477195769}, {"code": "def register_config(model_name: str) -> Coqpit\n    if model_name == \"xtts\":\n        try:\n            from xtts.config import XttsConfig\n\n            return XttsConfig\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"xtts not found, please install it with `pip install xtts` or `pip install xtts[all]`\"\n            )\n    else:\n        for path in [\n            f\"coqpit.models.{model_name}.config\",\n            f\"coqpit.models.{model_name.lower()}.config\",\n            f\"coqpit.models.{model_name.upper()}.config\",\n            f\"coqpit.models.{model_name.capitalize()}.config\",\n            f\"coqpit.models.{model_name.replace('_', '')}.config\",\n            f\"coqpit.models.{model_name.replace('_', '').lower()}.config\",\n            f\"coqpit.models.{model_name.replace('_', '').upper()}.config\",\n            f\"coqpit.models.{model_name.replace('_', '').capitalize()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '')}.config\",\n            f\"coqpit.models.{model_name.replace('-', '').lower()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '').upper()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '').capitalize()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '_')}.config\",\n            f\"coqpit.models.{model_name.replace('-', '_').lower()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '_').upper()}.config\",\n            f\"coqpit.models.{model_name.replace('-', '_').capitalize()}.config\",\n            f\"coqpit.models.{model_name.replace(' ', '')}.config\",\n            f\"coqpit.models.{model_name.replace(' ', '').lower()}.config\",\n            f\"coqpit.models.{model_name.replace(' ', '').upper()}.config\",\n            f\"coqpit.models.{model_name.replace(' ', '').capitalize()}.config\",\n            f\"coqpit.models.{model_name.replace(' ', '_')}.config\",\n            f\"coqpit.models.{model_name.replace", "CodeBleu_score": 20.577134652575072}, {"code": "def register_config(model_name: str) -> Coqpit\n    try:\n        config_class = getattr(\n            importlib.import_module(\n                f\"coqpit.configs.{model_name.replace('-', '_')}\"\n            ),\n            \"CONFIG_REGISTRY\",\n        )[model_name]\n    except ModuleNotFoundError:\n        config_class = getattr(\n            importlib.import_module(\n                f\"coqpit.configs.{model_name.replace('-', '_')}\"\n            ),\n            \"CONFIG_REGISTRY\",\n        )[model_name]\n    except KeyError:\n        raise ModuleNotFoundError(\n            f\"No configuration found for model {model_name}. Please make sure you have installed the correct package.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 31.72202087270628}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        config_dict = yaml.safe_load(open(config_path, \"r\"))\n    elif config_path.endswith(\".json\"):\n        config_dict = json.load(open(config_path, \"r\"))\n    else:\n        raise ValueError(f\"Unknown configuration file type: {config_path}\")\n\n    model_name = config_dict[\"model\"]\n    config_class = _CONFIG_CLASSES.get(model_name)\n    if config_class is None:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n\n    config = config_class()\n    config.from_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 39.30084305529515}, {"code": "def load_config(config_path: str) -> Coqpit\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        config_dict = yaml.safe_load(open(config_path, \"r\"))\n    elif config_path.endswith(\".json\"):\n        config_dict = json.load(open(config_path, \"r\"))\n    else:\n        raise ValueError(f\"Unknown config file type: {config_path}\")\n    model_name = config_dict.pop(\"model_name\")\n    config_class = get_config_class(model_name)\n    config = config_class(**config_dict)\n    return config\n\n", "CodeBleu_score": 31.68144973579204}, {"code": "def load_config(config_path: str) -> Coqpit\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Determine the file extension and read the content accordingly as YAML or JSON\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown config file type: {config_path}\")\n\n    # Process the configuration dictionary to obtain the model name\n    model_name = config_dict[\"model\"]\n\n    # Register the appropriate configuration class\n    config_class = CONFIG_REGISTRY.get(model_name)\n    if config_class is None:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n\n    # Create an instance of the configuration class, populate it with the loaded data, and return the configuration object\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 43.005359897805135}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = os.path.join(rootDir, filename)\n                yield fullPath\n\n", "CodeBleu_score": 78.372087774408}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    for (dirpath, dirnames, filenames) in walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = path.join(dirpath, filename).replace(\"\\\\\", \"/\")\n                yield fullPath\n\n", "CodeBleu_score": 69.74706821734738}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    for (dirpath, dirnames, filenames) in walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                imagePath = path.join(dirpath, filename).replace(\" \", \"\\\\ \")\n                yield imagePath\n\n", "CodeBleu_score": 70.05066030757841}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    # Clear the GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if the paths are provided\n    if xtts_checkpoint is None:\n        raise ValueError(\"No checkpoint path provided\")\n    if xtts_config is None:\n        raise ValueError(\"No config path provided\")\n    if xtts_vocab is None:\n        raise ValueError(\"No vocab path provided\")\n\n    # Load the configuration\n    print(\"Loading configuration...\")\n    config = XTTSConfig.from_json_file(xtts_config)\n\n    # Initialize the XTTS model\n    print(\"Initializing the model...\")\n    model = XTTSModel(config)\n\n    # Load the model checkpoint and vocabulary\n    print(\"Loading the model checkpoint and vocabulary...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    tokenizer = XTTSTokenizer(xtts_vocab)\n\n    # Move the model to the GPU if available\n    if torch.cuda.is_available():\n        print(\"Moving the model to the GPU...\")\n        model.cuda()\n\n    # Print a success message\n    print(\"Model loaded successfully!\")\n\n    return model, tokenizer\n\n", "CodeBleu_score": 44.65972525641821}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    torch.cuda.empty_cache()\n    if xtts_checkpoint is None:\n        print(\"Please provide a checkpoint path\")\n        return\n    if xtts_config is None:\n        print(\"Please provide a config path\")\n        return\n    if xtts_vocab is None:\n        print(\"Please provide a vocab path\")\n        return\n    config = XTTSConfig.from_pretrained(xtts_config)\n    model = XTTSModel(config=config)\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    print(\"Loaded model from {}\".format(xtts_checkpoint))\n    return model\n\n", "CodeBleu_score": 50.055002376208634}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    try:\n        torch.cuda.empty_cache()\n        if xtts_checkpoint is None:\n            raise ValueError(\n                \"No checkpoint path is provided. Please provide a valid checkpoint path.\"\n            )\n        if xtts_config is None:\n            raise ValueError(\n                \"No config path is provided. Please provide a valid config path.\"\n            )\n        if xtts_vocab is None:\n            raise ValueError(\n                \"No vocab path is provided. Please provide a valid vocab path.\"\n            )\n        print(\"Loading configuration...\")\n        config = XTTSConfig.from_pretrained(xtts_config)\n        print(\"Initializing model...\")\n        model = XTTSForSpeechSynthesis(config)\n        print(\"Loading model checkpoint...\")\n        model.load_state_dict(torch.load(xtts_checkpoint))\n        print(\"Loading vocabulary...\")\n        tokenizer = XTTSTokenizer.from_pretrained(xtts_vocab)\n        print(\"Moving model to GPU...\")\n        model.to(device)\n        print(\"Model loaded successfully!\")\n        return model, tokenizer\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None, None\n\n", "CodeBleu_score": 39.96068737824924}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.torch_specifications:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.sample_rate))\n    if num_frames > x.shape[0]:\n        num_frames = x.shape[0]\n    offsets = np.linspace(0, x.shape[0] - num_frames, num_eval, dtype=int)\n    if self.torch_specifications:\n        x = torch.from_numpy(x).float()\n        x = x.to(self.device)\n        x = x.view(1, 1, -1)\n    x = np.expand_dims(x, axis=0)\n    embeddings = []\n    for offset in offsets:\n        x_batch = x[:, :, offset:offset + num_frames]\n        if self.torch_specifications:\n            x_batch = x_batch.to(self.device)\n        embedding = self.model(x_batch)\n        embeddings.append(embedding.cpu().detach().numpy())\n    embeddings = np.concatenate(embeddings, axis=0)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings", "CodeBleu_score": 43.526922857715014}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust num_frames based on hop length\n    if self.hop_length is not None:\n        num_frames = int(num_frames * self.hop_length)\n    # Ensure num_frames does not exceed input length\n    num_frames = min(num_frames, x.shape[-1])\n    # Compute offset positions for evaluation\n    offsets = np.linspace(0, x.shape[-1] - num_frames, num_eval, dtype=int)\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    embeddings = []\n    for offset in offsets:\n        frames = x[..., offset : offset + num_frames]\n        embeddings.append(self.forward(frames))\n    embeddings = torch.cat(embeddings, dim=0)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = embeddings / torch.norm(embeddings, p=2, dim=-1, keepdim=True)\n    return embeddings\n\n", "CodeBleu_score": 47.43863843734236}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust the number of frames based on the hop length if using torch specifications\n    if self.use_torch and self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.sr))\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[0])\n    # Calculate offset positions for evaluation\n    offsets = np.linspace(0, x.shape[0] - num_frames, num_eval, dtype=int)\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    embeddings = np.concatenate([self.model.predict(x[o:o + num_frames]) for o in offsets], axis=0)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n        return np.mean(embeddings, axis=0)\n    return embeddings\n\n", "CodeBleu_score": 37.39388957922799}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n      downsample = nn.Sequential(\n          nn.Conv2d(self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False),\n          nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 86.0626155763906}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.54080921158837}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n      downsample = nn.Sequential(\n          nn.Conv2d(self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False),\n          nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    \"\"\"The function defines the forward pass of the network.\\nIt first applies the initial convolutional layer and batch normalization to the input x.\\nIt then applies the first residual block layer to the output of the initial convolutional layer.\\nIt then applies the second residual block layer to the output of the first residual block layer.\\nIt then applies the third residual block layer to the output of the second residual block layer.\\nIt then applies the fourth residual block layer to the output of the third residual block layer.\\nIt then applies the average pooling layer to the output of the fourth residual block layer.\\nIt then applies the fully connected layer to the output of the average pooling layer.\\nFinally, it returns the output of the fully connected layer.\"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n\n    return x\n", "CodeBleu_score": 78.27276819340155}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    # Reduce the number of classes to 10 if there are more than 10\n    num_classes = min(num_classes_in_batch, 10)\n    # Calculate the number of utterances per class\n    num_utterances = len(embeddings) // num_classes\n    # Create a list of colors to use for each class\n    colors = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"yellow\",\n        \"purple\",\n        \"orange\",\n        \"pink\",\n        \"brown\",\n        \"gray\",\n        \"black\",\n    ]\n    # Create a list of labels to use for each class\n    labels = [f\"Class {i}\" for i in range(num_classes)]\n    # Initialize a list to store the scatter plot data\n    scatter_data = []\n    # Loop through the classes\n    for i in range(num_classes):\n        # Select the embeddings for the current class\n        class_embeddings = embeddings[i * num_utterances : (i + 1) * num_utterances]\n        # Use UMAP to reduce the dimensionality of the embeddings\n        umap_embedding = umap.UMAP(n_components=2).fit_transform(class_embeddings)\n        # Create a scatter plot for the current class\n        scatter_data.append(\n            go.Scatter(\n                x=umap_embedding[:, 0],\n                y=umap_embedding[:, 1],\n                mode=\"markers\",\n                marker=dict(color=colors[i], size=8),\n                name=labels[i],\n            )\n        )\n    # Create a figure with the scatter plots\n    fig = go.Figure(data=scatter_data)\n    # Set the title of the figure\n    fig.update_layout(title=\"UMAP projection\")\n    # Set the aspect ratio to be equal\n    fig.update_layout(\n        xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n        yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n    )\n    # Save the figure as an image\n    fig.write_image(\"umap.png\")\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 22.026473008965304}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    # Limit the number of classes to 10\n    num_classes_in_batch = min(num_classes_in_batch, 10)\n    # Create a UMAP instance\n    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n    # Transform the embeddings using UMAP\n    umap_embeddings = umap_reducer.fit_transform(embeddings)\n    # Create a scatter plot of the UMAP embeddings\n    fig, ax = plt.subplots(figsize=(10, 10))\n    for i in range(num_classes_in_batch):\n        class_embeddings = umap_embeddings[i * 10:(i + 1) * 10]\n        ax.scatter(class_embeddings[:, 0], class_embeddings[:, 1], label=f\"Class {i}\")\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    fig.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 29.440558687919804}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    # Calculate the number of utterances per class\n    num_classes = len(set(embeddings.labels))\n    # If the number of classes is greater than 10, limit the number of classes to 10\n    if num_classes > 10:\n        num_classes = 10\n    # If the number of classes in the batch is greater than 10, limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    # Create a UMAP transformer with n_components=2\n    transformer = umap.UMAP(n_components=2)\n    # Fit the transformer to the embeddings\n    transformer.fit(embeddings.embeddings)\n    # Transform the embeddings\n    transformed_embeddings = transformer.transform(embeddings.embeddings)\n    # Create a scatter plot with colors representing different classes\n    plt.scatter(\n        transformed_embeddings[:, 0],\n        transformed_embeddings[:, 1],\n        c=embeddings.labels,\n        cmap=\"tab10\",\n        s=1,\n    )\n    # Set the title of the plot\n    plt.title(\"UMAP projection\")\n    # Set the aspect ratio of the plot to be equal\n    plt.axis(\"equal\")\n    # Save the plot as a figure\n    plt.savefig(\"umap.png\")\n    # Return the figure\n    return plt\n\n", "CodeBleu_score": 30.840071229151235}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n    # Calculate cosine similarity matrix\n    cosine_sim = []\n    for spk_id, spk_utts in dvecs.items():\n        # Calculate new centroids excluding current utterance\n        new_centroids = np.array(centroids)\n        for utt_id, utt_dvec in spk_utts.items():\n            new_centroids = new_centroids - utt_dvec\n        new_centroids = new_centroids / (len(spk_utts) - 1)\n        # Calculate cosine similarity with new centroids\n        for utt_id, utt_dvec in spk_utts.items():\n            sim = np.dot(new_centroids, utt_dvec.T) / (np.linalg.norm(new_centroids, axis=1) * np.linalg.norm(utt_dvec))\n            sim = np.clip(sim, 0, 1)\n            cosine_sim.append(sim)\n    cosine_sim = np.concatenate(cosine_sim, axis=0)\n    return cosine_sim", "CodeBleu_score": 32.59103784458265}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cosine_sim = []\n    for spk in range(self.num_spk):\n        spk_dvecs = dvecs[spk]\n        spk_centroids = centroids[spk]\n        for utt in range(self.num_utt[spk]):\n            utt_dvecs = np.delete(spk_dvecs, utt, axis=0)\n            utt_centroids = np.delete(spk_centroids, utt, axis=0)\n            utt_sim = np.dot(utt_dvecs, utt_centroids.T) / (np.linalg.norm(utt_dvecs, axis=1) * np.linalg.norm(utt_centroids, axis=1))\n            utt_sim = np.clip(utt_sim, a_min=1e-6, a_max=None)\n            cosine_sim.append(utt_sim)\n    cosine_sim = np.concatenate(cosine_sim)\n    cosine_sim = np.reshape(cosine_sim, (self.num_spk, self.num_utt[0], self.num_spk))\n    return cosine_sim\n", "CodeBleu_score": 29.602030703474686}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    sim_matrix = []\n    for i, speaker in enumerate(dvecs):\n        # For each speaker, calculate the cosine similarity between the speaker's d-vectors and the centroids\n        sim_matrix.append(\n            [\n                np.clip(\n                    np.dot(dvecs[i], centroids.T)\n                    / (\n                        np.linalg.norm(dvecs[i])\n                        * np.linalg.norm(centroids, axis=1)\n                    ),\n                    -1,\n                    1,\n                )\n                for dvecs in dvecs\n            ]\n        )\n    return np.vstack(sim_matrix)\n\n", "CodeBleu_score": 28.70210983590279}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding.\n        loss = self.softmax(cos_sim_matrix[i], dim=0)\n        # Stack the losses for each row into tensors.\n        losses.append(loss)\n    # Return the final stacked tensor of losses.\n    return torch.stack(losses)\n\n  def embed_loss_nce(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using noise contrastive estimation.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding.\n        loss = self.nce(cos_sim_matrix[i], dim=0)\n        # Stack the losses for each row into tensors.\n        losses.append(loss)\n    # Return the final stacked tensor of losses.\n    return torch.stack(losses)\n\n  def embed_loss_nce_neg_sample(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using noise contrastive estimation with negative sampling.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding.\n        loss = self.nce_neg_sample(cos_sim_matrix[i], dim=0)\n        # Stack the losses for each row into tensors.\n        losses.append(loss)\n    # Return the final stacked tensor of losses.\n    return torch.stack(losses)\n\n  def embed_loss_nce_neg_sample_batch(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using noise contrastive estimation with negative sampling.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\n", "CodeBleu_score": 32.73012209670644}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding\n        loss = -torch.log(torch.exp(cos_sim_matrix[i, i]) / torch.exp(cos_sim_matrix[i, :]).sum())\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 36.494732387172085}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for dvec in dvecs:\n      dvec = dvec.unsqueeze(0)\n      dvec = dvec.expand(dvec.size(0), cos_sim_matrix.size(0))\n      dvec = dvec.unsqueeze(1)\n      dvec = dvec.expand(dvec.size(0), cos_sim_matrix.size(0), cos_sim_matrix.size(1))\n      dvec = dvec.contiguous().view(-1, cos_sim_matrix.size(1))\n      cos_sim_matrix = cos_sim_matrix.unsqueeze(0)\n      cos_sim_matrix = cos_sim_matrix.expand(dvec.size(0), cos_sim_matrix.size(1), cos_sim_matrix.size(2))\n      cos_sim_matrix = cos_sim_matrix.contiguous().view(-1, cos_sim_matrix.size(2))\n      loss = self.softmax_loss(dvec, cos_sim_matrix)\n      loss = loss.view(dvec.size(0), cos_sim_matrix.size(1))\n      losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n\n  def embed_loss_softmax_batch(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    dvecs = dvecs.unsqueeze(1)\n    dvecs = dvecs.expand(dvecs.size(0), cos_sim_matrix.size(0), dvecs.size(2))\n    dvecs = dvecs.contiguous().view(-1, dvecs.size(2))\n    cos_sim_matrix = cos_sim_matrix.unsqueeze(0)\n    cos_sim_matrix = cos_sim_matrix.expand(dvecs.size(0), cos_sim_matrix.size(1), cos_sim_matrix.size(2))\n    cos_sim_matrix = cos_sim_matrix.contiguous().view(-1, cos_sim_matrix.size(2))\n    loss = self.softmax_loss(dvecs, cos_sim_", "CodeBleu_score": 24.31201897945868}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    # Compute the sigmoid of the cosine similarity matrix for each embedding\n    sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n    # Initialize the loss row\n    loss_row = []\n    # Iterate over the embeddings\n    for i, dvec in enumerate(dvecs):\n        # Exclude the current centroid from the sigmoid matrix\n        sigmoid_matrix_i = sigmoid_matrix[i]\n        sigmoid_matrix_i[i] = 0\n        # Compute the contrast loss\n        loss_i = torch.log(sigmoid_matrix_i[torch.argmax(sigmoid_matrix_i)]).unsqueeze(0)\n        # Append the loss to the loss row\n        loss_row.append(loss_i)\n    # Stack the losses for each row into tensors\n    loss_row = torch.cat(loss_row, dim=0)\n    # Return the final stacked tensor of losses\n    return loss_row\n\n", "CodeBleu_score": 22.185131790783473}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    losses = []\n    for i in range(len(dvecs)):\n      loss_row = []\n      for j in range(len(dvecs)):\n        if i == j:\n          continue\n        loss_row.append(self.sigmoid(cos_sim_matrix[i][j]))\n      losses.append(torch.stack(loss_row))\n    return torch.stack(losses)\n", "CodeBleu_score": 28.832749093652595}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss_row = []\n    for i, dvec in enumerate(dvecs):\n        sim_row = cos_sim_matrix[i]\n        sim_row = torch.sigmoid(sim_row)\n        sim_row[i] = 0\n        loss_row.append(torch.log(sim_row).sum())\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 19.963044813863917}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n    # Check if the input has at least two utterances per speaker\n    assert x.shape[1] >= 2, \"There must be at least two utterances per speaker.\"\n    # Calculate the centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1, keepdim=True)\n    # Calculate the cosine similarity matrix between the embeddings and centroids\n    sim_matrix = torch.matmul(x, centroids.transpose(1, 2))\n    # Scale and shift the similarity matrix\n    sim_matrix = (self.s * sim_matrix).squeeze()\n    # Calculate the embedding loss\n    loss = self.loss_fn(sim_matrix)\n    # Calculate the mean loss\n    return loss.mean()\n", "CodeBleu_score": 42.116227285350625}, {"code": "def forward(self, x, _label=None)\n    # Get the number of speakers and the number of utterances per speaker\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n    # Check that there are at least two utterances per speaker\n    if num_utts_per_speaker < 2:\n      raise ValueError(\"There must be at least two utterances per speaker.\")\n    # Reshape the input to have dimensions (num_speakers*num_utts_per_speaker, dvec_feats)\n    x = x.view(num_speakers*num_utts_per_speaker, dvec_feats)\n    # Compute the centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=0, keepdim=True)\n    # Compute the cosine similarity matrix between the embeddings and centroids\n    sim_matrix = torch.matmul(x, centroids.t())\n    # Scale and shift the similarity matrix\n    sim_matrix = (1 - self.alpha) * sim_matrix + self.alpha / num_utts_per_speaker\n    # Compute the embedding loss\n    embedding_loss = torch.mean(torch.clamp(self.s - sim_matrix, min=0) ** 2)\n    # Return the mean loss\n    return embedding_loss\n", "CodeBleu_score": 35.950120270315146}, {"code": "def forward(self, x, _label=None)\n    centroids = x.mean(dim=1)\n    centroids_norm = F.normalize(centroids, p=2, dim=-1)\n    x_norm = F.normalize(x, p=2, dim=-1)\n    sim_matrix = torch.matmul(x_norm, centroids_norm.transpose(1, 2))\n    sim_matrix = (self.w * sim_matrix.transpose(1, 2)) + self.b\n    sim_matrix = self.softmax(sim_matrix)\n    sim_matrix = (1 - self.s) * sim_matrix + self.s / x.size(1)\n    loss_matrix = self.ce_loss(sim_matrix)\n    loss_matrix = loss_matrix.mean(dim=1)\n    return loss_matrix.mean()\n\n", "CodeBleu_score": 29.595620086482484}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])oice_dirs = [\n        \"voices\",\n        \"voices/male\",\n        \"voices/female\",\n        \"voices/child\",\n        \"voices/other\",\n        \"voices/male/adult\",\n        \"voices/male/child\",\n        \"voices/female/adult\",\n        \"voices/female/child\",\n        \"voices/child/adult\",\n        \"voices/child/child\",\n        \"voices/other/adult\",\n        \"voices/other/child\",\n    ]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        if not os.path.isdir(voice_dir):\n            continue\n        for root, dirs, files in os.walk(voice_dir):\n            if not files:\n                continue\n            if any(file.endswith(\".npz\") for file in files):\n                voices[root] = [\n                    os.path.join(root, file) for file in files if file.endswith(\".npz\")\n                ]\n            else:\n                voices[root] = [\n                    os.path.join(root, file)\n                    for file in files\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\")\n                ]\n    return voices\n\n", "CodeBleu_score": 25.070367298195595}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])oices = {}\n    voice_dirs = [\n        *extra_voice_dirs,\n        *list(Path(os.path.join(os.path.dirname(__file__), \"voices\")).glob(\"**/*\")),\n    ]\n    for voice_dir in voice_dirs:\n        if voice_dir.is_dir():\n            voice_name = voice_dir.name\n            voices[voice_name] = []\n            for voice_file in voice_dir.glob(\"**/*\"):\n                if voice_file.is_file():\n                    if voice_file.suffix == \".npz\":\n                        voices[voice_name].append(voice_file)\n                    elif voice_file.suffix == \".wav\":\n                        voices[voice_name].append(voice_file)\n                    elif voice_file.suffix == \".mp3\":\n                        voices[voice_name].append(voice_file)\n    return voices\n\n", "CodeBleu_score": 24.626914525344727}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])oices = {}\n    for voice_dir in extra_voice_dirs:\n        voices[voice_dir] = []\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".wav\"):\n                    voices[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".mp3\"):\n                    voices[voice_dir].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 28.83279503709112}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   if voice == \"random\":\n        return None, None, None\n    voice_dirs = [get_voice_dir(model)] + extra_voice_dirs\n    voice_paths = get_voices(voice_dirs, voice)\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice} in {voice_dirs}\")\n    elif len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice} in {voice_dirs}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"mel\"], voice_data[\"pitch\"], voice_data[\"energy\"]\n    else:\n        voice_data = load_audio(voice_path)\n        voice_mel, voice_pitch, voice_energy = extract_features(voice_data, model)\n        voice_mel = np.expand_dims(voice_mel, axis=0)\n        voice_pitch = np.expand_dims(voice_pitch, axis=0)\n        voice_energy = np.expand_dims(voice_energy, axis=0)\n        voice_path = voice_path.replace(\".wav\", \".npz\")\n        np.savez(voice_path, mel=voice_mel, pitch=voice_pitch, energy=voice_energy)\n        return voice_mel, voice_pitch, voice_energy\n\n", "CodeBleu_score": 44.66321429523639}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voices found for {model} and {voice}\")\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {model} and {voice}: {voice_paths}\"\n        )\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"arr_0\"], voice_data[\"arr_1\"], voice_data[\"arr_2\"], voice_path\n    else:\n        voice_data = load_voice(model, voice, extra_voice_dirs)\n        np.savez(voice_path + \".npz\", *voice_data)\n        return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 47.37502086034492}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   if voice == \"random\":\n        return None, None, None\n    voice_dirs = [get_voice_dir(model)] + extra_voice_dirs\n    voice_paths = get_voices(voice_dirs, voice)\n    if len(voice_paths) == 0:\n        raise ValueError(f\"Could not find voice {voice} in {voice_dirs}\")\n    elif len(voice_paths) > 1:\n        raise ValueError(f\"Found multiple voices matching {voice} in {voice_dirs}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return np.load(voice_path)\n    else:\n        voice_data = load_voice(model, voice_path, extra_voice_dirs)\n        np.savez(voice_path + \".npz\", **voice_data)\n        return voice_data\n\n", "CodeBleu_score": 45.37074354745007}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    # Extract the tensor's dimensions\n    batch_size, num_heads, length, _ = pos_score.size()\n    # Create a zero tensor with an additional dimension\n    zero_tensor = pos_score.new_zeros(batch_size, num_heads, 1, length)\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    concat_tensor = torch.cat([zero_tensor, pos_score], dim=-1)\n    # Reshape and slice the concatenated tensor to achieve the relative shift\n    shifted_tensor = concat_tensor.view(batch_size, num_heads, length + 1, length)[:, :, 1:]\n    return shifted_tensor\n\n", "CodeBleu_score": 27.984657826206753}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    # Extract tensor dimensions\n    batch_size, num_heads, query_length, memory_length = pos_score.size()\n    # Create a zero tensor with an additional dimension\n    zero_tensor = pos_score.new_zeros(batch_size, num_heads, query_length, 1)\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    concat_tensor = torch.cat([zero_tensor, pos_score], dim=-1)\n    # Reshape and slice the concatenated tensor to achieve the relative shift\n    shifted_tensor = concat_tensor.view(batch_size, num_heads, query_length, memory_length + 1)[:, :, :, 1:].contiguous()\n    return shifted_tensor\n\n", "CodeBleu_score": 29.59989194103654}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, num_heads, query_length, key_length = pos_score.size()\n    pos_score_flat = pos_score.view(batch_size, num_heads, -1)\n    zeros = torch.zeros((batch_size, num_heads, key_length - query_length), dtype=pos_score.dtype, device=pos_score.device)\n    pos_score_flat = torch.cat((zeros, pos_score_flat), dim=-1)\n    pos_score = pos_score_flat.view_as(pos_score).unsqueeze(dim=-1)\n    return pos_score\n\n", "CodeBleu_score": 34.6280066953372}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    # Create an empty tensor to store the processed contexts\n    processed_contexts = torch.zeros(batch_size, max_len, self.hidden_dim).to(context.device)\n    # Iterate over each batch element\n    for i in range(batch_size):\n        # Extract the context up to the specified length\n        batch_context = context[i, :lens[i]]\n        # Apply the convolutional layers\n        conv_out = self.conv(batch_context.unsqueeze(0))\n        conv_out = conv_out.squeeze(0)\n        # Store the processed context\n        processed_contexts[i, :lens[i]] = conv_out\n    # Pad the processed contexts to create a consistent batch size\n    padded_contexts = torch.nn.utils.rnn.pad_sequence(processed_contexts, batch_first=True)\n    return padded_contexts\n", "CodeBleu_score": 30.131975787681046}, {"code": "def run_padded_sequence(self, context, lens)\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    context_size = context.size(2)\n    context = context.view(-1, context_size)\n    context = self.relu(self.dropout(self.conv1(context)))\n    context = self.relu(self.dropout(self.conv2(context)))\n    context = self.relu(self.dropout(self.conv3(context)))\n    context = context.view(batch_size, max_len, -1)\n    context = context.index_select(1, lens.view(-1))\n    context = context.view(batch_size, -1, self.out_channels)\n    context = self.pad(context)\n    return context", "CodeBleu_score": 25.577797502833832}, {"code": "def run_padded_sequence(self, context, lens)\n    batch_size = context.size(0)\n    max_len = lens.max()\n    context_padded = torch.zeros(batch_size, max_len, self.hidden_size).to(context.device)\n    for i in range(batch_size):\n        cur_len = lens[i]\n        context_padded[i, :cur_len] = self.forward(context[i, :cur_len])\n    return context_padded\n\n", "CodeBleu_score": 19.299573599720325}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # sort by length\n    sorted_lens, indices = torch.sort(lens, dim=0, descending=True)\n    sorted_context = context[indices]\n    # pack sequences\n    packed_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    # run function on packed sequences\n    packed_output = fn(packed_context)\n    # unpack sequences\n    unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n    # map results back to original indices\n    _, unsorted_indices = torch.sort(indices, dim=0)\n    unsorted_output = unpacked_output[unsorted_indices]\n    return unsorted_output\n", "CodeBleu_score": 15.543494273461084}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # sort the indices and lengths\n    indices = np.argsort(lens)\n    lens = lens[indices]\n    # sort the context sequences\n    context = context[indices]\n    # pack the sequences\n    packed_context = pack_sequence(context, lens)\n    # run the function on the packed sequences\n    packed_results = fn(packed_context)\n    # unpack the results\n    results = unpack_sequence(packed_results, lens)\n    # map the results back to the original indices\n    results = results[np.argsort(indices)]\n    # return the context in its original order\n    return context, results\n", "CodeBleu_score": 12.104506684555666}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # Sort the inputs based on their lengths\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, sorted_indices)\n    # Pack the context sequences\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    # Apply the function to the packed context\n    packed_output = fn(packed_context)\n    # Unpack the sequences\n    unpacked_output, unpacked_lens = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    # Map the results back to the original indices\n    _, original_indices = torch.sort(sorted_indices)\n    output = unpacked_output.index_select(0, original_indices)\n    return output", "CodeBleu_score": 18.032445841153418}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n      conv_block.remove_weight_norm()\n", "CodeBleu_score": 27.302018603576762}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n      conv_block.remove_weight_norm()\n\n  def forward(self, x):\n    \"\"\"The forward function of the model.\\nIt calls the forward method on the kernel_predictor, convt_pre, and conv_blocks to generate the output of the model.\\nIt returns the output of the model.\"\"\"\n    kernel = self.kernel_predictor(x)\n    x = self.convt_pre(x, kernel)\n    for conv_block in self.conv_blocks:\n      x = conv_block(x, kernel)\n    return x\n", "CodeBleu_score": 37.842476415201844}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n      conv_block.remove_weight_norm()\n\n  def forward(self, x):\n    \"\"\"The function performs a forward pass through the neural network.\\nIt applies the forward method of the kernel_predictor, upsamples the output using the upsample_kernel method, applies the forward method of the convt_pre layer, and iterates through the conv_blocks to apply the forward method of each block's weight.\"\"\"\n    kernel = self.kernel_predictor(x)\n    kernel = self.upsample_kernel(kernel)\n    x = self.convt_pre(x, kernel)\n    for conv_block in self.conv_blocks:\n      x = conv_block(x)\n    return x\n\n  def upsample_kernel(self, kernel):\n    \"\"\"The function upsamples the kernel using the upsample_kernel method of the kernel_predictor.\"\"\"\n    return self.kernel_predictor.upsample_kernel(kernel)\n\n", "CodeBleu_score": 36.19598047885092}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)   mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_\"):\n        mx_name = \"gluon_se_\" + mx_name[3:]\n    elif mx_name.startswith(\"senet_\"):\n        mx_name = \"gluon_senet_\" + mx_name[6:]\n    elif mx_name.startswith(\"inceptionv3\"):\n        mx_name = \"gluon_inceptionv3\"\n    return \"gluon_\" + mx_name\n\n", "CodeBleu_score": 33.25318194132228}, {"code": "def map_mx_to_torch_model(mx_name)   mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_\"):\n        mx_name = \"gluon_se_\" + mx_name[3:]\n    elif mx_name.startswith(\"senet_\"):\n        mx_name = \"gluon_se_\" + mx_name[6:]\n    elif mx_name == \"inceptionv3\":\n        mx_name = \"gluon_inceptionv3\"\n    mx_name = \"gluon_\" + mx_name\n    return mx_name\n\n", "CodeBleu_score": 34.64918271300495}, {"code": "def map_mx_to_torch_model(mx_name)   mx_name = mx_name.lower()\n    mx_name = mx_name.replace(\"se_\", \"\")\n    mx_name = mx_name.replace(\"senet_\", \"\")\n    mx_name = mx_name.replace(\"inceptionv3\", \"inception_v3\")\n    return f\"gluon_{mx_name}\"\n\n", "CodeBleu_score": 28.749946068912074}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n    num_samples = self._num_samples\n    if self._num_replicas > 1:\n      num_samples = math_ops.ceil(num_samples / self._num_replicas)\n    if self._batch_size > 1:\n      num_samples = num_samples * self._batch_size\n    return int(num_samples)", "CodeBleu_score": 27.799736661886588}, {"code": "def _num_samples_per_worker(self)\n    num_samples = self._num_samples\n    num_workers = self._num_workers\n    num_replicas = self._num_replicas\n    batch_size = self._batch_size\n\n    # Calculate the number of samples per worker\n    if num_workers > 1:\n        # If training or multiple replicas are used, round up the number of samples\n        num_samples = int(math.ceil(num_samples / num_workers))\n    if batch_size > 0:\n        # If a batch size is specified during training, adjust the number of samples to be a multiple of the batch size\n        num_samples = int(math.ceil(num_samples / batch_size) * batch_size)\n    return int(num_samples)", "CodeBleu_score": 32.17837801586283}, {"code": "def _num_samples_per_worker(self)\n    if self.training or self.num_replicas > 1:\n        num_samples_per_worker = math.ceil(self.num_samples / self.num_replicas)\n    else:\n        num_samples_per_worker = self.num_samples\n    if self.batch_size is not None:\n        num_samples_per_worker = (\n            (num_samples_per_worker + self.batch_size - 1) // self.batch_size\n        ) * self.batch_size\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 39.011564746143065}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    images = []\n    targets = []\n    if types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = {}\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if is_image_file(fname, types):\n                path = os.path.join(root, fname)\n                if leaf_name_only:\n                    path = os.path.basename(path)\n                item = path, get_class_name(root)\n                if item[1] not in class_to_idx:\n                    class_to_idx[item[1]] = len(class_to_idx)\n                images.append(item[0])\n                targets.append(class_to_idx[item[1]])\n    if sort:\n        images = sorted(images)\n        targets = sorted(targets)\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 30.77159491365626}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    # Find all images in the folder\n    images = []\n    for root, dirs, files in os.walk(folder):\n        for file in files:\n            if file.lower().endswith(tuple(types)) if types else file.lower().endswith(\n                    ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')):\n                path = os.path.join(root, file) if leaf_name_only else root\n                label = os.path.basename(os.path.dirname(path)).lower()\n                images.append((path, label))\n\n    # Create class_to_idx if it's not provided\n    if class_to_idx is None:\n        class_names = {class_name for _, class_name in images}\n        class_to_idx = {class_name: i for i, class_name in enumerate(sorted(class_names))}\n\n    # Convert the labels to class indices\n    targets = [class_to_idx[label] for _, label in images]\n\n    # Sort if needed\n    if sort:\n        images.sort()\n        targets.sort()\n\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 33.66359706185329}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = {}\n    if leaf_name_only:\n        def _get_class_name(f_path: str) -> str:\n            return os.path.basename(os.path.dirname(f_path))\n    else:\n        def _get_class_name(f_path: str) -> str:\n            return os.path.dirname(f_path)\n    images = []\n    dir = os.path.expanduser(folder)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if _has_file_allowed_extension(fname, types):\n                    path = os.path.join(root, fname)\n                    class_name = _get_class_name(path)\n                    if class_name not in class_to_idx:\n                        class_to_idx[class_name] = len(class_to_idx)\n                    item = (path, class_to_idx[class_name])\n                    images.append(item)\n    if sort:\n        images = sorted(images, key=lambda x: x[0])\n    return images, class_to_idx\n\n", "CodeBleu_score": 36.21160864450345}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return torch.initial_seed() % 2 ** 32\n    else:\n        return worker_info.seed % 2 ** 32\n\n", "CodeBleu_score": 34.33042638450784}, {"code": "def pytorch_worker_seed()\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.initial_seed()\n\n", "CodeBleu_score": 44.791216619705175}, {"code": "def pytorch_worker_seed()\n    try:\n        worker_info = torch.utils.data.get_worker_info()\n        seed = worker_info.seed\n    except Exception:\n        seed = get_wds_rank()\n    return seed\n\n", "CodeBleu_score": 40.129512527288405}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in (\"L\", \"RGB\"):\n        return solarize_add_lut(img, add, thresh)\n    return img\n\n", "CodeBleu_score": 9.244507210900602}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    if add < 0:\n        add = 0\n    elif add > 255:\n        add = 255\n    add = int(add)\n    if img.mode == \"L\":\n        return img.point(lambda i: i if i > thresh else i + add)\n    else:\n        return img.point(lambda i: i if i[0] > thresh and i[1] > thresh and i[2] > thresh else (i[0] + add, i[1] + add, i[2] + add))\n\n", "CodeBleu_score": 34.1284652121857}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    if img.mode == \"L\":\n        return img.point(lambda x: 255 - x if x < thresh else x + add)\n    if img.mode == \"RGB\":\n        return img.point(lambda x: 255 - x if x < thresh else x + add, \"R\").point(\n            lambda x: 255 - x if x < thresh else x + add, \"G\"\n        ).point(lambda x: 255 - x if x < thresh else x + add, \"B\")\n\n", "CodeBleu_score": 29.335726385069982}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n    transforms_list = list(transforms.keys())\n    probabilities = list(transforms.values())\n    probabilities = [p / sum(probabilities) for p in probabilities]\n    return transforms_list, probabilities\n\n", "CodeBleu_score": 21.39992132415359}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = np.array(transform_probs) / sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.318371327924858}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transforms_list = list(transforms.keys())\n    probs_list = list(transforms.values())\n    probs_list = [float(i) for i in probs_list]\n    probs_list = np.array(probs_list)\n    probs_list = probs_list / probs_list.sum()\n    return transforms_list, probs_list\n\n", "CodeBleu_score": 20.416430173084127}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for weight in mixing_weights:\n        seq = self.get_sequence(img)\n        mixed_img += weight * seq.augment_image(img)\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    mixed_img = Image.blend(img, mixed_img, m)\n    return mixed_img\n", "CodeBleu_score": 25.24569661587594}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        op_list = np.random.choice(self.ops, size=m, replace=False)\n        for op in op_list:\n            img = op(img)\n        mixed_img += w * img\n    mixed_img = np.clip(mixed_img, 0, 1)\n    mixed_img = Image.fromarray((mixed_img * 255).astype(np.uint8))\n    mixed_img = ImageEnhance.Color(mixed_img).enhance(self.color_factor)\n    mixed_img = ImageEnhance.Contrast(mixed_img).enhance(self.contrast_factor)\n    mixed_img = ImageEnhance.Brightness(mixed_img).enhance(self.brightness_factor)\n    mixed_img = ImageEnhance.Sharpness(mixed_img).enhance(self.sharpness_factor)\n    mixed_img = np.asarray(mixed_img) / 255\n    mixed_img = (\n        self.mixing_factor * mixed_img + (1 - self.mixing_factor) * img\n    ).astype(np.float32)\n    return mixed_img\n\n", "CodeBleu_score": 35.82900927974776}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros_like(img)\n    for w in mixing_weights:\n        with self.rng.state_context(self.rng.get_state()):\n            mixed_img += w * self._apply_seq(img)\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    return Image.blend(img, mixed_img, m)\n\n", "CodeBleu_score": 24.81213541881806}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        x = (sum(x) / len(x),) * n\n        logging.warning(\n            f\"The length of x ({len(x)}) does not match the required number of channels ({n}). The mean of x will be repeated {n} times.\"\n        )\n    assert len(x) == n, f\"The length of x ({len(x)}) does not match the required number of channels ({n}).\"\n    return x\n\n", "CodeBleu_score": 42.332752170262445}, {"code": "def adapt_to_chs(x, n)\n    assert n > 0, \"n must be greater than 0\"\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    else:\n        assert len(x) == n, \"x must be a tuple or list of length n\"\n    return x\n\n", "CodeBleu_score": 32.83689814497151}, {"code": "def adapt_to_chs(x, n)\n    assert isinstance(n, int)\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        x = tuple(np.mean(x) for _ in range(n))\n        print(f\"The number of channels in x ({len(x)}) does not match n ({n}). The mean of x has been repeated n times.\")\n    assert len(x) == n\n    return x\n", "CodeBleu_score": 48.661282393372154}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_seeding == 'all':\n        np.random.seed(worker_info.seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise TypeError('worker_seeding must be a callable or string')\n\n", "CodeBleu_score": 28.489833194026698}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return\n\n    worker_id = worker_info.id\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise TypeError(f'worker_seeding must be a string or a callable, got {type(worker_seeding)}')\n\n", "CodeBleu_score": 34.825669798461945}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    # Retrieve worker information\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        print(f\"Warning: Worker {worker_id} does not have worker info. Skipping seeding.\")\n        return\n\n    # Verify worker ID\n    if worker_id!= worker_info.id:\n        raise RuntimeError(f\"Worker seeding mismatch: {worker_id} vs {worker_info.id}\")\n\n    # Set seed\n    if callable(worker_seeding):\n        # Generate and set seed for random, torch, and numpy\n        seed = worker_seeding(worker_info.seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif isinstance(worker_seeding, str):\n        # Set numpy seed based on worker_info seed for 'all' seeding\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed)\n        else:\n            raise ValueError(f\"Invalid worker_seeding: {worker_seeding}\")\n    else:\n        raise TypeError(f\"Invalid type for worker_seeding: {type(worker_seeding)}\")\n\n", "CodeBleu_score": 40.117040773219045}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        (\n            (image_size / (image_size + CROP_PADDING))\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\n        ),\n        tf.int32,\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 58.93347788834787}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.shape(image)\n    padded_center_crop_size = tf.cast(\n        ((image_size / PADDING_CONSTANT) * tf.cast(image_shape, tf.float32)), tf.int32\n    )\n    offset_height = ((image_shape[0] - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size[1]) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size[0], padded_center_crop_size[1]\n    )\n    image = tf.image.resize(image, (image_size, image_size), method=resize_method)\n    return image\n\n", "CodeBleu_score": 33.69858152384324}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        (\n            (image_size / resize_method) * ((image_height / image_width) + 0.3)\n        ),\n        tf.int32,\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, image_size, image_size])\n    image = tf.image.decode_and_crop_jpeg(\n        image_bytes, crop_window, channels=3\n    )  # Decode and crop the image\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 45.01547730703269}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = decode_image(image_bytes, image_size, interpolation)\n    image = random_crop(image, image_size, image_size)\n    image = random_flip(image)\n    image = to_float32(image)\n    image = to_bfloat16(image) if use_bfloat16 else image\n    return image\n\n", "CodeBleu_score": 23.041725306567507}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode the image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Randomly crop the image\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    # Flip the image horizontally\n    image = tf.image.random_flip_left_right(image)\n    # Reshape the image\n    image = tf.reshape(image, [image_size, image_size, 3])\n    # Convert the image to the specified dtype\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 34.44426187503907}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = decode_image(image_bytes, image_size, interpolation)\n    image = random_crop(image)\n    image = random_flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 40.07776193495008}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if image_size is None:\n        image_size = [IMAGE_SIZE, IMAGE_SIZE]\n    # Decode the image from bytes\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    # Get the image shape\n    image_shape = tf.shape(image)\n    # Get the image height and width\n    height, width = image_shape[0], image_shape[1]\n    # Calculate the minimum dimension of the image\n    min_dim = tf.minimum(height, width)\n    # Calculate the offset for center cropping based on the minimum dimension\n    offset_height = (height - min_dim) // 2\n    offset_width = (width - min_dim) // 2\n    # Crop the image to the center\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, min_dim, min_dim\n    )\n    # Resize the image based on the interpolation method\n    if interpolation == 'bicubic':\n        image = tf.image.resize(image, image_size, method='bicubic')\n    elif interpolation == 'bilinear':\n        image = tf.image.resize(image, image_size, method='bilinear')\n    else:\n        raise ValueError(\n            f'Invalid interpolation method: {interpolation}. Must be \"bicubic\" or \"bilinear\".'\n        )\n    # Convert the image data type to bfloat16 if use_bfloat16 is True, otherwise convert it to float32\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    # Return the preprocessed image\n    return image\n\n", "CodeBleu_score": 40.58142761337993}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.resize_with_crop_or_pad(image, image_size + 16, image_size + 16)\n    if interpolation == 'bicubic':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC)\n    elif interpolation == 'lanczos3':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.LANCZOS3)\n    elif interpolation == 'lanczos5':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.LANCZOS5)\n    elif interpolation == 'nearest':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    elif interpolation == 'area':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.AREA)\n    else:\n        raise ValueError(f'Invalid interpolation method: {interpolation}')\n    image = tf.cast(image, dtype=tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 38.940284129759604}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if image_bytes is None:\n        tf.logging.info('Image is None')\n        return tf.zeros([image_size, image_size, 3])\n    # Decode the image as a JPEG file, preserving the RGB channels and returning a uint8 tensor.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Resize the image to the specified size using the specified interpolation method.\n    image = _resize_image(image, image_size, interpolation)\n    # Convert the image data type to either bfloat16 or float32 based on the use_bfloat16 flag.\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 48.54315776954833}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(self.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(\n                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n            )\n\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=1)\n    result = self.compute_metrics(preds, out_label_ids)\n    result[\"eval_loss\"] = eval_loss\n\n    output_eval_file = os.path.join(self.output_dir, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result\n\n", "CodeBleu_score": 42.8902818300074}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    output_file = os.path.join(self.output_dir, output_file)\n    model.eval()\n    eval_loss = 0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n                )\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(\n                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n            )\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=1)\n    result = compute_metrics(preds, out_label_ids)\n    result[\"eval_loss\"] = eval_loss\n    output_eval_file = os.path.join(self.output_dir, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n    return result\n\n", "CodeBleu_score": 46.69618178726954}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    eval_dataloader = eval_dataloader.with_format(\"torch\")\n    eval_dataloader.reset()\n    self.model = model\n    self.model.eval()\n    self.model.to(self.device)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in eval_dataloader:\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM and RoBERTa don't use segment_ids\n            outputs = self.model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(\n                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n            )\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=1)\n    result = self.compute_metrics(preds, out_label_ids)\n    result[\"eval_loss\"] = eval_loss\n    output_eval_file = os.path.join(self.output_dir, f\"{output_file}.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (", "CodeBleu_score": 44.95013518101763}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n    print(\"Rank1: {:.4f}\".format(metrics['rank1']))\n    print(\"Rank5: {:.4f}\".format(metrics['rank5']))\n    print(\"mIoU: {:.4f}\".format(metrics['mean_iou']))\n", "CodeBleu_score": 24.193846360853563}, {"code": " def print_computed_metrics(self, metrics)\n    print(\"Rank-1: {:.4f}\".format(metrics[\"rank-1\"]))\n    print(\"Rank-5: {:.4f}\".format(metrics[\"rank-5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"mIoU\"]))\n\n", "CodeBleu_score": 24.572311038185973}, {"code": " def print_computed_metrics(self, metrics)\n    print(f\"Rank@1: {metrics['rank1']*100:.4f}%, Rank@5: {metrics['rank5']*100:.4f}%, mIoU: {metrics['miou']*100:.4f}%\")\n\n  def print_epoch_metrics(self, epoch, train_metrics, val_metrics):\n    \"\"\"The function prints the metrics for the current epoch, including the epoch number, train loss, train accuracy, and validation accuracy.\\nIt retrieves these values from the provided metrics dictionaries and prints them formatted to four decimal places.\"\"\"\n    print(f\"Epoch {epoch+1}: Train Loss: {train_metrics['loss']*100:.4f}%, Train Accuracy: {train_metrics['accuracy']*100:.4f}%, Validation Accuracy: {val_metrics['accuracy']*100:.4f}%\")\n\n  def print_final_metrics(self, train_metrics, val_metrics, test_metrics):\n    \"\"\"The function prints the final metrics for the training, validation, and test sets.\\nIt retrieves these values from the provided metrics dictionaries and prints them formatted to four decimal places.\"\"\"\n    print(f\"Train Loss: {train_metrics['loss']*100:.4f}%, Train Accuracy: {train_metrics['accuracy']*100:.4f}%, Validation Accuracy: {val_metrics['accuracy']*100:.4f}%, Test Accuracy: {test_metrics['accuracy']*100:.4f}%\")\n\n  def save_model(self, model, optimizer, scheduler, epoch, loss, accuracy, val_accuracy):\n    \"\"\"The function saves the model, optimizer, and scheduler state dictionaries to the provided file path.\\nIt also saves the epoch number, loss, accuracy, and validation accuracy as a checkpoint.\"\"\"\n    torch.save({\n       'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n       'scheduler_state_dict': scheduler.state_dict(),\n        'epoch': epoch,\n        'loss': loss,\n        'accuracy': accuracy,\n        'val_accuracy': val_accuracy\n    }, self.model_save_path)\n\n  def load_model(self):\n    \"\"\"The function loads the model, optimizer, and scheduler state dictionaries from the provided file path.\\n", "CodeBleu_score": 28.301418514593575}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n    rank1 = 0\n    rank5 = 0\n    iou = 0\n    for segment, datum in zip(segments, data):\n        # get IoU and rank for each prediction\n        iou_values = np.array([self._iou(segment, pred) for pred in datum[\"predictions\"]])\n        ranks = np.argsort(iou_values)[::-1] + 1\n        # average top values\n        rank1 += ranks[0]\n        rank5 += ranks[4]\n        iou += iou_values[0]\n    # calculate percentage of rank 1 and rank 5\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    # calculate mean IoU\n    iou /= len(segments)\n    return rank1, rank5, iou\n", "CodeBleu_score": 29.861434276088435}, {"code": "def _eval_predictions(self, segments, data)\n    rank1, rank5, mean_iou = [], [], []\n    for segment, data_item in zip(segments, data):\n        ious = []\n        for pred in segment:\n            iou = self._calculate_iou(pred, data_item)\n            ious.append(iou)\n        ranked_ious = np.sort(ious)[::-1]\n        rank1.append(ranked_ious[0])\n        rank5.append(ranked_ious[:5].mean())\n        mean_iou.append(ranked_ious.mean())\n    rank1 = np.mean(rank1)\n    rank5 = np.mean(rank5)\n    mean_iou = np.mean(mean_iou)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 32.26536111667876}, {"code": "def _eval_predictions(self, segments, data)\n    # Get the segment and data IDs\n    seg_ids = [seg[\"id\"] for seg in segments]\n    data_ids = [data[\"id\"] for data in data]\n    # Check if there is a match between the segment and data IDs\n    if set(seg_ids)!= set(data_ids):\n        raise ValueError(\"Segment and data IDs do not match.\")\n    # Initialize lists to store IoUs and ranks\n    ious = []\n    ranks = []\n    # Iterate through each segment and data pair\n    for seg, d in zip(segments, data):\n        # Get the segment and data masks\n        seg_mask = seg[\"mask\"]\n        data_mask = d[\"mask\"]\n        # Calculate the IoU for the segment and data masks\n        iou = self._iou(seg_mask, data_mask)\n        # Append the IoU to the list of IoUs\n        ious.append(iou)\n        # Calculate the rank of the IoU among the segment and data masks\n        rank = self._rank(iou, seg_mask, data_mask)\n        # Append the rank to the list of ranks\n        ranks.append(rank)\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = np.mean(np.array(ranks) == 1) * 100\n    rank5 = np.mean(np.array(ranks) <= 5) * 100\n    # Calculate the mean IoU across all segments\n    mean_iou = np.mean(ious)\n    # Return rank1, rank5, and mean IoU\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 37.76469743291601}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.cuda(0)\n    for batch in eval_dataloader:\n        for key, value in batch.items():\n            if key!= \"labels\":\n                batch[key] = value.cuda(0)\n        with torch.no_grad():\n            output = model(**batch)\n        scores.append(output)\n    scores = self.process_scores(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 40.03917138506203}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in eval_dataloader:\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n            }\n            if self.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n                del inputs[\"token_type_ids\"]\n            if self.model_type in [\"xlnet\", \"xlm\"]:\n                inputs.update({\"cls_index\": batch[3], \"p_mask\": batch[4]})\n            outputs = model(**inputs)\n        scores.append(outputs[0].detach().cpu().numpy())\n    scores = np.vstack(scores)\n    if output_file is not None:\n        np.save(output_file, scores)\n    return scores\n\n", "CodeBleu_score": 36.69457531309545}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in eval_dataloader:\n        for k, v in batch.items():\n            batch[k] = v.to(self.device)\n        with torch.no_grad():\n            outputs = model(**batch)\n        scores.append(outputs)\n    scores = self.post_process(scores)\n    if output_file is not None:\n        self.save_results(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 40.457772876541746}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, \"There should be exactly two sets of scores\"\n    scores = [np.concatenate(s, axis=0) for s in scores]\n    self.full_scores = None\n    return np.dot(scores[0], scores[1].T)\n\n", "CodeBleu_score": 34.74290735204178}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    scores = [tf.concat(s, axis=0) for s in scores]\n    self._full_scores = None\n    return tf.matmul(scores[0], scores[1], transpose_b=True)\n", "CodeBleu_score": 35.772014779620996}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, \"There should be exactly two sets of scores\"\n    scores = [np.concatenate(score, axis=0) for score in scores]\n    self.full_scores = []\n    return np.dot(scores[0], scores[1].T)\n", "CodeBleu_score": 40.75490464914255}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\n    # Concatenate predictions\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n\n    # Check for prediction errors\n    pred_errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i]!= Y_true[i]:\n            pred_errors.append(i)\n\n    # Print samples of prediction errors\n    if len(pred_errors) > 0:\n        print(\"Sample of prediction errors:\")\n        for i in range(min(len(pred_errors), 10)):\n            print(f\"Index: {pred_errors[i]}, Predicted: {Y_pred[pred_errors[i]]}, True: {Y_true[pred_errors[i]]}\")\n\n    # Save predictions and true labels as a pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 41.4948556243876}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    if len(Y_pred)!= len(Y_true):\n      print('ERROR: Number of predictions and true labels do not match!')\n      return None\n    if output_file is not None:\n      with open(output_file, 'wb') as f:\n        pickle.dump((Y_pred, Y_true), f)\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n", "CodeBleu_score": 25.681750680405173}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump((Y_pred, Y_true), f)\n\n    # Check for prediction errors\n    pred_error_idx = np.where(Y_pred!= Y_true)[0]\n    if len(pred_error_idx) > 0:\n        print(\"There are {} prediction errors.\".format(len(pred_error_idx)))\n        print(\"Some examples of these errors:\")\n        print(Y_pred[pred_error_idx[:10]])\n        print(Y_true[pred_error_idx[:10]])\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}", "CodeBleu_score": 35.99178527058168}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n    for key in logging_outputs[0].keys():\n        if key!= \"loss\":\n            continue\n        losses = [log.get(key, 0) for log in logging_outputs]\n        avg_loss = sum(losses) / sum(log.get(\"sample_size\", 1) for log in logging_outputs)\n        logging_outputs[0][key] = avg_loss\n\n", "CodeBleu_score": 44.72797717459549}, {"code": "def reduce_metrics(logging_outputs) -> None\n    # Sum the loss values and sample sizes from the logs\n    total_loss = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    total_sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    # Log the average loss as a scalar with three decimal places\n    avg_loss = total_loss / total_sample_size\n    trainer.logger.log_metrics({\"loss\": round(avg_loss, 3)}, step=trainer.global_step)\n\n", "CodeBleu_score": 43.41532757023014}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size)\n\n", "CodeBleu_score": 62.06001936124095}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            [\n                torch.ones_like(cmasks),\n                torch.ones_like(vmasks)\n            ],\n            dim=1\n        )\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(cmasks),\n            torch.ones_like(vmasks)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 36.723052601547515}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    # If no attention mask is provided, create one using cmasks and vmasks\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            (len(cmasks), self.max_seq_length),\n            dtype=torch.long,\n            device=self.device\n        )\n        for i, cmask in enumerate(cmasks):\n            seq_len = len(cmask)\n            attention_mask[i, :seq_len] = cmask\n    # Concatenate zeros for visual tokens and ones for text tokens\n    token_type_ids = torch.zeros(\n        (len(cmasks), self.max_seq_length),\n        dtype=torch.long,\n        device=self.device\n    )\n    for i, vmask in enumerate(vmasks):\n        seq_len = len(vmask)\n        token_type_ids[i, seq_len:] = 1\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 41.77993025868224}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            [\n                cmasks,\n                vmasks\n            ],\n            dim=1\n        )\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(cmasks),\n            torch.ones_like(vmasks)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 35.54287854605058}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if attention_mask is None:\n        attention_mask = input_ids.ne(self.config.pad_token_id)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # extend attention_mask, l_t, r_t so that they are of appropriate length\n    if self.config.pad_token_id is None:\n        raise ValueError(\n            \"If no pad_token_id is defined, the attention mask needs to be defined and passed to this function.\"\n        )\n    extended_attention_mask = self.get_extended_attention_mask(\n        attention_mask, input_ids.shape, input_video_embeds.shape\n    )\n\n    # extend token_type_ids and position_ids\n    if self.config.add_video_embeds_to_text_input:\n        token_type_ids = torch.cat(\n            (\n                torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device),\n                torch.ones(\n                    input_video_embeds.shape, dtype=torch.long, device=input_ids.device\n                ),\n            ),\n            dim=1,\n        )\n    else:\n        token_type_ids = torch.cat(\n            (token_type_ids, torch.zeros(input_video_embeds.shape, dtype=torch.long, device=input_ids.device)),\n            dim=1,\n        )\n\n    # prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    # prepare inputs for video models\n    if self.config.add_video_embeds_to_text_input:\n        input_ids = torch.cat((input_ids, input_video_embeds), dim=1)\n        attention_mask = extended_attention_mask\n    else:\n        input_", "CodeBleu_score": 26.22320673019732}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if input_video_embeds is not None:\n        if attention_mask is None:\n            attention_mask = input_ids.ne(self.config.pad_token_id)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n            attention_mask, input_ids.shape, model_kwargs.get(\"device\", 0)\n        )\n        if token_type_ids is not None:\n            extended_token_type_ids = token_type_ids.unsqueeze(1).expand(\n                input_ids.shape\n            )\n        else:\n            extended_token_type_ids = None\n        # Prepare head mask if needed\n        head_mask = self.get_head_mask(\n            None, self.config.num_hidden_layers, model_kwargs.get(\"head_mask\", None)\n        )\n        # Prepare inputs for video generation models\n        input_video_embeds = input_video_embeds.unsqueeze(1).expand(\n            -1, input_ids.shape[1], -1\n        )\n        input_ids = torch.cat((input_ids, input_video_embeds), dim=-1)\n        attention_mask = torch.cat(\n            (attention_mask, attention_mask.new_ones(input_ids.shape[0], 1)), dim=-1\n        )\n        if extended_token_type_ids is not None:\n            extended_token_type_ids = torch.cat(\n                (extended_token_type_ids, extended_token_type_ids.new_ones(input_ids.shape[0], 1)),\n                dim=-1,\n            )\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": extended_token_type_ids,\n            \"head_mask\": head_mask,\n        }\n    else:\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n           ", "CodeBleu_score": 34.43910753611288}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    # Check if input_ids and input_video_embeds are provided\n    if input_ids is None and input_video_embeds is None:\n        raise ValueError(\"You have to specify either input_ids or input_video_embeds\")\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    if input_ids is not None and input_video_embeds is not None:\n        input_ids_length = input_ids.shape[-1]\n        input_video_embeds_length = input_video_embeds.shape[-1]\n        if input_ids_length + input_video_embeds_length > self.config.max_position_embeddings:\n            input_ids = input_ids[:, -self.config.max_position_embeddings + input_video_embeds_length :]\n            input_ids_length = input_ids.shape[-1]\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, -input_ids_length - input_video_embeds_length :]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, -input_ids_length - input_video_embeds_length :]\n\n    # Return a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 43.470826368215164}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose    if \"img_id\" in cfg.test_pipeline[0]:\n        cfg.test_pipeline[0].pop(\"img_id\")\n    load_idx = None\n    for i, t in enumerate(cfg.test_pipeline):\n        if t[\"type\"] == \"LoadImageFromFile\":\n            load_idx = i\n            break\n    assert load_idx is not None, \"LoadImageFromFile is not found in test_pipeline\"\n    cfg.test_pipeline[load_idx][\"type\"] = \"mmdet.InferencerLoader\"\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 25.240064918169363}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose    if 'img_id' in cfg.test_pipeline[0]:\n        cfg.test_pipeline[0].pop('img_id')\n    load_img_idx = next(\n        idx for idx, transform in enumerate(cfg.test_pipeline)\n        if transform['type'] == 'LoadImageFromFile')\n    cfg.test_pipeline[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 23.4720373594116}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose    pipeline = cfg.test_pipeline\n    meta_keys = pipeline[0].get('meta_keys', [])\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    if 'LoadImageFromFile' in pipeline:\n        load_image_idx = pipeline.index('LoadImageFromFile')\n        pipeline[load_image_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 26.1537775899554}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        inputs = [path for path in Path(inputs).glob(\"*.jpg\")]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.84980421140745}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        inputs = os.listdir(inputs)\n        inputs = [os.path.join(inputs, f) for f in inputs if f.endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"))]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.295896617773536}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        inputs = list(Path(inputs).glob(\"*.jpg\"))\n        inputs = [str(input) for input in inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 40.697149155020625}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    data = []\n    for input_ in inputs:\n        if isinstance(input_, dict):\n            if 'img' in input_:\n                input_ = self.pipeline(input_)\n            elif 'img_path' in input_:\n                input_ = self.pipeline(input_)\n            else:\n                raise ValueError(\n                    'Input must be a dictionary containing either an \"img\" or \"img_path\" key.'\n                )\n        data.append(input_)\n        if len(data) == chunk_size:\n            yield data\n            data = []\n    if data:\n        yield data", "CodeBleu_score": 34.206211150913234}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            chunk.append(self.pipeline(input))\n        else:\n            chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n", "CodeBleu_score": 24.398854085725866}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    for chunk in _chunkify(inputs, chunk_size):\n        chunk_data = []\n        for input in chunk:\n            if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n                chunk_data.append(self.pipeline(input))\n            else:\n                chunk_data.append(input)\n        yield chunk_data\n", "CodeBleu_score": 23.134216517391742}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if not no_save_pred:\n        self.init_pred_out_dir(pred_out_dir)\n\n    results = {}\n    if not return_datasamples:\n        preds = self.convert_to_dict(preds)\n        results['preds'] = preds\n    else:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.'\n            )\n        results['datasamples'] = preds\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result:\n        self.print_result(results)\n\n    return results\n\n", "CodeBleu_score": 47.266121936771775}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if not no_save_pred and not osp.exists(pred_out_dir):\n        os.makedirs(pred_out_dir)\n    result = dict()\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True, '\n                'please set return_datasamples to False or set pred_out_dir to an empty string.'\n            )\n        result['datasamples'] = []\n    else:\n        result['predictions'] = []\n    if visualization is not None:\n        result['visualization'] = visualization\n    if print_result:\n        print(preds)\n    if return_datasamples:\n        for pred in preds:\n            if isinstance(pred, BaseDataElement):\n                result['datasamples'].append(pred)\n            else:\n                result['datasamples'].append(\n                    self.dataset.parse_data_sample(pred, dataset_meta=self.dataset.metainfo)\n                )\n    else:\n        for pred in preds:\n            if isinstance(pred, BaseDataElement):\n                result['predictions'].append(pred.to_dict())\n            else:\n                result['predictions'].append(\n                    self.dataset.parse_data_sample(pred, dataset_meta=self.dataset.metainfo).to_dict()\n                )\n    return result", "CodeBleu_score": 44.184864619680454}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if not no_save_pred and pred_out_dir == '':\n        pred_out_dir = self.pred_out_dir\n    if pred_out_dir!= '':\n        if return_datasamples:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.'\n            )\n        else:\n            self.save_pred(pred_out_dir, preds, visualization)\n    if not return_datasamples:\n        preds = self.convert_to_dict(preds)\n    if print_result:\n        self.print_result(preds)\n    result = {\n        'preds': preds,\n        'visualization': visualization,\n    }\n    return result\n\n", "CodeBleu_score": 46.14839901575942}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # Copy the first transform in the dataset pipeline\n    first_transform = copy.deepcopy(cfg[\"dataset\"][\"pipeline\"][0])\n    # Modify the first transform to include only the 'Resize' transform\n    first_transform[\"transforms\"] = [\n        transform\n        for transform in first_transform[\"transforms\"]\n        if transform[\"type\"] == \"Resize\"\n    ]\n    # Copy the last transform in the pipeline\n    last_transform = copy.deepcopy(cfg[\"dataset\"][\"pipeline\"][-1])\n    # Combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = {\"transforms\": [first_transform, last_transform]}\n    # Return the constructed test pipeline\n    return test_pipeline\n\n", "CodeBleu_score": 30.959272844365987}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    dataset_pipeline = cfg.get(\"dataset_pipeline\")\n    test_pipeline = copy.deepcopy(dataset_pipeline)\n    test_pipeline[0] = {\"type\": \"Resize\", \"height\": 224, \"width\": 224}\n    test_pipeline.append(copy.deepcopy(dataset_pipeline[-1]))\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 16.166100811425938}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # Copy the first transform in the dataset pipeline\n    first_transform = copy.deepcopy(cfg.pipeline.dataset.transforms[0])\n    # Modify the first transform to include only the 'Resize' transform\n    first_transform.transforms = [Resize(cfg.pipeline.dataset.transforms[0].transforms[0].size)]\n    # Copy the last transform in the pipeline\n    last_transform = copy.deepcopy(cfg.pipeline.dataset.transforms[-1])\n    # Combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = Compose([first_transform, last_transform])\n    # Return the constructed test pipeline\n    return test_pipeline\n\n", "CodeBleu_score": 28.714141085593454}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    # prepare input data\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape, img_id=0,\n                video_len=video_len)\n    data = model.test_pipeline(data)\n    if next(model.parameters()).device!= torch.device('cpu'):\n        model.cpu()\n    model.eval()\n    with torch.no_grad():\n        results = model.forward(**data)\n    return results\n\n", "CodeBleu_score": 24.49565635102883}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape, img_id=0, video_len=video_len)\n    data = model.cfg.data.test.pipeline[1](data)\n    data = model.cfg.data.test.pipeline[3](data)\n    data = model.cfg.data.test.pipeline[4](data)\n    data = model.cfg.data.test.pipeline[6](data)\n    data = model.cfg.data.test.pipeline[7](data)\n    data = model.cfg.data.test.pipeline[8](data)\n    data = model.cfg.data.test.pipeline[9](data)\n    data = model.cfg.data.test.pipeline[10](data)\n    data = model.cfg.data.test.pipeline[11](data)\n    data = model.cfg.data.test.pipeline[12](data)\n    data = model.cfg.data.test.pipeline[13](data)\n    data = model.cfg.data.test.pipeline[14](data)\n    data = model.cfg.data.test.pipeline[15](data)\n    data = model.cfg.data.test.pipeline[16](data)\n    data = model.cfg.data.test.pipeline[17](data)\n    data = model.cfg.data.test.pipeline[18](data)\n    data = model.cfg.data.test.pipeline[19](data)\n    data = model.cfg.data.test.pipeline[20](data)\n    data = model.cfg.data.test.pipeline[21](data)\n    data = model.cfg.data.test.pipeline[22](data)\n    data = model.cfg.data.test.pipeline[23](data)\n    data = model.cfg.data.test.pipeline[24](data)\n    data = model.cfg.data.test.pipeline[25](data)\n    data = model.cfg.data.test.pipeline[26](data)\n    data = model.cfg.data.test.pipeline[27](data)\n    data = model.cfg.data.test.pipeline[28](data)\n    data = model.cfg.data.test.pipeline", "CodeBleu_score": 16.908586189104263}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    data = {\n        'img': img,\n        'frame_id': frame_id,\n        'ori_shape': img.shape[:2],\n        'img_id': 0,\n        'video_len': video_len\n    }\n    data = model.pipeline(data)\n    if next(model.parameters()).is_cuda:\n        data = collate([data], samples_per_gpu=1)\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 32.201334640800546}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        # truncate tokens from the longer sequence\n        if len(tokens_a) > len(tokens_b):\n            if rng.random() < 0.5:\n                del tokens_a[0]\n            else:\n                del tokens_a[-1]\n        else:\n            if rng.random() < 0.5:\n                del tokens_b[0]\n            else:\n                del tokens_b[-1]\n\n", "CodeBleu_score": 51.49097337010655}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        # truncate the longer sequence\n        if len(tokens_a) > len(tokens_b):\n            if rng.random() < 0.5:\n                del tokens_a[0]\n            else:\n                del tokens_a[-1]\n        else:\n            if rng.random() < 0.5:\n                del tokens_b[0]\n            else:\n                del tokens_b[-1]\n\n", "CodeBleu_score": 51.49097337010655}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        if len(tokens_a) > len(tokens_b):\n            if rng.randint(0, 1) == 0:\n                tokens_a.pop(0)\n            else:\n                tokens_a.pop()\n        else:\n            if rng.randint(0, 1) == 0:\n                tokens_b.pop(0)\n            else:\n                tokens_b.pop()\n\n", "CodeBleu_score": 47.79628903367013}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n    if isinstance(activation_string, str):\n        if activation_string == \"\":\n            return None\n        elif activation_string == \"relu\":\n            return tf.nn.relu\n        elif activation_string == \"tanh\":\n            return tf.nn.tanh\n        elif activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        elif activation_string == \"softmax\":\n            return tf.nn.softmax\n        else:\n            raise ValueError(\"Unsupported activation function: {}\".format(activation_string))\n    else:\n        return activation_string\n\n", "CodeBleu_score": 33.532711848912356}, {"code": "def get_activation(activation_string)\n    if isinstance(activation_string, str):\n        if activation_string == \"\":\n            return None\n        elif activation_string == \"relu\":\n            return tf.nn.relu\n        elif activation_string == \"elu\":\n            return tf.nn.elu\n        elif activation_string == \"leaky_relu\":\n            return tf.nn.leaky_relu\n        elif activation_string == \"tanh\":\n            return tf.nn.tanh\n        elif activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        else:\n            raise ValueError(\"Unsupported activation function.\")\n    else:\n        return activation_string\n\n", "CodeBleu_score": 29.06047738528066}, {"code": "def get_activation(activation_string)\n    if isinstance(activation_string, str):\n        if activation_string == '':\n            return None\n        elif activation_string =='sigmoid':\n            return tf.nn.sigmoid\n        elif activation_string == 'tanh':\n            return tf.nn.tanh\n        elif activation_string =='relu':\n            return tf.nn.relu\n        else:\n            raise ValueError('Unsupported activation function: {}'.format(activation_string))\n    else:\n        return activation_string\n\n", "CodeBleu_score": 30.896734721772884}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 52.87834546200205}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, rate=dropout_prob)\n", "CodeBleu_score": 52.813686512369564}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 52.87834546200205}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 54.47030159574046}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 54.47030159574046}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 59.90976989090129}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n    input_tensor = tf.convert_to_tensor(input_tensor)\n    input_tensor_shape = input_tensor.get_shape()\n    input_tensor_dims = input_tensor_shape.ndims\n    if input_tensor_dims is None:\n        raise ValueError(\"The number of dimensions of the input tensor is unknown.\")\n    if input_tensor_dims < 2:\n        raise ValueError(\"The number of dimensions of the input tensor must be at least 2.\")\n    if input_tensor_dims == 2:\n        return input_tensor\n    return tf.reshape(input_tensor, [-1, input_tensor_shape[-1]])\n\n", "CodeBleu_score": 23.157562015853944}, {"code": "def reshape_to_matrix(input_tensor)\n    input_tensor_rank = len(input_tensor.get_shape().as_list())\n    if input_tensor_rank == 2:\n        return input_tensor\n    elif input_tensor_rank > 2:\n        return tf.reshape(input_tensor, [-1, input_tensor.get_shape().as_list()[-1]])\n    else:\n        raise ValueError(\"The input tensor must be of rank 2 or higher.\")\n", "CodeBleu_score": 32.41915557939697}, {"code": "def reshape_to_matrix(input_tensor)\n    if input_tensor.get_shape().ndims < 2:\n        raise ValueError(\"Input tensor must be at least rank 2.\")\n    if input_tensor.get_shape().ndims == 2:\n        return input_tensor\n    return tf.reshape(input_tensor, [-1, input_tensor.get_shape().dims[-1].value])\n\n", "CodeBleu_score": 33.96841590989751}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L272\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L77\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L272\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L77\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L272\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L77\n    # for original code\n    # See https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/run_classifier.py#L272\n    # for original code\n    # See https://github.com/google-research/bert/blob/f", "CodeBleu_score": 0.8783979178716022}, {"code": "def get_train_examples(self, data_dir)\n    # The only difference between this and the previous function is that the label is \"contradictory\" instead of \"contradiction\"\n    # so we need to change it\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), encoding=\"utf-8\") as f:\n        next(f)\n        for line in f:\n            guid = \"train-%d\" % (len(examples))\n            text_a, text_b, label = line.strip().split(\"\\t\")\n            text_a = tokenization.convert_to_unicode(text_a)\n            text_b = tokenization.convert_to_unicode(text_b)\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 36.5262338723459}, {"code": "def get_train_examples(self, data_dir)\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, 'utf-8') for cell in line)\n            examples.append(\n                InputExample(guid=line[0], text_a=line[5], text_b=line[6], label=line[3]))\n    return examples\n\n", "CodeBleu_score": 21.78803863615815}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    label_map = {label: i for i, label in enumerate(label_list)}\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 79.55657327067625}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    label_map = {label: i for i, label in enumerate(label_list)}\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.99267938913188}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    label_map = {label: i for i, label in enumerate(label_list)}\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.99267938913188}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 80.2731767938083}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    # Get the output weights and bias\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n    # Compute logits\n    with tf.variable_scope(\"cls\"):\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n    # Compute log probabilities\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    # Convert labels to one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    # Compute per-example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    # Compute mean loss\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 77.63858344963157}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer()\n        )\n        with tf.variable_scope(\"loss\"):\n            if labels is None:\n                return None, None, None\n            # The labels are reshaped to shape [batch_size, 1]\n            # and then converted to one-hot encoding.\n            labels = tf.reshape(labels, [-1, 1])\n            one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n            # The logits are computed through matrix multiplication and bias addition.\n            logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n            logits = tf.nn.bias_add(logits, output_bias)\n            # The per-example loss is calculated as the negative sum of the product of one-hot labels and log probabilities.\n            per_example_loss = -tf.reduce_sum(\n                one_hot_labels * tf.nn.log_softmax(logits), axis=-1\n            )\n            # The mean loss is computed.\n            loss = tf.reduce_mean(per_example_loss)\n            return (\n                loss,\n                per_example_loss,\n                tf.nn.log_softmax(logits),\n            )\n\n", "CodeBleu_score": 63.73461543905705}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n    flat_offsets = torch.arange(0, sequence_tensor.shape[0] * sequence_tensor.shape[1], sequence_tensor.shape[1], dtype=torch.long, device=sequence_tensor.device)\n    flat_positions = positions + flat_offsets[:, None]\n    flat_sequence_tensor = sequence_tensor.flatten(0, 1)\n    output_tensor = flat_sequence_tensor.gather(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 22.287902247010795}, {"code": "def gather_indexes(sequence_tensor, positions)\n    flat_offsets = torch.arange(0, sequence_tensor.shape[0] * sequence_tensor.shape[1], sequence_tensor.shape[1], device=sequence_tensor.device)\n    flat_positions = positions + flat_offsets[:, None]\n    flat_sequence_tensor = sequence_tensor.flatten(0, 1)\n    output_tensor = flat_sequence_tensor.gather(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 21.64917521783673}, {"code": "def gather_indexes(sequence_tensor, positions)\n    flat_offsets = tf.reshape(\n        tf.range(0, tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1], [-1, 1]\n    )\n    flat_positions = flat_offsets + tf.reshape(positions, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [-1, int(sequence_tensor.shape[2])])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 27.412260880860202}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 76.3363458292989}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 76.3363458292989}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = normalize_text(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n", "CodeBleu_score": 65.93905998565808}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 58.3818442458788}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 58.3818442458788}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n    tokens = self.basic_tokenizer.tokenize(text)\n    return self.wordpiece_tokenizer.tokenize(tokens)\n", "CodeBleu_score": 14.172030445340427}, {"code": "def tokenize(self, text)\n    tokens = self.tokenizer.tokenize(text)\n    sub_tokens = []\n    for token in tokens:\n      sub_tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return sub_tokens\n", "CodeBleu_score": 40.8824846080419}, {"code": "def tokenize(self, text)\n    tokens = self.basic_tokenizer.tokenize(text)\n    return tokens\n", "CodeBleu_score": 6.923087678568543}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio, _ = librosa.load(path, sr=sample_rate, offset=offset, duration=duration)\n        audio = audio.astype(dtype)\n        return audio, False\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 28.143727183800994}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        with open(path, \"rb\") as f:\n            f.seek(offset)\n            data = np.frombuffer(f.read(duration), dtype=dtype)\n        return data, False\n    except Exception as e:\n        logging.error(f\"Error loading audio data from {path}:\\n{e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 30.989218139615403}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        data, sr = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        return data, False\n    except Exception as e:\n        print(f\"Failed to load audio: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 27.926089637978603}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    elif waveform.shape[-1] < n_channels:\n        return tf.tile(waveform, [1, 1, n_channels])[:, :, :n_channels]\n    else:\n        return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 48.494339732358625}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    waveform = tf.expand_dims(waveform, axis=-1)\n    if waveform.shape[-1] < n_channels:\n        waveform = tf.tile(waveform, [1, 1, n_channels])\n    return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 47.918383181372704}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape[0] >= n_channels:\n        return waveform[:n_channels, :]\n    else:\n        return tf.tile(waveform, [n_channels // waveform.shape[0], 1])[:n_channels, :]\n\n", "CodeBleu_score": 38.394839699090774}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    spectrogram_db_max = tf.reduce_max(spectrogram_db)\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, spectrogram_db_max - db_range, spectrogram_db_max\n    )\n    spectrogram_db_clipped_uint = tf.cast(\n        (spectrogram_db_clipped - (spectrogram_db_max - db_range))\n        / db_range\n        * 255.0,\n        tf.uint8,\n    )\n    return spectrogram_db_clipped_uint\n\n", "CodeBleu_score": 40.13724498844665}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    spectrogram_db_clipped = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    spectrogram_db_clipped_uint8 = tf.cast(\n        (spectrogram_db_clipped - (max_db - db_range))\n        * 255\n        / db_range,\n        tf.uint8,\n    )\n    return spectrogram_db_clipped_uint8\n\n", "CodeBleu_score": 42.120135527919146}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram_db = gain_to_db(spectrogram, db_range=db_range, **kwargs)\n    spectrogram_db_max = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, spectrogram_db_max - db_range, spectrogram_db_max\n    )\n    spectrogram_db_clipped_uint8 = tf.cast(\n        (spectrogram_db_clipped - (spectrogram_db_max - db_range))\n        / db_range\n        * 255.0,\n        tf.uint8,\n    )\n    return spectrogram_db_clipped_uint8\n\n", "CodeBleu_score": 38.8497859045437}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    waveform = tf.expand_dims(waveform, axis=-1)\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n        name=\"stft\",\n    )\n    spectrogram = tf.abs(stft) ** spec_exponent\n    spectrogram = tf.transpose(spectrogram, [0, 2, 1])\n    return spectrogram\n\n", "CodeBleu_score": 37.86910461581912}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Compute the STFT of the waveform\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: tf.signal.hann_window(x, periodic=False) ** window_exponent,\n    )\n    # Transpose and take the absolute value\n    spectrogram = tf.abs(tf.transpose(stft))\n    # Apply the specified exponent\n    if spec_exponent!= 1.0:\n        spectrogram = spectrogram ** spec_exponent\n    return spectrogram\n\n", "CodeBleu_score": 40.401057351672065}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Create a Hann window function\n    window = tf.signal.hann_window(frame_length, periodic=True, dtype=tf.dtypes.float32)\n    # Apply the window function to the waveform\n    windowed = waveform * tf.pow(window, window_exponent)\n    # Compute the Short-Time Fourier Transform (STFT)\n    stft = tf.signal.stft(\n        windowed,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=None,\n        pad_end=False,\n    )\n    # Transpose the STFT tensor\n    stft = tf.transpose(stft)\n    # Compute the absolute value of the STFT tensor\n    magnitude = tf.abs(stft)\n    # Apply the exponent to the magnitude tensor\n    magnitude = tf.pow(magnitude, spec_exponent)\n    return magnitude\n\n", "CodeBleu_score": 38.01518871009442}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, (tf.shape(spectrogram)[0], new_time_dim), method=method\n    )\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    adjusted_spectrogram = tf.image.resize_with_crop_or_pad(\n        resized_spectrogram, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n    return adjusted_spectrogram\n\n", "CodeBleu_score": 38.766196793195704}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(tf.cast(time_dim, tf.float32) * factor, tf.int32)\n    spectrogram = tf.expand_dims(spectrogram, axis=0)\n    spectrogram = tf.image.resize(spectrogram, (1, new_time_dim), method=method)\n    spectrogram = tf.squeeze(spectrogram, axis=0)\n    if factor > 1.0:\n        spectrogram = spectrogram[:, :time_dim]\n    else:\n        spectrogram = tf.pad(\n            spectrogram, [[0, 0], [0, time_dim - new_time_dim]], constant_values=0.0\n        )\n    return spectrogram\n\n", "CodeBleu_score": 40.65349447503359}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    spectrogram = tf.expand_dims(spectrogram, axis=0)\n    spectrogram = tf.image.resize(spectrogram, [new_time_dim, tf.shape(spectrogram)[2]], method=method)\n    spectrogram = tf.squeeze(spectrogram, axis=0)\n    if factor > 1:\n        spectrogram = spectrogram[:, :time_dim]\n    else:\n        spectrogram = tf.pad(spectrogram, [[0, 0], [0, time_dim - new_time_dim]])\n    return spectrogram\n\n", "CodeBleu_score": 39.824006616255815}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor based on the semitone shift\n    factor = 2 ** (semitone_shift / 12)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[tf.cast(tf.shape(spectrogram)[0] * factor, tf.int32), tf.shape(spectrogram)[1]],\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [0, 0],\n            [0, tf.shape(spectrogram)[0] - tf.shape(resized_spectrogram)[0]],\n        ],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 34.42112308359157}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    if semitone_shift == 0.0:\n        return spectrogram\n    else:\n        # Calculate the resizing factor\n        resizing_factor = 2.0 ** (semitone_shift / 12.0)\n        # Resize the spectrogram along the frequency axis using the specified method\n        resized_spectrogram = tf.image.resize(\n            spectrogram,\n            size=[tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resizing_factor, tf.int32)],\n            method=method,\n        )\n        # Pad the resized spectrogram to maintain the original dimensions\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            paddings=[[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]], [0, 0]],\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 42.336536332453434}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    semitone_shift = tf.cast(semitone_shift, tf.float32)\n    resizing_factor = tf.math.pow(2.0, semitone_shift / 12.0)\n    spectrogram_shape = tf.shape(spectrogram)\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [\n            tf.cast(spectrogram_shape[0] * resizing_factor, tf.int32),\n            spectrogram_shape[1],\n        ],\n        method=method,\n    )\n    spectrogram = tf.pad(\n        spectrogram,\n        [[0, spectrogram_shape[0] - tf.shape(spectrogram)[0]], [0, 0]],\n        constant_values=0.0,\n    )\n    return spectrogram\n\n", "CodeBleu_score": 30.54525312584743}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    conv_activation = params[\"conv_activation\"]\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leaky_relu\":\n        return LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Unsupported activation type: {conv_activation}\")\n\n", "CodeBleu_score": 41.44817894897178}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    else:\n        return nn.LeakyReLU(negative_slope=0.2)\n\n", "CodeBleu_score": 34.13471863812688}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid value for 'conv_activation': {params['conv_activation']}\"\n        )\n\n", "CodeBleu_score": 33.25671577729363}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    # Create a dictionary to store the logit masks for each instrument\n    logit_masks = {}\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for the current instrument\n        logit_mask = unet_model(input_tensor, instrument, params)\n        # Add the logit mask for the current instrument to the dictionary\n        logit_masks[instrument] = logit_mask\n    # Combine the logit masks using a softmax function along the specified axis\n    combined_logit_mask = tf.nn.softmax(tf.stack(list(logit_masks.values()), axis=0), axis=0)\n    # Create a dictionary to store the spectrograms for each instrument\n    spectrograms = {}\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Create a spectrogram for the current instrument by multiplying the corresponding logit mask with the input tensor\n        spectrogram = tf.multiply(combined_logit_mask[instrument], input_tensor)\n        # Add the spectrogram for the current instrument to the dictionary\n        spectrograms[instrument] = spectrogram\n    # Return the dictionary of spectrograms\n    return spectrograms\n\n", "CodeBleu_score": 34.57045678434286}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\n    # Create a dictionary to store the logit masks for each instrument\n    logit_masks = {}\n\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for each instrument\n        logit_mask = unet(input_tensor, instrument, params)\n        # Collect the logit masks\n        logit_masks[instrument] = logit_mask\n\n    # Combine the logit masks using a softmax function along the specified axis\n    combined_logit_mask = tf.nn.softmax(tf.stack(list(logit_masks.values()), axis=0), axis=0)\n\n    # Create an output dictionary where each entry is a spectrogram for an instrument\n    # obtained by multiplying the corresponding mask with the input tensor\n    spectrograms = {\n        instrument: combined_logit_mask[i] * input_tensor\n        for i, instrument in enumerate(instruments)\n    }\n\n    # Return the dictionary of instrument spectrograms\n    return spectrograms\n\n", "CodeBleu_score": 36.12991975822933}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet_model(input_tensor, instrument, params)\n        logit_masks.append(logit_mask)\n\n    logit_masks = tf.stack(logit_masks, axis=0)\n    softmax_masks = tf.nn.softmax(logit_masks, axis=0)\n\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = softmax_masks[i] * input_tensor\n\n    return spectrograms\n\n", "CodeBleu_score": 32.38088985908158}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n    with open(path, 'rb') as f:\n        hash_obj = hashlib.sha256()\n        while chunk := f.read(1024):\n            hash_obj.update(chunk)\n    return hash_obj.hexdigest()\n\n", "CodeBleu_score": 37.74592690656377}, {"code": "def compute_file_checksum(path)\n    hash_obj = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_obj.update(chunk)\n    return hash_obj.hexdigest()\n\n", "CodeBleu_score": 55.86401636764998}, {"code": "def compute_file_checksum(path)\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 57.44570069380956}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Unable to retrieve index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name][\"sha256\"]\n\n", "CodeBleu_score": 40.5129328768748}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Error: Could not retrieve checksum index from {url}.\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Error: Model name {name} not found in checksum index.\")\n    return index[name]\n\n", "CodeBleu_score": 39.046192499802004}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Failed to retrieve index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name][\"checksum\"]\n\n", "CodeBleu_score": 40.85056294678775}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n    url = self._get_url(name)\n    with tempfile.NamedTemporaryFile() as tmp_file:\n        logger.info(f\"Downloading {name} from {url}\")\n        with requests.get(url, stream=True, headers=self._headers) as r:\n            r.raise_for_status()\n            for chunk in r.iter_content(chunk_size=8192):\n                tmp_file.write(chunk)\n            tmp_file.flush()\n            self._check_integrity(tmp_file.name, self._get_expected_checksum(name))\n            logger.info(f\"Extracting {name} to {path}\")\n            with tarfile.open(tmp_file.name) as tar:\n                tar.extractall(path=path)\n            os.remove(tmp_file.name)\n            logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 31.72678423521852}, {"code": "def download(self, name: str, path: str) -> None:\n    # Construct the download URL\n    url = f\"https://download.pytorch.org/models/{name}\"\n    # Log the download initiation\n    logging.info(f\"Downloading {name} from {url}\")\n    # Stream the file using HTTP/2\n    with urllib.request.urlopen(url) as response, open(path, \"wb\") as file:\n        shutil.copyfileobj(response, file)\n    # Check the file's integrity\n    expected_checksum = _model_urls[name][\"md5\"]\n    actual_checksum = md5(path).hexdigest()\n    if actual_checksum!= expected_checksum:\n        raise RuntimeError(f\"Invalid checksum for {name}. Expected {expected_checksum}, got {actual_checksum}\")\n    # Extract the archive to the specified path\n    with tarfile.open(path, \"r:gz\") as archive:\n        archive.extractall(path=path)\n    # Delete the temporary file\n    os.remove(path)\n    # Log the extraction completion\n    logging.info(f\"Extracted {name} to {path}\")\n", "CodeBleu_score": 27.630457738337473}, {"code": "def download(self, name: str, path: str) -> None:\n    # Construct the download URL\n    url = self.model_urls[name]\n    # Log the download initiation\n    print(f\"Downloading {name} from {url}\")\n    # Stream the file using HTTP/2\n    with urllib.request.urlopen(url) as response, open(path, 'wb') as file:\n        shutil.copyfileobj(response, file)\n    # Check the file's integrity\n    expected_md5 = self.model_md5s[name]\n    if not check_md5(path, expected_md5):\n        raise RuntimeError(f\"MD5 mismatch for {name}\")\n    # Extract the archive\n    print(f\"Extracting {name} to {path}\")\n    with tarfile.open(path, 'r') as archive:\n        archive.extractall(path=path)\n    # Delete the temporary file\n    os.remove(path)\n    # Log the extraction completion\n    print(f\"Done extracting {name}\")\n", "CodeBleu_score": 22.84135504550259}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n    # If the descriptor is an embedded configuration\n    if descriptor.startswith(\"embedded:\"):\n        # Extract the configuration name from the descriptor\n        config_name = descriptor.replace(\"embedded:\", \"\")\n        # Read the corresponding JSON resource\n        config_file = f\"config/{config_name}.json\"\n        if not os.path.isfile(config_file):\n            raise FileNotFoundError(f\"Embedded configuration file not found: {config_file}\")\n        with open(config_file, \"r\") as f:\n            config = json.load(f)\n        return config\n    # If the descriptor is a standard file path\n    elif os.path.isfile(descriptor):\n        # Read the JSON content from the file\n        with open(descriptor, \"r\") as f:\n            config = json.load(f)\n        return config\n    # If the descriptor is not an embedded configuration or a valid file path\n    else:\n        raise ValueError(f\"Invalid configuration descriptor: {descriptor}\")\n\n", "CodeBleu_score": 46.2285891472894}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded:\"):\n        configuration_name = descriptor[9:]\n        configuration_path = os.path.join(\n            os.path.dirname(__file__), \"configurations\", configuration_name + \".json\"\n        )\n        if not os.path.exists(configuration_path):\n            raise ValueError(f\"Embedded configuration {configuration_name} not found.\")\n        with open(configuration_path, \"r\") as configuration_file:\n            return json.load(configuration_file)\n    elif os.path.exists(descriptor):\n        with open(descriptor, \"r\") as configuration_file:\n            return json.load(configuration_file)\n    else:\n        raise ValueError(f\"Configuration file {descriptor} not found.\")\n\n", "CodeBleu_score": 46.214487962487496}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        if name in EMBEDDED_CONFIGURATIONS:\n            return json.loads(EMBEDDED_CONFIGURATIONS[name])\n        else:\n            raise ValueError(f\"Embedded configuration '{name}' not found\")\n    elif os.path.exists(descriptor):\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n    else:\n        raise ValueError(f\"Configuration file '{descriptor}' not found\")\n\n", "CodeBleu_score": 51.97906711100369}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    tensor_list = [tensor for tensor in tensor_dict.values()]\n    tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    tensor_processed = func(tensor_concat)\n    tensor_split = tf.split(tensor_processed, len(tensor_list), axis=concat_axis)\n    tensor_dict_processed = {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), tensor_split)\n    }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 38.65810915327951}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    concat_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        return dict(zip(tensor_dict.keys(), tf.split(concat_tensor, len(tensor_dict))))\n    else:\n        return dict(\n            zip(tensor_dict.keys(), tf.split(concat_tensor, len(tensor_dict), axis=1))\n        )\n\n", "CodeBleu_score": 36.23380451598964}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be 0 or 1\")\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        processed_tensor = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        processed_tensor = tf.split(processed_tensor, len(tensor_dict), axis=1)\n    return dict(zip(tensor_dict.keys(), processed_tensor))\n\n", "CodeBleu_score": 39.456347822772166}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.math.round((tensor - min_value) / (max_value - min_value) * 255), tf.uint8\n    )\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.163913786281405}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor = tf.cast(\n        tf.math.round(\n            ((tensor - tensor_min) / (tensor_max - tensor_min)) * 255.0\n        ),\n        tf.uint8,\n    )\n    return {\n        tensor_key: tensor,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 58.83272067556782}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    tensor_min = tf.math.reduce_min(tensor)\n    tensor_max = tf.math.reduce_max(tensor)\n    tensor_uint8 = tf.cast(\n        (tensor - tensor_min) / (tensor_max - tensor_min) * 255, tf.uint8\n    )\n    return {\n        tensor_key: tensor_uint8,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 58.2703675098042}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = tf.math.floormod(segment_len - tensor_len, segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_len], [0, 0]])\n    segments = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n    return segments\n\n", "CodeBleu_score": 30.306772718676058}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = segment_len - (tensor.shape[0] % segment_len)\n    # Pad the tensor with zeros\n    padded_tensor = tf.pad(tensor, [[0, padding_needed], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    segments = tf.reshape(padded_tensor, [-1, segment_len, tensor.shape[1]])\n    return segments\n\n", "CodeBleu_score": 21.091696537099256}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = segment_len - (tensor_len % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 25.191154865974845}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - instr_spec.shape[0] % frame_length\n    # Create an extension of zeros\n    extension = torch.zeros(extra_rows, instr_spec.shape[1])\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, extension), dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[1] * F)\n    return instr_spec\n\n", "CodeBleu_score": 14.827687693729324}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    if instr_spec.shape[1] < frame_length:\n        pad_size = frame_length - instr_spec.shape[1]\n        instr_spec = tf.concat(\n            [instr_spec, tf.zeros([instr_spec.shape[0], pad_size, F])], axis=1\n        )\n    instr_spec = tf.reshape(instr_spec, [instr_spec.shape[0] * instr_spec.shape[1], F])\n    return instr_spec\n\n", "CodeBleu_score": 23.049155795498173}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = (frame_length - instr_spec.shape[0] % frame_length) % frame_length\n    # Create an extension of zeros\n    extra_rows = torch.zeros((num_extra_rows, instr_spec.shape[1]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, extra_rows), dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[0] // F, F)\n    return instr_spec\n\n", "CodeBleu_score": 16.03190230391673}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 17.895501042571066}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 17.895501042571066}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # Convert the DataFrame columns into a TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n\n    return dataset\n\n", "CodeBleu_score": 36.47017546402109}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = result and tf.equal(tf.shape(tensor_tf)[dim], length)\n    return result\n\n", "CodeBleu_score": 38.60254269438785}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 35.273186844439394}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        seed=audio_params[\"seed\"],\n    )\n    return builder.build(\n        train_csv_path=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 41.44990676602506}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache_dataset=audio_params[\"cache_dataset\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 34.81844896185918}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n    if self._instrument_builders is None:\n      self._instrument_builders = []\n      for instrument in self._instruments:\n        self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self._instrument_builders:\n      yield instrument_builder\n\n  def get_instrument_builder(self, instrument: Instrument) -> InstrumentDatasetBuilder:\n    \"\"\"The function iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\\nThe function then yields each builder from the list of instrument builders.\"\"\"\n    if self._instrument_builders is None:\n      self._instrument_builders = []\n      for instrument in self._instruments:\n        self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self._instrument_builders:\n      if instrument_builder.instrument == instrument:\n        return instrument_builder\n    return None\n\n  def get_instrument_dataset(self, instrument: Instrument) -> Dataset:\n    \"\"\"The function iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\\nThe function then yields each builder from the list of instrument builders.\"\"\"\n    instrument_builder = self.get_instrument_builder(instrument)\n    if instrument_builder is None:\n      return None\n    return instrument_builder.get_dataset()\n\n  def get_instrument_dataset_path(self, instrument: Instrument) -> Path:\n    \"\"\"The function iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\\nThe function then yields each builder from the list of instrument builders.\"\"\"\n    instrument_builder = self.get_instrument_builder(instrument)\n    if instrument_builder is None:\n      return None\n    return instrument_builder.get_dataset_path()\n\n  def get_instrument_dataset_name(self, instrument: Instrument) -> str:\n    \"\"\"The function iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\\nThe function then yields each builder from the list of instrument builders.\"\"\"\n    instrument_builder = self.get_instrument_builder(instrument)\n    if instrument_builder is None:\n      return None\n    return instrument_builder.get_dataset_name()\n\n  def get_instrument_dataset_path_str(self, instrument: Instrument) -> str:\n    \"\"\"The function iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each", "CodeBleu_score": 46.66431548165371}, {"code": "def instruments(self) -> Any\n    if self._instrument_builders is None:\n      self._instrument_builders = [InstrumentDatasetBuilder(instrument) for instrument in self._instruments]\n    for instrument_builder in self._instrument_builders:\n      yield instrument_builder\n\n  @property\n  def instruments_by_name(self) -> Dict[str, InstrumentDatasetBuilder]:\n    \"\"\"The function initializes instrument dataset builders if they are not already created.\\nIt then returns a dictionary of instrument dataset builders, where the keys are the instrument names and the values are the corresponding instrument dataset builders.\"\"\"\n    if self._instrument_builders is None:\n      self._instrument_builders = [InstrumentDatasetBuilder(instrument) for instrument in self._instruments]\n    return {instrument.name: instrument_builder for instrument_builder, instrument in zip(self._instrument_builders, self._instruments)}\n\n  def _instrument_builder_for_instrument(self, instrument: Instrument) -> InstrumentDatasetBuilder:\n    \"\"\"The function initializes instrument dataset builders if they are not already created.\\nIt then returns the instrument dataset builder for the given instrument.\"\"\"\n    if self._instrument_builders is None:\n      self._instrument_builders = [InstrumentDatasetBuilder(instrument) for instrument in self._instruments]\n    for instrument_builder in self._instrument_builders:\n      if instrument_builder.instrument == instrument:\n        return instrument_builder\n    raise ValueError(f'Instrument \"{instrument.name}\" not found in dataset builder')\n\n  def _instrument_builder_for_instrument_name(self, instrument_name: str) -> InstrumentDatasetBuilder:\n    \"\"\"The function initializes instrument dataset builders if they are not already created.\\nIt then returns the instrument dataset builder for the given instrument name.\"\"\"\n    if self._instrument_builders is None:\n      self._instrument_builders = [InstrumentDatasetBuilder(instrument) for instrument in self._instruments]\n    for instrument_builder in self._instrument_builders:\n      if instrument_builder.instrument.name == instrument_name:\n        return instrument_builder\n    raise ValueError(f'Instrument \"{instrument_name}\" not found in dataset builder')\n\n  def _instrument_builder_for_instrument_name_or_instrument(self, instrument_name_or_instrument: Union[str, Instrument]) -> InstrumentDatasetBuilder:\n    \"\"\"The function initializes instrument dataset builders if they are not already created.\\nIt then returns the instrument dataset builder for the given instrument name", "CodeBleu_score": 35.98840100499896}, {"code": "def instruments(self) -> Any\n    if self._instruments is None:\n      self._instruments = [InstrumentDatasetBuilder(self._instrument_name, self._instrument_path) for self._instrument_name, self._instrument_path in self._instrument_list]\n    for instrument in self._instruments:\n      yield instrument\n", "CodeBleu_score": 35.62998612204018}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    if wait:\n        while not os.path.exists(os.path.join(cache, \"cache_index.json\")):\n            logger.warning(\n                \"Waiting for cache index to be generated at %s\", cache\n            )\n            time.sleep(10)\n    os.makedirs(cache, exist_ok=True)\n    cache_path = os.path.join(cache, \"cache_index.json\")\n    if os.path.exists(cache_path):\n        with open(cache_path, \"r\") as f:\n            cache_index = json.load(f)\n    else:\n        cache_index = {}\n    if dataset in cache_index:\n        return cache_index[dataset]\n    if isinstance(dataset, str):\n        dataset = json.loads(dataset)\n    cache_index[dataset] = dataset\n    with open(cache_path, \"w\") as f:\n        json.dump(cache_index, f)\n    return dataset", "CodeBleu_score": 46.539858405697984}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    cache_dir = os.path.dirname(cache)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    if wait:\n        while not os.path.exists(cache + \".index\"):\n            logging.info(f\"Waiting for {cache + '.index'} to be created...\")\n            time.sleep(1)\n    with open(cache, \"wb\") as f:\n        pickle.dump(dataset, f)\n    return dataset\n\n", "CodeBleu_score": 47.5243990504981}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is not None:\n        if wait:\n            while not os.path.exists(os.path.join(cache, \"cache_index.json\")):\n                logger.info(\n                    f\"Waiting for cache index to be created at {os.path.join(cache, 'cache_index.json')}\"\n                )\n                time.sleep(10)\n        if not os.path.exists(cache):\n            os.makedirs(cache)\n        dataset.save_to_disk(cache)\n        return dataset\n    return dataset\n\n", "CodeBleu_score": 52.19701601212602}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    # Load the model directory using a ModelProvider and update the parameters\n    model_provider = ModelProvider(params[\"model_name\"])\n    params = model_provider.update_params(params)\n\n    # Set up the session configuration to limit GPU memory usage\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n\n    # Create a RunConfig with this session configuration\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n\n    # Initialize and return a TensorFlow estimator using the specified model function, model directory, parameters, and configuration\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 34.38043469192312}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model_dir(params[\"model_name\"])\n    params[\"model_dir\"] = model_dir\n    params[\"num_classes\"] = 2\n    params[\"label_smoothing\"] = 0.0\n    params[\"moving_average_decay\"] = None\n    params[\"learning_rate_decay_type\"] = \"exponential\"\n    params[\"learning_rate\"] = 0.00001\n    params[\"batch_size\"] = 1\n    params[\"num_epochs\"] = 1\n    params[\"num_epochs_per_decay\"] = 1\n    params[\"max_number_of_steps\"] = None\n    params[\"optimizer\"] = \"rmsprop\"\n    params[\"weight_decay\"] = 0.00004\n    params[\"momentum\"] = 0.9\n    params[\"use_batch_norm\"] = True\n    params[\"batch_norm_decay\"] = 0.997\n    params[\"batch_norm_epsilon\"] = 1e-5\n    params[\"batch_norm_scale\"] = True\n    params[\"batch_norm_zero_debias_moving_mean\"] = True\n    params[\"batch_norm_renorm\"] = False\n    params[\"batch_norm_renorm_clipping\"] = None\n    params[\"batch_norm_renorm_decay\"] = 0.99\n    params[\"batch_norm_renorm_epsilon\"] = 0.001\n    params[\"batch_norm_delay\"] = 0\n    params[\"use_depthwise\"] = False\n    params[\"depth_multiplier\"] = 1.0\n    params[\"min_depth\"] = None\n    params[\"divisible_by\"] = None\n    params[\"finegrain_classification_level\"] = 0\n    params[\"dropout_keep_prob\"] = 0.8\n    params[\"data_format\"] = \"channels_last\"\n    params[\"weight_decay_pattern\"] = \"^(.*dense_layer|.*logits|.*final_dense_layer|.*final_logits_layer)$\"\n    params[\"weight_decay_rate\"] = 0.00004\n    params[\"activation_fn\"] = \"relu\"\n    params[\"activation_fn_inference\"] = None\n    params[\"activation_fn_init\"] = \"relu\"\n    params[\"activation_fn_", "CodeBleu_score": 17.261534414670702}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider(params[\"model_dir\"])\n    params[\"model_provider\"] = model_provider\n    params[\"params\"] = params\n    if MWF:\n        params[\"model_provider\"].load_model_weights(params[\"model_dir\"], params[\"model_provider\"].model_name)\n    session_config = tf.ConfigProto(allow_soft_placement=True)\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(\n        model_fn=model_provider.model_fn, model_dir=params[\"model_dir\"], config=run_config, params=params\n    )\n\n", "CodeBleu_score": 40.02139232143321}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.ndim!= 2:\n        raise ValueError(\"The input waveform must be stereo.\")\n\n    audio_id = audio_descriptor.audio_id\n    audio_descriptor.audio_id = None\n    self.data_generator.update(waveform, audio_descriptor)\n    prediction = next(self.model.predict_generator(self.data_generator))\n    prediction.pop(\"audio_id\")\n    audio_descriptor.audio_id = audio_id\n    return prediction\n", "CodeBleu_score": 44.73418197109894}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if len(waveform.shape) == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    if waveform.shape[1] == 1:\n        waveform = np.concatenate([waveform, waveform], axis=1)\n    prediction_generator = self.model.separate(\n        waveform, audio_descriptor=audio_descriptor\n    )\n    prediction = next(prediction_generator)\n    if \"audio_id\" in prediction:\n        del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 51.20775595973234}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.shape[1]!= 2:\n        waveform = np.stack([waveform, waveform], axis=1)\n\n    data_generator = self.model.separate.data_generator(\n        [waveform], [audio_descriptor]\n    )\n    prediction = next(data_generator.generate())\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 46.62656058919234}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            tuple(int(ceil(image_shape[0] / stride)) for stride in config.BACKBONE_STRIDES),\n            tuple(int(ceil(image_shape[1] / stride)) for stride in config.BACKBONE_STRIDES),\n        ]\n    )\n\n", "CodeBleu_score": 72.28241632212973}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            tuple(int(ceil(image_shape[0] / stride)) for stride in config.BACKBONE_STRIDES),\n            tuple(int(ceil(image_shape[1] / stride)) for stride in config.BACKBONE_STRIDES),\n        ]\n    )\n\n", "CodeBleu_score": 72.28241632212973}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(np.ceil(image_shape[0] / stride)),\n                int(np.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 79.53916287162126}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n    # Split boxes into individual components\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    # Split window into individual components\n    w, h = tf.split(window, 2, axis=1)\n    # Clip each component of the boxes to lie within the window\n    x1 = tf.maximum(tf.minimum(x1, w), 0)\n    x2 = tf.maximum(tf.minimum(x2, w), 0)\n    y1 = tf.maximum(tf.minimum(y1, h), 0)\n    y2 = tf.maximum(tf.minimum(y2, h), 0)\n    # Reassemble clipped boxes into a single tensor\n    clipped_boxes = tf.concat([x1, y1, x2, y2], axis=1)\n    # Return clipped boxes with a fixed shape\n    return clipped_boxes\n\n", "CodeBleu_score": 24.52973269187197}, {"code": "def clip_boxes_graph(boxes, window)\n    # Split boxes into coordinates (x1, y1, x2, y2) and lengths (w, h)\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    w, h = x2 - x1, y2 - y1\n\n    # Split window into coordinates (x1, y1, x2, y2) and lengths (w, h)\n    window_x1, window_y1, window_x2, window_y2 = tf.split(window, 4, axis=1)\n    window_w, window_h = window_x2 - window_x1, window_y2 - window_y1\n\n    # Compute clipped coordinates and lengths\n    x1 = tf.maximum(x1, window_x1)\n    y1 = tf.maximum(y1, window_y1)\n    x2 = tf.minimum(x2, window_x2)\n    y2 = tf.minimum(y2, window_y2)\n    w = tf.maximum(x2 - x1, 0.0)\n    h = tf.maximum(y2 - y1, 0.0)\n\n    # Concatenate clipped coordinates and lengths\n    boxes = tf.concat([x1, y1, x2, y2], axis=1)\n    return boxes\n\n", "CodeBleu_score": 40.09496381169733}, {"code": "def clip_boxes_graph(boxes, window)\n    # Split boxes into components\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    # Split window into components\n    w1, w2 = tf.split(window, 2, axis=1)\n    # Clip components to lie within window\n    x1 = tf.maximum(tf.minimum(x1, w2), w1)\n    x2 = tf.maximum(tf.minimum(x2, w2), w1)\n    y1 = tf.maximum(tf.minimum(y1, w2), w1)\n    y2 = tf.maximum(tf.minimum(y2, w2), w1)\n    # Reassemble clipped components into the final tensor\n    clipped = tf.concat([x1, y1, x2, y2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 34.911583475273886}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Get the number of anchor ratios\n    num_anchors = len(anchors_per_location)\n    \n    # Define a 1x1 convolutional layer with shared weights across all locations\n    shared = KL.Conv2D(256, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')\n    \n    # Apply the shared convolutional layer to the feature map\n    x = shared(feature_map)\n    \n    # Define two separate 1x1 convolutional layers for class scores and bounding box predictions\n    x_class = KL.Conv2D(2 * num_anchors, (1, 1), activation='linear', name='rpn_class_raw')(x)\n    x_bbox = KL.Conv2D(4 * num_anchors, (1, 1), activation='linear', name='rpn_bbox_pred')(x)\n    \n    # Reshape the class scores to 2D\n    x_class = KL.Lambda(lambda t: K.reshape(t, [K.shape(t)[0], -1, 2]))(x_class)\n    \n    # Softmax activation for class probabilities\n    x_class = KL.Activation('softmax', name='rpn_class_xxx')(x_class)\n    \n    # Reshape the bounding box predictions to 2D\n    x_bbox = KL.Lambda(lambda t: K.reshape(t, [K.shape(t)[0], -1, 4]))(x_bbox)\n    \n    # Return the class logits, class probabilities, and bounding box predictions\n    return [x_class, x_bbox]\n", "CodeBleu_score": 36.03174276635237}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Get the number of anchors per location\n    num_anchors = len(anchors_per_location)\n    # Define the shared convolutional layer\n    shared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n    # Define the class score convolutional layer\n    x = KL.Conv2D(2 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    # Reshape the class score convolutional layer output\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    # Pass the class score convolutional layer output through a softmax activation to obtain class probabilities\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    # Define the bounding box prediction convolutional layer\n    x = KL.Conv2D(4 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    # Reshape the bounding box prediction convolutional layer output\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 47.49209588408726}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Get the number of anchors per location, which is the length of the anchors array\n    num_anchors = len(anchors_per_location)\n    \n    # Define a 1x1 convolutional layer with the number of filters as the number of anchors per location multiplied by 5\n    rpn = layers.Conv2D(num_anchors * 5, (1, 1), padding=\"same\", activation=\"linear\", kernel_initializer=\"he_normal\", name=\"rpn_conv_shared\")(feature_map)\n    \n    # Reshape the rpn output to (batch_size, num_anchors, 4) for bounding box predictions\n    x = layers.Reshape(\n        (num_anchors, 4), name=\"rpn_bbox_pred\"\n    )(rpn)\n    \n    # Reshape the rpn output to (batch_size, num_anchors, 2) for class scores\n    class_logits = layers.Reshape(\n        (num_anchors, 2), name=\"rpn_class_logits\"\n    )(rpn)\n    \n    # Reshape the rpn output to (batch_size, num_anchors, 2) for class probabilities\n    class_probs = layers.Activation(\n        \"softmax\", name=\"rpn_class_probs\"\n    )(class_logits)\n    \n    # Concatenate the class probabilities and bounding box predictions along the last axis\n    rpn_probs = layers.Concatenate(axis=2, name=\"rpn_probs\")([\n        class_probs, x\n    ])\n    \n    # Define the RPN model\n    rpn_model = keras.Model(\n        inputs=feature_map, outputs=rpn_probs, name=\"rpn_model\"\n    )\n    \n    return rpn_model\n\n", "CodeBleu_score": 20.365433979240517}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n    x = K.abs(y_true - y_pred)\n    x = K.switch(x < 1, 0.5 * x ** 2, x - 0.5)\n    return K.sum(x)\n\n", "CodeBleu_score": 16.984496638760362}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = tf.math.abs(y_true - y_pred)\n    less_than_one = tf.cast(tf.math.less(diff, 1.0), tf.float32)\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return tf.reduce_sum(loss, axis=-1)\n\n", "CodeBleu_score": 59.043299889260446}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = tf.keras.backend.abs(y_true - y_pred)\n    less_than_one = tf.keras.backend.cast(tf.keras.backend.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff ** 2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 48.872686897397635}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n    # Reset the graph\n    K.clear_session()\n\n    # Define the input layer\n    input_layer = Input(shape=x_train.shape[1:])\n\n    # Define the convolutional layers\n    conv1 = Conv2D(32, (3, 3), padding=\"same\")(input_layer)\n    conv1 = Activation(\"relu\")(conv1)\n    conv1 = Conv2D(32, (3, 3), padding=\"same\")(conv1)\n    conv1 = Activation(\"relu\")(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    # Define the dense layers\n    flat = Flatten()(pool1)\n    dense1 = Dense(128)(flat)\n    dense1 = Activation(\"relu\")(dense1)\n    dense2 = Dense(num_classes)(dense1)\n    output_layer = Activation(\"softmax\")(dense2)\n\n    # Define the model\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n    # Compile the model\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n\n    return model\n\n", "CodeBleu_score": 40.533793573835695}, {"code": "def build_model(x_train, num_classes)\n    # Reset the graph\n    tf.keras.backend.clear_session()\n    # Define the input layer\n    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n    # Define the convolutional layers\n    conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(input_layer)\n    conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(conv1)\n    # Define the max-pooling layer\n    pool1 = tf.keras.layers.MaxPooling2D((2, 2))(conv2)\n    # Flatten the output and pass it through dense layers\n    flatten = tf.keras.layers.Flatten()(pool1)\n    dense1 = tf.keras.layers.Dense(64, activation=\"relu\")(flatten)\n    output_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(dense1)\n    # Define the model\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n    # Compile the model\n    model.compile(\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 23.47292121553651}, {"code": "def build_model(x_train, num_classes)\n    # Reset the TensorFlow graph\n    K.clear_session()\n    # Define the input layer with the shape of the training data\n    inputs = Input(shape=x_train.shape[1:])\n    # Add the first convolutional layer with 32 filters and a kernel size of 3x3\n    x = Conv2D(32, (3, 3), activation=\"relu\")(inputs)\n    # Add the second convolutional layer with 64 filters and a kernel size of 3x3\n    x = Conv2D(64, (3, 3), activation=\"relu\")(x)\n    # Add a max-pooling layer with a pool size of 2x2\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Flatten the output\n    x = Flatten()(x)\n    # Add a dense layer with 128 units and ReLU activation\n    x = Dense(128, activation=\"relu\")(x)\n    # Add a dense layer with num_classes units and softmax activation for classification\n    outputs = Dense(num_classes, activation=\"softmax\")(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=outputs)\n    # Compile the model with the Adam optimizer, categorical cross-entropy loss, and accuracy metric\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n", "CodeBleu_score": 27.796128701362377}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([y1, x1, y2, x2])\n    return np.array(boxes)\n\n", "CodeBleu_score": 67.06565867661857}, {"code": "def extract_bboxes(mask)\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([y1, x1, y2, x2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 68.17888051144811}, {"code": "def extract_bboxes(mask)\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute the IoU values\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 55.49021365037661}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n    # 1. Compute areas of boxes1 and boxes2\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # 2. Compute overlaps to generate the matrix of size (boxes1.shape[0], boxes2.shape[0])\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # Area of boxes1\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    # Area of boxes2\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # Intersections\n    intersections = np.maximum(np.minimum(boxes1[:, 2], boxes2[:, 2]) - np.maximum(boxes1[:, 0], boxes2[:, 0]), 0) * np.maximum(np.minimum(boxes1[:, 3], boxes2[:, 3]) - np.maximum(boxes1[:, 1], boxes2[:, 1]), 0)\n    # Unions\n    unions = area1 + area2 - intersections\n    # Compute IoU\n    overlaps = intersections / unions\n    return overlaps\n", "CodeBleu_score": 38.86763002521875}, {"code": "def compute_overlaps(boxes1, boxes2)\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n    image = cv2.imread(self.image_info[image_id]['path'])\n    # If the image is not in RGB format, convert it to RGB\n    if len(image.shape)!= 3 or image.shape[2]!= 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # If the image has four channels, remove the alpha channel\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 54.27737801872474}, {"code": "def load_image(self, image_id):\n    info = self.image_info[image_id]\n    path = info['path']\n    image = read_image(path)\n    if image.ndim == 3 and image.shape[2] == 4:\n        image = image[:, :, :3]\n    if image.ndim == 2:\n        image = np.expand_dims(image, axis=2)\n    image = np.transpose(image, (1, 0, 2))\n    return image\n", "CodeBleu_score": 48.65970226202002}, {"code": "def load_image(self, image_id):\n    image_info = self.image_info[image_id]\n    image_path = image_info['path']\n    image = Image.open(image_path)\n    if image.mode == 'RGBA':\n        image = image.convert('RGB')\n    image = np.array(image)\n    return image\n", "CodeBleu_score": 42.552927351353276}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    fig = plt.figure(figsize=(14, 14 / cols * rows))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.title(title, fontsize=9)\n        plt.axis(\"off\")\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 53.028871645839224}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.1 * (rows / cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.grid(False)\n        plt.axis(\"off\")\n        plt.title(title, fontsize=14)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.show()\n\n", "CodeBleu_score": 48.44630678548285}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.2 * (rows/cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    for i in range(len(images)):\n        plt.subplot(rows, cols, i + 1)\n        plt.grid(False)\n        plt.xticks([])\n        plt.yticks([])\n        cmap = 'Blues' if type(images[0]) is not np.ndarray else cmap\n        norm = False if type(images[0]) is not np.ndarray else norm\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.xlabel(titles[i])\n    plt.tight_layout()\n    plt.show()\n", "CodeBleu_score": 40.01850274923977}]}
